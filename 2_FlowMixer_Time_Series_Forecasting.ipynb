{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiE2BD2hHadn"
      },
      "source": [
        "# FlowMixer Implementation and Experiments for Times Series Forecasting - TENSORFLOW\n",
        "## Supplementary Material\n",
        "\n",
        "This notebook provides the complete implementation and experimental validation of the FlowMixer architecture presented in our paper. It contains the following key components:\n",
        "\n",
        "### 1. Core Implementation\n",
        "- FlowMixer neural architecture with time and feature mixing layers\n",
        "- RevIN (Reversible Instance Normalization) implementation + TD-RevIn (Time Dependent)\n",
        "- Custom loss functions and evaluation metrics\n",
        "- Utility functions for data processing and model training\n",
        "\n",
        "### 2. Experimental Validation\n",
        "- Long-horizon time series forecasting experiments\n",
        "- Benchmark comparisons across multiple datasets (ETT, Weather, Traffic, etc.)\n",
        "\n",
        "- Visualization tools for model analysis and results presentation\n",
        "\n",
        "### 3. Usage Instructions\n",
        "To reproduce our experiments:\n",
        "1. Ensure all required dependencies are installed (TensorFlow, NumPy, Pandas)\n",
        "2. **Download the benchmark datasets to the `./data` directory**\n",
        "3. Run the cells in sequence to train and evaluate the models\n",
        "4. Use the visualization functions to analyze results\n",
        "\n",
        "File names should end in csv: ETTh1.csv ETTm2.csv ..etc\n",
        "Results are dislayed in tables at the end of training.\n",
        "\n",
        "### Environment Requirements\n",
        "- Python 3.7+\n",
        "- TensorFlow 2.x\n",
        "- NumPy\n",
        "- Pandas\n",
        "- Matplotlib\n",
        "- scikit-learn\n",
        "\n",
        "### References\n",
        "\n",
        "This paper uses code from TSMixer paper (Chen et al. 2023), and its associated repository.\n",
        "\n",
        "#### Please note This Notebook has results of the standard mixing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code"
      ],
      "metadata": {
        "id": "jbkj4chKheHj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WSjHEFdE5HZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import AdamW, SGD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Reproduction\n",
        "SEED = 888\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qp2Hn_zuE5HZ"
      },
      "outputs": [],
      "source": [
        "# @title DataLoader\n",
        "\n",
        "\n",
        "\"\"\"Load raw data and generate time series dataset.\"\"\"\n",
        "\n",
        "\n",
        "#DATA_DIR = 'gs://time_series_datasets'\n",
        "LOCAL_CACHE_DIR = './data/'\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "  \"\"\"Generate data loader from raw data.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, data, batch_size, seq_len, pred_len, feature_type, target='OT'\n",
        "  ):\n",
        "    self.data = data\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_len = seq_len\n",
        "    self.pred_len = pred_len\n",
        "    self.feature_type = feature_type\n",
        "    self.target = target\n",
        "    self.target_slice = slice(0, None)\n",
        "    self.n_feature=1\n",
        "    self._read_data()\n",
        "\n",
        "  def _read_data(self):\n",
        "    \"\"\"Load raw data and split datasets.\"\"\"\n",
        "\n",
        "    # copy data from cloud storage if not exists\n",
        "    if not os.path.isdir(LOCAL_CACHE_DIR):\n",
        "      os.mkdir(LOCAL_CACHE_DIR)\n",
        "\n",
        "    file_name = self.data + '.csv'\n",
        "    cache_filepath = os.path.join(LOCAL_CACHE_DIR, file_name)\n",
        "    #if not os.path.isfile(cache_filepath):\n",
        "    #  tf.io.gfile.copy(\n",
        "    #      os.path.join(DATA_DIR, file_name), cache_filepath, overwrite=True\n",
        "    #  )\n",
        "\n",
        "    df_raw = pd.read_csv(cache_filepath)\n",
        "\n",
        "    # S: univariate-univariate, M: multivariate-multivariate, MS:\n",
        "    # multivariate-univariate\n",
        "    df = df_raw.set_index('date')\n",
        "    if self.feature_type == 'S':\n",
        "      df = df[[self.target]]\n",
        "    elif self.feature_type == 'MS':\n",
        "      target_idx = df.columns.get_loc(self.target)\n",
        "      self.target_slice = slice(target_idx, target_idx + 1)\n",
        "\n",
        "    # split train/valid/test\n",
        "    n = len(df)\n",
        "    if self.data.startswith('ETTm'):\n",
        "      train_end = 12 * 30 * 24 * 4\n",
        "      val_end = train_end + 4 * 30 * 24 * 4\n",
        "      test_end = val_end + 4 * 30 * 24 * 4\n",
        "    elif self.data.startswith('ETTh'):\n",
        "      train_end = 12 * 30 * 24\n",
        "      val_end = train_end + 4 * 30 * 24\n",
        "      test_end = val_end + 4 * 30 * 24\n",
        "    else:\n",
        "      train_end = int(n * 0.7)\n",
        "      val_end = n - int(n * 0.2)\n",
        "      test_end = n\n",
        "    train_df = df[:train_end]\n",
        "    val_df = df[train_end - self.seq_len : val_end]\n",
        "    test_df = df[val_end - self.seq_len : test_end]\n",
        "\n",
        "    # standardize by training set\n",
        "    self.scaler = StandardScaler()\n",
        "    self.scaler.fit(train_df.values)\n",
        "\n",
        "    def scale_df(df, scaler):\n",
        "      data = scaler.transform(df.values)\n",
        "      return pd.DataFrame(data, index=df.index, columns=df.columns)\n",
        "\n",
        "    self.train_df = scale_df(train_df, self.scaler)\n",
        "    self.val_df = scale_df(val_df, self.scaler)\n",
        "    self.test_df = scale_df(test_df, self.scaler)\n",
        "    self.n_feature = self.train_df.shape[-1]\n",
        "\n",
        "  def _split_window(self, data):\n",
        "    inputs = data[:, : self.seq_len, :]\n",
        "    #labels = data[:, self.seq_len :, self.target_slice]\n",
        "    labels = data[:, self.pred_len:self.seq_len+self.pred_len, self.target_slice]\n",
        "    # Slicing doesn't preserve static shape information, so set the shapes\n",
        "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "    inputs.set_shape([None, self.seq_len, None])\n",
        "    #labels.set_shape([None, self.pred_len, None])\n",
        "    labels.set_shape([None, self.seq_len, None])\n",
        "    return inputs, labels\n",
        "\n",
        "  def _make_dataset(self, data, shuffle=True):\n",
        "    data = np.array(data, dtype=np.float32)\n",
        "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=data,\n",
        "        targets=None,\n",
        "        sequence_length=(self.seq_len + self.pred_len),\n",
        "        sequence_stride=1,\n",
        "        shuffle=shuffle,\n",
        "        batch_size=self.batch_size,\n",
        "    )\n",
        "    ds = ds.map(self._split_window)\n",
        "    return ds\n",
        "\n",
        "  def inverse_transform(self, data):\n",
        "    return self.scaler.inverse_transform(data)\n",
        "\n",
        "  def get_train(self, shuffle=True):\n",
        "    return self._make_dataset(self.train_df, shuffle=shuffle)\n",
        "\n",
        "  def get_val(self):\n",
        "    return self._make_dataset(self.val_df, shuffle=False)\n",
        "\n",
        "  def get_test(self):\n",
        "    return self._make_dataset(self.test_df, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ef7pV50rE5Ha"
      },
      "outputs": [],
      "source": [
        "# @title MSEs\n",
        "\n",
        "def mse_mod(y_true, y_pred, pred_len):\n",
        "    \"\"\"\n",
        "    Custom MSE loss function that focuses on the last pred_len elements.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (tf.Tensor): True values.\n",
        "    y_pred (tf.Tensor): Predicted values.\n",
        "    pred_len (int): Number of elements to focus on from the end.\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor: Computed MSE loss.\n",
        "    \"\"\"\n",
        "    y_true_last = y_true[:, -pred_len , :]\n",
        "    y_pred_last = y_pred[:, -pred_len , :]\n",
        "\n",
        "    # Compute MSE on the last pred_len elements\n",
        "    loss = tf.keras.losses.MeanSquaredError()(y_true_last, y_pred_last)\n",
        "\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFenrVEYapJG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title FlowMixer\n",
        "\n",
        "## REVIN / TD-REVIN\n",
        "class RevNorm(tf.keras.layers.Layer):\n",
        "    \"\"\"Reversible Instance Normalization.\n",
        "    This layer can normalize or denormalize data based on the mode specified.\n",
        "    Attributes:\n",
        "        axis (int): The axis over which to normalize the data.\n",
        "        eps (float): A small constant added to variance to avoid division by zero.\n",
        "        affine (bool): Whether to use affine transformation during normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, axis=1, eps=1e-5, affine=True, revin=1):\n",
        "        super().__init__()\n",
        "        self.axis = axis\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.affine_weight = None\n",
        "        self.affine_bias = None\n",
        "        self.mean = None\n",
        "        self.stdev = None\n",
        "        self.revin=revin\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.affine:\n",
        "            if self.revin == 1:\n",
        "              param_shape = (input_shape[-1],) # TYPE I Classic REVIN\n",
        "            else:\n",
        "              param_shape = (1,input_shape[-2],input_shape[-1]) # TYPE II TD-REVIN\n",
        "\n",
        "\n",
        "\n",
        "            self.affine_weight = self.add_weight(\n",
        "                name='affine_weight', shape=param_shape, initializer='ones', trainable=True\n",
        "            )\n",
        "            self.affine_bias = self.add_weight(\n",
        "                name='affine_bias', shape=param_shape, initializer='zeros', trainable=False\n",
        "            )\n",
        "\n",
        "\n",
        "    def call(self, x, mode):\n",
        "\n",
        "        if mode == 'norm':\n",
        "            self.mean, self.stdev = self._get_statistics(x)\n",
        "            x = self._normalize(x)\n",
        "        elif mode == 'denorm':\n",
        "            x = self._denormalize(x)\n",
        "        else:\n",
        "            raise NotImplementedError(\"Mode not supported, choose 'norm' or 'denorm'\")\n",
        "        return x\n",
        "\n",
        "    def _get_statistics(self, x):\n",
        "        mean = tf.stop_gradient(\n",
        "            tf.reduce_mean(x, axis=self.axis, keepdims=True)\n",
        "        )\n",
        "        stdev = tf.stop_gradient(\n",
        "            tf.sqrt(tf.math.reduce_variance(x, axis=self.axis, keepdims=True) + self.eps)\n",
        "        )\n",
        "        return mean, stdev\n",
        "\n",
        "    def _normalize(self, x):\n",
        "        x = (x - self.mean) / self.stdev\n",
        "        if self.affine:\n",
        "            x = x * self.affine_weight + self.affine_bias\n",
        "        return x\n",
        "\n",
        "    def _denormalize(self, x):\n",
        "        if self.affine:\n",
        "            x = (x - self.affine_bias) / self.affine_weight\n",
        "        x = x * self.stdev + self.mean\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# OPTIONAL\n",
        "class NoiseInjectionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, noise_factor=0.01, axis=1, noise_type='gaussian',\n",
        "                 trainable=False, apply_during_inference=False):\n",
        "        \"\"\"\n",
        "        Initializes the enhanced noise injection layer.\n",
        "        :param noise_factor: The factor of the signal energy by which the noise is scaled.\n",
        "        :param axis: The axis along which to calculate the signal energy and inject noise.\n",
        "        :param noise_type: The type of noise to inject ('gaussian' or 'uniform').\n",
        "        :param trainable: Whether the noise_factor should be trainable.\n",
        "        :param apply_during_inference: Whether to apply noise during inference.\n",
        "        \"\"\"\n",
        "        super(NoiseInjectionLayer, self).__init__()\n",
        "        self.axis = axis\n",
        "        self.noise_type = noise_type\n",
        "        self.apply_during_inference = apply_during_inference\n",
        "\n",
        "        if trainable:\n",
        "            self.noise_factor = tf.Variable(noise_factor,\n",
        "                                            constraint=lambda x: tf.clip_by_value(x, 0, 1),\n",
        "                                            trainable=True)\n",
        "        else:\n",
        "            self.noise_factor = noise_factor\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training is None:\n",
        "            training = tf.keras.backend.learning_phase()\n",
        "\n",
        "        if training or self.apply_during_inference:\n",
        "            # Calculate the signal energy along the specified axis\n",
        "            signal_energy = tf.reduce_mean(tf.square(inputs), axis=self.axis, keepdims=True)\n",
        "\n",
        "            # Calculate the standard deviation of the noise from signal energy\n",
        "            noise_std = tf.sqrt(signal_energy) * self.noise_factor\n",
        "\n",
        "            # Generate noise\n",
        "            if self.noise_type == 'gaussian':\n",
        "                noise = tf.random.normal(shape=tf.shape(inputs), mean=0.0, stddev=1.0)\n",
        "            elif self.noise_type == 'uniform':\n",
        "                noise = tf.random.uniform(shape=tf.shape(inputs), minval=-1.0, maxval=1.0)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported noise type: {self.noise_type}\")\n",
        "\n",
        "            # Scale and add noise to the input\n",
        "            return inputs + noise * noise_std\n",
        "        else:\n",
        "            return inputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(NoiseInjectionLayer, self).get_config()\n",
        "        config.update({\n",
        "            'noise_factor': self.noise_factor.numpy() if isinstance(self.noise_factor, tf.Variable) else self.noise_factor,\n",
        "            'axis': self.axis,\n",
        "            'noise_type': self.noise_type,\n",
        "            'trainable': isinstance(self.noise_factor, tf.Variable),\n",
        "            'apply_during_inference': self.apply_during_inference\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "## MIXER\n",
        "class MixingModule(tf.keras.layers.Layer):\n",
        "    def __init__(self, pred_len, **kwargs):\n",
        "        super(MixingModule, self).__init__(**kwargs)\n",
        "        self.pred_len = pred_len\n",
        "        self.wt = None\n",
        "\n",
        "    def wt_initializer(self, shape, dtype=tf.float32):\n",
        "        seq_len = shape[0]\n",
        "        initial_wt = tf.random.uniform(shape, minval=-0.1, maxval=0.1, dtype=dtype)\n",
        "\n",
        "        # Create the desired structure\n",
        "        for i in range(seq_len - self.pred_len):\n",
        "            initial_wt = tf.tensor_scatter_nd_update(\n",
        "                initial_wt,\n",
        "                [[i , i+self.pred_len]],\n",
        "                [1.0]\n",
        "            )\n",
        "\n",
        "        return initial_wt\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initialize weights for feature transformation\n",
        "        self.W = self.add_weight(name='W',\n",
        "                                  shape=(input_shape[-2], input_shape[-2]),\n",
        "                                  #initializer=self.wt_initializer,\n",
        "                                  initializer='glorot_normal',\n",
        "                                  trainable=True)\n",
        "\n",
        "\n",
        "        self.scale_t = self.add_weight(name='scale_t', shape=(), initializer='zeros', trainable=True)\n",
        "        self.scale_f = self.add_weight(name='scale_f', shape=(), initializer='ones', trainable=True)\n",
        "\n",
        "        self.K = self.add_weight(name='K',\n",
        "                                 shape=(input_shape[-1], input_shape[-1]),\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 trainable=True)\n",
        "        self.Q = self.add_weight(name='Q',\n",
        "                                 shape=(input_shape[-1], input_shape[-1]),\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 trainable=True)\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        Wt = self.scale_t * tf.eye(inputs.shape[-2]) + self.W * self.W\n",
        "\n",
        "\n",
        "\n",
        "        Wf_T = tf.eye(inputs.shape[-1])+self.scale_f*self.scale_f*tf.nn.softmax(self.Q@tf.transpose(self.K)/tf.sqrt(1e-16+inputs.shape[-1]),axis=0)\n",
        "        Wf_T = Wf_T/(1.0+self.scale_f*self.scale_f)\n",
        "\n",
        "\n",
        "        x= x + Wt @ x @ Wf_T\n",
        "\n",
        "        return x\n",
        "\n",
        "    def defWt(self):\n",
        "        Wt = self.scale_t * tf.eye(self.wt.shape[-1]) + self.wt * self.wt\n",
        "        return Wt\n",
        "\n",
        "    def defWf(self):\n",
        "        Wf = tf.eye(self.K.shape[-1])+self.scale_f*self.scale_f*tf.nn.softmax(self.Q@tf.transpose(self.K)/tf.sqrt(1e-16+self.K.shape[-1]),axis=0)\n",
        "        Wf = Wf/(1.0+self.scale_f*self.scale_f)\n",
        "        return Wf\n",
        "\n",
        "\n",
        "## FlowMixer\n",
        "class FlowMixer(tf.keras.Model):\n",
        "    def __init__(self, seq_len, nb_features, pred_len, dropout_rate, noise_factor, revin=1):\n",
        "        super(FlowMixer, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.nb_features = nb_features\n",
        "        self.dropout_rate=dropout_rate\n",
        "        self.noise_factor = noise_factor\n",
        "        self.revin=revin\n",
        "        self.pred_len = pred_len\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.noise_layer = NoiseInjectionLayer(noise_factor=self.noise_factor, axis=1, noise_type='uniform',\n",
        "                                            trainable=False, apply_during_inference=False)# Optional\n",
        "\n",
        "\n",
        "        self.mixer_layer = MixingModule(seq_len)\n",
        "\n",
        "        self.drop_layer=tf.keras.layers.Dropout(self.dropout_rate)\n",
        "\n",
        "        self.revin_layer = RevNorm(axis=-2, revin=self.revin)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "\n",
        "        x = inputs\n",
        "        x = self.revin_layer(x, mode='norm')\n",
        "\n",
        "        x = self.drop_layer(x)\n",
        "        if training:\n",
        "            x = self.noise_layer(x)\n",
        "\n",
        "        x = self.mixer_layer(x)\n",
        "        x = self.revin_layer(x, mode='denorm')\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOq8mEcyCEiE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Train and Evaluate\n",
        "\n",
        "\n",
        "def custom_evaluation(model, data, pred_len):\n",
        "    mse = tf.keras.metrics.MeanSquaredError()\n",
        "    mae = tf.keras.metrics.MeanAbsoluteError()\n",
        "\n",
        "    for inputs, labels in data:\n",
        "        forecasts = model(inputs, training=False)\n",
        "        labels_last = labels[:, -pred_len:, :]\n",
        "        forecasts_last = forecasts[:, -pred_len:, :]\n",
        "        mse.update_state(labels_last, forecasts_last)\n",
        "        mae.update_state(labels_last, forecasts_last)\n",
        "\n",
        "    return mse.result().numpy(), mae.result().numpy()\n",
        "\n",
        "class CustomEvaluation(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data, pred_len):\n",
        "        super(CustomEvaluation, self).__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.pred_len = pred_len\n",
        "        self.best_mse = float('inf')\n",
        "        self.best_mae = float('inf')\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        mse, mae = custom_evaluation(self.model, self.validation_data, self.pred_len)# self.validation\n",
        "        logs['val_custom_mse'] = mse\n",
        "        logs['val_custom_mae'] = mae\n",
        "        if mse < self.best_mse:\n",
        "            self.best_mse = mse\n",
        "            self.best_mae = mae\n",
        "\n",
        "def train_and_evaluate(data_loader, seq_len, pred_len, dropout_rate, noise_factor=0.00, revin=2, epochs=100, learning_rate=1e-4, mopt='sgd'):\n",
        "    train_data = data_loader.get_train()\n",
        "    val_data = data_loader.get_val()\n",
        "    test_data = data_loader.get_test()\n",
        "    nb_features = data_loader.n_feature\n",
        "\n",
        "    model = FlowMixer(seq_len, nb_features, pred_len, dropout_rate, noise_factor, revin=revin)\n",
        "    if mopt=='sgd':\n",
        "      optimizer = SGD(learning_rate=learning_rate, weight_decay=1e-6, momentum=0.9)\n",
        "    elif mopt=='adamw':\n",
        "      optimizer = AdamW(learning_rate=learning_rate, weight_decay=1e-6)\n",
        "    else:\n",
        "      optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse', 'mae'])\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    if not os.path.exists('./model_checkpoints'):\n",
        "        os.makedirs('./model_checkpoints')\n",
        "\n",
        "    early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_custom_mse',\n",
        "        patience=24,#100,#24\n",
        "        mode='min',\n",
        "        restore_best_weights=True  # This will restore best weights automatically\n",
        "    )\n",
        "    reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_mse',\n",
        "        factor=0.2,\n",
        "        patience=8,#100,#8\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    )\n",
        "    #custom_eval_callback = CustomEvaluation(test_data, pred_len)# cgange it back\n",
        "    custom_eval_callback = CustomEvaluation(val_data, pred_len)# change it back\n",
        "    # Add checkpoint callback\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=f'./model_checkpoints/model_{seq_len}_{pred_len}_{dropout_rate}.weights.h5',\n",
        "        monitor='val_mse',\n",
        "        save_best_only=True,\n",
        "        mode='min',\n",
        "        save_weights_only=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        train_data,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_data,\n",
        "        callbacks=[early_stop_callback, reduce_lr_callback, custom_eval_callback, checkpoint_callback],\n",
        "        verbose=2,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_mse, test_mae = custom_evaluation(model, test_data, pred_len)\n",
        "    return custom_eval_callback.best_mse, custom_eval_callback.best_mae, test_mse, test_mae, model\n",
        "\n",
        "def run_experiments(data_name='ETTh1', seq_len_=1024, horizons=[96,192,336,720], dropout_rates=[0.1, 0.2, 0.3], batch_size=32, revin=1, epochs=100, learning_rate=1e-4, mopt='sgd'):\n",
        "    results = []\n",
        "    models = {}\n",
        "\n",
        "    for pred_len in horizons:\n",
        "        if pred_len > seq_len_:\n",
        "            seq_len = pred_len # PADDING\n",
        "        else:\n",
        "            seq_len = seq_len_\n",
        "\n",
        "        data_loader = DataLoader(data=data_name, feature_type='M', batch_size=batch_size,\n",
        "                                 seq_len=seq_len, pred_len=pred_len) # M for Multivariate\n",
        "\n",
        "        for dropout_rate in dropout_rates:\n",
        "            print(f\"Running experiment: horizon={pred_len}, dropout_rate={dropout_rate}\")\n",
        "            val_mse, val_mae, test_mse, test_mae, model = train_and_evaluate(data_loader, seq_len, pred_len, dropout_rate, revin=revin, epochs=epochs, learning_rate=learning_rate, mopt=mopt)\n",
        "\n",
        "            results.append({\n",
        "                'horizon': pred_len,\n",
        "                'dropout_rate': dropout_rate,\n",
        "                'val_mse': val_mse,\n",
        "                'val_mae': val_mae,\n",
        "                'test_mse': test_mse,\n",
        "                'test_mae': test_mae\n",
        "            })\n",
        "\n",
        "            # Store the model\n",
        "            models[(pred_len, dropout_rate)] = model\n",
        "\n",
        "            results_dir = './flowmixer_results'\n",
        "            if not os.path.exists(results_dir):\n",
        "                os.makedirs(results_dir)\n",
        "            results_file = os.path.join(results_dir, f'{data_name}_experiment_results.csv')\n",
        "            #results_gdir = '/content/drive/MyDrive/FlowMixer/flowmixer_results_nmi'\n",
        "            #results_gfile = os.path.join(results_gdir, f'{data_name}_experiment_results.csv')\n",
        "            # Save results after each experiment\n",
        "            df = pd.DataFrame(results)\n",
        "            df.to_csv(results_file, index=False)\n",
        "            #df.to_csv(results_gfile, index=False)\n",
        "\n",
        "\n",
        "    return results, models\n",
        "\n",
        "\n",
        "def display_results_tables(results):\n",
        "   \"\"\"\n",
        "   Display MSE and MAE results in formatted tables.\n",
        "\n",
        "   Args:\n",
        "       results: List of dictionaries containing results with keys:\n",
        "               horizon, dropout_rate, val_mse, val_mae, test_mse, test_mae\n",
        "   \"\"\"\n",
        "   # Create DataFrame from results\n",
        "   df = pd.DataFrame(results)\n",
        "\n",
        "   # Create pivot tables\n",
        "   mse_table = pd.pivot_table(df, values='test_mse', index='horizon', columns='dropout_rate')\n",
        "   mae_table = pd.pivot_table(df, values='test_mae', index='horizon', columns='dropout_rate')\n",
        "\n",
        "   # Format column headers to show dropout rates as percentages\n",
        "   mse_table.columns = [f'DR={x:.0%}' for x in mse_table.columns]\n",
        "   mae_table.columns = [f'DR={x:.0%}' for x in mae_table.columns]\n",
        "\n",
        "   # Rename index\n",
        "   mse_table.index.name = 'Horizon'\n",
        "   mae_table.index.name = 'Horizon'\n",
        "\n",
        "   # Display tables with nice formatting\n",
        "   print(\"\\nMSE Results:\")\n",
        "   print(\"=\" * 50)\n",
        "   print(mse_table.round(4))\n",
        "\n",
        "   print(\"\\nMAE Results:\")\n",
        "   print(\"=\" * 50)\n",
        "   print(mae_table.round(4))\n",
        "\n",
        "# Usage:\n",
        "#display_results_tables(results[0])  #\n",
        "\n",
        "# Example usage:\n",
        "# results, models = run_experiments()\n",
        "#\n",
        "# # To access a specific model:\n",
        "# model = models[(96, 0.1)]  # Gets the model for horizon=96 and dropout_rate=0.1\n",
        "#\n",
        "# # You can now use this model for further predictions or analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "wEY0D-bxwx-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETTh1"
      ],
      "metadata": {
        "id": "6rCpXxlC0W9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEyASCqIEQF0",
        "outputId": "24ba5044-2cf1-497c-f741-7ace40331736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: horizon=96, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_17', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "236/236 - 5s - 22ms/step - loss: 0.4244 - mae: 0.4586 - mse: 0.4244 - val_loss: 0.7782 - val_mae: 0.6197 - val_mse: 0.7782 - learning_rate: 0.1000 - val_custom_mse: 0.9545 - val_custom_mae: 0.6896\n",
            "Epoch 2/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.4035 - mae: 0.4482 - mse: 0.4035 - val_loss: 0.7290 - val_mae: 0.5983 - val_mse: 0.7290 - learning_rate: 0.1000 - val_custom_mse: 0.9243 - val_custom_mae: 0.6760\n",
            "Epoch 3/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.3830 - mae: 0.4364 - mse: 0.3830 - val_loss: 0.6764 - val_mae: 0.5751 - val_mse: 0.6764 - learning_rate: 0.1000 - val_custom_mse: 0.9031 - val_custom_mae: 0.6662\n",
            "Epoch 4/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3580 - mae: 0.4216 - mse: 0.3580 - val_loss: 0.6162 - val_mae: 0.5483 - val_mse: 0.6162 - learning_rate: 0.1000 - val_custom_mse: 0.8869 - val_custom_mae: 0.6590\n",
            "Epoch 5/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3271 - mae: 0.4024 - mse: 0.3271 - val_loss: 0.5442 - val_mae: 0.5141 - val_mse: 0.5442 - learning_rate: 0.1000 - val_custom_mse: 0.8711 - val_custom_mae: 0.6521\n",
            "Epoch 6/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2895 - mae: 0.3777 - mse: 0.2895 - val_loss: 0.4643 - val_mae: 0.4741 - val_mse: 0.4643 - learning_rate: 0.1000 - val_custom_mse: 0.8648 - val_custom_mae: 0.6503\n",
            "Epoch 7/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2471 - mae: 0.3474 - mse: 0.2471 - val_loss: 0.3782 - val_mae: 0.4238 - val_mse: 0.3782 - learning_rate: 0.1000 - val_custom_mse: 0.8468 - val_custom_mae: 0.6417\n",
            "Epoch 8/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2039 - mae: 0.3132 - mse: 0.2039 - val_loss: 0.3012 - val_mae: 0.3736 - val_mse: 0.3012 - learning_rate: 0.1000 - val_custom_mse: 0.8427 - val_custom_mae: 0.6401\n",
            "Epoch 9/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1650 - mae: 0.2787 - mse: 0.1650 - val_loss: 0.2382 - val_mae: 0.3252 - val_mse: 0.2382 - learning_rate: 0.1000 - val_custom_mse: 0.8264 - val_custom_mae: 0.6315\n",
            "Epoch 10/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1342 - mae: 0.2475 - mse: 0.1342 - val_loss: 0.1944 - val_mae: 0.2860 - val_mse: 0.1944 - learning_rate: 0.1000 - val_custom_mse: 0.8169 - val_custom_mae: 0.6259\n",
            "Epoch 11/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1124 - mae: 0.2222 - mse: 0.1124 - val_loss: 0.1667 - val_mae: 0.2576 - val_mse: 0.1667 - learning_rate: 0.1000 - val_custom_mse: 0.8037 - val_custom_mae: 0.6180\n",
            "Epoch 12/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0979 - mae: 0.2033 - mse: 0.0979 - val_loss: 0.1502 - val_mae: 0.2386 - val_mse: 0.1502 - learning_rate: 0.1000 - val_custom_mse: 0.7930 - val_custom_mae: 0.6112\n",
            "Epoch 13/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0884 - mae: 0.1896 - mse: 0.0884 - val_loss: 0.1400 - val_mae: 0.2256 - val_mse: 0.1400 - learning_rate: 0.1000 - val_custom_mse: 0.7855 - val_custom_mae: 0.6061\n",
            "Epoch 14/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0818 - mae: 0.1794 - mse: 0.0818 - val_loss: 0.1328 - val_mae: 0.2156 - val_mse: 0.1328 - learning_rate: 0.1000 - val_custom_mse: 0.7801 - val_custom_mae: 0.6023\n",
            "Epoch 15/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0770 - mae: 0.1713 - mse: 0.0770 - val_loss: 0.1276 - val_mae: 0.2087 - val_mse: 0.1276 - learning_rate: 0.1000 - val_custom_mse: 0.7735 - val_custom_mae: 0.5980\n",
            "Epoch 16/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0732 - mae: 0.1646 - mse: 0.0732 - val_loss: 0.1229 - val_mae: 0.2013 - val_mse: 0.1229 - learning_rate: 0.1000 - val_custom_mse: 0.7699 - val_custom_mae: 0.5957\n",
            "Epoch 17/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0699 - mae: 0.1587 - mse: 0.0699 - val_loss: 0.1189 - val_mae: 0.1952 - val_mse: 0.1189 - learning_rate: 0.1000 - val_custom_mse: 0.7653 - val_custom_mae: 0.5930\n",
            "Epoch 18/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0671 - mae: 0.1534 - mse: 0.0671 - val_loss: 0.1156 - val_mae: 0.1903 - val_mse: 0.1156 - learning_rate: 0.1000 - val_custom_mse: 0.7600 - val_custom_mae: 0.5899\n",
            "Epoch 19/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0646 - mae: 0.1485 - mse: 0.0646 - val_loss: 0.1121 - val_mae: 0.1845 - val_mse: 0.1121 - learning_rate: 0.1000 - val_custom_mse: 0.7558 - val_custom_mae: 0.5877\n",
            "Epoch 20/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0624 - mae: 0.1440 - mse: 0.0624 - val_loss: 0.1090 - val_mae: 0.1790 - val_mse: 0.1090 - learning_rate: 0.1000 - val_custom_mse: 0.7520 - val_custom_mae: 0.5859\n",
            "Epoch 21/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0604 - mae: 0.1398 - mse: 0.0604 - val_loss: 0.1065 - val_mae: 0.1752 - val_mse: 0.1065 - learning_rate: 0.1000 - val_custom_mse: 0.7478 - val_custom_mae: 0.5836\n",
            "Epoch 22/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0585 - mae: 0.1359 - mse: 0.0585 - val_loss: 0.1037 - val_mae: 0.1698 - val_mse: 0.1037 - learning_rate: 0.1000 - val_custom_mse: 0.7446 - val_custom_mae: 0.5823\n",
            "Epoch 23/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0569 - mae: 0.1323 - mse: 0.0569 - val_loss: 0.1017 - val_mae: 0.1662 - val_mse: 0.1017 - learning_rate: 0.1000 - val_custom_mse: 0.7415 - val_custom_mae: 0.5806\n",
            "Epoch 24/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0553 - mae: 0.1289 - mse: 0.0553 - val_loss: 0.1000 - val_mae: 0.1635 - val_mse: 0.1000 - learning_rate: 0.1000 - val_custom_mse: 0.7382 - val_custom_mae: 0.5787\n",
            "Epoch 25/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0539 - mae: 0.1257 - mse: 0.0539 - val_loss: 0.0982 - val_mae: 0.1603 - val_mse: 0.0982 - learning_rate: 0.1000 - val_custom_mse: 0.7357 - val_custom_mae: 0.5774\n",
            "Epoch 26/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0526 - mae: 0.1226 - mse: 0.0526 - val_loss: 0.0961 - val_mae: 0.1562 - val_mse: 0.0961 - learning_rate: 0.1000 - val_custom_mse: 0.7327 - val_custom_mae: 0.5761\n",
            "Epoch 27/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0514 - mae: 0.1198 - mse: 0.0514 - val_loss: 0.0942 - val_mae: 0.1520 - val_mse: 0.0942 - learning_rate: 0.1000 - val_custom_mse: 0.7308 - val_custom_mae: 0.5755\n",
            "Epoch 28/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0503 - mae: 0.1171 - mse: 0.0503 - val_loss: 0.0930 - val_mae: 0.1498 - val_mse: 0.0930 - learning_rate: 0.1000 - val_custom_mse: 0.7291 - val_custom_mae: 0.5744\n",
            "Epoch 29/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0492 - mae: 0.1146 - mse: 0.0492 - val_loss: 0.0912 - val_mae: 0.1456 - val_mse: 0.0912 - learning_rate: 0.1000 - val_custom_mse: 0.7270 - val_custom_mae: 0.5738\n",
            "Epoch 30/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0483 - mae: 0.1122 - mse: 0.0483 - val_loss: 0.0899 - val_mae: 0.1427 - val_mse: 0.0899 - learning_rate: 0.1000 - val_custom_mse: 0.7251 - val_custom_mae: 0.5730\n",
            "Epoch 31/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0474 - mae: 0.1099 - mse: 0.0474 - val_loss: 0.0887 - val_mae: 0.1403 - val_mse: 0.0887 - learning_rate: 0.1000 - val_custom_mse: 0.7235 - val_custom_mae: 0.5723\n",
            "Epoch 32/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0465 - mae: 0.1078 - mse: 0.0465 - val_loss: 0.0876 - val_mae: 0.1383 - val_mse: 0.0876 - learning_rate: 0.1000 - val_custom_mse: 0.7215 - val_custom_mae: 0.5712\n",
            "Epoch 33/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0457 - mae: 0.1057 - mse: 0.0457 - val_loss: 0.0864 - val_mae: 0.1353 - val_mse: 0.0864 - learning_rate: 0.1000 - val_custom_mse: 0.7206 - val_custom_mae: 0.5711\n",
            "Epoch 34/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0450 - mae: 0.1038 - mse: 0.0450 - val_loss: 0.0853 - val_mae: 0.1327 - val_mse: 0.0853 - learning_rate: 0.1000 - val_custom_mse: 0.7194 - val_custom_mae: 0.5707\n",
            "Epoch 35/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0443 - mae: 0.1020 - mse: 0.0443 - val_loss: 0.0847 - val_mae: 0.1316 - val_mse: 0.0847 - learning_rate: 0.1000 - val_custom_mse: 0.7183 - val_custom_mae: 0.5697\n",
            "Epoch 36/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0437 - mae: 0.1002 - mse: 0.0437 - val_loss: 0.0837 - val_mae: 0.1294 - val_mse: 0.0837 - learning_rate: 0.1000 - val_custom_mse: 0.7172 - val_custom_mae: 0.5693\n",
            "Epoch 37/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0430 - mae: 0.0985 - mse: 0.0430 - val_loss: 0.0828 - val_mae: 0.1272 - val_mse: 0.0828 - learning_rate: 0.1000 - val_custom_mse: 0.7160 - val_custom_mae: 0.5689\n",
            "Epoch 38/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0425 - mae: 0.0970 - mse: 0.0425 - val_loss: 0.0823 - val_mae: 0.1260 - val_mse: 0.0823 - learning_rate: 0.1000 - val_custom_mse: 0.7155 - val_custom_mae: 0.5683\n",
            "Epoch 39/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0419 - mae: 0.0955 - mse: 0.0419 - val_loss: 0.0813 - val_mae: 0.1235 - val_mse: 0.0813 - learning_rate: 0.1000 - val_custom_mse: 0.7142 - val_custom_mae: 0.5681\n",
            "Epoch 40/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0414 - mae: 0.0940 - mse: 0.0414 - val_loss: 0.0806 - val_mae: 0.1215 - val_mse: 0.0806 - learning_rate: 0.1000 - val_custom_mse: 0.7136 - val_custom_mae: 0.5680\n",
            "Epoch 41/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0409 - mae: 0.0926 - mse: 0.0409 - val_loss: 0.0800 - val_mae: 0.1205 - val_mse: 0.0800 - learning_rate: 0.1000 - val_custom_mse: 0.7125 - val_custom_mae: 0.5673\n",
            "Epoch 42/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0405 - mae: 0.0913 - mse: 0.0405 - val_loss: 0.0797 - val_mae: 0.1197 - val_mse: 0.0797 - learning_rate: 0.1000 - val_custom_mse: 0.7128 - val_custom_mae: 0.5671\n",
            "Epoch 43/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0401 - mae: 0.0900 - mse: 0.0401 - val_loss: 0.0788 - val_mae: 0.1173 - val_mse: 0.0788 - learning_rate: 0.1000 - val_custom_mse: 0.7113 - val_custom_mae: 0.5668\n",
            "Epoch 44/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0396 - mae: 0.0888 - mse: 0.0396 - val_loss: 0.0782 - val_mae: 0.1155 - val_mse: 0.0782 - learning_rate: 0.1000 - val_custom_mse: 0.7108 - val_custom_mae: 0.5669\n",
            "Epoch 45/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0393 - mae: 0.0876 - mse: 0.0393 - val_loss: 0.0778 - val_mae: 0.1146 - val_mse: 0.0778 - learning_rate: 0.1000 - val_custom_mse: 0.7104 - val_custom_mae: 0.5664\n",
            "Epoch 46/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0389 - mae: 0.0865 - mse: 0.0389 - val_loss: 0.0772 - val_mae: 0.1132 - val_mse: 0.0772 - learning_rate: 0.1000 - val_custom_mse: 0.7096 - val_custom_mae: 0.5662\n",
            "Epoch 47/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0385 - mae: 0.0854 - mse: 0.0385 - val_loss: 0.0768 - val_mae: 0.1122 - val_mse: 0.0768 - learning_rate: 0.1000 - val_custom_mse: 0.7094 - val_custom_mae: 0.5659\n",
            "Epoch 48/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0382 - mae: 0.0844 - mse: 0.0382 - val_loss: 0.0763 - val_mae: 0.1105 - val_mse: 0.0763 - learning_rate: 0.1000 - val_custom_mse: 0.7091 - val_custom_mae: 0.5660\n",
            "Epoch 49/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0379 - mae: 0.0833 - mse: 0.0379 - val_loss: 0.0759 - val_mae: 0.1092 - val_mse: 0.0759 - learning_rate: 0.1000 - val_custom_mse: 0.7086 - val_custom_mae: 0.5661\n",
            "Epoch 50/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0376 - mae: 0.0824 - mse: 0.0376 - val_loss: 0.0755 - val_mae: 0.1081 - val_mse: 0.0755 - learning_rate: 0.1000 - val_custom_mse: 0.7080 - val_custom_mae: 0.5658\n",
            "Epoch 51/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0373 - mae: 0.0815 - mse: 0.0373 - val_loss: 0.0752 - val_mae: 0.1072 - val_mse: 0.0752 - learning_rate: 0.1000 - val_custom_mse: 0.7086 - val_custom_mae: 0.5658\n",
            "Epoch 52/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0370 - mae: 0.0805 - mse: 0.0370 - val_loss: 0.0748 - val_mae: 0.1060 - val_mse: 0.0748 - learning_rate: 0.1000 - val_custom_mse: 0.7079 - val_custom_mae: 0.5657\n",
            "Epoch 53/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0368 - mae: 0.0797 - mse: 0.0368 - val_loss: 0.0744 - val_mae: 0.1049 - val_mse: 0.0744 - learning_rate: 0.1000 - val_custom_mse: 0.7077 - val_custom_mae: 0.5657\n",
            "Epoch 54/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0365 - mae: 0.0788 - mse: 0.0365 - val_loss: 0.0741 - val_mae: 0.1040 - val_mse: 0.0741 - learning_rate: 0.1000 - val_custom_mse: 0.7071 - val_custom_mae: 0.5653\n",
            "Epoch 55/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0363 - mae: 0.0781 - mse: 0.0363 - val_loss: 0.0739 - val_mae: 0.1037 - val_mse: 0.0739 - learning_rate: 0.1000 - val_custom_mse: 0.7072 - val_custom_mae: 0.5663\n",
            "Epoch 56/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0361 - mae: 0.0774 - mse: 0.0361 - val_loss: 0.0735 - val_mae: 0.1023 - val_mse: 0.0735 - learning_rate: 0.1000 - val_custom_mse: 0.7068 - val_custom_mae: 0.5652\n",
            "Epoch 57/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0359 - mae: 0.0765 - mse: 0.0359 - val_loss: 0.0732 - val_mae: 0.1013 - val_mse: 0.0732 - learning_rate: 0.1000 - val_custom_mse: 0.7062 - val_custom_mae: 0.5649\n",
            "Epoch 58/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0357 - mae: 0.0758 - mse: 0.0357 - val_loss: 0.0729 - val_mae: 0.1003 - val_mse: 0.0729 - learning_rate: 0.1000 - val_custom_mse: 0.7061 - val_custom_mae: 0.5652\n",
            "Epoch 59/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0355 - mae: 0.0750 - mse: 0.0355 - val_loss: 0.0727 - val_mae: 0.0996 - val_mse: 0.0727 - learning_rate: 0.1000 - val_custom_mse: 0.7056 - val_custom_mae: 0.5646\n",
            "Epoch 60/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0353 - mae: 0.0744 - mse: 0.0353 - val_loss: 0.0725 - val_mae: 0.0988 - val_mse: 0.0725 - learning_rate: 0.1000 - val_custom_mse: 0.7059 - val_custom_mae: 0.5648\n",
            "Epoch 61/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0351 - mae: 0.0737 - mse: 0.0351 - val_loss: 0.0722 - val_mae: 0.0979 - val_mse: 0.0722 - learning_rate: 0.1000 - val_custom_mse: 0.7054 - val_custom_mae: 0.5647\n",
            "Epoch 62/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0349 - mae: 0.0731 - mse: 0.0349 - val_loss: 0.0720 - val_mae: 0.0972 - val_mse: 0.0720 - learning_rate: 0.1000 - val_custom_mse: 0.7052 - val_custom_mae: 0.5644\n",
            "Epoch 63/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0348 - mae: 0.0724 - mse: 0.0348 - val_loss: 0.0718 - val_mae: 0.0963 - val_mse: 0.0718 - learning_rate: 0.1000 - val_custom_mse: 0.7052 - val_custom_mae: 0.5648\n",
            "Epoch 64/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0346 - mae: 0.0719 - mse: 0.0346 - val_loss: 0.0715 - val_mae: 0.0957 - val_mse: 0.0715 - learning_rate: 0.1000 - val_custom_mse: 0.7046 - val_custom_mae: 0.5642\n",
            "Epoch 65/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0345 - mae: 0.0713 - mse: 0.0345 - val_loss: 0.0713 - val_mae: 0.0949 - val_mse: 0.0713 - learning_rate: 0.1000 - val_custom_mse: 0.7044 - val_custom_mae: 0.5642\n",
            "Epoch 66/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0343 - mae: 0.0707 - mse: 0.0343 - val_loss: 0.0713 - val_mae: 0.0951 - val_mse: 0.0713 - learning_rate: 0.1000 - val_custom_mse: 0.7045 - val_custom_mae: 0.5637\n",
            "Epoch 67/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0342 - mae: 0.0702 - mse: 0.0342 - val_loss: 0.0710 - val_mae: 0.0938 - val_mse: 0.0710 - learning_rate: 0.1000 - val_custom_mse: 0.7039 - val_custom_mae: 0.5638\n",
            "Epoch 68/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0341 - mae: 0.0696 - mse: 0.0341 - val_loss: 0.0708 - val_mae: 0.0929 - val_mse: 0.0708 - learning_rate: 0.1000 - val_custom_mse: 0.7038 - val_custom_mae: 0.5640\n",
            "Epoch 69/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0339 - mae: 0.0691 - mse: 0.0339 - val_loss: 0.0706 - val_mae: 0.0924 - val_mse: 0.0706 - learning_rate: 0.1000 - val_custom_mse: 0.7038 - val_custom_mae: 0.5638\n",
            "Epoch 70/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0338 - mae: 0.0686 - mse: 0.0338 - val_loss: 0.0705 - val_mae: 0.0916 - val_mse: 0.0705 - learning_rate: 0.1000 - val_custom_mse: 0.7039 - val_custom_mae: 0.5640\n",
            "Epoch 71/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0337 - mae: 0.0681 - mse: 0.0337 - val_loss: 0.0703 - val_mae: 0.0910 - val_mse: 0.0703 - learning_rate: 0.1000 - val_custom_mse: 0.7033 - val_custom_mae: 0.5638\n",
            "Epoch 72/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0336 - mae: 0.0676 - mse: 0.0336 - val_loss: 0.0701 - val_mae: 0.0904 - val_mse: 0.0701 - learning_rate: 0.1000 - val_custom_mse: 0.7034 - val_custom_mae: 0.5639\n",
            "Epoch 73/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0335 - mae: 0.0672 - mse: 0.0335 - val_loss: 0.0701 - val_mae: 0.0905 - val_mse: 0.0701 - learning_rate: 0.1000 - val_custom_mse: 0.7028 - val_custom_mae: 0.5643\n",
            "Epoch 74/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0334 - mae: 0.0668 - mse: 0.0334 - val_loss: 0.0699 - val_mae: 0.0894 - val_mse: 0.0699 - learning_rate: 0.1000 - val_custom_mse: 0.7030 - val_custom_mae: 0.5636\n",
            "Epoch 75/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0333 - mae: 0.0663 - mse: 0.0333 - val_loss: 0.0697 - val_mae: 0.0887 - val_mse: 0.0697 - learning_rate: 0.1000 - val_custom_mse: 0.7025 - val_custom_mae: 0.5637\n",
            "Epoch 76/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0332 - mae: 0.0659 - mse: 0.0332 - val_loss: 0.0696 - val_mae: 0.0882 - val_mse: 0.0696 - learning_rate: 0.1000 - val_custom_mse: 0.7026 - val_custom_mae: 0.5636\n",
            "Epoch 77/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0331 - mae: 0.0654 - mse: 0.0331 - val_loss: 0.0695 - val_mae: 0.0878 - val_mse: 0.0695 - learning_rate: 0.1000 - val_custom_mse: 0.7023 - val_custom_mae: 0.5633\n",
            "Epoch 78/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0330 - mae: 0.0651 - mse: 0.0330 - val_loss: 0.0693 - val_mae: 0.0873 - val_mse: 0.0693 - learning_rate: 0.1000 - val_custom_mse: 0.7023 - val_custom_mae: 0.5632\n",
            "Epoch 79/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0329 - mae: 0.0646 - mse: 0.0329 - val_loss: 0.0692 - val_mae: 0.0868 - val_mse: 0.0692 - learning_rate: 0.1000 - val_custom_mse: 0.7021 - val_custom_mae: 0.5632\n",
            "Epoch 80/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0328 - mae: 0.0643 - mse: 0.0328 - val_loss: 0.0691 - val_mae: 0.0862 - val_mse: 0.0691 - learning_rate: 0.1000 - val_custom_mse: 0.7019 - val_custom_mae: 0.5633\n",
            "Epoch 81/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0327 - mae: 0.0639 - mse: 0.0327 - val_loss: 0.0690 - val_mae: 0.0858 - val_mse: 0.0690 - learning_rate: 0.1000 - val_custom_mse: 0.7019 - val_custom_mae: 0.5632\n",
            "Epoch 82/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0327 - mae: 0.0635 - mse: 0.0327 - val_loss: 0.0689 - val_mae: 0.0854 - val_mse: 0.0689 - learning_rate: 0.1000 - val_custom_mse: 0.7022 - val_custom_mae: 0.5636\n",
            "Epoch 83/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0326 - mae: 0.0633 - mse: 0.0326 - val_loss: 0.0688 - val_mae: 0.0852 - val_mse: 0.0688 - learning_rate: 0.1000 - val_custom_mse: 0.7015 - val_custom_mae: 0.5629\n",
            "Epoch 84/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0325 - mae: 0.0629 - mse: 0.0325 - val_loss: 0.0690 - val_mae: 0.0873 - val_mse: 0.0690 - learning_rate: 0.1000 - val_custom_mse: 0.7017 - val_custom_mae: 0.5646\n",
            "Epoch 85/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0325 - mae: 0.0628 - mse: 0.0325 - val_loss: 0.0686 - val_mae: 0.0841 - val_mse: 0.0686 - learning_rate: 0.1000 - val_custom_mse: 0.7014 - val_custom_mae: 0.5633\n",
            "Epoch 86/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0324 - mae: 0.0622 - mse: 0.0324 - val_loss: 0.0685 - val_mae: 0.0838 - val_mse: 0.0685 - learning_rate: 0.1000 - val_custom_mse: 0.7012 - val_custom_mae: 0.5635\n",
            "Epoch 87/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0323 - mae: 0.0619 - mse: 0.0323 - val_loss: 0.0684 - val_mae: 0.0832 - val_mse: 0.0684 - learning_rate: 0.1000 - val_custom_mse: 0.7009 - val_custom_mae: 0.5631\n",
            "Epoch 88/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0322 - mae: 0.0616 - mse: 0.0322 - val_loss: 0.0683 - val_mae: 0.0830 - val_mse: 0.0683 - learning_rate: 0.1000 - val_custom_mse: 0.7009 - val_custom_mae: 0.5629\n",
            "Epoch 89/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0322 - mae: 0.0612 - mse: 0.0322 - val_loss: 0.0682 - val_mae: 0.0824 - val_mse: 0.0682 - learning_rate: 0.1000 - val_custom_mse: 0.7006 - val_custom_mae: 0.5629\n",
            "Epoch 90/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0321 - mae: 0.0610 - mse: 0.0321 - val_loss: 0.0682 - val_mae: 0.0821 - val_mse: 0.0682 - learning_rate: 0.1000 - val_custom_mse: 0.7011 - val_custom_mae: 0.5629\n",
            "Epoch 91/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0321 - mae: 0.0607 - mse: 0.0321 - val_loss: 0.0681 - val_mae: 0.0818 - val_mse: 0.0681 - learning_rate: 0.1000 - val_custom_mse: 0.7006 - val_custom_mae: 0.5627\n",
            "Epoch 92/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0320 - mae: 0.0604 - mse: 0.0320 - val_loss: 0.0680 - val_mae: 0.0814 - val_mse: 0.0680 - learning_rate: 0.1000 - val_custom_mse: 0.7008 - val_custom_mae: 0.5629\n",
            "Epoch 93/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0320 - mae: 0.0601 - mse: 0.0320 - val_loss: 0.0680 - val_mae: 0.0813 - val_mse: 0.0680 - learning_rate: 0.1000 - val_custom_mse: 0.7009 - val_custom_mae: 0.5627\n",
            "Epoch 94/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0319 - mae: 0.0599 - mse: 0.0319 - val_loss: 0.0679 - val_mae: 0.0807 - val_mse: 0.0679 - learning_rate: 0.1000 - val_custom_mse: 0.7007 - val_custom_mae: 0.5631\n",
            "Epoch 95/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0319 - mae: 0.0596 - mse: 0.0319 - val_loss: 0.0678 - val_mae: 0.0803 - val_mse: 0.0678 - learning_rate: 0.1000 - val_custom_mse: 0.7003 - val_custom_mae: 0.5628\n",
            "Epoch 96/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0318 - mae: 0.0594 - mse: 0.0318 - val_loss: 0.0677 - val_mae: 0.0800 - val_mse: 0.0677 - learning_rate: 0.1000 - val_custom_mse: 0.7001 - val_custom_mae: 0.5627\n",
            "Epoch 97/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0318 - mae: 0.0591 - mse: 0.0318 - val_loss: 0.0676 - val_mae: 0.0797 - val_mse: 0.0676 - learning_rate: 0.1000 - val_custom_mse: 0.7000 - val_custom_mae: 0.5626\n",
            "Epoch 98/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0317 - mae: 0.0589 - mse: 0.0317 - val_loss: 0.0676 - val_mae: 0.0795 - val_mse: 0.0676 - learning_rate: 0.1000 - val_custom_mse: 0.6999 - val_custom_mae: 0.5629\n",
            "Epoch 99/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0317 - mae: 0.0586 - mse: 0.0317 - val_loss: 0.0676 - val_mae: 0.0792 - val_mse: 0.0676 - learning_rate: 0.1000 - val_custom_mse: 0.7003 - val_custom_mae: 0.5626\n",
            "Epoch 100/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0316 - mae: 0.0584 - mse: 0.0316 - val_loss: 0.0675 - val_mae: 0.0790 - val_mse: 0.0675 - learning_rate: 0.1000 - val_custom_mse: 0.7003 - val_custom_mae: 0.5626\n",
            "Running experiment: horizon=96, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_18', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "236/236 - 6s - 27ms/step - loss: 0.4381 - mae: 0.4675 - mse: 0.4381 - val_loss: 0.7752 - val_mae: 0.6214 - val_mse: 0.7752 - learning_rate: 0.1000 - val_custom_mse: 0.9619 - val_custom_mae: 0.6949\n",
            "Epoch 2/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.4117 - mae: 0.4540 - mse: 0.4117 - val_loss: 0.7301 - val_mae: 0.6031 - val_mse: 0.7301 - learning_rate: 0.1000 - val_custom_mse: 0.9471 - val_custom_mae: 0.6885\n",
            "Epoch 3/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.3879 - mae: 0.4404 - mse: 0.3879 - val_loss: 0.6734 - val_mae: 0.5781 - val_mse: 0.6734 - learning_rate: 0.1000 - val_custom_mse: 0.9249 - val_custom_mae: 0.6782\n",
            "Epoch 4/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3594 - mae: 0.4234 - mse: 0.3594 - val_loss: 0.6103 - val_mae: 0.5503 - val_mse: 0.6103 - learning_rate: 0.1000 - val_custom_mse: 0.9128 - val_custom_mae: 0.6731\n",
            "Epoch 5/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3249 - mae: 0.4019 - mse: 0.3249 - val_loss: 0.5283 - val_mae: 0.5089 - val_mse: 0.5283 - learning_rate: 0.1000 - val_custom_mse: 0.8847 - val_custom_mae: 0.6595\n",
            "Epoch 6/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2844 - mae: 0.3748 - mse: 0.2844 - val_loss: 0.4470 - val_mae: 0.4666 - val_mse: 0.4470 - learning_rate: 0.1000 - val_custom_mse: 0.8796 - val_custom_mae: 0.6576\n",
            "Epoch 7/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2407 - mae: 0.3429 - mse: 0.2407 - val_loss: 0.3604 - val_mae: 0.4143 - val_mse: 0.3604 - learning_rate: 0.1000 - val_custom_mse: 0.8601 - val_custom_mae: 0.6480\n",
            "Epoch 8/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1988 - mae: 0.3088 - mse: 0.1988 - val_loss: 0.2852 - val_mae: 0.3624 - val_mse: 0.2852 - learning_rate: 0.1000 - val_custom_mse: 0.8387 - val_custom_mae: 0.6371\n",
            "Epoch 9/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1637 - mae: 0.2766 - mse: 0.1637 - val_loss: 0.2294 - val_mae: 0.3185 - val_mse: 0.2294 - learning_rate: 0.1000 - val_custom_mse: 0.8268 - val_custom_mae: 0.6305\n",
            "Epoch 10/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1377 - mae: 0.2498 - mse: 0.1377 - val_loss: 0.1915 - val_mae: 0.2842 - val_mse: 0.1915 - learning_rate: 0.1000 - val_custom_mse: 0.8133 - val_custom_mae: 0.6228\n",
            "Epoch 11/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1204 - mae: 0.2298 - mse: 0.1204 - val_loss: 0.1677 - val_mae: 0.2593 - val_mse: 0.1677 - learning_rate: 0.1000 - val_custom_mse: 0.8053 - val_custom_mae: 0.6175\n",
            "Epoch 12/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1093 - mae: 0.2158 - mse: 0.1093 - val_loss: 0.1533 - val_mae: 0.2433 - val_mse: 0.1533 - learning_rate: 0.1000 - val_custom_mse: 0.7930 - val_custom_mae: 0.6100\n",
            "Epoch 13/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1021 - mae: 0.2060 - mse: 0.1021 - val_loss: 0.1436 - val_mae: 0.2314 - val_mse: 0.1436 - learning_rate: 0.1000 - val_custom_mse: 0.7857 - val_custom_mae: 0.6052\n",
            "Epoch 14/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0972 - mae: 0.1988 - mse: 0.0972 - val_loss: 0.1366 - val_mae: 0.2217 - val_mse: 0.1366 - learning_rate: 0.1000 - val_custom_mse: 0.7811 - val_custom_mae: 0.6022\n",
            "Epoch 15/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0934 - mae: 0.1931 - mse: 0.0934 - val_loss: 0.1311 - val_mae: 0.2145 - val_mse: 0.1311 - learning_rate: 0.1000 - val_custom_mse: 0.7757 - val_custom_mae: 0.5988\n",
            "Epoch 16/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0903 - mae: 0.1883 - mse: 0.0903 - val_loss: 0.1263 - val_mae: 0.2073 - val_mse: 0.1263 - learning_rate: 0.1000 - val_custom_mse: 0.7717 - val_custom_mae: 0.5965\n",
            "Epoch 17/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0878 - mae: 0.1841 - mse: 0.0878 - val_loss: 0.1221 - val_mae: 0.2012 - val_mse: 0.1221 - learning_rate: 0.1000 - val_custom_mse: 0.7678 - val_custom_mae: 0.5945\n",
            "Epoch 18/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0856 - mae: 0.1803 - mse: 0.0856 - val_loss: 0.1191 - val_mae: 0.1972 - val_mse: 0.1191 - learning_rate: 0.1000 - val_custom_mse: 0.7619 - val_custom_mae: 0.5910\n",
            "Epoch 19/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0836 - mae: 0.1769 - mse: 0.0836 - val_loss: 0.1161 - val_mae: 0.1928 - val_mse: 0.1161 - learning_rate: 0.1000 - val_custom_mse: 0.7578 - val_custom_mae: 0.5887\n",
            "Epoch 20/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0818 - mae: 0.1738 - mse: 0.0818 - val_loss: 0.1130 - val_mae: 0.1879 - val_mse: 0.1130 - learning_rate: 0.1000 - val_custom_mse: 0.7537 - val_custom_mae: 0.5869\n",
            "Epoch 21/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0802 - mae: 0.1709 - mse: 0.0802 - val_loss: 0.1100 - val_mae: 0.1828 - val_mse: 0.1100 - learning_rate: 0.1000 - val_custom_mse: 0.7504 - val_custom_mae: 0.5856\n",
            "Epoch 22/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0788 - mae: 0.1682 - mse: 0.0788 - val_loss: 0.1080 - val_mae: 0.1798 - val_mse: 0.1080 - learning_rate: 0.1000 - val_custom_mse: 0.7468 - val_custom_mae: 0.5836\n",
            "Epoch 23/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0776 - mae: 0.1658 - mse: 0.0776 - val_loss: 0.1062 - val_mae: 0.1773 - val_mse: 0.1062 - learning_rate: 0.1000 - val_custom_mse: 0.7437 - val_custom_mae: 0.5818\n",
            "Epoch 24/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0764 - mae: 0.1636 - mse: 0.0764 - val_loss: 0.1040 - val_mae: 0.1734 - val_mse: 0.1040 - learning_rate: 0.1000 - val_custom_mse: 0.7409 - val_custom_mae: 0.5806\n",
            "Epoch 25/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0753 - mae: 0.1614 - mse: 0.0753 - val_loss: 0.1022 - val_mae: 0.1703 - val_mse: 0.1022 - learning_rate: 0.1000 - val_custom_mse: 0.7385 - val_custom_mae: 0.5795\n",
            "Epoch 26/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0744 - mae: 0.1595 - mse: 0.0744 - val_loss: 0.1004 - val_mae: 0.1671 - val_mse: 0.1004 - learning_rate: 0.1000 - val_custom_mse: 0.7365 - val_custom_mae: 0.5788\n",
            "Epoch 27/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0735 - mae: 0.1577 - mse: 0.0735 - val_loss: 0.0988 - val_mae: 0.1644 - val_mse: 0.0988 - learning_rate: 0.1000 - val_custom_mse: 0.7343 - val_custom_mae: 0.5779\n",
            "Epoch 28/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0728 - mae: 0.1561 - mse: 0.0728 - val_loss: 0.0976 - val_mae: 0.1625 - val_mse: 0.0976 - learning_rate: 0.1000 - val_custom_mse: 0.7322 - val_custom_mae: 0.5767\n",
            "Epoch 29/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0721 - mae: 0.1546 - mse: 0.0721 - val_loss: 0.0964 - val_mae: 0.1604 - val_mse: 0.0964 - learning_rate: 0.1000 - val_custom_mse: 0.7300 - val_custom_mae: 0.5758\n",
            "Epoch 30/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0714 - mae: 0.1531 - mse: 0.0714 - val_loss: 0.0951 - val_mae: 0.1579 - val_mse: 0.0951 - learning_rate: 0.1000 - val_custom_mse: 0.7285 - val_custom_mae: 0.5753\n",
            "Epoch 31/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0708 - mae: 0.1518 - mse: 0.0708 - val_loss: 0.0940 - val_mae: 0.1559 - val_mse: 0.0940 - learning_rate: 0.1000 - val_custom_mse: 0.7270 - val_custom_mae: 0.5746\n",
            "Epoch 32/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0703 - mae: 0.1506 - mse: 0.0703 - val_loss: 0.0931 - val_mae: 0.1543 - val_mse: 0.0931 - learning_rate: 0.1000 - val_custom_mse: 0.7255 - val_custom_mae: 0.5738\n",
            "Epoch 33/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0698 - mae: 0.1494 - mse: 0.0698 - val_loss: 0.0922 - val_mae: 0.1526 - val_mse: 0.0922 - learning_rate: 0.1000 - val_custom_mse: 0.7242 - val_custom_mae: 0.5733\n",
            "Epoch 34/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0693 - mae: 0.1483 - mse: 0.0693 - val_loss: 0.0913 - val_mae: 0.1509 - val_mse: 0.0913 - learning_rate: 0.1000 - val_custom_mse: 0.7230 - val_custom_mae: 0.5729\n",
            "Epoch 35/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0689 - mae: 0.1474 - mse: 0.0689 - val_loss: 0.0908 - val_mae: 0.1502 - val_mse: 0.0908 - learning_rate: 0.1000 - val_custom_mse: 0.7217 - val_custom_mae: 0.5718\n",
            "Epoch 36/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0684 - mae: 0.1464 - mse: 0.0684 - val_loss: 0.0900 - val_mae: 0.1487 - val_mse: 0.0900 - learning_rate: 0.1000 - val_custom_mse: 0.7208 - val_custom_mae: 0.5715\n",
            "Epoch 37/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0681 - mae: 0.1456 - mse: 0.0681 - val_loss: 0.0892 - val_mae: 0.1469 - val_mse: 0.0892 - learning_rate: 0.1000 - val_custom_mse: 0.7199 - val_custom_mae: 0.5715\n",
            "Epoch 38/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0678 - mae: 0.1448 - mse: 0.0678 - val_loss: 0.0886 - val_mae: 0.1459 - val_mse: 0.0886 - learning_rate: 0.1000 - val_custom_mse: 0.7192 - val_custom_mae: 0.5711\n",
            "Epoch 39/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0675 - mae: 0.1440 - mse: 0.0675 - val_loss: 0.0881 - val_mae: 0.1449 - val_mse: 0.0881 - learning_rate: 0.1000 - val_custom_mse: 0.7185 - val_custom_mae: 0.5707\n",
            "Epoch 40/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0672 - mae: 0.1433 - mse: 0.0672 - val_loss: 0.0876 - val_mae: 0.1440 - val_mse: 0.0876 - learning_rate: 0.1000 - val_custom_mse: 0.7173 - val_custom_mae: 0.5702\n",
            "Epoch 41/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0670 - mae: 0.1427 - mse: 0.0670 - val_loss: 0.0871 - val_mae: 0.1432 - val_mse: 0.0871 - learning_rate: 0.1000 - val_custom_mse: 0.7168 - val_custom_mae: 0.5699\n",
            "Epoch 42/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0668 - mae: 0.1421 - mse: 0.0668 - val_loss: 0.0867 - val_mae: 0.1422 - val_mse: 0.0867 - learning_rate: 0.1000 - val_custom_mse: 0.7160 - val_custom_mae: 0.5696\n",
            "Epoch 43/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0665 - mae: 0.1415 - mse: 0.0665 - val_loss: 0.0861 - val_mae: 0.1412 - val_mse: 0.0861 - learning_rate: 0.1000 - val_custom_mse: 0.7151 - val_custom_mae: 0.5695\n",
            "Epoch 44/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0663 - mae: 0.1410 - mse: 0.0663 - val_loss: 0.0857 - val_mae: 0.1404 - val_mse: 0.0857 - learning_rate: 0.1000 - val_custom_mse: 0.7144 - val_custom_mae: 0.5693\n",
            "Epoch 45/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0661 - mae: 0.1405 - mse: 0.0661 - val_loss: 0.0855 - val_mae: 0.1400 - val_mse: 0.0855 - learning_rate: 0.1000 - val_custom_mse: 0.7139 - val_custom_mae: 0.5687\n",
            "Epoch 46/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0660 - mae: 0.1400 - mse: 0.0660 - val_loss: 0.0851 - val_mae: 0.1391 - val_mse: 0.0851 - learning_rate: 0.1000 - val_custom_mse: 0.7137 - val_custom_mae: 0.5688\n",
            "Epoch 47/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0657 - mae: 0.1396 - mse: 0.0657 - val_loss: 0.0848 - val_mae: 0.1386 - val_mse: 0.0848 - learning_rate: 0.1000 - val_custom_mse: 0.7129 - val_custom_mae: 0.5684\n",
            "Epoch 48/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0656 - mae: 0.1392 - mse: 0.0656 - val_loss: 0.0844 - val_mae: 0.1378 - val_mse: 0.0844 - learning_rate: 0.1000 - val_custom_mse: 0.7126 - val_custom_mae: 0.5687\n",
            "Epoch 49/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0654 - mae: 0.1388 - mse: 0.0654 - val_loss: 0.0842 - val_mae: 0.1373 - val_mse: 0.0842 - learning_rate: 0.1000 - val_custom_mse: 0.7120 - val_custom_mae: 0.5681\n",
            "Epoch 50/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0653 - mae: 0.1384 - mse: 0.0653 - val_loss: 0.0839 - val_mae: 0.1369 - val_mse: 0.0839 - learning_rate: 0.1000 - val_custom_mse: 0.7115 - val_custom_mae: 0.5678\n",
            "Epoch 51/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0652 - mae: 0.1381 - mse: 0.0652 - val_loss: 0.0837 - val_mae: 0.1363 - val_mse: 0.0837 - learning_rate: 0.1000 - val_custom_mse: 0.7110 - val_custom_mae: 0.5679\n",
            "Epoch 52/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0650 - mae: 0.1377 - mse: 0.0650 - val_loss: 0.0835 - val_mae: 0.1360 - val_mse: 0.0835 - learning_rate: 0.1000 - val_custom_mse: 0.7103 - val_custom_mae: 0.5673\n",
            "Epoch 53/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0649 - mae: 0.1374 - mse: 0.0649 - val_loss: 0.0833 - val_mae: 0.1357 - val_mse: 0.0833 - learning_rate: 0.1000 - val_custom_mse: 0.7099 - val_custom_mae: 0.5670\n",
            "Epoch 54/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0648 - mae: 0.1371 - mse: 0.0648 - val_loss: 0.0831 - val_mae: 0.1351 - val_mse: 0.0831 - learning_rate: 0.1000 - val_custom_mse: 0.7101 - val_custom_mae: 0.5673\n",
            "Epoch 55/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0646 - mae: 0.1368 - mse: 0.0646 - val_loss: 0.0829 - val_mae: 0.1346 - val_mse: 0.0829 - learning_rate: 0.1000 - val_custom_mse: 0.7095 - val_custom_mae: 0.5671\n",
            "Epoch 56/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0645 - mae: 0.1366 - mse: 0.0645 - val_loss: 0.0828 - val_mae: 0.1345 - val_mse: 0.0828 - learning_rate: 0.1000 - val_custom_mse: 0.7096 - val_custom_mae: 0.5668\n",
            "Epoch 57/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0645 - mae: 0.1364 - mse: 0.0645 - val_loss: 0.0825 - val_mae: 0.1339 - val_mse: 0.0825 - learning_rate: 0.1000 - val_custom_mse: 0.7088 - val_custom_mae: 0.5668\n",
            "Epoch 58/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0644 - mae: 0.1362 - mse: 0.0644 - val_loss: 0.0824 - val_mae: 0.1337 - val_mse: 0.0824 - learning_rate: 0.1000 - val_custom_mse: 0.7088 - val_custom_mae: 0.5666\n",
            "Epoch 59/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0643 - mae: 0.1359 - mse: 0.0643 - val_loss: 0.0822 - val_mae: 0.1333 - val_mse: 0.0822 - learning_rate: 0.1000 - val_custom_mse: 0.7084 - val_custom_mae: 0.5668\n",
            "Epoch 60/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0642 - mae: 0.1357 - mse: 0.0642 - val_loss: 0.0822 - val_mae: 0.1333 - val_mse: 0.0822 - learning_rate: 0.1000 - val_custom_mse: 0.7082 - val_custom_mae: 0.5666\n",
            "Epoch 61/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0641 - mae: 0.1354 - mse: 0.0641 - val_loss: 0.0823 - val_mae: 0.1335 - val_mse: 0.0823 - learning_rate: 0.1000 - val_custom_mse: 0.7079 - val_custom_mae: 0.5665\n",
            "Epoch 62/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0640 - mae: 0.1352 - mse: 0.0640 - val_loss: 0.0821 - val_mae: 0.1331 - val_mse: 0.0821 - learning_rate: 0.1000 - val_custom_mse: 0.7082 - val_custom_mae: 0.5667\n",
            "Epoch 63/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0639 - mae: 0.1351 - mse: 0.0639 - val_loss: 0.0820 - val_mae: 0.1329 - val_mse: 0.0820 - learning_rate: 0.1000 - val_custom_mse: 0.7079 - val_custom_mae: 0.5664\n",
            "Epoch 64/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0639 - mae: 0.1350 - mse: 0.0639 - val_loss: 0.0819 - val_mae: 0.1327 - val_mse: 0.0819 - learning_rate: 0.1000 - val_custom_mse: 0.7081 - val_custom_mae: 0.5663\n",
            "Epoch 65/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0637 - mae: 0.1348 - mse: 0.0637 - val_loss: 0.0818 - val_mae: 0.1324 - val_mse: 0.0818 - learning_rate: 0.1000 - val_custom_mse: 0.7074 - val_custom_mae: 0.5662\n",
            "Epoch 66/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0637 - mae: 0.1347 - mse: 0.0637 - val_loss: 0.0817 - val_mae: 0.1322 - val_mse: 0.0817 - learning_rate: 0.1000 - val_custom_mse: 0.7073 - val_custom_mae: 0.5661\n",
            "Epoch 67/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0637 - mae: 0.1346 - mse: 0.0637 - val_loss: 0.0816 - val_mae: 0.1320 - val_mse: 0.0816 - learning_rate: 0.1000 - val_custom_mse: 0.7068 - val_custom_mae: 0.5658\n",
            "Epoch 68/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0636 - mae: 0.1345 - mse: 0.0636 - val_loss: 0.0814 - val_mae: 0.1316 - val_mse: 0.0814 - learning_rate: 0.1000 - val_custom_mse: 0.7071 - val_custom_mae: 0.5663\n",
            "Epoch 69/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0635 - mae: 0.1343 - mse: 0.0635 - val_loss: 0.0814 - val_mae: 0.1314 - val_mse: 0.0814 - learning_rate: 0.1000 - val_custom_mse: 0.7072 - val_custom_mae: 0.5661\n",
            "Epoch 70/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0634 - mae: 0.1342 - mse: 0.0634 - val_loss: 0.0813 - val_mae: 0.1315 - val_mse: 0.0813 - learning_rate: 0.1000 - val_custom_mse: 0.7071 - val_custom_mae: 0.5666\n",
            "Epoch 71/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0634 - mae: 0.1342 - mse: 0.0634 - val_loss: 0.0812 - val_mae: 0.1312 - val_mse: 0.0812 - learning_rate: 0.1000 - val_custom_mse: 0.7069 - val_custom_mae: 0.5661\n",
            "Epoch 72/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0633 - mae: 0.1340 - mse: 0.0633 - val_loss: 0.0812 - val_mae: 0.1312 - val_mse: 0.0812 - learning_rate: 0.1000 - val_custom_mse: 0.7065 - val_custom_mae: 0.5655\n",
            "Epoch 73/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0633 - mae: 0.1339 - mse: 0.0633 - val_loss: 0.0811 - val_mae: 0.1308 - val_mse: 0.0811 - learning_rate: 0.1000 - val_custom_mse: 0.7065 - val_custom_mae: 0.5657\n",
            "Epoch 74/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0632 - mae: 0.1338 - mse: 0.0632 - val_loss: 0.0810 - val_mae: 0.1307 - val_mse: 0.0810 - learning_rate: 0.1000 - val_custom_mse: 0.7065 - val_custom_mae: 0.5662\n",
            "Epoch 75/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0632 - mae: 0.1337 - mse: 0.0632 - val_loss: 0.0810 - val_mae: 0.1308 - val_mse: 0.0810 - learning_rate: 0.1000 - val_custom_mse: 0.7062 - val_custom_mae: 0.5654\n",
            "Epoch 76/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0631 - mae: 0.1335 - mse: 0.0631 - val_loss: 0.0809 - val_mae: 0.1305 - val_mse: 0.0809 - learning_rate: 0.1000 - val_custom_mse: 0.7060 - val_custom_mae: 0.5654\n",
            "Epoch 77/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0630 - mae: 0.1335 - mse: 0.0630 - val_loss: 0.0808 - val_mae: 0.1304 - val_mse: 0.0808 - learning_rate: 0.1000 - val_custom_mse: 0.7055 - val_custom_mae: 0.5655\n",
            "Epoch 78/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0630 - mae: 0.1334 - mse: 0.0630 - val_loss: 0.0808 - val_mae: 0.1304 - val_mse: 0.0808 - learning_rate: 0.1000 - val_custom_mse: 0.7050 - val_custom_mae: 0.5652\n",
            "Epoch 79/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0630 - mae: 0.1334 - mse: 0.0630 - val_loss: 0.0807 - val_mae: 0.1301 - val_mse: 0.0807 - learning_rate: 0.1000 - val_custom_mse: 0.7051 - val_custom_mae: 0.5655\n",
            "Epoch 80/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0629 - mae: 0.1333 - mse: 0.0629 - val_loss: 0.0807 - val_mae: 0.1300 - val_mse: 0.0807 - learning_rate: 0.1000 - val_custom_mse: 0.7051 - val_custom_mae: 0.5654\n",
            "Epoch 81/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0629 - mae: 0.1332 - mse: 0.0629 - val_loss: 0.0807 - val_mae: 0.1300 - val_mse: 0.0807 - learning_rate: 0.1000 - val_custom_mse: 0.7051 - val_custom_mae: 0.5652\n",
            "Epoch 82/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0629 - mae: 0.1332 - mse: 0.0629 - val_loss: 0.0806 - val_mae: 0.1299 - val_mse: 0.0806 - learning_rate: 0.1000 - val_custom_mse: 0.7053 - val_custom_mae: 0.5654\n",
            "Epoch 83/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0629 - mae: 0.1331 - mse: 0.0629 - val_loss: 0.0805 - val_mae: 0.1297 - val_mse: 0.0805 - learning_rate: 0.1000 - val_custom_mse: 0.7048 - val_custom_mae: 0.5654\n",
            "Epoch 84/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0628 - mae: 0.1330 - mse: 0.0628 - val_loss: 0.0805 - val_mae: 0.1297 - val_mse: 0.0805 - learning_rate: 0.1000 - val_custom_mse: 0.7048 - val_custom_mae: 0.5652\n",
            "Epoch 85/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0628 - mae: 0.1330 - mse: 0.0628 - val_loss: 0.0805 - val_mae: 0.1297 - val_mse: 0.0805 - learning_rate: 0.1000 - val_custom_mse: 0.7049 - val_custom_mae: 0.5652\n",
            "Epoch 86/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0627 - mae: 0.1329 - mse: 0.0627 - val_loss: 0.0805 - val_mae: 0.1296 - val_mse: 0.0805 - learning_rate: 0.1000 - val_custom_mse: 0.7049 - val_custom_mae: 0.5655\n",
            "Epoch 87/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0626 - mae: 0.1328 - mse: 0.0626 - val_loss: 0.0806 - val_mae: 0.1302 - val_mse: 0.0806 - learning_rate: 0.1000 - val_custom_mse: 0.7046 - val_custom_mae: 0.5644\n",
            "Epoch 88/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0627 - mae: 0.1329 - mse: 0.0627 - val_loss: 0.0804 - val_mae: 0.1295 - val_mse: 0.0804 - learning_rate: 0.1000 - val_custom_mse: 0.7045 - val_custom_mae: 0.5648\n",
            "Epoch 89/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0627 - mae: 0.1328 - mse: 0.0627 - val_loss: 0.0804 - val_mae: 0.1297 - val_mse: 0.0804 - learning_rate: 0.1000 - val_custom_mse: 0.7040 - val_custom_mae: 0.5645\n",
            "Epoch 90/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0626 - mae: 0.1328 - mse: 0.0626 - val_loss: 0.0803 - val_mae: 0.1294 - val_mse: 0.0803 - learning_rate: 0.1000 - val_custom_mse: 0.7036 - val_custom_mae: 0.5645\n",
            "Epoch 91/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0626 - mae: 0.1327 - mse: 0.0626 - val_loss: 0.0803 - val_mae: 0.1294 - val_mse: 0.0803 - learning_rate: 0.1000 - val_custom_mse: 0.7040 - val_custom_mae: 0.5647\n",
            "Epoch 92/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0625 - mae: 0.1327 - mse: 0.0625 - val_loss: 0.0803 - val_mae: 0.1294 - val_mse: 0.0803 - learning_rate: 0.1000 - val_custom_mse: 0.7038 - val_custom_mae: 0.5653\n",
            "Epoch 93/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0625 - mae: 0.1326 - mse: 0.0625 - val_loss: 0.0804 - val_mae: 0.1295 - val_mse: 0.0804 - learning_rate: 0.1000 - val_custom_mse: 0.7040 - val_custom_mae: 0.5645\n",
            "Epoch 94/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0625 - mae: 0.1326 - mse: 0.0625 - val_loss: 0.0803 - val_mae: 0.1292 - val_mse: 0.0803 - learning_rate: 0.1000 - val_custom_mse: 0.7038 - val_custom_mae: 0.5647\n",
            "Epoch 95/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0625 - mae: 0.1326 - mse: 0.0625 - val_loss: 0.0805 - val_mae: 0.1298 - val_mse: 0.0805 - learning_rate: 0.1000 - val_custom_mse: 0.7038 - val_custom_mae: 0.5641\n",
            "Epoch 96/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0624 - mae: 0.1325 - mse: 0.0624 - val_loss: 0.0802 - val_mae: 0.1291 - val_mse: 0.0802 - learning_rate: 0.1000 - val_custom_mse: 0.7034 - val_custom_mae: 0.5646\n",
            "Epoch 97/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0624 - mae: 0.1325 - mse: 0.0624 - val_loss: 0.0802 - val_mae: 0.1291 - val_mse: 0.0802 - learning_rate: 0.1000 - val_custom_mse: 0.7032 - val_custom_mae: 0.5647\n",
            "Epoch 98/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0624 - mae: 0.1325 - mse: 0.0624 - val_loss: 0.0802 - val_mae: 0.1290 - val_mse: 0.0802 - learning_rate: 0.1000 - val_custom_mse: 0.7034 - val_custom_mae: 0.5644\n",
            "Epoch 99/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0623 - mae: 0.1324 - mse: 0.0623 - val_loss: 0.0802 - val_mae: 0.1291 - val_mse: 0.0802 - learning_rate: 0.1000 - val_custom_mse: 0.7033 - val_custom_mae: 0.5651\n",
            "Epoch 100/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0623 - mae: 0.1324 - mse: 0.0623 - val_loss: 0.0801 - val_mae: 0.1289 - val_mse: 0.0801 - learning_rate: 0.1000 - val_custom_mse: 0.7031 - val_custom_mae: 0.5646\n",
            "Running experiment: horizon=96, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_19', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "236/236 - 6s - 26ms/step - loss: 0.4525 - mae: 0.4766 - mse: 0.4525 - val_loss: 0.7805 - val_mae: 0.6258 - val_mse: 0.7805 - learning_rate: 0.1000 - val_custom_mse: 0.9736 - val_custom_mae: 0.7012\n",
            "Epoch 2/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.4212 - mae: 0.4605 - mse: 0.4212 - val_loss: 0.7408 - val_mae: 0.6110 - val_mse: 0.7408 - learning_rate: 0.1000 - val_custom_mse: 0.9696 - val_custom_mae: 0.6999\n",
            "Epoch 3/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3947 - mae: 0.4454 - mse: 0.3947 - val_loss: 0.6824 - val_mae: 0.5856 - val_mse: 0.6824 - learning_rate: 0.1000 - val_custom_mse: 0.9473 - val_custom_mae: 0.6897\n",
            "Epoch 4/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3634 - mae: 0.4269 - mse: 0.3634 - val_loss: 0.6038 - val_mae: 0.5479 - val_mse: 0.6038 - learning_rate: 0.1000 - val_custom_mse: 0.9088 - val_custom_mae: 0.6716\n",
            "Epoch 5/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3262 - mae: 0.4035 - mse: 0.3262 - val_loss: 0.5221 - val_mae: 0.5074 - val_mse: 0.5221 - learning_rate: 0.1000 - val_custom_mse: 0.8865 - val_custom_mae: 0.6611\n",
            "Epoch 6/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2838 - mae: 0.3749 - mse: 0.2838 - val_loss: 0.4360 - val_mae: 0.4609 - val_mse: 0.4360 - learning_rate: 0.1000 - val_custom_mse: 0.8691 - val_custom_mae: 0.6529\n",
            "Epoch 7/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.2400 - mae: 0.3424 - mse: 0.2400 - val_loss: 0.3495 - val_mae: 0.4073 - val_mse: 0.3495 - learning_rate: 0.1000 - val_custom_mse: 0.8414 - val_custom_mae: 0.6387\n",
            "Epoch 8/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2004 - mae: 0.3097 - mse: 0.2004 - val_loss: 0.2835 - val_mae: 0.3623 - val_mse: 0.2835 - learning_rate: 0.1000 - val_custom_mse: 0.8398 - val_custom_mae: 0.6377\n",
            "Epoch 9/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1692 - mae: 0.2810 - mse: 0.1692 - val_loss: 0.2300 - val_mae: 0.3200 - val_mse: 0.2300 - learning_rate: 0.1000 - val_custom_mse: 0.8162 - val_custom_mae: 0.6250\n",
            "Epoch 10/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1474 - mae: 0.2587 - mse: 0.1474 - val_loss: 0.1961 - val_mae: 0.2896 - val_mse: 0.1961 - learning_rate: 0.1000 - val_custom_mse: 0.8087 - val_custom_mae: 0.6201\n",
            "Epoch 11/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1331 - mae: 0.2431 - mse: 0.1331 - val_loss: 0.1742 - val_mae: 0.2681 - val_mse: 0.1742 - learning_rate: 0.1000 - val_custom_mse: 0.7944 - val_custom_mae: 0.6117\n",
            "Epoch 12/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1240 - mae: 0.2326 - mse: 0.1240 - val_loss: 0.1602 - val_mae: 0.2523 - val_mse: 0.1602 - learning_rate: 0.1000 - val_custom_mse: 0.7906 - val_custom_mae: 0.6089\n",
            "Epoch 13/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1180 - mae: 0.2252 - mse: 0.1180 - val_loss: 0.1508 - val_mae: 0.2416 - val_mse: 0.1508 - learning_rate: 0.1000 - val_custom_mse: 0.7830 - val_custom_mae: 0.6041\n",
            "Epoch 14/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1137 - mae: 0.2198 - mse: 0.1137 - val_loss: 0.1438 - val_mae: 0.2329 - val_mse: 0.1438 - learning_rate: 0.1000 - val_custom_mse: 0.7782 - val_custom_mae: 0.6010\n",
            "Epoch 15/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1103 - mae: 0.2154 - mse: 0.1103 - val_loss: 0.1384 - val_mae: 0.2261 - val_mse: 0.1384 - learning_rate: 0.1000 - val_custom_mse: 0.7727 - val_custom_mae: 0.5977\n",
            "Epoch 16/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1076 - mae: 0.2117 - mse: 0.1076 - val_loss: 0.1335 - val_mae: 0.2196 - val_mse: 0.1335 - learning_rate: 0.1000 - val_custom_mse: 0.7691 - val_custom_mae: 0.5958\n",
            "Epoch 17/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1052 - mae: 0.2084 - mse: 0.1052 - val_loss: 0.1297 - val_mae: 0.2144 - val_mse: 0.1297 - learning_rate: 0.1000 - val_custom_mse: 0.7654 - val_custom_mae: 0.5937\n",
            "Epoch 18/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1032 - mae: 0.2055 - mse: 0.1032 - val_loss: 0.1261 - val_mae: 0.2094 - val_mse: 0.1261 - learning_rate: 0.1000 - val_custom_mse: 0.7611 - val_custom_mae: 0.5917\n",
            "Epoch 19/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1013 - mae: 0.2029 - mse: 0.1013 - val_loss: 0.1231 - val_mae: 0.2051 - val_mse: 0.1231 - learning_rate: 0.1000 - val_custom_mse: 0.7572 - val_custom_mae: 0.5898\n",
            "Epoch 20/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0998 - mae: 0.2005 - mse: 0.0998 - val_loss: 0.1204 - val_mae: 0.2014 - val_mse: 0.1204 - learning_rate: 0.1000 - val_custom_mse: 0.7538 - val_custom_mae: 0.5880\n",
            "Epoch 21/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0983 - mae: 0.1983 - mse: 0.0983 - val_loss: 0.1178 - val_mae: 0.1973 - val_mse: 0.1178 - learning_rate: 0.1000 - val_custom_mse: 0.7508 - val_custom_mae: 0.5871\n",
            "Epoch 22/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0971 - mae: 0.1964 - mse: 0.0971 - val_loss: 0.1162 - val_mae: 0.1956 - val_mse: 0.1162 - learning_rate: 0.1000 - val_custom_mse: 0.7468 - val_custom_mae: 0.5843\n",
            "Epoch 23/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0960 - mae: 0.1947 - mse: 0.0960 - val_loss: 0.1140 - val_mae: 0.1922 - val_mse: 0.1140 - learning_rate: 0.1000 - val_custom_mse: 0.7443 - val_custom_mae: 0.5834\n",
            "Epoch 24/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0949 - mae: 0.1931 - mse: 0.0949 - val_loss: 0.1122 - val_mae: 0.1895 - val_mse: 0.1122 - learning_rate: 0.1000 - val_custom_mse: 0.7418 - val_custom_mae: 0.5823\n",
            "Epoch 25/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0940 - mae: 0.1916 - mse: 0.0940 - val_loss: 0.1105 - val_mae: 0.1869 - val_mse: 0.1105 - learning_rate: 0.1000 - val_custom_mse: 0.7398 - val_custom_mae: 0.5815\n",
            "Epoch 26/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0931 - mae: 0.1903 - mse: 0.0931 - val_loss: 0.1090 - val_mae: 0.1844 - val_mse: 0.1090 - learning_rate: 0.1000 - val_custom_mse: 0.7374 - val_custom_mae: 0.5807\n",
            "Epoch 27/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0924 - mae: 0.1891 - mse: 0.0924 - val_loss: 0.1078 - val_mae: 0.1827 - val_mse: 0.1078 - learning_rate: 0.1000 - val_custom_mse: 0.7352 - val_custom_mae: 0.5793\n",
            "Epoch 28/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0917 - mae: 0.1880 - mse: 0.0917 - val_loss: 0.1066 - val_mae: 0.1805 - val_mse: 0.1066 - learning_rate: 0.1000 - val_custom_mse: 0.7340 - val_custom_mae: 0.5795\n",
            "Epoch 29/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0911 - mae: 0.1870 - mse: 0.0911 - val_loss: 0.1056 - val_mae: 0.1791 - val_mse: 0.1056 - learning_rate: 0.1000 - val_custom_mse: 0.7318 - val_custom_mae: 0.5781\n",
            "Epoch 30/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0905 - mae: 0.1861 - mse: 0.0905 - val_loss: 0.1047 - val_mae: 0.1779 - val_mse: 0.1047 - learning_rate: 0.1000 - val_custom_mse: 0.7301 - val_custom_mae: 0.5769\n",
            "Epoch 31/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0900 - mae: 0.1852 - mse: 0.0900 - val_loss: 0.1037 - val_mae: 0.1762 - val_mse: 0.1037 - learning_rate: 0.1000 - val_custom_mse: 0.7290 - val_custom_mae: 0.5770\n",
            "Epoch 32/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0895 - mae: 0.1845 - mse: 0.0895 - val_loss: 0.1030 - val_mae: 0.1752 - val_mse: 0.1030 - learning_rate: 0.1000 - val_custom_mse: 0.7271 - val_custom_mae: 0.5756\n",
            "Epoch 33/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0890 - mae: 0.1837 - mse: 0.0890 - val_loss: 0.1023 - val_mae: 0.1741 - val_mse: 0.1023 - learning_rate: 0.1000 - val_custom_mse: 0.7260 - val_custom_mae: 0.5750\n",
            "Epoch 34/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0887 - mae: 0.1831 - mse: 0.0887 - val_loss: 0.1017 - val_mae: 0.1731 - val_mse: 0.1017 - learning_rate: 0.1000 - val_custom_mse: 0.7248 - val_custom_mae: 0.5744\n",
            "Epoch 35/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0883 - mae: 0.1825 - mse: 0.0883 - val_loss: 0.1011 - val_mae: 0.1721 - val_mse: 0.1011 - learning_rate: 0.1000 - val_custom_mse: 0.7240 - val_custom_mae: 0.5740\n",
            "Epoch 36/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0880 - mae: 0.1819 - mse: 0.0880 - val_loss: 0.1005 - val_mae: 0.1709 - val_mse: 0.1005 - learning_rate: 0.1000 - val_custom_mse: 0.7232 - val_custom_mae: 0.5742\n",
            "Epoch 37/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0877 - mae: 0.1815 - mse: 0.0877 - val_loss: 0.1000 - val_mae: 0.1703 - val_mse: 0.1000 - learning_rate: 0.1000 - val_custom_mse: 0.7221 - val_custom_mae: 0.5734\n",
            "Epoch 38/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0873 - mae: 0.1809 - mse: 0.0873 - val_loss: 0.0995 - val_mae: 0.1696 - val_mse: 0.0995 - learning_rate: 0.1000 - val_custom_mse: 0.7213 - val_custom_mae: 0.5731\n",
            "Epoch 39/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0870 - mae: 0.1805 - mse: 0.0870 - val_loss: 0.0991 - val_mae: 0.1689 - val_mse: 0.0991 - learning_rate: 0.1000 - val_custom_mse: 0.7204 - val_custom_mae: 0.5725\n",
            "Epoch 40/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0869 - mae: 0.1802 - mse: 0.0869 - val_loss: 0.0987 - val_mae: 0.1683 - val_mse: 0.0987 - learning_rate: 0.1000 - val_custom_mse: 0.7197 - val_custom_mae: 0.5723\n",
            "Epoch 41/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0865 - mae: 0.1797 - mse: 0.0865 - val_loss: 0.0984 - val_mae: 0.1677 - val_mse: 0.0984 - learning_rate: 0.1000 - val_custom_mse: 0.7193 - val_custom_mae: 0.5721\n",
            "Epoch 42/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0863 - mae: 0.1794 - mse: 0.0863 - val_loss: 0.0982 - val_mae: 0.1675 - val_mse: 0.0982 - learning_rate: 0.1000 - val_custom_mse: 0.7179 - val_custom_mae: 0.5711\n",
            "Epoch 43/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0861 - mae: 0.1789 - mse: 0.0861 - val_loss: 0.0984 - val_mae: 0.1677 - val_mse: 0.0984 - learning_rate: 0.1000 - val_custom_mse: 0.7181 - val_custom_mae: 0.5719\n",
            "Epoch 44/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0859 - mae: 0.1785 - mse: 0.0859 - val_loss: 0.0980 - val_mae: 0.1671 - val_mse: 0.0980 - learning_rate: 0.1000 - val_custom_mse: 0.7169 - val_custom_mae: 0.5710\n",
            "Epoch 45/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0857 - mae: 0.1783 - mse: 0.0857 - val_loss: 0.0977 - val_mae: 0.1664 - val_mse: 0.0977 - learning_rate: 0.1000 - val_custom_mse: 0.7168 - val_custom_mae: 0.5711\n",
            "Epoch 46/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0855 - mae: 0.1780 - mse: 0.0855 - val_loss: 0.0974 - val_mae: 0.1660 - val_mse: 0.0974 - learning_rate: 0.1000 - val_custom_mse: 0.7165 - val_custom_mae: 0.5712\n",
            "Epoch 47/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0853 - mae: 0.1778 - mse: 0.0853 - val_loss: 0.0972 - val_mae: 0.1657 - val_mse: 0.0972 - learning_rate: 0.1000 - val_custom_mse: 0.7158 - val_custom_mae: 0.5703\n",
            "Epoch 48/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0852 - mae: 0.1777 - mse: 0.0852 - val_loss: 0.0971 - val_mae: 0.1655 - val_mse: 0.0971 - learning_rate: 0.1000 - val_custom_mse: 0.7157 - val_custom_mae: 0.5699\n",
            "Epoch 49/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0850 - mae: 0.1774 - mse: 0.0850 - val_loss: 0.0968 - val_mae: 0.1650 - val_mse: 0.0968 - learning_rate: 0.1000 - val_custom_mse: 0.7152 - val_custom_mae: 0.5699\n",
            "Epoch 50/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0848 - mae: 0.1773 - mse: 0.0848 - val_loss: 0.0965 - val_mae: 0.1645 - val_mse: 0.0965 - learning_rate: 0.1000 - val_custom_mse: 0.7151 - val_custom_mae: 0.5702\n",
            "Epoch 51/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0847 - mae: 0.1771 - mse: 0.0847 - val_loss: 0.0964 - val_mae: 0.1643 - val_mse: 0.0964 - learning_rate: 0.1000 - val_custom_mse: 0.7145 - val_custom_mae: 0.5699\n",
            "Epoch 52/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0845 - mae: 0.1769 - mse: 0.0845 - val_loss: 0.0963 - val_mae: 0.1640 - val_mse: 0.0963 - learning_rate: 0.1000 - val_custom_mse: 0.7146 - val_custom_mae: 0.5701\n",
            "Epoch 53/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0844 - mae: 0.1767 - mse: 0.0844 - val_loss: 0.0961 - val_mae: 0.1638 - val_mse: 0.0961 - learning_rate: 0.1000 - val_custom_mse: 0.7139 - val_custom_mae: 0.5696\n",
            "Epoch 54/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0843 - mae: 0.1766 - mse: 0.0843 - val_loss: 0.0961 - val_mae: 0.1638 - val_mse: 0.0961 - learning_rate: 0.1000 - val_custom_mse: 0.7136 - val_custom_mae: 0.5695\n",
            "Epoch 55/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0841 - mae: 0.1764 - mse: 0.0841 - val_loss: 0.0960 - val_mae: 0.1635 - val_mse: 0.0960 - learning_rate: 0.1000 - val_custom_mse: 0.7135 - val_custom_mae: 0.5691\n",
            "Epoch 56/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0840 - mae: 0.1763 - mse: 0.0840 - val_loss: 0.0958 - val_mae: 0.1633 - val_mse: 0.0958 - learning_rate: 0.1000 - val_custom_mse: 0.7130 - val_custom_mae: 0.5693\n",
            "Epoch 57/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0839 - mae: 0.1762 - mse: 0.0839 - val_loss: 0.0958 - val_mae: 0.1634 - val_mse: 0.0958 - learning_rate: 0.1000 - val_custom_mse: 0.7130 - val_custom_mae: 0.5687\n",
            "Epoch 58/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0839 - mae: 0.1761 - mse: 0.0839 - val_loss: 0.0958 - val_mae: 0.1634 - val_mse: 0.0958 - learning_rate: 0.1000 - val_custom_mse: 0.7134 - val_custom_mae: 0.5703\n",
            "Epoch 59/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0839 - mae: 0.1761 - mse: 0.0839 - val_loss: 0.0955 - val_mae: 0.1627 - val_mse: 0.0955 - learning_rate: 0.1000 - val_custom_mse: 0.7126 - val_custom_mae: 0.5695\n",
            "Epoch 60/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0836 - mae: 0.1758 - mse: 0.0836 - val_loss: 0.0954 - val_mae: 0.1627 - val_mse: 0.0954 - learning_rate: 0.1000 - val_custom_mse: 0.7122 - val_custom_mae: 0.5687\n",
            "Epoch 61/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0835 - mae: 0.1757 - mse: 0.0835 - val_loss: 0.0953 - val_mae: 0.1625 - val_mse: 0.0953 - learning_rate: 0.1000 - val_custom_mse: 0.7119 - val_custom_mae: 0.5689\n",
            "Epoch 62/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0835 - mae: 0.1756 - mse: 0.0835 - val_loss: 0.0953 - val_mae: 0.1625 - val_mse: 0.0953 - learning_rate: 0.1000 - val_custom_mse: 0.7116 - val_custom_mae: 0.5685\n",
            "Epoch 63/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0834 - mae: 0.1755 - mse: 0.0834 - val_loss: 0.0953 - val_mae: 0.1625 - val_mse: 0.0953 - learning_rate: 0.1000 - val_custom_mse: 0.7115 - val_custom_mae: 0.5683\n",
            "Epoch 64/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0833 - mae: 0.1754 - mse: 0.0833 - val_loss: 0.0952 - val_mae: 0.1624 - val_mse: 0.0952 - learning_rate: 0.1000 - val_custom_mse: 0.7113 - val_custom_mae: 0.5682\n",
            "Epoch 65/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0833 - mae: 0.1753 - mse: 0.0833 - val_loss: 0.0952 - val_mae: 0.1623 - val_mse: 0.0952 - learning_rate: 0.1000 - val_custom_mse: 0.7113 - val_custom_mae: 0.5684\n",
            "Epoch 66/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0831 - mae: 0.1752 - mse: 0.0831 - val_loss: 0.0952 - val_mae: 0.1622 - val_mse: 0.0952 - learning_rate: 0.1000 - val_custom_mse: 0.7116 - val_custom_mae: 0.5690\n",
            "Epoch 67/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0831 - mae: 0.1752 - mse: 0.0831 - val_loss: 0.0951 - val_mae: 0.1621 - val_mse: 0.0951 - learning_rate: 0.1000 - val_custom_mse: 0.7112 - val_custom_mae: 0.5684\n",
            "Epoch 68/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0830 - mae: 0.1751 - mse: 0.0830 - val_loss: 0.0950 - val_mae: 0.1619 - val_mse: 0.0950 - learning_rate: 0.1000 - val_custom_mse: 0.7110 - val_custom_mae: 0.5685\n",
            "Epoch 69/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0829 - mae: 0.1750 - mse: 0.0829 - val_loss: 0.0951 - val_mae: 0.1621 - val_mse: 0.0951 - learning_rate: 0.1000 - val_custom_mse: 0.7106 - val_custom_mae: 0.5677\n",
            "Epoch 70/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0829 - mae: 0.1750 - mse: 0.0829 - val_loss: 0.0950 - val_mae: 0.1620 - val_mse: 0.0950 - learning_rate: 0.1000 - val_custom_mse: 0.7113 - val_custom_mae: 0.5691\n",
            "Epoch 71/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0828 - mae: 0.1749 - mse: 0.0828 - val_loss: 0.0949 - val_mae: 0.1618 - val_mse: 0.0949 - learning_rate: 0.1000 - val_custom_mse: 0.7111 - val_custom_mae: 0.5684\n",
            "Epoch 72/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0827 - mae: 0.1748 - mse: 0.0827 - val_loss: 0.0949 - val_mae: 0.1618 - val_mse: 0.0949 - learning_rate: 0.1000 - val_custom_mse: 0.7102 - val_custom_mae: 0.5678\n",
            "Epoch 73/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0827 - mae: 0.1748 - mse: 0.0827 - val_loss: 0.0948 - val_mae: 0.1616 - val_mse: 0.0948 - learning_rate: 0.1000 - val_custom_mse: 0.7102 - val_custom_mae: 0.5684\n",
            "Epoch 74/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0826 - mae: 0.1746 - mse: 0.0826 - val_loss: 0.0947 - val_mae: 0.1615 - val_mse: 0.0947 - learning_rate: 0.1000 - val_custom_mse: 0.7097 - val_custom_mae: 0.5679\n",
            "Epoch 75/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0826 - mae: 0.1746 - mse: 0.0826 - val_loss: 0.0949 - val_mae: 0.1619 - val_mse: 0.0949 - learning_rate: 0.1000 - val_custom_mse: 0.7101 - val_custom_mae: 0.5689\n",
            "Epoch 76/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0825 - mae: 0.1746 - mse: 0.0825 - val_loss: 0.0948 - val_mae: 0.1617 - val_mse: 0.0948 - learning_rate: 0.1000 - val_custom_mse: 0.7103 - val_custom_mae: 0.5689\n",
            "Epoch 77/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0825 - mae: 0.1746 - mse: 0.0825 - val_loss: 0.0947 - val_mae: 0.1615 - val_mse: 0.0947 - learning_rate: 0.1000 - val_custom_mse: 0.7093 - val_custom_mae: 0.5678\n",
            "Epoch 78/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0823 - mae: 0.1744 - mse: 0.0823 - val_loss: 0.0947 - val_mae: 0.1615 - val_mse: 0.0947 - learning_rate: 0.1000 - val_custom_mse: 0.7097 - val_custom_mae: 0.5682\n",
            "Epoch 79/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0823 - mae: 0.1744 - mse: 0.0823 - val_loss: 0.0948 - val_mae: 0.1617 - val_mse: 0.0948 - learning_rate: 0.1000 - val_custom_mse: 0.7091 - val_custom_mae: 0.5677\n",
            "Epoch 80/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0823 - mae: 0.1743 - mse: 0.0823 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.1000 - val_custom_mse: 0.7088 - val_custom_mae: 0.5674\n",
            "Epoch 81/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0822 - mae: 0.1743 - mse: 0.0822 - val_loss: 0.0948 - val_mae: 0.1615 - val_mse: 0.0948 - learning_rate: 0.1000 - val_custom_mse: 0.7095 - val_custom_mae: 0.5677\n",
            "Epoch 82/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0822 - mae: 0.1743 - mse: 0.0822 - val_loss: 0.0947 - val_mae: 0.1614 - val_mse: 0.0947 - learning_rate: 0.1000 - val_custom_mse: 0.7090 - val_custom_mae: 0.5678\n",
            "Epoch 83/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0821 - mae: 0.1742 - mse: 0.0821 - val_loss: 0.0947 - val_mae: 0.1615 - val_mse: 0.0947 - learning_rate: 0.1000 - val_custom_mse: 0.7087 - val_custom_mae: 0.5672\n",
            "Epoch 84/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0821 - mae: 0.1742 - mse: 0.0821 - val_loss: 0.0948 - val_mae: 0.1617 - val_mse: 0.0948 - learning_rate: 0.1000 - val_custom_mse: 0.7090 - val_custom_mae: 0.5672\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "236/236 - 2s - 8ms/step - loss: 0.0820 - mae: 0.1741 - mse: 0.0820 - val_loss: 0.0946 - val_mae: 0.1615 - val_mse: 0.0946 - learning_rate: 0.1000 - val_custom_mse: 0.7074 - val_custom_mae: 0.5668\n",
            "Epoch 86/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0821 - mae: 0.1741 - mse: 0.0821 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0200 - val_custom_mse: 0.7086 - val_custom_mae: 0.5675\n",
            "Epoch 87/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0820 - mae: 0.1741 - mse: 0.0820 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0200 - val_custom_mse: 0.7086 - val_custom_mae: 0.5674\n",
            "Epoch 88/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0820 - mae: 0.1740 - mse: 0.0820 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0200 - val_custom_mse: 0.7084 - val_custom_mae: 0.5673\n",
            "Epoch 89/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1740 - mse: 0.0819 - val_loss: 0.0946 - val_mae: 0.1614 - val_mse: 0.0946 - learning_rate: 0.0200 - val_custom_mse: 0.7084 - val_custom_mae: 0.5673\n",
            "Epoch 90/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0820 - mae: 0.1741 - mse: 0.0820 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0200 - val_custom_mse: 0.7082 - val_custom_mae: 0.5672\n",
            "Epoch 91/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1740 - mse: 0.0819 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0200 - val_custom_mse: 0.7085 - val_custom_mae: 0.5674\n",
            "Epoch 92/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0820 - mae: 0.1740 - mse: 0.0820 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0200 - val_custom_mse: 0.7084 - val_custom_mae: 0.5673\n",
            "Epoch 93/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1740 - mse: 0.0819 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0200 - val_custom_mse: 0.7084 - val_custom_mae: 0.5673\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1740 - mse: 0.0819 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0200 - val_custom_mse: 0.7084 - val_custom_mae: 0.5672\n",
            "Epoch 95/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1741 - mse: 0.0819 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0040 - val_custom_mse: 0.7081 - val_custom_mae: 0.5672\n",
            "Epoch 96/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1739 - mse: 0.0819 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0040 - val_custom_mse: 0.7082 - val_custom_mae: 0.5672\n",
            "Epoch 97/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0820 - mae: 0.1740 - mse: 0.0820 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0040 - val_custom_mse: 0.7083 - val_custom_mae: 0.5672\n",
            "Epoch 98/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0818 - mae: 0.1739 - mse: 0.0818 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0040 - val_custom_mse: 0.7083 - val_custom_mae: 0.5672\n",
            "Epoch 99/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1740 - mse: 0.0819 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0040 - val_custom_mse: 0.7082 - val_custom_mae: 0.5672\n",
            "Epoch 100/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1740 - mse: 0.0819 - val_loss: 0.0946 - val_mae: 0.1613 - val_mse: 0.0946 - learning_rate: 0.0040 - val_custom_mse: 0.7082 - val_custom_mae: 0.5672\n",
            "Running experiment: horizon=96, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_20', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "236/236 - 6s - 26ms/step - loss: 0.4671 - mae: 0.4857 - mse: 0.4671 - val_loss: 0.8125 - val_mae: 0.6444 - val_mse: 0.8125 - learning_rate: 0.1000 - val_custom_mse: 1.0309 - val_custom_mae: 0.7274\n",
            "Epoch 2/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.4306 - mae: 0.4670 - mse: 0.4306 - val_loss: 0.7544 - val_mae: 0.6193 - val_mse: 0.7544 - learning_rate: 0.1000 - val_custom_mse: 0.9980 - val_custom_mae: 0.7127\n",
            "Epoch 3/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.4009 - mae: 0.4501 - mse: 0.4009 - val_loss: 0.6835 - val_mae: 0.5874 - val_mse: 0.6835 - learning_rate: 0.1000 - val_custom_mse: 0.9592 - val_custom_mae: 0.6951\n",
            "Epoch 4/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3661 - mae: 0.4296 - mse: 0.3661 - val_loss: 0.6083 - val_mae: 0.5528 - val_mse: 0.6083 - learning_rate: 0.1000 - val_custom_mse: 0.9336 - val_custom_mae: 0.6834\n",
            "Epoch 5/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3256 - mae: 0.4040 - mse: 0.3256 - val_loss: 0.5136 - val_mae: 0.5041 - val_mse: 0.5136 - learning_rate: 0.1000 - val_custom_mse: 0.8929 - val_custom_mae: 0.6640\n",
            "Epoch 6/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.2814 - mae: 0.3736 - mse: 0.2814 - val_loss: 0.4292 - val_mae: 0.4581 - val_mse: 0.4292 - learning_rate: 0.1000 - val_custom_mse: 0.8802 - val_custom_mae: 0.6578\n",
            "Epoch 7/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.2382 - mae: 0.3408 - mse: 0.2382 - val_loss: 0.3442 - val_mae: 0.4050 - val_mse: 0.3442 - learning_rate: 0.1000 - val_custom_mse: 0.8532 - val_custom_mae: 0.6441\n",
            "Epoch 8/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.2015 - mae: 0.3100 - mse: 0.2015 - val_loss: 0.2784 - val_mae: 0.3590 - val_mse: 0.2784 - learning_rate: 0.1000 - val_custom_mse: 0.8359 - val_custom_mae: 0.6350\n",
            "Epoch 9/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.1742 - mae: 0.2850 - mse: 0.1742 - val_loss: 0.2295 - val_mae: 0.3205 - val_mse: 0.2295 - learning_rate: 0.1000 - val_custom_mse: 0.8133 - val_custom_mae: 0.6226\n",
            "Epoch 10/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1557 - mae: 0.2667 - mse: 0.1557 - val_loss: 0.1989 - val_mae: 0.2936 - val_mse: 0.1989 - learning_rate: 0.1000 - val_custom_mse: 0.8045 - val_custom_mae: 0.6173\n",
            "Epoch 11/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1442 - mae: 0.2547 - mse: 0.1442 - val_loss: 0.1793 - val_mae: 0.2744 - val_mse: 0.1793 - learning_rate: 0.1000 - val_custom_mse: 0.7975 - val_custom_mae: 0.6129\n",
            "Epoch 12/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1367 - mae: 0.2467 - mse: 0.1367 - val_loss: 0.1662 - val_mae: 0.2609 - val_mse: 0.1662 - learning_rate: 0.1000 - val_custom_mse: 0.7880 - val_custom_mae: 0.6072\n",
            "Epoch 13/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1317 - mae: 0.2412 - mse: 0.1317 - val_loss: 0.1573 - val_mae: 0.2506 - val_mse: 0.1573 - learning_rate: 0.1000 - val_custom_mse: 0.7835 - val_custom_mae: 0.6045\n",
            "Epoch 14/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1281 - mae: 0.2371 - mse: 0.1281 - val_loss: 0.1506 - val_mae: 0.2427 - val_mse: 0.1506 - learning_rate: 0.1000 - val_custom_mse: 0.7787 - val_custom_mae: 0.6018\n",
            "Epoch 15/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1251 - mae: 0.2337 - mse: 0.1251 - val_loss: 0.1456 - val_mae: 0.2370 - val_mse: 0.1456 - learning_rate: 0.1000 - val_custom_mse: 0.7713 - val_custom_mae: 0.5974\n",
            "Epoch 16/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1227 - mae: 0.2307 - mse: 0.1227 - val_loss: 0.1413 - val_mae: 0.2317 - val_mse: 0.1413 - learning_rate: 0.1000 - val_custom_mse: 0.7673 - val_custom_mae: 0.5952\n",
            "Epoch 17/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1206 - mae: 0.2282 - mse: 0.1206 - val_loss: 0.1377 - val_mae: 0.2271 - val_mse: 0.1377 - learning_rate: 0.1000 - val_custom_mse: 0.7638 - val_custom_mae: 0.5932\n",
            "Epoch 18/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1188 - mae: 0.2260 - mse: 0.1188 - val_loss: 0.1344 - val_mae: 0.2225 - val_mse: 0.1344 - learning_rate: 0.1000 - val_custom_mse: 0.7604 - val_custom_mae: 0.5918\n",
            "Epoch 19/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1172 - mae: 0.2240 - mse: 0.1172 - val_loss: 0.1319 - val_mae: 0.2197 - val_mse: 0.1319 - learning_rate: 0.1000 - val_custom_mse: 0.7557 - val_custom_mae: 0.5890\n",
            "Epoch 20/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1158 - mae: 0.2221 - mse: 0.1158 - val_loss: 0.1293 - val_mae: 0.2160 - val_mse: 0.1293 - learning_rate: 0.1000 - val_custom_mse: 0.7533 - val_custom_mae: 0.5881\n",
            "Epoch 21/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1144 - mae: 0.2204 - mse: 0.1144 - val_loss: 0.1271 - val_mae: 0.2129 - val_mse: 0.1271 - learning_rate: 0.1000 - val_custom_mse: 0.7505 - val_custom_mae: 0.5871\n",
            "Epoch 22/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1134 - mae: 0.2190 - mse: 0.1134 - val_loss: 0.1252 - val_mae: 0.2102 - val_mse: 0.1252 - learning_rate: 0.1000 - val_custom_mse: 0.7479 - val_custom_mae: 0.5858\n",
            "Epoch 23/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1123 - mae: 0.2177 - mse: 0.1123 - val_loss: 0.1235 - val_mae: 0.2077 - val_mse: 0.1235 - learning_rate: 0.1000 - val_custom_mse: 0.7457 - val_custom_mae: 0.5851\n",
            "Epoch 24/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1114 - mae: 0.2165 - mse: 0.1114 - val_loss: 0.1220 - val_mae: 0.2057 - val_mse: 0.1220 - learning_rate: 0.1000 - val_custom_mse: 0.7434 - val_custom_mae: 0.5837\n",
            "Epoch 25/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1106 - mae: 0.2154 - mse: 0.1106 - val_loss: 0.1208 - val_mae: 0.2042 - val_mse: 0.1208 - learning_rate: 0.1000 - val_custom_mse: 0.7407 - val_custom_mae: 0.5821\n",
            "Epoch 26/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1099 - mae: 0.2144 - mse: 0.1099 - val_loss: 0.1196 - val_mae: 0.2025 - val_mse: 0.1196 - learning_rate: 0.1000 - val_custom_mse: 0.7386 - val_custom_mae: 0.5811\n",
            "Epoch 27/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1091 - mae: 0.2134 - mse: 0.1091 - val_loss: 0.1186 - val_mae: 0.2011 - val_mse: 0.1186 - learning_rate: 0.1000 - val_custom_mse: 0.7366 - val_custom_mae: 0.5799\n",
            "Epoch 28/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1085 - mae: 0.2127 - mse: 0.1085 - val_loss: 0.1174 - val_mae: 0.1994 - val_mse: 0.1174 - learning_rate: 0.1000 - val_custom_mse: 0.7353 - val_custom_mae: 0.5798\n",
            "Epoch 29/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1079 - mae: 0.2119 - mse: 0.1079 - val_loss: 0.1165 - val_mae: 0.1978 - val_mse: 0.1165 - learning_rate: 0.1000 - val_custom_mse: 0.7346 - val_custom_mae: 0.5799\n",
            "Epoch 30/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1073 - mae: 0.2112 - mse: 0.1073 - val_loss: 0.1157 - val_mae: 0.1966 - val_mse: 0.1157 - learning_rate: 0.1000 - val_custom_mse: 0.7331 - val_custom_mae: 0.5794\n",
            "Epoch 31/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1069 - mae: 0.2106 - mse: 0.1069 - val_loss: 0.1150 - val_mae: 0.1957 - val_mse: 0.1150 - learning_rate: 0.1000 - val_custom_mse: 0.7323 - val_custom_mae: 0.5786\n",
            "Epoch 32/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1064 - mae: 0.2099 - mse: 0.1064 - val_loss: 0.1144 - val_mae: 0.1948 - val_mse: 0.1144 - learning_rate: 0.1000 - val_custom_mse: 0.7306 - val_custom_mae: 0.5776\n",
            "Epoch 33/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1060 - mae: 0.2094 - mse: 0.1060 - val_loss: 0.1138 - val_mae: 0.1938 - val_mse: 0.1138 - learning_rate: 0.1000 - val_custom_mse: 0.7298 - val_custom_mae: 0.5774\n",
            "Epoch 34/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1057 - mae: 0.2090 - mse: 0.1057 - val_loss: 0.1133 - val_mae: 0.1930 - val_mse: 0.1133 - learning_rate: 0.1000 - val_custom_mse: 0.7288 - val_custom_mae: 0.5769\n",
            "Epoch 35/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1051 - mae: 0.2084 - mse: 0.1051 - val_loss: 0.1127 - val_mae: 0.1922 - val_mse: 0.1127 - learning_rate: 0.1000 - val_custom_mse: 0.7277 - val_custom_mae: 0.5767\n",
            "Epoch 36/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1050 - mae: 0.2081 - mse: 0.1050 - val_loss: 0.1123 - val_mae: 0.1916 - val_mse: 0.1123 - learning_rate: 0.1000 - val_custom_mse: 0.7268 - val_custom_mae: 0.5760\n",
            "Epoch 37/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1046 - mae: 0.2077 - mse: 0.1046 - val_loss: 0.1118 - val_mae: 0.1910 - val_mse: 0.1118 - learning_rate: 0.1000 - val_custom_mse: 0.7255 - val_custom_mae: 0.5756\n",
            "Epoch 38/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1044 - mae: 0.2073 - mse: 0.1044 - val_loss: 0.1115 - val_mae: 0.1906 - val_mse: 0.1115 - learning_rate: 0.1000 - val_custom_mse: 0.7249 - val_custom_mae: 0.5752\n",
            "Epoch 39/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1040 - mae: 0.2069 - mse: 0.1040 - val_loss: 0.1112 - val_mae: 0.1900 - val_mse: 0.1112 - learning_rate: 0.1000 - val_custom_mse: 0.7245 - val_custom_mae: 0.5749\n",
            "Epoch 40/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1039 - mae: 0.2067 - mse: 0.1039 - val_loss: 0.1109 - val_mae: 0.1894 - val_mse: 0.1109 - learning_rate: 0.1000 - val_custom_mse: 0.7242 - val_custom_mae: 0.5752\n",
            "Epoch 41/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1036 - mae: 0.2062 - mse: 0.1036 - val_loss: 0.1117 - val_mae: 0.1909 - val_mse: 0.1117 - learning_rate: 0.1000 - val_custom_mse: 0.7236 - val_custom_mae: 0.5750\n",
            "Epoch 42/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1032 - mae: 0.2057 - mse: 0.1032 - val_loss: 0.1112 - val_mae: 0.1900 - val_mse: 0.1112 - learning_rate: 0.1000 - val_custom_mse: 0.7231 - val_custom_mae: 0.5745\n",
            "Epoch 43/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1030 - mae: 0.2055 - mse: 0.1030 - val_loss: 0.1108 - val_mae: 0.1895 - val_mse: 0.1108 - learning_rate: 0.1000 - val_custom_mse: 0.7223 - val_custom_mae: 0.5743\n",
            "Epoch 44/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1028 - mae: 0.2053 - mse: 0.1028 - val_loss: 0.1105 - val_mae: 0.1889 - val_mse: 0.1105 - learning_rate: 0.1000 - val_custom_mse: 0.7219 - val_custom_mae: 0.5737\n",
            "Epoch 45/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1025 - mae: 0.2050 - mse: 0.1025 - val_loss: 0.1102 - val_mae: 0.1884 - val_mse: 0.1102 - learning_rate: 0.1000 - val_custom_mse: 0.7217 - val_custom_mae: 0.5742\n",
            "Epoch 46/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1023 - mae: 0.2048 - mse: 0.1023 - val_loss: 0.1100 - val_mae: 0.1881 - val_mse: 0.1100 - learning_rate: 0.1000 - val_custom_mse: 0.7208 - val_custom_mae: 0.5731\n",
            "Epoch 47/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1021 - mae: 0.2046 - mse: 0.1021 - val_loss: 0.1099 - val_mae: 0.1879 - val_mse: 0.1099 - learning_rate: 0.1000 - val_custom_mse: 0.7209 - val_custom_mae: 0.5735\n",
            "Epoch 48/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1020 - mae: 0.2045 - mse: 0.1020 - val_loss: 0.1097 - val_mae: 0.1878 - val_mse: 0.1097 - learning_rate: 0.1000 - val_custom_mse: 0.7200 - val_custom_mae: 0.5730\n",
            "Epoch 49/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1018 - mae: 0.2043 - mse: 0.1018 - val_loss: 0.1095 - val_mae: 0.1877 - val_mse: 0.1095 - learning_rate: 0.1000 - val_custom_mse: 0.7187 - val_custom_mae: 0.5720\n",
            "Epoch 50/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1016 - mae: 0.2041 - mse: 0.1016 - val_loss: 0.1092 - val_mae: 0.1870 - val_mse: 0.1092 - learning_rate: 0.1000 - val_custom_mse: 0.7193 - val_custom_mae: 0.5729\n",
            "Epoch 51/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1016 - mae: 0.2040 - mse: 0.1016 - val_loss: 0.1090 - val_mae: 0.1867 - val_mse: 0.1090 - learning_rate: 0.1000 - val_custom_mse: 0.7186 - val_custom_mae: 0.5723\n",
            "Epoch 52/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1014 - mae: 0.2038 - mse: 0.1014 - val_loss: 0.1092 - val_mae: 0.1870 - val_mse: 0.1092 - learning_rate: 0.1000 - val_custom_mse: 0.7189 - val_custom_mae: 0.5729\n",
            "Epoch 53/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1013 - mae: 0.2037 - mse: 0.1013 - val_loss: 0.1090 - val_mae: 0.1866 - val_mse: 0.1090 - learning_rate: 0.1000 - val_custom_mse: 0.7189 - val_custom_mae: 0.5727\n",
            "Epoch 54/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1011 - mae: 0.2034 - mse: 0.1011 - val_loss: 0.1088 - val_mae: 0.1864 - val_mse: 0.1088 - learning_rate: 0.1000 - val_custom_mse: 0.7184 - val_custom_mae: 0.5722\n",
            "Epoch 55/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1010 - mae: 0.2034 - mse: 0.1010 - val_loss: 0.1088 - val_mae: 0.1863 - val_mse: 0.1088 - learning_rate: 0.1000 - val_custom_mse: 0.7180 - val_custom_mae: 0.5721\n",
            "Epoch 56/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1009 - mae: 0.2033 - mse: 0.1009 - val_loss: 0.1087 - val_mae: 0.1861 - val_mse: 0.1087 - learning_rate: 0.1000 - val_custom_mse: 0.7182 - val_custom_mae: 0.5725\n",
            "Epoch 57/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1008 - mae: 0.2032 - mse: 0.1008 - val_loss: 0.1086 - val_mae: 0.1861 - val_mse: 0.1086 - learning_rate: 0.1000 - val_custom_mse: 0.7177 - val_custom_mae: 0.5717\n",
            "Epoch 58/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1006 - mae: 0.2030 - mse: 0.1006 - val_loss: 0.1084 - val_mae: 0.1858 - val_mse: 0.1084 - learning_rate: 0.1000 - val_custom_mse: 0.7170 - val_custom_mae: 0.5712\n",
            "Epoch 59/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1005 - mae: 0.2029 - mse: 0.1005 - val_loss: 0.1084 - val_mae: 0.1858 - val_mse: 0.1084 - learning_rate: 0.1000 - val_custom_mse: 0.7168 - val_custom_mae: 0.5717\n",
            "Epoch 60/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1004 - mae: 0.2028 - mse: 0.1004 - val_loss: 0.1083 - val_mae: 0.1856 - val_mse: 0.1083 - learning_rate: 0.1000 - val_custom_mse: 0.7162 - val_custom_mae: 0.5713\n",
            "Epoch 61/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1004 - mae: 0.2027 - mse: 0.1004 - val_loss: 0.1083 - val_mae: 0.1856 - val_mse: 0.1083 - learning_rate: 0.1000 - val_custom_mse: 0.7161 - val_custom_mae: 0.5712\n",
            "Epoch 62/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1003 - mae: 0.2026 - mse: 0.1003 - val_loss: 0.1083 - val_mae: 0.1856 - val_mse: 0.1083 - learning_rate: 0.1000 - val_custom_mse: 0.7160 - val_custom_mae: 0.5720\n",
            "Epoch 63/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1001 - mae: 0.2025 - mse: 0.1001 - val_loss: 0.1081 - val_mae: 0.1855 - val_mse: 0.1081 - learning_rate: 0.1000 - val_custom_mse: 0.7154 - val_custom_mae: 0.5711\n",
            "Epoch 64/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1001 - mae: 0.2024 - mse: 0.1001 - val_loss: 0.1080 - val_mae: 0.1852 - val_mse: 0.1080 - learning_rate: 0.1000 - val_custom_mse: 0.7154 - val_custom_mae: 0.5712\n",
            "Epoch 65/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1000 - mae: 0.2024 - mse: 0.1000 - val_loss: 0.1080 - val_mae: 0.1852 - val_mse: 0.1080 - learning_rate: 0.1000 - val_custom_mse: 0.7152 - val_custom_mae: 0.5710\n",
            "Epoch 66/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0999 - mae: 0.2022 - mse: 0.0999 - val_loss: 0.1080 - val_mae: 0.1853 - val_mse: 0.1080 - learning_rate: 0.1000 - val_custom_mse: 0.7151 - val_custom_mae: 0.5710\n",
            "Epoch 67/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0998 - mae: 0.2021 - mse: 0.0998 - val_loss: 0.1080 - val_mae: 0.1852 - val_mse: 0.1080 - learning_rate: 0.1000 - val_custom_mse: 0.7150 - val_custom_mae: 0.5708\n",
            "Epoch 68/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0996 - mae: 0.2020 - mse: 0.0996 - val_loss: 0.1079 - val_mae: 0.1850 - val_mse: 0.1079 - learning_rate: 0.1000 - val_custom_mse: 0.7147 - val_custom_mae: 0.5708\n",
            "Epoch 69/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0995 - mae: 0.2019 - mse: 0.0995 - val_loss: 0.1079 - val_mae: 0.1851 - val_mse: 0.1079 - learning_rate: 0.1000 - val_custom_mse: 0.7144 - val_custom_mae: 0.5705\n",
            "Epoch 70/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0996 - mae: 0.2019 - mse: 0.0996 - val_loss: 0.1078 - val_mae: 0.1851 - val_mse: 0.1078 - learning_rate: 0.1000 - val_custom_mse: 0.7141 - val_custom_mae: 0.5706\n",
            "Epoch 71/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0995 - mae: 0.2018 - mse: 0.0995 - val_loss: 0.1078 - val_mae: 0.1849 - val_mse: 0.1078 - learning_rate: 0.1000 - val_custom_mse: 0.7138 - val_custom_mae: 0.5703\n",
            "Epoch 72/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0995 - mae: 0.2018 - mse: 0.0995 - val_loss: 0.1078 - val_mae: 0.1850 - val_mse: 0.1078 - learning_rate: 0.1000 - val_custom_mse: 0.7134 - val_custom_mae: 0.5699\n",
            "Epoch 73/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0993 - mae: 0.2016 - mse: 0.0993 - val_loss: 0.1078 - val_mae: 0.1849 - val_mse: 0.1078 - learning_rate: 0.1000 - val_custom_mse: 0.7140 - val_custom_mae: 0.5704\n",
            "Epoch 74/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0993 - mae: 0.2016 - mse: 0.0993 - val_loss: 0.1078 - val_mae: 0.1849 - val_mse: 0.1078 - learning_rate: 0.1000 - val_custom_mse: 0.7137 - val_custom_mae: 0.5705\n",
            "Epoch 75/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0993 - mae: 0.2016 - mse: 0.0993 - val_loss: 0.1077 - val_mae: 0.1848 - val_mse: 0.1077 - learning_rate: 0.1000 - val_custom_mse: 0.7133 - val_custom_mae: 0.5700\n",
            "Epoch 76/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0991 - mae: 0.2015 - mse: 0.0991 - val_loss: 0.1076 - val_mae: 0.1847 - val_mse: 0.1076 - learning_rate: 0.1000 - val_custom_mse: 0.7134 - val_custom_mae: 0.5702\n",
            "Epoch 77/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0990 - mae: 0.2014 - mse: 0.0990 - val_loss: 0.1077 - val_mae: 0.1849 - val_mse: 0.1077 - learning_rate: 0.1000 - val_custom_mse: 0.7133 - val_custom_mae: 0.5706\n",
            "Epoch 78/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0989 - mae: 0.2013 - mse: 0.0989 - val_loss: 0.1077 - val_mae: 0.1847 - val_mse: 0.1077 - learning_rate: 0.1000 - val_custom_mse: 0.7138 - val_custom_mae: 0.5701\n",
            "Epoch 79/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0989 - mae: 0.2012 - mse: 0.0989 - val_loss: 0.1076 - val_mae: 0.1847 - val_mse: 0.1076 - learning_rate: 0.1000 - val_custom_mse: 0.7131 - val_custom_mae: 0.5703\n",
            "Epoch 80/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0989 - mae: 0.2013 - mse: 0.0989 - val_loss: 0.1077 - val_mae: 0.1849 - val_mse: 0.1077 - learning_rate: 0.1000 - val_custom_mse: 0.7136 - val_custom_mae: 0.5704\n",
            "Epoch 81/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0988 - mae: 0.2011 - mse: 0.0988 - val_loss: 0.1077 - val_mae: 0.1849 - val_mse: 0.1077 - learning_rate: 0.1000 - val_custom_mse: 0.7133 - val_custom_mae: 0.5702\n",
            "Epoch 82/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0988 - mae: 0.2011 - mse: 0.0988 - val_loss: 0.1075 - val_mae: 0.1846 - val_mse: 0.1075 - learning_rate: 0.1000 - val_custom_mse: 0.7130 - val_custom_mae: 0.5704\n",
            "Epoch 83/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0987 - mae: 0.2010 - mse: 0.0987 - val_loss: 0.1077 - val_mae: 0.1849 - val_mse: 0.1077 - learning_rate: 0.1000 - val_custom_mse: 0.7127 - val_custom_mae: 0.5694\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "236/236 - 2s - 8ms/step - loss: 0.0986 - mae: 0.2010 - mse: 0.0986 - val_loss: 0.1077 - val_mae: 0.1849 - val_mse: 0.1077 - learning_rate: 0.1000 - val_custom_mse: 0.7129 - val_custom_mae: 0.5698\n",
            "Epoch 85/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0986 - mae: 0.2010 - mse: 0.0986 - val_loss: 0.1075 - val_mae: 0.1844 - val_mse: 0.1075 - learning_rate: 0.0200 - val_custom_mse: 0.7122 - val_custom_mae: 0.5698\n",
            "Epoch 86/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0986 - mae: 0.2009 - mse: 0.0986 - val_loss: 0.1075 - val_mae: 0.1844 - val_mse: 0.1075 - learning_rate: 0.0200 - val_custom_mse: 0.7127 - val_custom_mae: 0.5700\n",
            "Epoch 87/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0985 - mae: 0.2009 - mse: 0.0985 - val_loss: 0.1075 - val_mae: 0.1844 - val_mse: 0.1075 - learning_rate: 0.0200 - val_custom_mse: 0.7129 - val_custom_mae: 0.5700\n",
            "Epoch 88/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0986 - mae: 0.2009 - mse: 0.0986 - val_loss: 0.1074 - val_mae: 0.1844 - val_mse: 0.1074 - learning_rate: 0.0200 - val_custom_mse: 0.7126 - val_custom_mae: 0.5699\n",
            "Epoch 89/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0984 - mae: 0.2008 - mse: 0.0984 - val_loss: 0.1074 - val_mae: 0.1843 - val_mse: 0.1074 - learning_rate: 0.0200 - val_custom_mse: 0.7125 - val_custom_mae: 0.5699\n",
            "Epoch 90/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0985 - mae: 0.2008 - mse: 0.0985 - val_loss: 0.1075 - val_mae: 0.1844 - val_mse: 0.1075 - learning_rate: 0.0200 - val_custom_mse: 0.7124 - val_custom_mae: 0.5698\n",
            "Epoch 91/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0985 - mae: 0.2009 - mse: 0.0985 - val_loss: 0.1074 - val_mae: 0.1844 - val_mse: 0.1074 - learning_rate: 0.0200 - val_custom_mse: 0.7123 - val_custom_mae: 0.5698\n",
            "Epoch 92/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0985 - mae: 0.2008 - mse: 0.0985 - val_loss: 0.1074 - val_mae: 0.1843 - val_mse: 0.1074 - learning_rate: 0.0200 - val_custom_mse: 0.7124 - val_custom_mae: 0.5698\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "236/236 - 2s - 8ms/step - loss: 0.0986 - mae: 0.2009 - mse: 0.0986 - val_loss: 0.1075 - val_mae: 0.1844 - val_mse: 0.1075 - learning_rate: 0.0200 - val_custom_mse: 0.7125 - val_custom_mae: 0.5698\n",
            "Epoch 94/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0985 - mae: 0.2009 - mse: 0.0985 - val_loss: 0.1074 - val_mae: 0.1843 - val_mse: 0.1074 - learning_rate: 0.0040 - val_custom_mse: 0.7124 - val_custom_mae: 0.5698\n",
            "Epoch 95/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0985 - mae: 0.2008 - mse: 0.0985 - val_loss: 0.1074 - val_mae: 0.1843 - val_mse: 0.1074 - learning_rate: 0.0040 - val_custom_mse: 0.7123 - val_custom_mae: 0.5698\n",
            "Epoch 96/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0985 - mae: 0.2008 - mse: 0.0985 - val_loss: 0.1074 - val_mae: 0.1843 - val_mse: 0.1074 - learning_rate: 0.0040 - val_custom_mse: 0.7123 - val_custom_mae: 0.5698\n",
            "Epoch 97/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0984 - mae: 0.2008 - mse: 0.0984 - val_loss: 0.1074 - val_mae: 0.1843 - val_mse: 0.1074 - learning_rate: 0.0040 - val_custom_mse: 0.7123 - val_custom_mae: 0.5698\n",
            "Epoch 98/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0985 - mae: 0.2008 - mse: 0.0985 - val_loss: 0.1074 - val_mae: 0.1843 - val_mse: 0.1074 - learning_rate: 0.0040 - val_custom_mse: 0.7123 - val_custom_mae: 0.5698\n",
            "Epoch 99/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0984 - mae: 0.2008 - mse: 0.0984 - val_loss: 0.1074 - val_mae: 0.1842 - val_mse: 0.1074 - learning_rate: 0.0040 - val_custom_mse: 0.7122 - val_custom_mae: 0.5697\n",
            "Epoch 100/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0985 - mae: 0.2008 - mse: 0.0985 - val_loss: 0.1074 - val_mae: 0.1843 - val_mse: 0.1074 - learning_rate: 0.0040 - val_custom_mse: 0.7125 - val_custom_mae: 0.5698\n",
            "Running experiment: horizon=192, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_21', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "233/233 - 5s - 23ms/step - loss: 0.4818 - mae: 0.4925 - mse: 0.4818 - val_loss: 1.0303 - val_mae: 0.7142 - val_mse: 1.0303 - learning_rate: 0.1000 - val_custom_mse: 1.2783 - val_custom_mae: 0.8013\n",
            "Epoch 2/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.4497 - mae: 0.4773 - mse: 0.4497 - val_loss: 0.9809 - val_mae: 0.6948 - val_mse: 0.9809 - learning_rate: 0.1000 - val_custom_mse: 1.2675 - val_custom_mae: 0.7963\n",
            "Epoch 3/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4206 - mae: 0.4614 - mse: 0.4206 - val_loss: 0.8967 - val_mae: 0.6606 - val_mse: 0.8967 - learning_rate: 0.1000 - val_custom_mse: 1.2144 - val_custom_mae: 0.7730\n",
            "Epoch 4/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3889 - mae: 0.4430 - mse: 0.3889 - val_loss: 0.8004 - val_mae: 0.6202 - val_mse: 0.8004 - learning_rate: 0.1000 - val_custom_mse: 1.1763 - val_custom_mae: 0.7573\n",
            "Epoch 5/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3520 - mae: 0.4195 - mse: 0.3520 - val_loss: 0.6809 - val_mae: 0.5736 - val_mse: 0.6809 - learning_rate: 0.1000 - val_custom_mse: 1.1388 - val_custom_mae: 0.7482\n",
            "Epoch 6/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.3081 - mae: 0.3896 - mse: 0.3081 - val_loss: 0.5636 - val_mae: 0.5174 - val_mse: 0.5636 - learning_rate: 0.1000 - val_custom_mse: 1.1051 - val_custom_mae: 0.7375\n",
            "Epoch 7/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.2602 - mae: 0.3544 - mse: 0.2602 - val_loss: 0.4589 - val_mae: 0.4565 - val_mse: 0.4589 - learning_rate: 0.1000 - val_custom_mse: 1.0691 - val_custom_mae: 0.7219\n",
            "Epoch 8/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2165 - mae: 0.3181 - mse: 0.2165 - val_loss: 0.3756 - val_mae: 0.4007 - val_mse: 0.3756 - learning_rate: 0.1000 - val_custom_mse: 1.0387 - val_custom_mae: 0.7084\n",
            "Epoch 9/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1817 - mae: 0.2854 - mse: 0.1817 - val_loss: 0.3180 - val_mae: 0.3555 - val_mse: 0.3180 - learning_rate: 0.1000 - val_custom_mse: 1.0146 - val_custom_mae: 0.6971\n",
            "Epoch 10/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1571 - mae: 0.2592 - mse: 0.1571 - val_loss: 0.2819 - val_mae: 0.3223 - val_mse: 0.2819 - learning_rate: 0.1000 - val_custom_mse: 0.9965 - val_custom_mae: 0.6885\n",
            "Epoch 11/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1409 - mae: 0.2400 - mse: 0.1409 - val_loss: 0.2632 - val_mae: 0.3032 - val_mse: 0.2632 - learning_rate: 0.1000 - val_custom_mse: 0.9864 - val_custom_mae: 0.6814\n",
            "Epoch 12/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1304 - mae: 0.2263 - mse: 0.1304 - val_loss: 0.2526 - val_mae: 0.2912 - val_mse: 0.2526 - learning_rate: 0.1000 - val_custom_mse: 0.9819 - val_custom_mae: 0.6771\n",
            "Epoch 13/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1232 - mae: 0.2162 - mse: 0.1232 - val_loss: 0.2459 - val_mae: 0.2828 - val_mse: 0.2459 - learning_rate: 0.1000 - val_custom_mse: 0.9800 - val_custom_mae: 0.6741\n",
            "Epoch 14/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1180 - mae: 0.2083 - mse: 0.1180 - val_loss: 0.2390 - val_mae: 0.2734 - val_mse: 0.2390 - learning_rate: 0.1000 - val_custom_mse: 0.9764 - val_custom_mae: 0.6721\n",
            "Epoch 15/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1138 - mae: 0.2016 - mse: 0.1138 - val_loss: 0.2333 - val_mae: 0.2657 - val_mse: 0.2333 - learning_rate: 0.1000 - val_custom_mse: 0.9721 - val_custom_mae: 0.6702\n",
            "Epoch 16/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1104 - mae: 0.1958 - mse: 0.1104 - val_loss: 0.2310 - val_mae: 0.2614 - val_mse: 0.2310 - learning_rate: 0.1000 - val_custom_mse: 0.9754 - val_custom_mae: 0.6695\n",
            "Epoch 17/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1074 - mae: 0.1906 - mse: 0.1074 - val_loss: 0.2268 - val_mae: 0.2550 - val_mse: 0.2268 - learning_rate: 0.1000 - val_custom_mse: 0.9720 - val_custom_mae: 0.6679\n",
            "Epoch 18/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1047 - mae: 0.1858 - mse: 0.1047 - val_loss: 0.2234 - val_mae: 0.2494 - val_mse: 0.2234 - learning_rate: 0.1000 - val_custom_mse: 0.9709 - val_custom_mae: 0.6670\n",
            "Epoch 19/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1024 - mae: 0.1814 - mse: 0.1024 - val_loss: 0.2218 - val_mae: 0.2463 - val_mse: 0.2218 - learning_rate: 0.1000 - val_custom_mse: 0.9732 - val_custom_mae: 0.6665\n",
            "Epoch 20/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1003 - mae: 0.1773 - mse: 0.1003 - val_loss: 0.2194 - val_mae: 0.2417 - val_mse: 0.2194 - learning_rate: 0.1000 - val_custom_mse: 0.9736 - val_custom_mae: 0.6662\n",
            "Epoch 21/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0983 - mae: 0.1734 - mse: 0.0983 - val_loss: 0.2164 - val_mae: 0.2367 - val_mse: 0.2164 - learning_rate: 0.1000 - val_custom_mse: 0.9707 - val_custom_mae: 0.6649\n",
            "Epoch 22/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0965 - mae: 0.1698 - mse: 0.0965 - val_loss: 0.2138 - val_mae: 0.2320 - val_mse: 0.2138 - learning_rate: 0.1000 - val_custom_mse: 0.9685 - val_custom_mae: 0.6643\n",
            "Epoch 23/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0949 - mae: 0.1665 - mse: 0.0949 - val_loss: 0.2122 - val_mae: 0.2291 - val_mse: 0.2122 - learning_rate: 0.1000 - val_custom_mse: 0.9690 - val_custom_mae: 0.6635\n",
            "Epoch 24/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0934 - mae: 0.1634 - mse: 0.0934 - val_loss: 0.2103 - val_mae: 0.2255 - val_mse: 0.2103 - learning_rate: 0.1000 - val_custom_mse: 0.9681 - val_custom_mae: 0.6631\n",
            "Epoch 25/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0920 - mae: 0.1604 - mse: 0.0920 - val_loss: 0.2078 - val_mae: 0.2212 - val_mse: 0.2078 - learning_rate: 0.1000 - val_custom_mse: 0.9646 - val_custom_mae: 0.6621\n",
            "Epoch 26/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0908 - mae: 0.1576 - mse: 0.0908 - val_loss: 0.2073 - val_mae: 0.2192 - val_mse: 0.2073 - learning_rate: 0.1000 - val_custom_mse: 0.9679 - val_custom_mae: 0.6623\n",
            "Epoch 27/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0896 - mae: 0.1550 - mse: 0.0896 - val_loss: 0.2052 - val_mae: 0.2154 - val_mse: 0.2052 - learning_rate: 0.1000 - val_custom_mse: 0.9652 - val_custom_mae: 0.6617\n",
            "Epoch 28/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0885 - mae: 0.1525 - mse: 0.0885 - val_loss: 0.2030 - val_mae: 0.2114 - val_mse: 0.2030 - learning_rate: 0.1000 - val_custom_mse: 0.9614 - val_custom_mae: 0.6608\n",
            "Epoch 29/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0875 - mae: 0.1502 - mse: 0.0875 - val_loss: 0.2024 - val_mae: 0.2097 - val_mse: 0.2024 - learning_rate: 0.1000 - val_custom_mse: 0.9636 - val_custom_mae: 0.6609\n",
            "Epoch 30/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0865 - mae: 0.1479 - mse: 0.0865 - val_loss: 0.2010 - val_mae: 0.2069 - val_mse: 0.2010 - learning_rate: 0.1000 - val_custom_mse: 0.9626 - val_custom_mae: 0.6607\n",
            "Epoch 31/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0856 - mae: 0.1459 - mse: 0.0856 - val_loss: 0.1996 - val_mae: 0.2042 - val_mse: 0.1996 - learning_rate: 0.1000 - val_custom_mse: 0.9606 - val_custom_mae: 0.6601\n",
            "Epoch 32/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0848 - mae: 0.1439 - mse: 0.0848 - val_loss: 0.1984 - val_mae: 0.2020 - val_mse: 0.1984 - learning_rate: 0.1000 - val_custom_mse: 0.9588 - val_custom_mae: 0.6593\n",
            "Epoch 33/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0840 - mae: 0.1420 - mse: 0.0840 - val_loss: 0.1984 - val_mae: 0.2009 - val_mse: 0.1984 - learning_rate: 0.1000 - val_custom_mse: 0.9631 - val_custom_mae: 0.6599\n",
            "Epoch 34/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0833 - mae: 0.1402 - mse: 0.0833 - val_loss: 0.1971 - val_mae: 0.1982 - val_mse: 0.1971 - learning_rate: 0.1000 - val_custom_mse: 0.9610 - val_custom_mae: 0.6595\n",
            "Epoch 35/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0826 - mae: 0.1385 - mse: 0.0826 - val_loss: 0.1960 - val_mae: 0.1959 - val_mse: 0.1960 - learning_rate: 0.1000 - val_custom_mse: 0.9601 - val_custom_mae: 0.6592\n",
            "Epoch 36/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0820 - mae: 0.1369 - mse: 0.0820 - val_loss: 0.1950 - val_mae: 0.1937 - val_mse: 0.1950 - learning_rate: 0.1000 - val_custom_mse: 0.9586 - val_custom_mae: 0.6589\n",
            "Epoch 37/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0814 - mae: 0.1353 - mse: 0.0814 - val_loss: 0.1950 - val_mae: 0.1929 - val_mse: 0.1950 - learning_rate: 0.1000 - val_custom_mse: 0.9619 - val_custom_mae: 0.6592\n",
            "Epoch 38/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0808 - mae: 0.1339 - mse: 0.0808 - val_loss: 0.1940 - val_mae: 0.1906 - val_mse: 0.1940 - learning_rate: 0.1000 - val_custom_mse: 0.9606 - val_custom_mae: 0.6590\n",
            "Epoch 39/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0803 - mae: 0.1324 - mse: 0.0803 - val_loss: 0.1934 - val_mae: 0.1892 - val_mse: 0.1934 - learning_rate: 0.1000 - val_custom_mse: 0.9602 - val_custom_mae: 0.6588\n",
            "Epoch 40/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0797 - mae: 0.1311 - mse: 0.0797 - val_loss: 0.1929 - val_mae: 0.1878 - val_mse: 0.1929 - learning_rate: 0.1000 - val_custom_mse: 0.9609 - val_custom_mae: 0.6587\n",
            "Epoch 41/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0793 - mae: 0.1298 - mse: 0.0793 - val_loss: 0.1919 - val_mae: 0.1857 - val_mse: 0.1919 - learning_rate: 0.1000 - val_custom_mse: 0.9587 - val_custom_mae: 0.6582\n",
            "Epoch 42/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0788 - mae: 0.1286 - mse: 0.0788 - val_loss: 0.1909 - val_mae: 0.1839 - val_mse: 0.1909 - learning_rate: 0.1000 - val_custom_mse: 0.9568 - val_custom_mae: 0.6583\n",
            "Epoch 43/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0784 - mae: 0.1274 - mse: 0.0784 - val_loss: 0.1904 - val_mae: 0.1826 - val_mse: 0.1904 - learning_rate: 0.1000 - val_custom_mse: 0.9570 - val_custom_mae: 0.6583\n",
            "Epoch 44/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0780 - mae: 0.1262 - mse: 0.0780 - val_loss: 0.1897 - val_mae: 0.1813 - val_mse: 0.1897 - learning_rate: 0.1000 - val_custom_mse: 0.9553 - val_custom_mae: 0.6578\n",
            "Epoch 45/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0776 - mae: 0.1252 - mse: 0.0776 - val_loss: 0.1895 - val_mae: 0.1800 - val_mse: 0.1895 - learning_rate: 0.1000 - val_custom_mse: 0.9567 - val_custom_mae: 0.6583\n",
            "Epoch 46/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0772 - mae: 0.1241 - mse: 0.0772 - val_loss: 0.1900 - val_mae: 0.1798 - val_mse: 0.1900 - learning_rate: 0.1000 - val_custom_mse: 0.9610 - val_custom_mae: 0.6590\n",
            "Epoch 47/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0769 - mae: 0.1231 - mse: 0.0769 - val_loss: 0.1889 - val_mae: 0.1778 - val_mse: 0.1889 - learning_rate: 0.1000 - val_custom_mse: 0.9581 - val_custom_mae: 0.6582\n",
            "Epoch 48/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0765 - mae: 0.1221 - mse: 0.0765 - val_loss: 0.1888 - val_mae: 0.1773 - val_mse: 0.1888 - learning_rate: 0.1000 - val_custom_mse: 0.9589 - val_custom_mae: 0.6580\n",
            "Epoch 49/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0762 - mae: 0.1212 - mse: 0.0762 - val_loss: 0.1884 - val_mae: 0.1757 - val_mse: 0.1884 - learning_rate: 0.1000 - val_custom_mse: 0.9594 - val_custom_mae: 0.6583\n",
            "Epoch 50/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0759 - mae: 0.1203 - mse: 0.0759 - val_loss: 0.1872 - val_mae: 0.1744 - val_mse: 0.1872 - learning_rate: 0.1000 - val_custom_mse: 0.9547 - val_custom_mae: 0.6574\n",
            "Epoch 51/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0756 - mae: 0.1194 - mse: 0.0756 - val_loss: 0.1874 - val_mae: 0.1736 - val_mse: 0.1874 - learning_rate: 0.1000 - val_custom_mse: 0.9573 - val_custom_mae: 0.6576\n",
            "Epoch 52/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0754 - mae: 0.1186 - mse: 0.0754 - val_loss: 0.1866 - val_mae: 0.1725 - val_mse: 0.1866 - learning_rate: 0.1000 - val_custom_mse: 0.9549 - val_custom_mae: 0.6572\n",
            "Epoch 53/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0751 - mae: 0.1178 - mse: 0.0751 - val_loss: 0.1861 - val_mae: 0.1718 - val_mse: 0.1861 - learning_rate: 0.1000 - val_custom_mse: 0.9538 - val_custom_mae: 0.6578\n",
            "Epoch 54/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0749 - mae: 0.1171 - mse: 0.0749 - val_loss: 0.1865 - val_mae: 0.1709 - val_mse: 0.1865 - learning_rate: 0.1000 - val_custom_mse: 0.9576 - val_custom_mae: 0.6579\n",
            "Epoch 55/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0746 - mae: 0.1163 - mse: 0.0746 - val_loss: 0.1861 - val_mae: 0.1696 - val_mse: 0.1861 - learning_rate: 0.1000 - val_custom_mse: 0.9568 - val_custom_mae: 0.6578\n",
            "Epoch 56/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0744 - mae: 0.1155 - mse: 0.0744 - val_loss: 0.1859 - val_mae: 0.1685 - val_mse: 0.1859 - learning_rate: 0.1000 - val_custom_mse: 0.9574 - val_custom_mae: 0.6581\n",
            "Epoch 57/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0742 - mae: 0.1148 - mse: 0.0742 - val_loss: 0.1871 - val_mae: 0.1695 - val_mse: 0.1871 - learning_rate: 0.1000 - val_custom_mse: 0.9633 - val_custom_mae: 0.6589\n",
            "Epoch 58/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0740 - mae: 0.1142 - mse: 0.0740 - val_loss: 0.1859 - val_mae: 0.1674 - val_mse: 0.1859 - learning_rate: 0.1000 - val_custom_mse: 0.9597 - val_custom_mae: 0.6582\n",
            "Epoch 59/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0738 - mae: 0.1135 - mse: 0.0738 - val_loss: 0.1850 - val_mae: 0.1664 - val_mse: 0.1850 - learning_rate: 0.1000 - val_custom_mse: 0.9560 - val_custom_mae: 0.6575\n",
            "Epoch 60/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0736 - mae: 0.1130 - mse: 0.0736 - val_loss: 0.1849 - val_mae: 0.1653 - val_mse: 0.1849 - learning_rate: 0.1000 - val_custom_mse: 0.9572 - val_custom_mae: 0.6580\n",
            "Epoch 61/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0735 - mae: 0.1123 - mse: 0.0735 - val_loss: 0.1851 - val_mae: 0.1649 - val_mse: 0.1851 - learning_rate: 0.1000 - val_custom_mse: 0.9587 - val_custom_mae: 0.6582\n",
            "Epoch 62/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0733 - mae: 0.1117 - mse: 0.0733 - val_loss: 0.1848 - val_mae: 0.1639 - val_mse: 0.1848 - learning_rate: 0.1000 - val_custom_mse: 0.9587 - val_custom_mae: 0.6582\n",
            "Epoch 63/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0731 - mae: 0.1112 - mse: 0.0731 - val_loss: 0.1847 - val_mae: 0.1636 - val_mse: 0.1847 - learning_rate: 0.1000 - val_custom_mse: 0.9585 - val_custom_mae: 0.6581\n",
            "Epoch 64/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0730 - mae: 0.1106 - mse: 0.0730 - val_loss: 0.1843 - val_mae: 0.1631 - val_mse: 0.1843 - learning_rate: 0.1000 - val_custom_mse: 0.9576 - val_custom_mae: 0.6579\n",
            "Epoch 65/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0728 - mae: 0.1102 - mse: 0.0728 - val_loss: 0.1844 - val_mae: 0.1621 - val_mse: 0.1844 - learning_rate: 0.1000 - val_custom_mse: 0.9589 - val_custom_mae: 0.6584\n",
            "Epoch 66/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0727 - mae: 0.1096 - mse: 0.0727 - val_loss: 0.1841 - val_mae: 0.1615 - val_mse: 0.1841 - learning_rate: 0.1000 - val_custom_mse: 0.9579 - val_custom_mae: 0.6582\n",
            "Epoch 67/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0726 - mae: 0.1091 - mse: 0.0726 - val_loss: 0.1838 - val_mae: 0.1608 - val_mse: 0.1838 - learning_rate: 0.1000 - val_custom_mse: 0.9575 - val_custom_mae: 0.6580\n",
            "Epoch 68/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0725 - mae: 0.1086 - mse: 0.0725 - val_loss: 0.1843 - val_mae: 0.1607 - val_mse: 0.1843 - learning_rate: 0.1000 - val_custom_mse: 0.9604 - val_custom_mae: 0.6584\n",
            "Epoch 69/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0723 - mae: 0.1081 - mse: 0.0723 - val_loss: 0.1837 - val_mae: 0.1594 - val_mse: 0.1837 - learning_rate: 0.1000 - val_custom_mse: 0.9586 - val_custom_mae: 0.6583\n",
            "Epoch 70/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0722 - mae: 0.1077 - mse: 0.0722 - val_loss: 0.1835 - val_mae: 0.1591 - val_mse: 0.1835 - learning_rate: 0.1000 - val_custom_mse: 0.9583 - val_custom_mae: 0.6583\n",
            "Epoch 71/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0721 - mae: 0.1073 - mse: 0.0721 - val_loss: 0.1841 - val_mae: 0.1592 - val_mse: 0.1841 - learning_rate: 0.1000 - val_custom_mse: 0.9615 - val_custom_mae: 0.6590\n",
            "Epoch 72/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0720 - mae: 0.1069 - mse: 0.0720 - val_loss: 0.1839 - val_mae: 0.1588 - val_mse: 0.1839 - learning_rate: 0.1000 - val_custom_mse: 0.9609 - val_custom_mae: 0.6588\n",
            "Epoch 73/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0719 - mae: 0.1065 - mse: 0.0719 - val_loss: 0.1826 - val_mae: 0.1576 - val_mse: 0.1826 - learning_rate: 0.1000 - val_custom_mse: 0.9548 - val_custom_mae: 0.6577\n",
            "Epoch 74/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0718 - mae: 0.1061 - mse: 0.0718 - val_loss: 0.1837 - val_mae: 0.1581 - val_mse: 0.1837 - learning_rate: 0.1000 - val_custom_mse: 0.9608 - val_custom_mae: 0.6584\n",
            "Epoch 75/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0717 - mae: 0.1057 - mse: 0.0717 - val_loss: 0.1829 - val_mae: 0.1565 - val_mse: 0.1829 - learning_rate: 0.1000 - val_custom_mse: 0.9577 - val_custom_mae: 0.6580\n",
            "Epoch 76/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0716 - mae: 0.1053 - mse: 0.0716 - val_loss: 0.1827 - val_mae: 0.1559 - val_mse: 0.1827 - learning_rate: 0.1000 - val_custom_mse: 0.9572 - val_custom_mae: 0.6581\n",
            "Epoch 77/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0715 - mae: 0.1049 - mse: 0.0715 - val_loss: 0.1824 - val_mae: 0.1554 - val_mse: 0.1824 - learning_rate: 0.1000 - val_custom_mse: 0.9564 - val_custom_mae: 0.6580\n",
            "Epoch 78/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0714 - mae: 0.1046 - mse: 0.0714 - val_loss: 0.1825 - val_mae: 0.1551 - val_mse: 0.1825 - learning_rate: 0.1000 - val_custom_mse: 0.9573 - val_custom_mae: 0.6580\n",
            "Epoch 79/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0714 - mae: 0.1042 - mse: 0.0714 - val_loss: 0.1827 - val_mae: 0.1547 - val_mse: 0.1827 - learning_rate: 0.1000 - val_custom_mse: 0.9590 - val_custom_mae: 0.6583\n",
            "Epoch 80/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0713 - mae: 0.1039 - mse: 0.0713 - val_loss: 0.1827 - val_mae: 0.1547 - val_mse: 0.1827 - learning_rate: 0.1000 - val_custom_mse: 0.9588 - val_custom_mae: 0.6585\n",
            "Epoch 81/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0712 - mae: 0.1036 - mse: 0.0712 - val_loss: 0.1825 - val_mae: 0.1540 - val_mse: 0.1825 - learning_rate: 0.1000 - val_custom_mse: 0.9586 - val_custom_mae: 0.6581\n",
            "Epoch 82/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0711 - mae: 0.1033 - mse: 0.0711 - val_loss: 0.1821 - val_mae: 0.1533 - val_mse: 0.1821 - learning_rate: 0.1000 - val_custom_mse: 0.9571 - val_custom_mae: 0.6581\n",
            "Epoch 83/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0711 - mae: 0.1029 - mse: 0.0711 - val_loss: 0.1816 - val_mae: 0.1543 - val_mse: 0.1816 - learning_rate: 0.1000 - val_custom_mse: 0.9536 - val_custom_mae: 0.6579\n",
            "Epoch 84/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0710 - mae: 0.1028 - mse: 0.0710 - val_loss: 0.1820 - val_mae: 0.1527 - val_mse: 0.1820 - learning_rate: 0.1000 - val_custom_mse: 0.9572 - val_custom_mae: 0.6583\n",
            "Epoch 85/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0709 - mae: 0.1024 - mse: 0.0709 - val_loss: 0.1815 - val_mae: 0.1537 - val_mse: 0.1815 - learning_rate: 0.1000 - val_custom_mse: 0.9538 - val_custom_mae: 0.6581\n",
            "Epoch 86/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0709 - mae: 0.1023 - mse: 0.0709 - val_loss: 0.1818 - val_mae: 0.1520 - val_mse: 0.1818 - learning_rate: 0.1000 - val_custom_mse: 0.9569 - val_custom_mae: 0.6581\n",
            "Epoch 87/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0708 - mae: 0.1018 - mse: 0.0708 - val_loss: 0.1817 - val_mae: 0.1516 - val_mse: 0.1817 - learning_rate: 0.1000 - val_custom_mse: 0.9567 - val_custom_mae: 0.6583\n",
            "Epoch 88/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0708 - mae: 0.1016 - mse: 0.0708 - val_loss: 0.1822 - val_mae: 0.1519 - val_mse: 0.1822 - learning_rate: 0.1000 - val_custom_mse: 0.9594 - val_custom_mae: 0.6585\n",
            "Epoch 89/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0707 - mae: 0.1014 - mse: 0.0707 - val_loss: 0.1824 - val_mae: 0.1513 - val_mse: 0.1824 - learning_rate: 0.1000 - val_custom_mse: 0.9609 - val_custom_mae: 0.6589\n",
            "Epoch 90/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0707 - mae: 0.1011 - mse: 0.0707 - val_loss: 0.1821 - val_mae: 0.1515 - val_mse: 0.1821 - learning_rate: 0.1000 - val_custom_mse: 0.9590 - val_custom_mae: 0.6582\n",
            "Epoch 91/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0706 - mae: 0.1009 - mse: 0.0706 - val_loss: 0.1814 - val_mae: 0.1506 - val_mse: 0.1814 - learning_rate: 0.1000 - val_custom_mse: 0.9560 - val_custom_mae: 0.6581\n",
            "Epoch 92/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0705 - mae: 0.1006 - mse: 0.0705 - val_loss: 0.1812 - val_mae: 0.1498 - val_mse: 0.1812 - learning_rate: 0.1000 - val_custom_mse: 0.9557 - val_custom_mae: 0.6578\n",
            "Epoch 93/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0705 - mae: 0.1005 - mse: 0.0705 - val_loss: 0.1818 - val_mae: 0.1499 - val_mse: 0.1818 - learning_rate: 0.1000 - val_custom_mse: 0.9588 - val_custom_mae: 0.6585\n",
            "Epoch 94/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0704 - mae: 0.1002 - mse: 0.0704 - val_loss: 0.1821 - val_mae: 0.1499 - val_mse: 0.1821 - learning_rate: 0.1000 - val_custom_mse: 0.9606 - val_custom_mae: 0.6590\n",
            "Epoch 95/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0704 - mae: 0.0999 - mse: 0.0704 - val_loss: 0.1811 - val_mae: 0.1494 - val_mse: 0.1811 - learning_rate: 0.1000 - val_custom_mse: 0.9556 - val_custom_mae: 0.6579\n",
            "Epoch 96/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0704 - mae: 0.0998 - mse: 0.0704 - val_loss: 0.1821 - val_mae: 0.1497 - val_mse: 0.1821 - learning_rate: 0.1000 - val_custom_mse: 0.9610 - val_custom_mae: 0.6588\n",
            "Epoch 97/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0703 - mae: 0.0995 - mse: 0.0703 - val_loss: 0.1821 - val_mae: 0.1494 - val_mse: 0.1821 - learning_rate: 0.1000 - val_custom_mse: 0.9612 - val_custom_mae: 0.6589\n",
            "Epoch 98/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0703 - mae: 0.0994 - mse: 0.0703 - val_loss: 0.1814 - val_mae: 0.1483 - val_mse: 0.1814 - learning_rate: 0.1000 - val_custom_mse: 0.9581 - val_custom_mae: 0.6583\n",
            "Epoch 99/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0702 - mae: 0.0992 - mse: 0.0702 - val_loss: 0.1807 - val_mae: 0.1489 - val_mse: 0.1807 - learning_rate: 0.1000 - val_custom_mse: 0.9536 - val_custom_mae: 0.6577\n",
            "Epoch 100/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0702 - mae: 0.0990 - mse: 0.0702 - val_loss: 0.1815 - val_mae: 0.1477 - val_mse: 0.1815 - learning_rate: 0.1000 - val_custom_mse: 0.9589 - val_custom_mae: 0.6586\n",
            "Running experiment: horizon=192, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_22', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "233/233 - 6s - 26ms/step - loss: 0.4921 - mae: 0.4988 - mse: 0.4921 - val_loss: 1.0212 - val_mae: 0.7165 - val_mse: 1.0212 - learning_rate: 0.1000 - val_custom_mse: 1.2845 - val_custom_mae: 0.8085\n",
            "Epoch 2/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.4583 - mae: 0.4823 - mse: 0.4583 - val_loss: 0.9467 - val_mae: 0.6896 - val_mse: 0.9467 - learning_rate: 0.1000 - val_custom_mse: 1.2486 - val_custom_mae: 0.7960\n",
            "Epoch 3/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.4279 - mae: 0.4655 - mse: 0.4279 - val_loss: 0.8619 - val_mae: 0.6559 - val_mse: 0.8619 - learning_rate: 0.1000 - val_custom_mse: 1.2082 - val_custom_mae: 0.7799\n",
            "Epoch 4/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3923 - mae: 0.4447 - mse: 0.3923 - val_loss: 0.7646 - val_mae: 0.6163 - val_mse: 0.7646 - learning_rate: 0.1000 - val_custom_mse: 1.1721 - val_custom_mae: 0.7667\n",
            "Epoch 5/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3506 - mae: 0.4188 - mse: 0.3506 - val_loss: 0.6579 - val_mae: 0.5637 - val_mse: 0.6579 - learning_rate: 0.1000 - val_custom_mse: 1.1233 - val_custom_mae: 0.7432\n",
            "Epoch 6/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3041 - mae: 0.3874 - mse: 0.3041 - val_loss: 0.5474 - val_mae: 0.5077 - val_mse: 0.5474 - learning_rate: 0.1000 - val_custom_mse: 1.0867 - val_custom_mae: 0.7282\n",
            "Epoch 7/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2577 - mae: 0.3527 - mse: 0.2577 - val_loss: 0.4458 - val_mae: 0.4497 - val_mse: 0.4458 - learning_rate: 0.1000 - val_custom_mse: 1.0552 - val_custom_mae: 0.7165\n",
            "Epoch 8/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2172 - mae: 0.3187 - mse: 0.2172 - val_loss: 0.3687 - val_mae: 0.3973 - val_mse: 0.3687 - learning_rate: 0.1000 - val_custom_mse: 1.0273 - val_custom_mae: 0.7039\n",
            "Epoch 9/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1863 - mae: 0.2896 - mse: 0.1863 - val_loss: 0.3169 - val_mae: 0.3562 - val_mse: 0.3169 - learning_rate: 0.1000 - val_custom_mse: 1.0064 - val_custom_mae: 0.6933\n",
            "Epoch 10/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1652 - mae: 0.2675 - mse: 0.1652 - val_loss: 0.2849 - val_mae: 0.3268 - val_mse: 0.2849 - learning_rate: 0.1000 - val_custom_mse: 0.9923 - val_custom_mae: 0.6858\n",
            "Epoch 11/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1517 - mae: 0.2521 - mse: 0.1517 - val_loss: 0.2673 - val_mae: 0.3087 - val_mse: 0.2673 - learning_rate: 0.1000 - val_custom_mse: 0.9856 - val_custom_mae: 0.6803\n",
            "Epoch 12/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1429 - mae: 0.2414 - mse: 0.1429 - val_loss: 0.2560 - val_mae: 0.2957 - val_mse: 0.2560 - learning_rate: 0.1000 - val_custom_mse: 0.9815 - val_custom_mae: 0.6769\n",
            "Epoch 13/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1370 - mae: 0.2337 - mse: 0.1370 - val_loss: 0.2484 - val_mae: 0.2863 - val_mse: 0.2484 - learning_rate: 0.1000 - val_custom_mse: 0.9788 - val_custom_mae: 0.6740\n",
            "Epoch 14/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1325 - mae: 0.2276 - mse: 0.1325 - val_loss: 0.2428 - val_mae: 0.2789 - val_mse: 0.2428 - learning_rate: 0.1000 - val_custom_mse: 0.9779 - val_custom_mae: 0.6723\n",
            "Epoch 15/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1291 - mae: 0.2226 - mse: 0.1291 - val_loss: 0.2362 - val_mae: 0.2706 - val_mse: 0.2362 - learning_rate: 0.1000 - val_custom_mse: 0.9707 - val_custom_mae: 0.6701\n",
            "Epoch 16/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.1262 - mae: 0.2182 - mse: 0.1262 - val_loss: 0.2343 - val_mae: 0.2667 - val_mse: 0.2343 - learning_rate: 0.1000 - val_custom_mse: 0.9762 - val_custom_mae: 0.6698\n",
            "Epoch 17/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1237 - mae: 0.2142 - mse: 0.1237 - val_loss: 0.2298 - val_mae: 0.2604 - val_mse: 0.2298 - learning_rate: 0.1000 - val_custom_mse: 0.9722 - val_custom_mae: 0.6681\n",
            "Epoch 18/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1214 - mae: 0.2106 - mse: 0.1214 - val_loss: 0.2262 - val_mae: 0.2552 - val_mse: 0.2262 - learning_rate: 0.1000 - val_custom_mse: 0.9697 - val_custom_mae: 0.6671\n",
            "Epoch 19/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1195 - mae: 0.2074 - mse: 0.1195 - val_loss: 0.2238 - val_mae: 0.2510 - val_mse: 0.2238 - learning_rate: 0.1000 - val_custom_mse: 0.9701 - val_custom_mae: 0.6664\n",
            "Epoch 20/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1178 - mae: 0.2044 - mse: 0.1178 - val_loss: 0.2207 - val_mae: 0.2465 - val_mse: 0.2207 - learning_rate: 0.1000 - val_custom_mse: 0.9672 - val_custom_mae: 0.6653\n",
            "Epoch 21/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1162 - mae: 0.2017 - mse: 0.1162 - val_loss: 0.2203 - val_mae: 0.2444 - val_mse: 0.2203 - learning_rate: 0.1000 - val_custom_mse: 0.9724 - val_custom_mae: 0.6656\n",
            "Epoch 22/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1149 - mae: 0.1992 - mse: 0.1149 - val_loss: 0.2164 - val_mae: 0.2393 - val_mse: 0.2164 - learning_rate: 0.1000 - val_custom_mse: 0.9654 - val_custom_mae: 0.6640\n",
            "Epoch 23/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1136 - mae: 0.1968 - mse: 0.1136 - val_loss: 0.2155 - val_mae: 0.2369 - val_mse: 0.2155 - learning_rate: 0.1000 - val_custom_mse: 0.9688 - val_custom_mae: 0.6643\n",
            "Epoch 24/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1125 - mae: 0.1947 - mse: 0.1125 - val_loss: 0.2137 - val_mae: 0.2337 - val_mse: 0.2137 - learning_rate: 0.1000 - val_custom_mse: 0.9676 - val_custom_mae: 0.6637\n",
            "Epoch 25/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1114 - mae: 0.1927 - mse: 0.1114 - val_loss: 0.2109 - val_mae: 0.2302 - val_mse: 0.2109 - learning_rate: 0.1000 - val_custom_mse: 0.9620 - val_custom_mae: 0.6626\n",
            "Epoch 26/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1105 - mae: 0.1909 - mse: 0.1105 - val_loss: 0.2099 - val_mae: 0.2279 - val_mse: 0.2099 - learning_rate: 0.1000 - val_custom_mse: 0.9633 - val_custom_mae: 0.6625\n",
            "Epoch 27/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1096 - mae: 0.1892 - mse: 0.1096 - val_loss: 0.2088 - val_mae: 0.2256 - val_mse: 0.2088 - learning_rate: 0.1000 - val_custom_mse: 0.9635 - val_custom_mae: 0.6621\n",
            "Epoch 28/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1089 - mae: 0.1876 - mse: 0.1089 - val_loss: 0.2078 - val_mae: 0.2235 - val_mse: 0.2078 - learning_rate: 0.1000 - val_custom_mse: 0.9639 - val_custom_mae: 0.6618\n",
            "Epoch 29/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.1081 - mae: 0.1861 - mse: 0.1081 - val_loss: 0.2071 - val_mae: 0.2219 - val_mse: 0.2071 - learning_rate: 0.1000 - val_custom_mse: 0.9652 - val_custom_mae: 0.6618\n",
            "Epoch 30/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1075 - mae: 0.1848 - mse: 0.1075 - val_loss: 0.2054 - val_mae: 0.2195 - val_mse: 0.2054 - learning_rate: 0.1000 - val_custom_mse: 0.9621 - val_custom_mae: 0.6609\n",
            "Epoch 31/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1070 - mae: 0.1836 - mse: 0.1070 - val_loss: 0.2040 - val_mae: 0.2174 - val_mse: 0.2040 - learning_rate: 0.1000 - val_custom_mse: 0.9599 - val_custom_mae: 0.6607\n",
            "Epoch 32/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1064 - mae: 0.1824 - mse: 0.1064 - val_loss: 0.2034 - val_mae: 0.2160 - val_mse: 0.2034 - learning_rate: 0.1000 - val_custom_mse: 0.9607 - val_custom_mae: 0.6607\n",
            "Epoch 33/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1058 - mae: 0.1813 - mse: 0.1058 - val_loss: 0.2029 - val_mae: 0.2145 - val_mse: 0.2029 - learning_rate: 0.1000 - val_custom_mse: 0.9615 - val_custom_mae: 0.6605\n",
            "Epoch 34/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1054 - mae: 0.1802 - mse: 0.1054 - val_loss: 0.2017 - val_mae: 0.2128 - val_mse: 0.2017 - learning_rate: 0.1000 - val_custom_mse: 0.9594 - val_custom_mae: 0.6601\n",
            "Epoch 35/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1050 - mae: 0.1793 - mse: 0.1050 - val_loss: 0.2006 - val_mae: 0.2114 - val_mse: 0.2006 - learning_rate: 0.1000 - val_custom_mse: 0.9568 - val_custom_mae: 0.6597\n",
            "Epoch 36/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1046 - mae: 0.1784 - mse: 0.1046 - val_loss: 0.2000 - val_mae: 0.2102 - val_mse: 0.2000 - learning_rate: 0.1000 - val_custom_mse: 0.9567 - val_custom_mae: 0.6596\n",
            "Epoch 37/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1042 - mae: 0.1776 - mse: 0.1042 - val_loss: 0.1996 - val_mae: 0.2090 - val_mse: 0.1996 - learning_rate: 0.1000 - val_custom_mse: 0.9574 - val_custom_mae: 0.6595\n",
            "Epoch 38/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1039 - mae: 0.1768 - mse: 0.1039 - val_loss: 0.1992 - val_mae: 0.2079 - val_mse: 0.1992 - learning_rate: 0.1000 - val_custom_mse: 0.9582 - val_custom_mae: 0.6596\n",
            "Epoch 39/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1035 - mae: 0.1760 - mse: 0.1035 - val_loss: 0.1986 - val_mae: 0.2070 - val_mse: 0.1986 - learning_rate: 0.1000 - val_custom_mse: 0.9573 - val_custom_mae: 0.6593\n",
            "Epoch 40/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1032 - mae: 0.1753 - mse: 0.1032 - val_loss: 0.1983 - val_mae: 0.2060 - val_mse: 0.1983 - learning_rate: 0.1000 - val_custom_mse: 0.9586 - val_custom_mae: 0.6595\n",
            "Epoch 41/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1029 - mae: 0.1747 - mse: 0.1029 - val_loss: 0.1975 - val_mae: 0.2051 - val_mse: 0.1975 - learning_rate: 0.1000 - val_custom_mse: 0.9562 - val_custom_mae: 0.6590\n",
            "Epoch 42/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.1026 - mae: 0.1741 - mse: 0.1026 - val_loss: 0.1977 - val_mae: 0.2044 - val_mse: 0.1977 - learning_rate: 0.1000 - val_custom_mse: 0.9593 - val_custom_mae: 0.6593\n",
            "Epoch 43/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1024 - mae: 0.1735 - mse: 0.1024 - val_loss: 0.1977 - val_mae: 0.2039 - val_mse: 0.1977 - learning_rate: 0.1000 - val_custom_mse: 0.9606 - val_custom_mae: 0.6595\n",
            "Epoch 44/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1022 - mae: 0.1730 - mse: 0.1022 - val_loss: 0.1975 - val_mae: 0.2031 - val_mse: 0.1975 - learning_rate: 0.1000 - val_custom_mse: 0.9616 - val_custom_mae: 0.6598\n",
            "Epoch 45/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1020 - mae: 0.1725 - mse: 0.1020 - val_loss: 0.1969 - val_mae: 0.2022 - val_mse: 0.1969 - learning_rate: 0.1000 - val_custom_mse: 0.9605 - val_custom_mae: 0.6595\n",
            "Epoch 46/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1018 - mae: 0.1720 - mse: 0.1018 - val_loss: 0.1964 - val_mae: 0.2015 - val_mse: 0.1964 - learning_rate: 0.1000 - val_custom_mse: 0.9593 - val_custom_mae: 0.6592\n",
            "Epoch 47/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1016 - mae: 0.1715 - mse: 0.1016 - val_loss: 0.1961 - val_mae: 0.2008 - val_mse: 0.1961 - learning_rate: 0.1000 - val_custom_mse: 0.9592 - val_custom_mae: 0.6591\n",
            "Epoch 48/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1014 - mae: 0.1711 - mse: 0.1014 - val_loss: 0.1947 - val_mae: 0.2006 - val_mse: 0.1947 - learning_rate: 0.1000 - val_custom_mse: 0.9521 - val_custom_mae: 0.6578\n",
            "Epoch 49/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1012 - mae: 0.1708 - mse: 0.1012 - val_loss: 0.1959 - val_mae: 0.1999 - val_mse: 0.1959 - learning_rate: 0.1000 - val_custom_mse: 0.9603 - val_custom_mae: 0.6594\n",
            "Epoch 50/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1011 - mae: 0.1704 - mse: 0.1011 - val_loss: 0.1950 - val_mae: 0.1991 - val_mse: 0.1950 - learning_rate: 0.1000 - val_custom_mse: 0.9573 - val_custom_mae: 0.6591\n",
            "Epoch 51/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1009 - mae: 0.1700 - mse: 0.1009 - val_loss: 0.1950 - val_mae: 0.1988 - val_mse: 0.1950 - learning_rate: 0.1000 - val_custom_mse: 0.9584 - val_custom_mae: 0.6593\n",
            "Epoch 52/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1008 - mae: 0.1698 - mse: 0.1008 - val_loss: 0.1949 - val_mae: 0.1983 - val_mse: 0.1949 - learning_rate: 0.1000 - val_custom_mse: 0.9585 - val_custom_mae: 0.6594\n",
            "Epoch 53/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1007 - mae: 0.1694 - mse: 0.1007 - val_loss: 0.1948 - val_mae: 0.1979 - val_mse: 0.1948 - learning_rate: 0.1000 - val_custom_mse: 0.9594 - val_custom_mae: 0.6594\n",
            "Epoch 54/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1005 - mae: 0.1689 - mse: 0.1005 - val_loss: 0.1946 - val_mae: 0.1981 - val_mse: 0.1946 - learning_rate: 0.1000 - val_custom_mse: 0.9577 - val_custom_mae: 0.6590\n",
            "Epoch 55/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1004 - mae: 0.1686 - mse: 0.1004 - val_loss: 0.1946 - val_mae: 0.1978 - val_mse: 0.1946 - learning_rate: 0.1000 - val_custom_mse: 0.9585 - val_custom_mae: 0.6591\n",
            "Epoch 56/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1003 - mae: 0.1685 - mse: 0.1003 - val_loss: 0.1943 - val_mae: 0.1972 - val_mse: 0.1943 - learning_rate: 0.1000 - val_custom_mse: 0.9576 - val_custom_mae: 0.6587\n",
            "Epoch 57/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1001 - mae: 0.1683 - mse: 0.1001 - val_loss: 0.1941 - val_mae: 0.1970 - val_mse: 0.1941 - learning_rate: 0.1000 - val_custom_mse: 0.9571 - val_custom_mae: 0.6588\n",
            "Epoch 58/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1000 - mae: 0.1680 - mse: 0.1000 - val_loss: 0.1934 - val_mae: 0.1969 - val_mse: 0.1934 - learning_rate: 0.1000 - val_custom_mse: 0.9541 - val_custom_mae: 0.6586\n",
            "Epoch 59/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0999 - mae: 0.1678 - mse: 0.0999 - val_loss: 0.1937 - val_mae: 0.1963 - val_mse: 0.1937 - learning_rate: 0.1000 - val_custom_mse: 0.9569 - val_custom_mae: 0.6588\n",
            "Epoch 60/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0999 - mae: 0.1676 - mse: 0.0999 - val_loss: 0.1934 - val_mae: 0.1960 - val_mse: 0.1934 - learning_rate: 0.1000 - val_custom_mse: 0.9560 - val_custom_mae: 0.6587\n",
            "Epoch 61/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0998 - mae: 0.1675 - mse: 0.0998 - val_loss: 0.1931 - val_mae: 0.1960 - val_mse: 0.1931 - learning_rate: 0.1000 - val_custom_mse: 0.9542 - val_custom_mae: 0.6584\n",
            "Epoch 62/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0997 - mae: 0.1673 - mse: 0.0997 - val_loss: 0.1939 - val_mae: 0.1957 - val_mse: 0.1939 - learning_rate: 0.1000 - val_custom_mse: 0.9592 - val_custom_mae: 0.6590\n",
            "Epoch 63/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0996 - mae: 0.1671 - mse: 0.0996 - val_loss: 0.1936 - val_mae: 0.1953 - val_mse: 0.1936 - learning_rate: 0.1000 - val_custom_mse: 0.9581 - val_custom_mae: 0.6587\n",
            "Epoch 64/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0995 - mae: 0.1669 - mse: 0.0995 - val_loss: 0.1934 - val_mae: 0.1951 - val_mse: 0.1934 - learning_rate: 0.1000 - val_custom_mse: 0.9578 - val_custom_mae: 0.6590\n",
            "Epoch 65/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0995 - mae: 0.1668 - mse: 0.0995 - val_loss: 0.1937 - val_mae: 0.1951 - val_mse: 0.1937 - learning_rate: 0.1000 - val_custom_mse: 0.9596 - val_custom_mae: 0.6593\n",
            "Epoch 66/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0994 - mae: 0.1666 - mse: 0.0994 - val_loss: 0.1935 - val_mae: 0.1948 - val_mse: 0.1935 - learning_rate: 0.1000 - val_custom_mse: 0.9593 - val_custom_mae: 0.6593\n",
            "Epoch 67/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0993 - mae: 0.1665 - mse: 0.0993 - val_loss: 0.1929 - val_mae: 0.1946 - val_mse: 0.1929 - learning_rate: 0.1000 - val_custom_mse: 0.9561 - val_custom_mae: 0.6587\n",
            "Epoch 68/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0992 - mae: 0.1663 - mse: 0.0992 - val_loss: 0.1939 - val_mae: 0.1949 - val_mse: 0.1939 - learning_rate: 0.1000 - val_custom_mse: 0.9611 - val_custom_mae: 0.6594\n",
            "Epoch 69/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0992 - mae: 0.1663 - mse: 0.0992 - val_loss: 0.1936 - val_mae: 0.1947 - val_mse: 0.1936 - learning_rate: 0.1000 - val_custom_mse: 0.9600 - val_custom_mae: 0.6590\n",
            "Epoch 70/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0991 - mae: 0.1661 - mse: 0.0991 - val_loss: 0.1934 - val_mae: 0.1943 - val_mse: 0.1934 - learning_rate: 0.1000 - val_custom_mse: 0.9594 - val_custom_mae: 0.6590\n",
            "Epoch 71/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0991 - mae: 0.1660 - mse: 0.0991 - val_loss: 0.1934 - val_mae: 0.1943 - val_mse: 0.1934 - learning_rate: 0.1000 - val_custom_mse: 0.9598 - val_custom_mae: 0.6589\n",
            "Epoch 72/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0990 - mae: 0.1659 - mse: 0.0990 - val_loss: 0.1929 - val_mae: 0.1939 - val_mse: 0.1929 - learning_rate: 0.1000 - val_custom_mse: 0.9575 - val_custom_mae: 0.6589\n",
            "Epoch 73/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0990 - mae: 0.1658 - mse: 0.0990 - val_loss: 0.1926 - val_mae: 0.1937 - val_mse: 0.1926 - learning_rate: 0.1000 - val_custom_mse: 0.9563 - val_custom_mae: 0.6587\n",
            "Epoch 74/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0989 - mae: 0.1658 - mse: 0.0989 - val_loss: 0.1929 - val_mae: 0.1939 - val_mse: 0.1929 - learning_rate: 0.1000 - val_custom_mse: 0.9576 - val_custom_mae: 0.6592\n",
            "Epoch 75/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.0989 - mae: 0.1656 - mse: 0.0989 - val_loss: 0.1933 - val_mae: 0.1940 - val_mse: 0.1933 - learning_rate: 0.1000 - val_custom_mse: 0.9598 - val_custom_mae: 0.6592\n",
            "Epoch 76/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0988 - mae: 0.1656 - mse: 0.0988 - val_loss: 0.1931 - val_mae: 0.1935 - val_mse: 0.1931 - learning_rate: 0.1000 - val_custom_mse: 0.9592 - val_custom_mae: 0.6594\n",
            "Epoch 77/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0988 - mae: 0.1654 - mse: 0.0988 - val_loss: 0.1927 - val_mae: 0.1935 - val_mse: 0.1927 - learning_rate: 0.1000 - val_custom_mse: 0.9574 - val_custom_mae: 0.6592\n",
            "Epoch 78/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0987 - mae: 0.1654 - mse: 0.0987 - val_loss: 0.1924 - val_mae: 0.1933 - val_mse: 0.1924 - learning_rate: 0.1000 - val_custom_mse: 0.9563 - val_custom_mae: 0.6588\n",
            "Epoch 79/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0987 - mae: 0.1653 - mse: 0.0987 - val_loss: 0.1925 - val_mae: 0.1933 - val_mse: 0.1925 - learning_rate: 0.1000 - val_custom_mse: 0.9565 - val_custom_mae: 0.6589\n",
            "Epoch 80/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0986 - mae: 0.1653 - mse: 0.0986 - val_loss: 0.1922 - val_mae: 0.1930 - val_mse: 0.1922 - learning_rate: 0.1000 - val_custom_mse: 0.9552 - val_custom_mae: 0.6584\n",
            "Epoch 81/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0986 - mae: 0.1651 - mse: 0.0986 - val_loss: 0.1933 - val_mae: 0.1933 - val_mse: 0.1933 - learning_rate: 0.1000 - val_custom_mse: 0.9612 - val_custom_mae: 0.6597\n",
            "Epoch 82/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0985 - mae: 0.1651 - mse: 0.0985 - val_loss: 0.1926 - val_mae: 0.1931 - val_mse: 0.1926 - learning_rate: 0.1000 - val_custom_mse: 0.9577 - val_custom_mae: 0.6588\n",
            "Epoch 83/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0985 - mae: 0.1650 - mse: 0.0985 - val_loss: 0.1927 - val_mae: 0.1932 - val_mse: 0.1927 - learning_rate: 0.1000 - val_custom_mse: 0.9580 - val_custom_mae: 0.6587\n",
            "Epoch 84/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0984 - mae: 0.1650 - mse: 0.0984 - val_loss: 0.1925 - val_mae: 0.1930 - val_mse: 0.1925 - learning_rate: 0.1000 - val_custom_mse: 0.9568 - val_custom_mae: 0.6585\n",
            "Epoch 85/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0984 - mae: 0.1649 - mse: 0.0984 - val_loss: 0.1926 - val_mae: 0.1929 - val_mse: 0.1926 - learning_rate: 0.1000 - val_custom_mse: 0.9581 - val_custom_mae: 0.6594\n",
            "Epoch 86/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0983 - mae: 0.1649 - mse: 0.0983 - val_loss: 0.1928 - val_mae: 0.1929 - val_mse: 0.1928 - learning_rate: 0.1000 - val_custom_mse: 0.9591 - val_custom_mae: 0.6591\n",
            "Epoch 87/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0984 - mae: 0.1649 - mse: 0.0984 - val_loss: 0.1921 - val_mae: 0.1928 - val_mse: 0.1921 - learning_rate: 0.1000 - val_custom_mse: 0.9553 - val_custom_mae: 0.6584\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "233/233 - 2s - 8ms/step - loss: 0.0983 - mae: 0.1648 - mse: 0.0983 - val_loss: 0.1928 - val_mae: 0.1930 - val_mse: 0.1928 - learning_rate: 0.1000 - val_custom_mse: 0.9587 - val_custom_mae: 0.6590\n",
            "Epoch 89/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0983 - mae: 0.1648 - mse: 0.0983 - val_loss: 0.1927 - val_mae: 0.1929 - val_mse: 0.1927 - learning_rate: 0.0200 - val_custom_mse: 0.9584 - val_custom_mae: 0.6591\n",
            "Epoch 90/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0984 - mae: 0.1648 - mse: 0.0984 - val_loss: 0.1928 - val_mae: 0.1929 - val_mse: 0.1928 - learning_rate: 0.0200 - val_custom_mse: 0.9589 - val_custom_mae: 0.6591\n",
            "Epoch 91/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0983 - mae: 0.1648 - mse: 0.0983 - val_loss: 0.1925 - val_mae: 0.1927 - val_mse: 0.1925 - learning_rate: 0.0200 - val_custom_mse: 0.9577 - val_custom_mae: 0.6590\n",
            "Epoch 92/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0982 - mae: 0.1647 - mse: 0.0982 - val_loss: 0.1927 - val_mae: 0.1928 - val_mse: 0.1927 - learning_rate: 0.0200 - val_custom_mse: 0.9584 - val_custom_mae: 0.6591\n",
            "Epoch 93/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0982 - mae: 0.1647 - mse: 0.0982 - val_loss: 0.1927 - val_mae: 0.1928 - val_mse: 0.1927 - learning_rate: 0.0200 - val_custom_mse: 0.9583 - val_custom_mae: 0.6591\n",
            "Epoch 94/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0983 - mae: 0.1647 - mse: 0.0983 - val_loss: 0.1925 - val_mae: 0.1927 - val_mse: 0.1925 - learning_rate: 0.0200 - val_custom_mse: 0.9576 - val_custom_mae: 0.6589\n",
            "Epoch 95/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0983 - mae: 0.1647 - mse: 0.0983 - val_loss: 0.1926 - val_mae: 0.1928 - val_mse: 0.1926 - learning_rate: 0.0200 - val_custom_mse: 0.9582 - val_custom_mae: 0.6590\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "233/233 - 2s - 8ms/step - loss: 0.0982 - mae: 0.1647 - mse: 0.0982 - val_loss: 0.1926 - val_mae: 0.1928 - val_mse: 0.1926 - learning_rate: 0.0200 - val_custom_mse: 0.9581 - val_custom_mae: 0.6590\n",
            "Epoch 97/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0982 - mae: 0.1646 - mse: 0.0982 - val_loss: 0.1925 - val_mae: 0.1928 - val_mse: 0.1925 - learning_rate: 0.0040 - val_custom_mse: 0.9573 - val_custom_mae: 0.6588\n",
            "Epoch 98/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0982 - mae: 0.1646 - mse: 0.0982 - val_loss: 0.1925 - val_mae: 0.1927 - val_mse: 0.1925 - learning_rate: 0.0040 - val_custom_mse: 0.9575 - val_custom_mae: 0.6589\n",
            "Epoch 99/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0982 - mae: 0.1646 - mse: 0.0982 - val_loss: 0.1925 - val_mae: 0.1928 - val_mse: 0.1925 - learning_rate: 0.0040 - val_custom_mse: 0.9577 - val_custom_mae: 0.6589\n",
            "Epoch 100/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0982 - mae: 0.1647 - mse: 0.0982 - val_loss: 0.1926 - val_mae: 0.1927 - val_mse: 0.1926 - learning_rate: 0.0040 - val_custom_mse: 0.9578 - val_custom_mae: 0.6590\n",
            "Running experiment: horizon=192, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_23', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "233/233 - 6s - 26ms/step - loss: 0.5023 - mae: 0.5050 - mse: 0.5023 - val_loss: 1.0182 - val_mae: 0.7187 - val_mse: 1.0182 - learning_rate: 0.1000 - val_custom_mse: 1.2935 - val_custom_mae: 0.8146\n",
            "Epoch 2/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.4635 - mae: 0.4860 - mse: 0.4635 - val_loss: 0.9394 - val_mae: 0.6864 - val_mse: 0.9394 - learning_rate: 0.1000 - val_custom_mse: 1.2422 - val_custom_mae: 0.7932\n",
            "Epoch 3/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4307 - mae: 0.4678 - mse: 0.4307 - val_loss: 0.8518 - val_mae: 0.6519 - val_mse: 0.8518 - learning_rate: 0.1000 - val_custom_mse: 1.2006 - val_custom_mae: 0.7770\n",
            "Epoch 4/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3928 - mae: 0.4456 - mse: 0.3928 - val_loss: 0.7519 - val_mae: 0.6099 - val_mse: 0.7519 - learning_rate: 0.1000 - val_custom_mse: 1.1595 - val_custom_mae: 0.7609\n",
            "Epoch 5/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.3490 - mae: 0.4182 - mse: 0.3490 - val_loss: 0.6420 - val_mae: 0.5584 - val_mse: 0.6420 - learning_rate: 0.1000 - val_custom_mse: 1.1180 - val_custom_mae: 0.7435\n",
            "Epoch 6/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3020 - mae: 0.3860 - mse: 0.3020 - val_loss: 0.5326 - val_mae: 0.5021 - val_mse: 0.5326 - learning_rate: 0.1000 - val_custom_mse: 1.0820 - val_custom_mae: 0.7295\n",
            "Epoch 7/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.2571 - mae: 0.3518 - mse: 0.2571 - val_loss: 0.4376 - val_mae: 0.4449 - val_mse: 0.4376 - learning_rate: 0.1000 - val_custom_mse: 1.0473 - val_custom_mae: 0.7136\n",
            "Epoch 8/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.2197 - mae: 0.3200 - mse: 0.2197 - val_loss: 0.3665 - val_mae: 0.3959 - val_mse: 0.3665 - learning_rate: 0.1000 - val_custom_mse: 1.0207 - val_custom_mae: 0.7005\n",
            "Epoch 9/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1925 - mae: 0.2944 - mse: 0.1925 - val_loss: 0.3176 - val_mae: 0.3575 - val_mse: 0.3176 - learning_rate: 0.1000 - val_custom_mse: 1.0023 - val_custom_mae: 0.6925\n",
            "Epoch 10/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1744 - mae: 0.2760 - mse: 0.1744 - val_loss: 0.2895 - val_mae: 0.3320 - val_mse: 0.2895 - learning_rate: 0.1000 - val_custom_mse: 0.9908 - val_custom_mae: 0.6844\n",
            "Epoch 11/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.1629 - mae: 0.2637 - mse: 0.1629 - val_loss: 0.2721 - val_mae: 0.3145 - val_mse: 0.2721 - learning_rate: 0.1000 - val_custom_mse: 0.9845 - val_custom_mae: 0.6795\n",
            "Epoch 12/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1555 - mae: 0.2554 - mse: 0.1555 - val_loss: 0.2595 - val_mae: 0.3008 - val_mse: 0.2595 - learning_rate: 0.1000 - val_custom_mse: 0.9783 - val_custom_mae: 0.6764\n",
            "Epoch 13/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1505 - mae: 0.2494 - mse: 0.1505 - val_loss: 0.2518 - val_mae: 0.2915 - val_mse: 0.2518 - learning_rate: 0.1000 - val_custom_mse: 0.9756 - val_custom_mae: 0.6739\n",
            "Epoch 14/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1466 - mae: 0.2447 - mse: 0.1466 - val_loss: 0.2471 - val_mae: 0.2852 - val_mse: 0.2471 - learning_rate: 0.1000 - val_custom_mse: 0.9766 - val_custom_mae: 0.6725\n",
            "Epoch 15/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1436 - mae: 0.2407 - mse: 0.1436 - val_loss: 0.2438 - val_mae: 0.2805 - val_mse: 0.2438 - learning_rate: 0.1000 - val_custom_mse: 0.9779 - val_custom_mae: 0.6712\n",
            "Epoch 16/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1411 - mae: 0.2374 - mse: 0.1411 - val_loss: 0.2385 - val_mae: 0.2739 - val_mse: 0.2385 - learning_rate: 0.1000 - val_custom_mse: 0.9739 - val_custom_mae: 0.6699\n",
            "Epoch 17/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1389 - mae: 0.2344 - mse: 0.1389 - val_loss: 0.2352 - val_mae: 0.2692 - val_mse: 0.2352 - learning_rate: 0.1000 - val_custom_mse: 0.9726 - val_custom_mae: 0.6687\n",
            "Epoch 18/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1370 - mae: 0.2317 - mse: 0.1370 - val_loss: 0.2316 - val_mae: 0.2645 - val_mse: 0.2316 - learning_rate: 0.1000 - val_custom_mse: 0.9697 - val_custom_mae: 0.6677\n",
            "Epoch 19/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1352 - mae: 0.2293 - mse: 0.1352 - val_loss: 0.2290 - val_mae: 0.2608 - val_mse: 0.2290 - learning_rate: 0.1000 - val_custom_mse: 0.9683 - val_custom_mae: 0.6667\n",
            "Epoch 20/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1338 - mae: 0.2272 - mse: 0.1338 - val_loss: 0.2276 - val_mae: 0.2580 - val_mse: 0.2276 - learning_rate: 0.1000 - val_custom_mse: 0.9704 - val_custom_mae: 0.6666\n",
            "Epoch 21/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2252 - mse: 0.1326 - val_loss: 0.2261 - val_mae: 0.2556 - val_mse: 0.2261 - learning_rate: 0.1000 - val_custom_mse: 0.9711 - val_custom_mae: 0.6662\n",
            "Epoch 22/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1314 - mae: 0.2235 - mse: 0.1314 - val_loss: 0.2235 - val_mae: 0.2521 - val_mse: 0.2235 - learning_rate: 0.1000 - val_custom_mse: 0.9679 - val_custom_mae: 0.6651\n",
            "Epoch 23/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.1303 - mae: 0.2218 - mse: 0.1303 - val_loss: 0.2216 - val_mae: 0.2493 - val_mse: 0.2216 - learning_rate: 0.1000 - val_custom_mse: 0.9667 - val_custom_mae: 0.6647\n",
            "Epoch 24/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.1293 - mae: 0.2204 - mse: 0.1293 - val_loss: 0.2197 - val_mae: 0.2466 - val_mse: 0.2197 - learning_rate: 0.1000 - val_custom_mse: 0.9643 - val_custom_mae: 0.6639\n",
            "Epoch 25/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1284 - mae: 0.2190 - mse: 0.1284 - val_loss: 0.2175 - val_mae: 0.2440 - val_mse: 0.2175 - learning_rate: 0.1000 - val_custom_mse: 0.9601 - val_custom_mae: 0.6631\n",
            "Epoch 26/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1276 - mae: 0.2178 - mse: 0.1276 - val_loss: 0.2175 - val_mae: 0.2427 - val_mse: 0.2175 - learning_rate: 0.1000 - val_custom_mse: 0.9641 - val_custom_mae: 0.6630\n",
            "Epoch 27/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1269 - mae: 0.2167 - mse: 0.1269 - val_loss: 0.2158 - val_mae: 0.2404 - val_mse: 0.2158 - learning_rate: 0.1000 - val_custom_mse: 0.9616 - val_custom_mae: 0.6627\n",
            "Epoch 28/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1262 - mae: 0.2157 - mse: 0.1262 - val_loss: 0.2154 - val_mae: 0.2393 - val_mse: 0.2154 - learning_rate: 0.1000 - val_custom_mse: 0.9634 - val_custom_mae: 0.6626\n",
            "Epoch 29/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1256 - mae: 0.2147 - mse: 0.1256 - val_loss: 0.2138 - val_mae: 0.2374 - val_mse: 0.2138 - learning_rate: 0.1000 - val_custom_mse: 0.9603 - val_custom_mae: 0.6621\n",
            "Epoch 30/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1251 - mae: 0.2139 - mse: 0.1251 - val_loss: 0.2133 - val_mae: 0.2362 - val_mse: 0.2133 - learning_rate: 0.1000 - val_custom_mse: 0.9612 - val_custom_mae: 0.6619\n",
            "Epoch 31/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1246 - mae: 0.2131 - mse: 0.1246 - val_loss: 0.2132 - val_mae: 0.2355 - val_mse: 0.2132 - learning_rate: 0.1000 - val_custom_mse: 0.9633 - val_custom_mae: 0.6620\n",
            "Epoch 32/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1241 - mae: 0.2124 - mse: 0.1241 - val_loss: 0.2123 - val_mae: 0.2341 - val_mse: 0.2123 - learning_rate: 0.1000 - val_custom_mse: 0.9624 - val_custom_mae: 0.6618\n",
            "Epoch 33/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1237 - mae: 0.2117 - mse: 0.1237 - val_loss: 0.2116 - val_mae: 0.2330 - val_mse: 0.2116 - learning_rate: 0.1000 - val_custom_mse: 0.9616 - val_custom_mae: 0.6616\n",
            "Epoch 34/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1233 - mae: 0.2111 - mse: 0.1233 - val_loss: 0.2114 - val_mae: 0.2324 - val_mse: 0.2114 - learning_rate: 0.1000 - val_custom_mse: 0.9623 - val_custom_mae: 0.6614\n",
            "Epoch 35/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1229 - mae: 0.2105 - mse: 0.1229 - val_loss: 0.2111 - val_mae: 0.2316 - val_mse: 0.2111 - learning_rate: 0.1000 - val_custom_mse: 0.9629 - val_custom_mae: 0.6614\n",
            "Epoch 36/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1225 - mae: 0.2100 - mse: 0.1225 - val_loss: 0.2097 - val_mae: 0.2301 - val_mse: 0.2097 - learning_rate: 0.1000 - val_custom_mse: 0.9591 - val_custom_mae: 0.6609\n",
            "Epoch 37/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1223 - mae: 0.2095 - mse: 0.1223 - val_loss: 0.2097 - val_mae: 0.2296 - val_mse: 0.2097 - learning_rate: 0.1000 - val_custom_mse: 0.9607 - val_custom_mae: 0.6608\n",
            "Epoch 38/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1219 - mae: 0.2090 - mse: 0.1219 - val_loss: 0.2083 - val_mae: 0.2284 - val_mse: 0.2083 - learning_rate: 0.1000 - val_custom_mse: 0.9561 - val_custom_mae: 0.6601\n",
            "Epoch 39/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1217 - mae: 0.2086 - mse: 0.1217 - val_loss: 0.2089 - val_mae: 0.2283 - val_mse: 0.2089 - learning_rate: 0.1000 - val_custom_mse: 0.9601 - val_custom_mae: 0.6605\n",
            "Epoch 40/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1213 - mae: 0.2082 - mse: 0.1213 - val_loss: 0.2081 - val_mae: 0.2274 - val_mse: 0.2081 - learning_rate: 0.1000 - val_custom_mse: 0.9582 - val_custom_mae: 0.6605\n",
            "Epoch 41/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1211 - mae: 0.2079 - mse: 0.1211 - val_loss: 0.2081 - val_mae: 0.2271 - val_mse: 0.2081 - learning_rate: 0.1000 - val_custom_mse: 0.9591 - val_custom_mae: 0.6602\n",
            "Epoch 42/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1210 - mae: 0.2076 - mse: 0.1210 - val_loss: 0.2071 - val_mae: 0.2262 - val_mse: 0.2071 - learning_rate: 0.1000 - val_custom_mse: 0.9554 - val_custom_mae: 0.6599\n",
            "Epoch 43/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1207 - mae: 0.2073 - mse: 0.1207 - val_loss: 0.2074 - val_mae: 0.2259 - val_mse: 0.2074 - learning_rate: 0.1000 - val_custom_mse: 0.9581 - val_custom_mae: 0.6600\n",
            "Epoch 44/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1205 - mae: 0.2070 - mse: 0.1205 - val_loss: 0.2072 - val_mae: 0.2258 - val_mse: 0.2072 - learning_rate: 0.1000 - val_custom_mse: 0.9574 - val_custom_mae: 0.6600\n",
            "Epoch 45/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1202 - mae: 0.2064 - mse: 0.1202 - val_loss: 0.2072 - val_mae: 0.2258 - val_mse: 0.2072 - learning_rate: 0.1000 - val_custom_mse: 0.9569 - val_custom_mae: 0.6597\n",
            "Epoch 46/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1200 - mae: 0.2062 - mse: 0.1200 - val_loss: 0.2070 - val_mae: 0.2253 - val_mse: 0.2070 - learning_rate: 0.1000 - val_custom_mse: 0.9573 - val_custom_mae: 0.6599\n",
            "Epoch 47/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1199 - mae: 0.2061 - mse: 0.1199 - val_loss: 0.2070 - val_mae: 0.2249 - val_mse: 0.2070 - learning_rate: 0.1000 - val_custom_mse: 0.9582 - val_custom_mae: 0.6601\n",
            "Epoch 48/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1197 - mae: 0.2058 - mse: 0.1197 - val_loss: 0.2079 - val_mae: 0.2254 - val_mse: 0.2079 - learning_rate: 0.1000 - val_custom_mse: 0.9625 - val_custom_mae: 0.6608\n",
            "Epoch 49/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1195 - mae: 0.2057 - mse: 0.1195 - val_loss: 0.2074 - val_mae: 0.2248 - val_mse: 0.2074 - learning_rate: 0.1000 - val_custom_mse: 0.9610 - val_custom_mae: 0.6604\n",
            "Epoch 50/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1194 - mae: 0.2055 - mse: 0.1194 - val_loss: 0.2065 - val_mae: 0.2241 - val_mse: 0.2065 - learning_rate: 0.1000 - val_custom_mse: 0.9578 - val_custom_mae: 0.6598\n",
            "Epoch 51/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1192 - mae: 0.2053 - mse: 0.1192 - val_loss: 0.2067 - val_mae: 0.2239 - val_mse: 0.2067 - learning_rate: 0.1000 - val_custom_mse: 0.9598 - val_custom_mae: 0.6602\n",
            "Epoch 52/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1191 - mae: 0.2051 - mse: 0.1191 - val_loss: 0.2064 - val_mae: 0.2236 - val_mse: 0.2064 - learning_rate: 0.1000 - val_custom_mse: 0.9588 - val_custom_mae: 0.6599\n",
            "Epoch 53/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1190 - mae: 0.2050 - mse: 0.1190 - val_loss: 0.2067 - val_mae: 0.2237 - val_mse: 0.2067 - learning_rate: 0.1000 - val_custom_mse: 0.9607 - val_custom_mae: 0.6603\n",
            "Epoch 54/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1189 - mae: 0.2049 - mse: 0.1189 - val_loss: 0.2067 - val_mae: 0.2236 - val_mse: 0.2067 - learning_rate: 0.1000 - val_custom_mse: 0.9606 - val_custom_mae: 0.6601\n",
            "Epoch 55/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1187 - mae: 0.2047 - mse: 0.1187 - val_loss: 0.2068 - val_mae: 0.2236 - val_mse: 0.2068 - learning_rate: 0.1000 - val_custom_mse: 0.9615 - val_custom_mae: 0.6603\n",
            "Epoch 56/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1186 - mae: 0.2046 - mse: 0.1186 - val_loss: 0.2059 - val_mae: 0.2230 - val_mse: 0.2059 - learning_rate: 0.1000 - val_custom_mse: 0.9577 - val_custom_mae: 0.6597\n",
            "Epoch 57/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1184 - mae: 0.2044 - mse: 0.1184 - val_loss: 0.2059 - val_mae: 0.2229 - val_mse: 0.2059 - learning_rate: 0.1000 - val_custom_mse: 0.9580 - val_custom_mae: 0.6596\n",
            "Epoch 58/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1184 - mae: 0.2044 - mse: 0.1184 - val_loss: 0.2061 - val_mae: 0.2227 - val_mse: 0.2061 - learning_rate: 0.1000 - val_custom_mse: 0.9596 - val_custom_mae: 0.6600\n",
            "Epoch 59/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1182 - mae: 0.2042 - mse: 0.1182 - val_loss: 0.2057 - val_mae: 0.2225 - val_mse: 0.2057 - learning_rate: 0.1000 - val_custom_mse: 0.9577 - val_custom_mae: 0.6598\n",
            "Epoch 60/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1182 - mae: 0.2041 - mse: 0.1182 - val_loss: 0.2051 - val_mae: 0.2224 - val_mse: 0.2051 - learning_rate: 0.1000 - val_custom_mse: 0.9549 - val_custom_mae: 0.6595\n",
            "Epoch 61/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1181 - mae: 0.2041 - mse: 0.1181 - val_loss: 0.2061 - val_mae: 0.2226 - val_mse: 0.2061 - learning_rate: 0.1000 - val_custom_mse: 0.9601 - val_custom_mae: 0.6602\n",
            "Epoch 62/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1180 - mae: 0.2039 - mse: 0.1180 - val_loss: 0.2057 - val_mae: 0.2223 - val_mse: 0.2057 - learning_rate: 0.1000 - val_custom_mse: 0.9583 - val_custom_mae: 0.6600\n",
            "Epoch 63/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1178 - mae: 0.2038 - mse: 0.1178 - val_loss: 0.2058 - val_mae: 0.2221 - val_mse: 0.2058 - learning_rate: 0.1000 - val_custom_mse: 0.9598 - val_custom_mae: 0.6602\n",
            "Epoch 64/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1178 - mae: 0.2037 - mse: 0.1178 - val_loss: 0.2049 - val_mae: 0.2218 - val_mse: 0.2049 - learning_rate: 0.1000 - val_custom_mse: 0.9551 - val_custom_mae: 0.6594\n",
            "Epoch 65/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1177 - mae: 0.2036 - mse: 0.1177 - val_loss: 0.2055 - val_mae: 0.2220 - val_mse: 0.2055 - learning_rate: 0.1000 - val_custom_mse: 0.9577 - val_custom_mae: 0.6597\n",
            "Epoch 66/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1177 - mae: 0.2036 - mse: 0.1177 - val_loss: 0.2054 - val_mae: 0.2218 - val_mse: 0.2054 - learning_rate: 0.1000 - val_custom_mse: 0.9581 - val_custom_mae: 0.6599\n",
            "Epoch 67/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1175 - mae: 0.2035 - mse: 0.1175 - val_loss: 0.2056 - val_mae: 0.2218 - val_mse: 0.2056 - learning_rate: 0.1000 - val_custom_mse: 0.9592 - val_custom_mae: 0.6601\n",
            "Epoch 68/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1174 - mae: 0.2034 - mse: 0.1174 - val_loss: 0.2046 - val_mae: 0.2218 - val_mse: 0.2046 - learning_rate: 0.1000 - val_custom_mse: 0.9537 - val_custom_mae: 0.6593\n",
            "Epoch 69/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1174 - mae: 0.2033 - mse: 0.1174 - val_loss: 0.2046 - val_mae: 0.2215 - val_mse: 0.2046 - learning_rate: 0.1000 - val_custom_mse: 0.9545 - val_custom_mae: 0.6595\n",
            "Epoch 70/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1173 - mae: 0.2032 - mse: 0.1173 - val_loss: 0.2054 - val_mae: 0.2216 - val_mse: 0.2054 - learning_rate: 0.1000 - val_custom_mse: 0.9581 - val_custom_mae: 0.6597\n",
            "Epoch 71/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1173 - mae: 0.2032 - mse: 0.1173 - val_loss: 0.2057 - val_mae: 0.2218 - val_mse: 0.2057 - learning_rate: 0.1000 - val_custom_mse: 0.9602 - val_custom_mae: 0.6607\n",
            "Epoch 72/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1172 - mae: 0.2031 - mse: 0.1172 - val_loss: 0.2047 - val_mae: 0.2215 - val_mse: 0.2047 - learning_rate: 0.1000 - val_custom_mse: 0.9548 - val_custom_mae: 0.6594\n",
            "Epoch 73/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1171 - mae: 0.2030 - mse: 0.1171 - val_loss: 0.2059 - val_mae: 0.2218 - val_mse: 0.2059 - learning_rate: 0.1000 - val_custom_mse: 0.9604 - val_custom_mae: 0.6601\n",
            "Epoch 74/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1170 - mae: 0.2029 - mse: 0.1170 - val_loss: 0.2051 - val_mae: 0.2213 - val_mse: 0.2051 - learning_rate: 0.1000 - val_custom_mse: 0.9573 - val_custom_mae: 0.6597\n",
            "Epoch 75/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1170 - mae: 0.2029 - mse: 0.1170 - val_loss: 0.2058 - val_mae: 0.2216 - val_mse: 0.2058 - learning_rate: 0.1000 - val_custom_mse: 0.9604 - val_custom_mae: 0.6604\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1169 - mae: 0.2028 - mse: 0.1169 - val_loss: 0.2056 - val_mae: 0.2215 - val_mse: 0.2056 - learning_rate: 0.1000 - val_custom_mse: 0.9599 - val_custom_mae: 0.6606\n",
            "Epoch 77/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1168 - mae: 0.2029 - mse: 0.1168 - val_loss: 0.2051 - val_mae: 0.2213 - val_mse: 0.2051 - learning_rate: 0.0200 - val_custom_mse: 0.9577 - val_custom_mae: 0.6600\n",
            "Epoch 78/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1168 - mae: 0.2027 - mse: 0.1168 - val_loss: 0.2055 - val_mae: 0.2214 - val_mse: 0.2055 - learning_rate: 0.0200 - val_custom_mse: 0.9592 - val_custom_mae: 0.6602\n",
            "Epoch 79/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1167 - mae: 0.2027 - mse: 0.1167 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 0.0200 - val_custom_mse: 0.9582 - val_custom_mae: 0.6600\n",
            "Epoch 80/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2053 - val_mae: 0.2213 - val_mse: 0.2053 - learning_rate: 0.0200 - val_custom_mse: 0.9587 - val_custom_mae: 0.6600\n",
            "Epoch 81/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1168 - mae: 0.2027 - mse: 0.1168 - val_loss: 0.2053 - val_mae: 0.2213 - val_mse: 0.2053 - learning_rate: 0.0200 - val_custom_mse: 0.9587 - val_custom_mae: 0.6601\n",
            "Epoch 82/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1167 - mae: 0.2027 - mse: 0.1167 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 0.0200 - val_custom_mse: 0.9584 - val_custom_mae: 0.6601\n",
            "Epoch 83/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1167 - mae: 0.2026 - mse: 0.1167 - val_loss: 0.2054 - val_mae: 0.2213 - val_mse: 0.2054 - learning_rate: 0.0200 - val_custom_mse: 0.9591 - val_custom_mae: 0.6602\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2054 - val_mae: 0.2214 - val_mse: 0.2054 - learning_rate: 0.0200 - val_custom_mse: 0.9590 - val_custom_mae: 0.6601\n",
            "Epoch 85/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 0.0040 - val_custom_mse: 0.9582 - val_custom_mae: 0.6599\n",
            "Epoch 86/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1165 - mae: 0.2025 - mse: 0.1165 - val_loss: 0.2053 - val_mae: 0.2213 - val_mse: 0.2053 - learning_rate: 0.0040 - val_custom_mse: 0.9584 - val_custom_mae: 0.6600\n",
            "Epoch 87/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1167 - mae: 0.2026 - mse: 0.1167 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 0.0040 - val_custom_mse: 0.9583 - val_custom_mae: 0.6600\n",
            "Epoch 88/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 0.0040 - val_custom_mse: 0.9582 - val_custom_mae: 0.6599\n",
            "Epoch 89/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2053 - val_mae: 0.2213 - val_mse: 0.2053 - learning_rate: 0.0040 - val_custom_mse: 0.9584 - val_custom_mae: 0.6600\n",
            "Epoch 90/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1167 - mae: 0.2026 - mse: 0.1167 - val_loss: 0.2053 - val_mae: 0.2213 - val_mse: 0.2053 - learning_rate: 0.0040 - val_custom_mse: 0.9583 - val_custom_mae: 0.6600\n",
            "Epoch 91/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2053 - val_mae: 0.2213 - val_mse: 0.2053 - learning_rate: 0.0040 - val_custom_mse: 0.9584 - val_custom_mae: 0.6600\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1167 - mae: 0.2026 - mse: 0.1167 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 0.0040 - val_custom_mse: 0.9582 - val_custom_mae: 0.6599\n",
            "Epoch 93/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1167 - mae: 0.2026 - mse: 0.1167 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9581 - val_custom_mae: 0.6599\n",
            "Epoch 94/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9580 - val_custom_mae: 0.6599\n",
            "Epoch 95/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1167 - mae: 0.2026 - mse: 0.1167 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9580 - val_custom_mae: 0.6599\n",
            "Epoch 96/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9581 - val_custom_mae: 0.6599\n",
            "Epoch 97/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1165 - mae: 0.2026 - mse: 0.1165 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9580 - val_custom_mae: 0.6599\n",
            "Epoch 98/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9581 - val_custom_mae: 0.6599\n",
            "Epoch 99/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1167 - mae: 0.2026 - mse: 0.1167 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9579 - val_custom_mae: 0.6599\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 100: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1166 - mae: 0.2026 - mse: 0.1166 - val_loss: 0.2052 - val_mae: 0.2213 - val_mse: 0.2052 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9580 - val_custom_mae: 0.6599\n",
            "Running experiment: horizon=192, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_24', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "233/233 - 6s - 26ms/step - loss: 0.5126 - mae: 0.5112 - mse: 0.5126 - val_loss: 1.0151 - val_mae: 0.7178 - val_mse: 1.0151 - learning_rate: 0.1000 - val_custom_mse: 1.2919 - val_custom_mae: 0.8143\n",
            "Epoch 2/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.4690 - mae: 0.4899 - mse: 0.4690 - val_loss: 0.9401 - val_mae: 0.6917 - val_mse: 0.9401 - learning_rate: 0.1000 - val_custom_mse: 1.2624 - val_custom_mae: 0.8049\n",
            "Epoch 3/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4341 - mae: 0.4705 - mse: 0.4341 - val_loss: 0.8449 - val_mae: 0.6506 - val_mse: 0.8449 - learning_rate: 0.1000 - val_custom_mse: 1.1999 - val_custom_mae: 0.7781\n",
            "Epoch 4/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3940 - mae: 0.4470 - mse: 0.3940 - val_loss: 0.7482 - val_mae: 0.6128 - val_mse: 0.7482 - learning_rate: 0.1000 - val_custom_mse: 1.1715 - val_custom_mae: 0.7699\n",
            "Epoch 5/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3488 - mae: 0.4188 - mse: 0.3488 - val_loss: 0.6324 - val_mae: 0.5550 - val_mse: 0.6324 - learning_rate: 0.1000 - val_custom_mse: 1.1110 - val_custom_mae: 0.7418\n",
            "Epoch 6/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3019 - mae: 0.3862 - mse: 0.3019 - val_loss: 0.5255 - val_mae: 0.4989 - val_mse: 0.5255 - learning_rate: 0.1000 - val_custom_mse: 1.0744 - val_custom_mae: 0.7264\n",
            "Epoch 7/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2587 - mae: 0.3530 - mse: 0.2587 - val_loss: 0.4344 - val_mae: 0.4441 - val_mse: 0.4344 - learning_rate: 0.1000 - val_custom_mse: 1.0419 - val_custom_mae: 0.7114\n",
            "Epoch 8/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2243 - mae: 0.3238 - mse: 0.2243 - val_loss: 0.3681 - val_mae: 0.3981 - val_mse: 0.3681 - learning_rate: 0.1000 - val_custom_mse: 1.0179 - val_custom_mae: 0.6983\n",
            "Epoch 9/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2004 - mae: 0.3016 - mse: 0.2004 - val_loss: 0.3236 - val_mae: 0.3635 - val_mse: 0.3236 - learning_rate: 0.1000 - val_custom_mse: 1.0020 - val_custom_mae: 0.6901\n",
            "Epoch 10/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1849 - mae: 0.2866 - mse: 0.1849 - val_loss: 0.2966 - val_mae: 0.3396 - val_mse: 0.2966 - learning_rate: 0.1000 - val_custom_mse: 0.9921 - val_custom_mae: 0.6838\n",
            "Epoch 11/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1755 - mae: 0.2771 - mse: 0.1755 - val_loss: 0.2806 - val_mae: 0.3239 - val_mse: 0.2806 - learning_rate: 0.1000 - val_custom_mse: 0.9880 - val_custom_mae: 0.6799\n",
            "Epoch 12/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1693 - mae: 0.2706 - mse: 0.1693 - val_loss: 0.2704 - val_mae: 0.3128 - val_mse: 0.2704 - learning_rate: 0.1000 - val_custom_mse: 0.9861 - val_custom_mae: 0.6775\n",
            "Epoch 13/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1650 - mae: 0.2660 - mse: 0.1650 - val_loss: 0.2618 - val_mae: 0.3033 - val_mse: 0.2618 - learning_rate: 0.1000 - val_custom_mse: 0.9816 - val_custom_mae: 0.6753\n",
            "Epoch 14/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1614 - mae: 0.2621 - mse: 0.1614 - val_loss: 0.2562 - val_mae: 0.2967 - val_mse: 0.2562 - learning_rate: 0.1000 - val_custom_mse: 0.9801 - val_custom_mae: 0.6737\n",
            "Epoch 15/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1587 - mae: 0.2590 - mse: 0.1587 - val_loss: 0.2520 - val_mae: 0.2913 - val_mse: 0.2520 - learning_rate: 0.1000 - val_custom_mse: 0.9793 - val_custom_mae: 0.6725\n",
            "Epoch 16/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1563 - mae: 0.2562 - mse: 0.1563 - val_loss: 0.2480 - val_mae: 0.2864 - val_mse: 0.2480 - learning_rate: 0.1000 - val_custom_mse: 0.9780 - val_custom_mae: 0.6716\n",
            "Epoch 17/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1542 - mae: 0.2538 - mse: 0.1542 - val_loss: 0.2450 - val_mae: 0.2823 - val_mse: 0.2450 - learning_rate: 0.1000 - val_custom_mse: 0.9771 - val_custom_mae: 0.6706\n",
            "Epoch 18/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1524 - mae: 0.2515 - mse: 0.1524 - val_loss: 0.2414 - val_mae: 0.2779 - val_mse: 0.2414 - learning_rate: 0.1000 - val_custom_mse: 0.9735 - val_custom_mae: 0.6693\n",
            "Epoch 19/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1507 - mae: 0.2495 - mse: 0.1507 - val_loss: 0.2385 - val_mae: 0.2744 - val_mse: 0.2385 - learning_rate: 0.1000 - val_custom_mse: 0.9711 - val_custom_mae: 0.6685\n",
            "Epoch 20/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1492 - mae: 0.2477 - mse: 0.1492 - val_loss: 0.2362 - val_mae: 0.2714 - val_mse: 0.2362 - learning_rate: 0.1000 - val_custom_mse: 0.9700 - val_custom_mae: 0.6677\n",
            "Epoch 21/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1479 - mae: 0.2461 - mse: 0.1479 - val_loss: 0.2352 - val_mae: 0.2695 - val_mse: 0.2352 - learning_rate: 0.1000 - val_custom_mse: 0.9721 - val_custom_mae: 0.6673\n",
            "Epoch 22/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1468 - mae: 0.2446 - mse: 0.1468 - val_loss: 0.2323 - val_mae: 0.2661 - val_mse: 0.2323 - learning_rate: 0.1000 - val_custom_mse: 0.9672 - val_custom_mae: 0.6665\n",
            "Epoch 23/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1457 - mae: 0.2433 - mse: 0.1457 - val_loss: 0.2308 - val_mae: 0.2640 - val_mse: 0.2308 - learning_rate: 0.1000 - val_custom_mse: 0.9669 - val_custom_mae: 0.6660\n",
            "Epoch 24/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1447 - mae: 0.2420 - mse: 0.1447 - val_loss: 0.2300 - val_mae: 0.2625 - val_mse: 0.2300 - learning_rate: 0.1000 - val_custom_mse: 0.9684 - val_custom_mae: 0.6658\n",
            "Epoch 25/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1438 - mae: 0.2409 - mse: 0.1438 - val_loss: 0.2285 - val_mae: 0.2604 - val_mse: 0.2285 - learning_rate: 0.1000 - val_custom_mse: 0.9665 - val_custom_mae: 0.6650\n",
            "Epoch 26/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1429 - mae: 0.2398 - mse: 0.1429 - val_loss: 0.2271 - val_mae: 0.2585 - val_mse: 0.2271 - learning_rate: 0.1000 - val_custom_mse: 0.9654 - val_custom_mae: 0.6647\n",
            "Epoch 27/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1423 - mae: 0.2390 - mse: 0.1423 - val_loss: 0.2259 - val_mae: 0.2569 - val_mse: 0.2259 - learning_rate: 0.1000 - val_custom_mse: 0.9639 - val_custom_mae: 0.6642\n",
            "Epoch 28/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1416 - mae: 0.2381 - mse: 0.1416 - val_loss: 0.2250 - val_mae: 0.2555 - val_mse: 0.2250 - learning_rate: 0.1000 - val_custom_mse: 0.9635 - val_custom_mae: 0.6641\n",
            "Epoch 29/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1410 - mae: 0.2374 - mse: 0.1410 - val_loss: 0.2247 - val_mae: 0.2547 - val_mse: 0.2247 - learning_rate: 0.1000 - val_custom_mse: 0.9652 - val_custom_mae: 0.6638\n",
            "Epoch 30/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1404 - mae: 0.2366 - mse: 0.1404 - val_loss: 0.2231 - val_mae: 0.2530 - val_mse: 0.2231 - learning_rate: 0.1000 - val_custom_mse: 0.9613 - val_custom_mae: 0.6633\n",
            "Epoch 31/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1400 - mae: 0.2360 - mse: 0.1400 - val_loss: 0.2228 - val_mae: 0.2522 - val_mse: 0.2228 - learning_rate: 0.1000 - val_custom_mse: 0.9624 - val_custom_mae: 0.6632\n",
            "Epoch 32/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1395 - mae: 0.2354 - mse: 0.1395 - val_loss: 0.2222 - val_mae: 0.2512 - val_mse: 0.2222 - learning_rate: 0.1000 - val_custom_mse: 0.9625 - val_custom_mae: 0.6633\n",
            "Epoch 33/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1392 - mae: 0.2349 - mse: 0.1392 - val_loss: 0.2212 - val_mae: 0.2502 - val_mse: 0.2212 - learning_rate: 0.1000 - val_custom_mse: 0.9597 - val_custom_mae: 0.6626\n",
            "Epoch 34/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.1386 - mae: 0.2343 - mse: 0.1386 - val_loss: 0.2215 - val_mae: 0.2498 - val_mse: 0.2215 - learning_rate: 0.1000 - val_custom_mse: 0.9633 - val_custom_mae: 0.6628\n",
            "Epoch 35/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1383 - mae: 0.2339 - mse: 0.1383 - val_loss: 0.2212 - val_mae: 0.2491 - val_mse: 0.2212 - learning_rate: 0.1000 - val_custom_mse: 0.9635 - val_custom_mae: 0.6627\n",
            "Epoch 36/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1379 - mae: 0.2334 - mse: 0.1379 - val_loss: 0.2210 - val_mae: 0.2487 - val_mse: 0.2210 - learning_rate: 0.1000 - val_custom_mse: 0.9642 - val_custom_mae: 0.6627\n",
            "Epoch 37/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1377 - mae: 0.2331 - mse: 0.1377 - val_loss: 0.2198 - val_mae: 0.2476 - val_mse: 0.2198 - learning_rate: 0.1000 - val_custom_mse: 0.9603 - val_custom_mae: 0.6620\n",
            "Epoch 38/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1372 - mae: 0.2324 - mse: 0.1372 - val_loss: 0.2202 - val_mae: 0.2482 - val_mse: 0.2202 - learning_rate: 0.1000 - val_custom_mse: 0.9599 - val_custom_mae: 0.6622\n",
            "Epoch 39/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1369 - mae: 0.2320 - mse: 0.1369 - val_loss: 0.2199 - val_mae: 0.2476 - val_mse: 0.2199 - learning_rate: 0.1000 - val_custom_mse: 0.9603 - val_custom_mae: 0.6618\n",
            "Epoch 40/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1366 - mae: 0.2317 - mse: 0.1366 - val_loss: 0.2188 - val_mae: 0.2467 - val_mse: 0.2188 - learning_rate: 0.1000 - val_custom_mse: 0.9570 - val_custom_mae: 0.6611\n",
            "Epoch 41/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1363 - mae: 0.2315 - mse: 0.1363 - val_loss: 0.2190 - val_mae: 0.2462 - val_mse: 0.2190 - learning_rate: 0.1000 - val_custom_mse: 0.9596 - val_custom_mae: 0.6616\n",
            "Epoch 42/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1361 - mae: 0.2312 - mse: 0.1361 - val_loss: 0.2190 - val_mae: 0.2460 - val_mse: 0.2190 - learning_rate: 0.1000 - val_custom_mse: 0.9604 - val_custom_mae: 0.6617\n",
            "Epoch 43/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1360 - mae: 0.2310 - mse: 0.1360 - val_loss: 0.2179 - val_mae: 0.2453 - val_mse: 0.2179 - learning_rate: 0.1000 - val_custom_mse: 0.9565 - val_custom_mae: 0.6612\n",
            "Epoch 44/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1358 - mae: 0.2308 - mse: 0.1358 - val_loss: 0.2181 - val_mae: 0.2449 - val_mse: 0.2181 - learning_rate: 0.1000 - val_custom_mse: 0.9586 - val_custom_mae: 0.6611\n",
            "Epoch 45/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1355 - mae: 0.2305 - mse: 0.1355 - val_loss: 0.2184 - val_mae: 0.2449 - val_mse: 0.2184 - learning_rate: 0.1000 - val_custom_mse: 0.9605 - val_custom_mae: 0.6615\n",
            "Epoch 46/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1353 - mae: 0.2304 - mse: 0.1353 - val_loss: 0.2180 - val_mae: 0.2446 - val_mse: 0.2180 - learning_rate: 0.1000 - val_custom_mse: 0.9594 - val_custom_mae: 0.6612\n",
            "Epoch 47/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1350 - mae: 0.2301 - mse: 0.1350 - val_loss: 0.2184 - val_mae: 0.2445 - val_mse: 0.2184 - learning_rate: 0.1000 - val_custom_mse: 0.9616 - val_custom_mae: 0.6617\n",
            "Epoch 48/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1349 - mae: 0.2299 - mse: 0.1349 - val_loss: 0.2177 - val_mae: 0.2440 - val_mse: 0.2177 - learning_rate: 0.1000 - val_custom_mse: 0.9591 - val_custom_mae: 0.6611\n",
            "Epoch 49/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1347 - mae: 0.2297 - mse: 0.1347 - val_loss: 0.2186 - val_mae: 0.2444 - val_mse: 0.2186 - learning_rate: 0.1000 - val_custom_mse: 0.9633 - val_custom_mae: 0.6616\n",
            "Epoch 50/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1346 - mae: 0.2295 - mse: 0.1346 - val_loss: 0.2171 - val_mae: 0.2435 - val_mse: 0.2171 - learning_rate: 0.1000 - val_custom_mse: 0.9574 - val_custom_mae: 0.6609\n",
            "Epoch 51/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1345 - mae: 0.2294 - mse: 0.1345 - val_loss: 0.2175 - val_mae: 0.2434 - val_mse: 0.2175 - learning_rate: 0.1000 - val_custom_mse: 0.9602 - val_custom_mae: 0.6614\n",
            "Epoch 52/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1342 - mae: 0.2292 - mse: 0.1342 - val_loss: 0.2175 - val_mae: 0.2433 - val_mse: 0.2175 - learning_rate: 0.1000 - val_custom_mse: 0.9605 - val_custom_mae: 0.6612\n",
            "Epoch 53/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1341 - mae: 0.2291 - mse: 0.1341 - val_loss: 0.2172 - val_mae: 0.2430 - val_mse: 0.2172 - learning_rate: 0.1000 - val_custom_mse: 0.9590 - val_custom_mae: 0.6606\n",
            "Epoch 54/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1339 - mae: 0.2289 - mse: 0.1339 - val_loss: 0.2169 - val_mae: 0.2430 - val_mse: 0.2169 - learning_rate: 0.1000 - val_custom_mse: 0.9579 - val_custom_mae: 0.6606\n",
            "Epoch 55/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1338 - mae: 0.2288 - mse: 0.1338 - val_loss: 0.2168 - val_mae: 0.2428 - val_mse: 0.2168 - learning_rate: 0.1000 - val_custom_mse: 0.9574 - val_custom_mae: 0.6606\n",
            "Epoch 56/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1337 - mae: 0.2287 - mse: 0.1337 - val_loss: 0.2167 - val_mae: 0.2427 - val_mse: 0.2167 - learning_rate: 0.1000 - val_custom_mse: 0.9572 - val_custom_mae: 0.6607\n",
            "Epoch 57/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1336 - mae: 0.2285 - mse: 0.1336 - val_loss: 0.2168 - val_mae: 0.2427 - val_mse: 0.2168 - learning_rate: 0.1000 - val_custom_mse: 0.9582 - val_custom_mae: 0.6609\n",
            "Epoch 58/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1335 - mae: 0.2285 - mse: 0.1335 - val_loss: 0.2175 - val_mae: 0.2430 - val_mse: 0.2175 - learning_rate: 0.1000 - val_custom_mse: 0.9612 - val_custom_mae: 0.6610\n",
            "Epoch 59/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1333 - mae: 0.2283 - mse: 0.1333 - val_loss: 0.2167 - val_mae: 0.2424 - val_mse: 0.2167 - learning_rate: 0.1000 - val_custom_mse: 0.9584 - val_custom_mae: 0.6606\n",
            "Epoch 60/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1333 - mae: 0.2282 - mse: 0.1333 - val_loss: 0.2169 - val_mae: 0.2426 - val_mse: 0.2169 - learning_rate: 0.1000 - val_custom_mse: 0.9590 - val_custom_mae: 0.6608\n",
            "Epoch 61/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1332 - mae: 0.2282 - mse: 0.1332 - val_loss: 0.2174 - val_mae: 0.2427 - val_mse: 0.2174 - learning_rate: 0.1000 - val_custom_mse: 0.9614 - val_custom_mae: 0.6611\n",
            "Epoch 62/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1331 - mae: 0.2280 - mse: 0.1331 - val_loss: 0.2170 - val_mae: 0.2424 - val_mse: 0.2170 - learning_rate: 0.1000 - val_custom_mse: 0.9602 - val_custom_mae: 0.6609\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1330 - mae: 0.2280 - mse: 0.1330 - val_loss: 0.2179 - val_mae: 0.2431 - val_mse: 0.2179 - learning_rate: 0.1000 - val_custom_mse: 0.9636 - val_custom_mae: 0.6615\n",
            "Epoch 64/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1330 - mae: 0.2279 - mse: 0.1330 - val_loss: 0.2166 - val_mae: 0.2422 - val_mse: 0.2166 - learning_rate: 0.0200 - val_custom_mse: 0.9586 - val_custom_mae: 0.6606\n",
            "Epoch 65/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1328 - mae: 0.2278 - mse: 0.1328 - val_loss: 0.2165 - val_mae: 0.2422 - val_mse: 0.2165 - learning_rate: 0.0200 - val_custom_mse: 0.9576 - val_custom_mae: 0.6603\n",
            "Epoch 66/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1328 - mae: 0.2278 - mse: 0.1328 - val_loss: 0.2166 - val_mae: 0.2423 - val_mse: 0.2166 - learning_rate: 0.0200 - val_custom_mse: 0.9581 - val_custom_mae: 0.6607\n",
            "Epoch 67/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1328 - mae: 0.2278 - mse: 0.1328 - val_loss: 0.2166 - val_mae: 0.2422 - val_mse: 0.2166 - learning_rate: 0.0200 - val_custom_mse: 0.9582 - val_custom_mae: 0.6605\n",
            "Epoch 68/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2277 - mse: 0.1327 - val_loss: 0.2165 - val_mae: 0.2421 - val_mse: 0.2165 - learning_rate: 0.0200 - val_custom_mse: 0.9582 - val_custom_mae: 0.6606\n",
            "Epoch 69/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1328 - mae: 0.2277 - mse: 0.1328 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 0.0200 - val_custom_mse: 0.9583 - val_custom_mae: 0.6606\n",
            "Epoch 70/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2277 - mse: 0.1327 - val_loss: 0.2165 - val_mae: 0.2422 - val_mse: 0.2165 - learning_rate: 0.0200 - val_custom_mse: 0.9577 - val_custom_mae: 0.6605\n",
            "Epoch 71/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2276 - mse: 0.1327 - val_loss: 0.2165 - val_mae: 0.2421 - val_mse: 0.2165 - learning_rate: 0.0200 - val_custom_mse: 0.9580 - val_custom_mae: 0.6606\n",
            "Epoch 72/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2165 - val_mae: 0.2422 - val_mse: 0.2165 - learning_rate: 0.0200 - val_custom_mse: 0.9580 - val_custom_mae: 0.6605\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2165 - val_mae: 0.2422 - val_mse: 0.2165 - learning_rate: 0.0200 - val_custom_mse: 0.9580 - val_custom_mae: 0.6606\n",
            "Epoch 74/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 0.0040 - val_custom_mse: 0.9585 - val_custom_mae: 0.6606\n",
            "Epoch 75/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2277 - mse: 0.1327 - val_loss: 0.2165 - val_mae: 0.2421 - val_mse: 0.2165 - learning_rate: 0.0040 - val_custom_mse: 0.9582 - val_custom_mae: 0.6606\n",
            "Epoch 76/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2165 - val_mae: 0.2421 - val_mse: 0.2165 - learning_rate: 0.0040 - val_custom_mse: 0.9581 - val_custom_mae: 0.6606\n",
            "Epoch 77/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1328 - mae: 0.2277 - mse: 0.1328 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 0.0040 - val_custom_mse: 0.9586 - val_custom_mae: 0.6607\n",
            "Epoch 78/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2276 - mse: 0.1327 - val_loss: 0.2165 - val_mae: 0.2421 - val_mse: 0.2165 - learning_rate: 0.0040 - val_custom_mse: 0.9582 - val_custom_mae: 0.6606\n",
            "Epoch 79/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2165 - val_mae: 0.2421 - val_mse: 0.2165 - learning_rate: 0.0040 - val_custom_mse: 0.9582 - val_custom_mae: 0.6606\n",
            "Epoch 80/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2165 - val_mae: 0.2421 - val_mse: 0.2165 - learning_rate: 0.0040 - val_custom_mse: 0.9584 - val_custom_mae: 0.6606\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2165 - val_mae: 0.2421 - val_mse: 0.2165 - learning_rate: 0.0040 - val_custom_mse: 0.9582 - val_custom_mae: 0.6606\n",
            "Epoch 82/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9586 - val_custom_mae: 0.6607\n",
            "Epoch 83/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2276 - mse: 0.1327 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9588 - val_custom_mae: 0.6607\n",
            "Epoch 84/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9586 - val_custom_mae: 0.6607\n",
            "Epoch 85/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9586 - val_custom_mae: 0.6607\n",
            "Epoch 86/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9588 - val_custom_mae: 0.6607\n",
            "Epoch 87/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9588 - val_custom_mae: 0.6607\n",
            "Epoch 88/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9587 - val_custom_mae: 0.6607\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 8.0000e-04 - val_custom_mse: 0.9587 - val_custom_mae: 0.6607\n",
            "Epoch 90/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 1.6000e-04 - val_custom_mse: 0.9587 - val_custom_mae: 0.6607\n",
            "Epoch 91/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 1.6000e-04 - val_custom_mse: 0.9587 - val_custom_mae: 0.6607\n",
            "Epoch 92/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2277 - mse: 0.1327 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 1.6000e-04 - val_custom_mse: 0.9588 - val_custom_mae: 0.6607\n",
            "Epoch 93/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2276 - mse: 0.1327 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 1.6000e-04 - val_custom_mse: 0.9588 - val_custom_mae: 0.6607\n",
            "Epoch 94/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2276 - mse: 0.1327 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 1.6000e-04 - val_custom_mse: 0.9588 - val_custom_mae: 0.6607\n",
            "Epoch 95/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 1.6000e-04 - val_custom_mse: 0.9588 - val_custom_mae: 0.6607\n",
            "Epoch 96/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 1.6000e-04 - val_custom_mse: 0.9588 - val_custom_mae: 0.6607\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2276 - mse: 0.1327 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 1.6000e-04 - val_custom_mse: 0.9588 - val_custom_mae: 0.6607\n",
            "Epoch 98/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 3.2000e-05 - val_custom_mse: 0.9587 - val_custom_mae: 0.6607\n",
            "Epoch 99/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2276 - mse: 0.1326 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 3.2000e-05 - val_custom_mse: 0.9587 - val_custom_mae: 0.6607\n",
            "Epoch 100/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1327 - mae: 0.2276 - mse: 0.1327 - val_loss: 0.2166 - val_mae: 0.2421 - val_mse: 0.2166 - learning_rate: 3.2000e-05 - val_custom_mse: 0.9587 - val_custom_mae: 0.6607\n",
            "Running experiment: horizon=336, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_25', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228/228 - 5s - 23ms/step - loss: 0.5309 - mae: 0.5203 - mse: 0.5309 - val_loss: 1.1932 - val_mae: 0.7826 - val_mse: 1.1932 - learning_rate: 0.1000 - val_custom_mse: 1.4540 - val_custom_mae: 0.8687\n",
            "Epoch 2/100\n",
            "228/228 - 2s - 9ms/step - loss: 0.4964 - mae: 0.5043 - mse: 0.4964 - val_loss: 1.1065 - val_mae: 0.7535 - val_mse: 1.1065 - learning_rate: 0.1000 - val_custom_mse: 1.4093 - val_custom_mae: 0.8555\n",
            "Epoch 3/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4669 - mae: 0.4882 - mse: 0.4669 - val_loss: 1.0246 - val_mae: 0.7209 - val_mse: 1.0246 - learning_rate: 0.1000 - val_custom_mse: 1.3668 - val_custom_mae: 0.8373\n",
            "Epoch 4/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4346 - mae: 0.4697 - mse: 0.4346 - val_loss: 0.9321 - val_mae: 0.6849 - val_mse: 0.9321 - learning_rate: 0.1000 - val_custom_mse: 1.3337 - val_custom_mae: 0.8262\n",
            "Epoch 5/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3990 - mae: 0.4472 - mse: 0.3990 - val_loss: 0.8366 - val_mae: 0.6410 - val_mse: 0.8366 - learning_rate: 0.1000 - val_custom_mse: 1.3009 - val_custom_mae: 0.8113\n",
            "Epoch 6/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3566 - mae: 0.4192 - mse: 0.3566 - val_loss: 0.7273 - val_mae: 0.5884 - val_mse: 0.7273 - learning_rate: 0.1000 - val_custom_mse: 1.2631 - val_custom_mae: 0.7971\n",
            "Epoch 7/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3140 - mae: 0.3883 - mse: 0.3140 - val_loss: 0.6356 - val_mae: 0.5362 - val_mse: 0.6356 - learning_rate: 0.1000 - val_custom_mse: 1.2329 - val_custom_mae: 0.7827\n",
            "Epoch 8/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2757 - mae: 0.3572 - mse: 0.2757 - val_loss: 0.5573 - val_mae: 0.4856 - val_mse: 0.5573 - learning_rate: 0.1000 - val_custom_mse: 1.2069 - val_custom_mae: 0.7721\n",
            "Epoch 9/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2453 - mae: 0.3295 - mse: 0.2453 - val_loss: 0.5084 - val_mae: 0.4478 - val_mse: 0.5084 - learning_rate: 0.1000 - val_custom_mse: 1.1915 - val_custom_mae: 0.7628\n",
            "Epoch 10/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2234 - mae: 0.3074 - mse: 0.2234 - val_loss: 0.4762 - val_mae: 0.4191 - val_mse: 0.4762 - learning_rate: 0.1000 - val_custom_mse: 1.1809 - val_custom_mae: 0.7565\n",
            "Epoch 11/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2086 - mae: 0.2910 - mse: 0.2086 - val_loss: 0.4587 - val_mae: 0.4011 - val_mse: 0.4587 - learning_rate: 0.1000 - val_custom_mse: 1.1767 - val_custom_mae: 0.7521\n",
            "Epoch 12/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1986 - mae: 0.2790 - mse: 0.1986 - val_loss: 0.4481 - val_mae: 0.3890 - val_mse: 0.4481 - learning_rate: 0.1000 - val_custom_mse: 1.1749 - val_custom_mae: 0.7492\n",
            "Epoch 13/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1916 - mae: 0.2699 - mse: 0.1916 - val_loss: 0.4440 - val_mae: 0.3820 - val_mse: 0.4440 - learning_rate: 0.1000 - val_custom_mse: 1.1798 - val_custom_mae: 0.7483\n",
            "Epoch 14/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1864 - mae: 0.2627 - mse: 0.1864 - val_loss: 0.4375 - val_mae: 0.3737 - val_mse: 0.4375 - learning_rate: 0.1000 - val_custom_mse: 1.1779 - val_custom_mae: 0.7466\n",
            "Epoch 15/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1822 - mae: 0.2566 - mse: 0.1822 - val_loss: 0.4350 - val_mae: 0.3687 - val_mse: 0.4350 - learning_rate: 0.1000 - val_custom_mse: 1.1817 - val_custom_mae: 0.7463\n",
            "Epoch 16/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1788 - mae: 0.2512 - mse: 0.1788 - val_loss: 0.4304 - val_mae: 0.3624 - val_mse: 0.4304 - learning_rate: 0.1000 - val_custom_mse: 1.1802 - val_custom_mae: 0.7454\n",
            "Epoch 17/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1758 - mae: 0.2465 - mse: 0.1758 - val_loss: 0.4285 - val_mae: 0.3580 - val_mse: 0.4285 - learning_rate: 0.1000 - val_custom_mse: 1.1831 - val_custom_mae: 0.7454\n",
            "Epoch 18/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1732 - mae: 0.2422 - mse: 0.1732 - val_loss: 0.4262 - val_mae: 0.3536 - val_mse: 0.4262 - learning_rate: 0.1000 - val_custom_mse: 1.1844 - val_custom_mae: 0.7452\n",
            "Epoch 19/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1709 - mae: 0.2382 - mse: 0.1709 - val_loss: 0.4230 - val_mae: 0.3488 - val_mse: 0.4230 - learning_rate: 0.1000 - val_custom_mse: 1.1830 - val_custom_mae: 0.7445\n",
            "Epoch 20/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1688 - mae: 0.2345 - mse: 0.1688 - val_loss: 0.4206 - val_mae: 0.3445 - val_mse: 0.4206 - learning_rate: 0.1000 - val_custom_mse: 1.1827 - val_custom_mae: 0.7444\n",
            "Epoch 21/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1669 - mae: 0.2311 - mse: 0.1669 - val_loss: 0.4199 - val_mae: 0.3415 - val_mse: 0.4199 - learning_rate: 0.1000 - val_custom_mse: 1.1859 - val_custom_mae: 0.7447\n",
            "Epoch 22/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1652 - mae: 0.2279 - mse: 0.1652 - val_loss: 0.4184 - val_mae: 0.3383 - val_mse: 0.4184 - learning_rate: 0.1000 - val_custom_mse: 1.1868 - val_custom_mae: 0.7446\n",
            "Epoch 23/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1636 - mae: 0.2250 - mse: 0.1636 - val_loss: 0.4178 - val_mae: 0.3357 - val_mse: 0.4178 - learning_rate: 0.1000 - val_custom_mse: 1.1895 - val_custom_mae: 0.7450\n",
            "Epoch 24/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1622 - mae: 0.2222 - mse: 0.1622 - val_loss: 0.4163 - val_mae: 0.3325 - val_mse: 0.4163 - learning_rate: 0.1000 - val_custom_mse: 1.1898 - val_custom_mae: 0.7452\n",
            "Epoch 25/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1609 - mae: 0.2196 - mse: 0.1609 - val_loss: 0.4168 - val_mae: 0.3316 - val_mse: 0.4168 - learning_rate: 0.1000 - val_custom_mse: 1.1939 - val_custom_mae: 0.7461\n",
            "Epoch 26/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1597 - mae: 0.2172 - mse: 0.1597 - val_loss: 0.4118 - val_mae: 0.3260 - val_mse: 0.4118 - learning_rate: 0.1000 - val_custom_mse: 1.1853 - val_custom_mae: 0.7438\n",
            "Epoch 27/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1585 - mae: 0.2150 - mse: 0.1585 - val_loss: 0.4134 - val_mae: 0.3256 - val_mse: 0.4134 - learning_rate: 0.1000 - val_custom_mse: 1.1922 - val_custom_mae: 0.7455\n",
            "Epoch 28/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1575 - mae: 0.2129 - mse: 0.1575 - val_loss: 0.4099 - val_mae: 0.3211 - val_mse: 0.4099 - learning_rate: 0.1000 - val_custom_mse: 1.1866 - val_custom_mae: 0.7441\n",
            "Epoch 29/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1565 - mae: 0.2108 - mse: 0.1565 - val_loss: 0.4095 - val_mae: 0.3189 - val_mse: 0.4095 - learning_rate: 0.1000 - val_custom_mse: 1.1888 - val_custom_mae: 0.7444\n",
            "Epoch 30/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1556 - mae: 0.2090 - mse: 0.1556 - val_loss: 0.4092 - val_mae: 0.3176 - val_mse: 0.4092 - learning_rate: 0.1000 - val_custom_mse: 1.1901 - val_custom_mae: 0.7449\n",
            "Epoch 31/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1548 - mae: 0.2072 - mse: 0.1548 - val_loss: 0.4058 - val_mae: 0.3143 - val_mse: 0.4058 - learning_rate: 0.1000 - val_custom_mse: 1.1830 - val_custom_mae: 0.7431\n",
            "Epoch 32/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1540 - mae: 0.2055 - mse: 0.1540 - val_loss: 0.4064 - val_mae: 0.3129 - val_mse: 0.4064 - learning_rate: 0.1000 - val_custom_mse: 1.1872 - val_custom_mae: 0.7441\n",
            "Epoch 33/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1533 - mae: 0.2039 - mse: 0.1533 - val_loss: 0.4045 - val_mae: 0.3106 - val_mse: 0.4045 - learning_rate: 0.1000 - val_custom_mse: 1.1841 - val_custom_mae: 0.7433\n",
            "Epoch 34/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1526 - mae: 0.2024 - mse: 0.1526 - val_loss: 0.4061 - val_mae: 0.3101 - val_mse: 0.4061 - learning_rate: 0.1000 - val_custom_mse: 1.1906 - val_custom_mae: 0.7448\n",
            "Epoch 35/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1519 - mae: 0.2010 - mse: 0.1519 - val_loss: 0.4041 - val_mae: 0.3076 - val_mse: 0.4041 - learning_rate: 0.1000 - val_custom_mse: 1.1871 - val_custom_mae: 0.7440\n",
            "Epoch 36/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1513 - mae: 0.1996 - mse: 0.1513 - val_loss: 0.4041 - val_mae: 0.3064 - val_mse: 0.4041 - learning_rate: 0.1000 - val_custom_mse: 1.1889 - val_custom_mae: 0.7444\n",
            "Epoch 37/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1507 - mae: 0.1984 - mse: 0.1507 - val_loss: 0.4037 - val_mae: 0.3048 - val_mse: 0.4037 - learning_rate: 0.1000 - val_custom_mse: 1.1899 - val_custom_mae: 0.7446\n",
            "Epoch 38/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1502 - mae: 0.1971 - mse: 0.1502 - val_loss: 0.4033 - val_mae: 0.3038 - val_mse: 0.4033 - learning_rate: 0.1000 - val_custom_mse: 1.1901 - val_custom_mae: 0.7449\n",
            "Epoch 39/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1497 - mae: 0.1961 - mse: 0.1497 - val_loss: 0.4049 - val_mae: 0.3037 - val_mse: 0.4049 - learning_rate: 0.1000 - val_custom_mse: 1.1958 - val_custom_mae: 0.7462\n",
            "Epoch 40/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1492 - mae: 0.1949 - mse: 0.1492 - val_loss: 0.4036 - val_mae: 0.3016 - val_mse: 0.4036 - learning_rate: 0.1000 - val_custom_mse: 1.1939 - val_custom_mae: 0.7459\n",
            "Epoch 41/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1488 - mae: 0.1938 - mse: 0.1488 - val_loss: 0.4037 - val_mae: 0.3013 - val_mse: 0.4037 - learning_rate: 0.1000 - val_custom_mse: 1.1949 - val_custom_mae: 0.7461\n",
            "Epoch 42/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1483 - mae: 0.1928 - mse: 0.1483 - val_loss: 0.4016 - val_mae: 0.2988 - val_mse: 0.4016 - learning_rate: 0.1000 - val_custom_mse: 1.1909 - val_custom_mae: 0.7450\n",
            "Epoch 43/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1480 - mae: 0.1919 - mse: 0.1480 - val_loss: 0.4009 - val_mae: 0.2983 - val_mse: 0.4009 - learning_rate: 0.1000 - val_custom_mse: 1.1898 - val_custom_mae: 0.7450\n",
            "Epoch 44/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1476 - mae: 0.1910 - mse: 0.1476 - val_loss: 0.4017 - val_mae: 0.2970 - val_mse: 0.4017 - learning_rate: 0.1000 - val_custom_mse: 1.1938 - val_custom_mae: 0.7457\n",
            "Epoch 45/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1472 - mae: 0.1900 - mse: 0.1472 - val_loss: 0.4024 - val_mae: 0.2971 - val_mse: 0.4024 - learning_rate: 0.1000 - val_custom_mse: 1.1965 - val_custom_mae: 0.7465\n",
            "Epoch 46/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1468 - mae: 0.1892 - mse: 0.1468 - val_loss: 0.4004 - val_mae: 0.2953 - val_mse: 0.4004 - learning_rate: 0.1000 - val_custom_mse: 1.1917 - val_custom_mae: 0.7456\n",
            "Epoch 47/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1465 - mae: 0.1884 - mse: 0.1465 - val_loss: 0.3999 - val_mae: 0.2946 - val_mse: 0.3999 - learning_rate: 0.1000 - val_custom_mse: 1.1914 - val_custom_mae: 0.7455\n",
            "Epoch 48/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1462 - mae: 0.1876 - mse: 0.1462 - val_loss: 0.4006 - val_mae: 0.2935 - val_mse: 0.4006 - learning_rate: 0.1000 - val_custom_mse: 1.1946 - val_custom_mae: 0.7463\n",
            "Epoch 49/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1459 - mae: 0.1868 - mse: 0.1459 - val_loss: 0.3996 - val_mae: 0.2919 - val_mse: 0.3996 - learning_rate: 0.1000 - val_custom_mse: 1.1930 - val_custom_mae: 0.7461\n",
            "Epoch 50/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1456 - mae: 0.1861 - mse: 0.1456 - val_loss: 0.3999 - val_mae: 0.2918 - val_mse: 0.3999 - learning_rate: 0.1000 - val_custom_mse: 1.1944 - val_custom_mae: 0.7463\n",
            "Epoch 51/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1453 - mae: 0.1855 - mse: 0.1453 - val_loss: 0.3989 - val_mae: 0.2907 - val_mse: 0.3989 - learning_rate: 0.1000 - val_custom_mse: 1.1925 - val_custom_mae: 0.7459\n",
            "Epoch 52/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1450 - mae: 0.1848 - mse: 0.1450 - val_loss: 0.4009 - val_mae: 0.2912 - val_mse: 0.4009 - learning_rate: 0.1000 - val_custom_mse: 1.1988 - val_custom_mae: 0.7476\n",
            "Epoch 53/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1448 - mae: 0.1841 - mse: 0.1448 - val_loss: 0.4008 - val_mae: 0.2910 - val_mse: 0.4008 - learning_rate: 0.1000 - val_custom_mse: 1.1986 - val_custom_mae: 0.7474\n",
            "Epoch 54/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1446 - mae: 0.1836 - mse: 0.1446 - val_loss: 0.3990 - val_mae: 0.2889 - val_mse: 0.3990 - learning_rate: 0.1000 - val_custom_mse: 1.1948 - val_custom_mae: 0.7466\n",
            "Epoch 55/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1443 - mae: 0.1830 - mse: 0.1443 - val_loss: 0.3992 - val_mae: 0.2882 - val_mse: 0.3992 - learning_rate: 0.1000 - val_custom_mse: 1.1961 - val_custom_mae: 0.7471\n",
            "Epoch 56/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1441 - mae: 0.1824 - mse: 0.1441 - val_loss: 0.3998 - val_mae: 0.2888 - val_mse: 0.3998 - learning_rate: 0.1000 - val_custom_mse: 1.1981 - val_custom_mae: 0.7475\n",
            "Epoch 57/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1439 - mae: 0.1818 - mse: 0.1439 - val_loss: 0.3982 - val_mae: 0.2864 - val_mse: 0.3982 - learning_rate: 0.1000 - val_custom_mse: 1.1948 - val_custom_mae: 0.7467\n",
            "Epoch 58/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1437 - mae: 0.1812 - mse: 0.1437 - val_loss: 0.3989 - val_mae: 0.2870 - val_mse: 0.3989 - learning_rate: 0.1000 - val_custom_mse: 1.1971 - val_custom_mae: 0.7473\n",
            "Epoch 59/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1435 - mae: 0.1807 - mse: 0.1435 - val_loss: 0.3982 - val_mae: 0.2861 - val_mse: 0.3982 - learning_rate: 0.1000 - val_custom_mse: 1.1956 - val_custom_mae: 0.7472\n",
            "Epoch 60/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1433 - mae: 0.1801 - mse: 0.1433 - val_loss: 0.3983 - val_mae: 0.2854 - val_mse: 0.3983 - learning_rate: 0.1000 - val_custom_mse: 1.1965 - val_custom_mae: 0.7474\n",
            "Epoch 61/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1431 - mae: 0.1797 - mse: 0.1431 - val_loss: 0.3999 - val_mae: 0.2856 - val_mse: 0.3999 - learning_rate: 0.1000 - val_custom_mse: 1.2016 - val_custom_mae: 0.7486\n",
            "Epoch 62/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1429 - mae: 0.1793 - mse: 0.1429 - val_loss: 0.4009 - val_mae: 0.2858 - val_mse: 0.4009 - learning_rate: 0.1000 - val_custom_mse: 1.2047 - val_custom_mae: 0.7497\n",
            "Epoch 63/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1428 - mae: 0.1788 - mse: 0.1428 - val_loss: 0.3997 - val_mae: 0.2848 - val_mse: 0.3997 - learning_rate: 0.1000 - val_custom_mse: 1.2017 - val_custom_mae: 0.7487\n",
            "Epoch 64/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1426 - mae: 0.1784 - mse: 0.1426 - val_loss: 0.3997 - val_mae: 0.2845 - val_mse: 0.3997 - learning_rate: 0.1000 - val_custom_mse: 1.2024 - val_custom_mae: 0.7490\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1425 - mae: 0.1779 - mse: 0.1425 - val_loss: 0.3982 - val_mae: 0.2831 - val_mse: 0.3982 - learning_rate: 0.1000 - val_custom_mse: 1.1985 - val_custom_mae: 0.7481\n",
            "Epoch 66/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1423 - mae: 0.1776 - mse: 0.1423 - val_loss: 0.3977 - val_mae: 0.2825 - val_mse: 0.3977 - learning_rate: 0.0200 - val_custom_mse: 1.1972 - val_custom_mae: 0.7475\n",
            "Epoch 67/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1423 - mae: 0.1773 - mse: 0.1423 - val_loss: 0.3982 - val_mae: 0.2825 - val_mse: 0.3982 - learning_rate: 0.0200 - val_custom_mse: 1.1990 - val_custom_mae: 0.7479\n",
            "Epoch 68/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1422 - mae: 0.1772 - mse: 0.1422 - val_loss: 0.3976 - val_mae: 0.2823 - val_mse: 0.3976 - learning_rate: 0.0200 - val_custom_mse: 1.1974 - val_custom_mae: 0.7475\n",
            "Epoch 69/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1422 - mae: 0.1772 - mse: 0.1422 - val_loss: 0.3981 - val_mae: 0.2824 - val_mse: 0.3981 - learning_rate: 0.0200 - val_custom_mse: 1.1989 - val_custom_mae: 0.7480\n",
            "Epoch 70/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1422 - mae: 0.1771 - mse: 0.1422 - val_loss: 0.3971 - val_mae: 0.2820 - val_mse: 0.3971 - learning_rate: 0.0200 - val_custom_mse: 1.1958 - val_custom_mae: 0.7472\n",
            "Epoch 71/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1421 - mae: 0.1770 - mse: 0.1421 - val_loss: 0.3978 - val_mae: 0.2821 - val_mse: 0.3978 - learning_rate: 0.0200 - val_custom_mse: 1.1982 - val_custom_mae: 0.7478\n",
            "Epoch 72/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1421 - mae: 0.1769 - mse: 0.1421 - val_loss: 0.3973 - val_mae: 0.2817 - val_mse: 0.3973 - learning_rate: 0.0200 - val_custom_mse: 1.1969 - val_custom_mae: 0.7475\n",
            "Epoch 73/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1421 - mae: 0.1768 - mse: 0.1421 - val_loss: 0.3977 - val_mae: 0.2820 - val_mse: 0.3977 - learning_rate: 0.0200 - val_custom_mse: 1.1980 - val_custom_mae: 0.7477\n",
            "Epoch 74/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1421 - mae: 0.1768 - mse: 0.1421 - val_loss: 0.3974 - val_mae: 0.2816 - val_mse: 0.3974 - learning_rate: 0.0200 - val_custom_mse: 1.1972 - val_custom_mae: 0.7476\n",
            "Epoch 75/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1420 - mae: 0.1767 - mse: 0.1420 - val_loss: 0.3978 - val_mae: 0.2817 - val_mse: 0.3978 - learning_rate: 0.0200 - val_custom_mse: 1.1983 - val_custom_mae: 0.7478\n",
            "Epoch 76/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1420 - mae: 0.1766 - mse: 0.1420 - val_loss: 0.3975 - val_mae: 0.2815 - val_mse: 0.3975 - learning_rate: 0.0200 - val_custom_mse: 1.1976 - val_custom_mae: 0.7477\n",
            "Epoch 77/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1420 - mae: 0.1765 - mse: 0.1420 - val_loss: 0.3975 - val_mae: 0.2816 - val_mse: 0.3975 - learning_rate: 0.0200 - val_custom_mse: 1.1977 - val_custom_mae: 0.7477\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1420 - mae: 0.1764 - mse: 0.1420 - val_loss: 0.3973 - val_mae: 0.2816 - val_mse: 0.3973 - learning_rate: 0.0200 - val_custom_mse: 1.1971 - val_custom_mae: 0.7476\n",
            "Epoch 79/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1765 - mse: 0.1419 - val_loss: 0.3972 - val_mae: 0.2814 - val_mse: 0.3972 - learning_rate: 0.0040 - val_custom_mse: 1.1968 - val_custom_mae: 0.7476\n",
            "Epoch 80/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1764 - mse: 0.1419 - val_loss: 0.3973 - val_mae: 0.2813 - val_mse: 0.3973 - learning_rate: 0.0040 - val_custom_mse: 1.1971 - val_custom_mae: 0.7477\n",
            "Epoch 81/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1763 - mse: 0.1419 - val_loss: 0.3972 - val_mae: 0.2813 - val_mse: 0.3972 - learning_rate: 0.0040 - val_custom_mse: 1.1968 - val_custom_mae: 0.7476\n",
            "Epoch 82/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1763 - mse: 0.1419 - val_loss: 0.3971 - val_mae: 0.2813 - val_mse: 0.3971 - learning_rate: 0.0040 - val_custom_mse: 1.1968 - val_custom_mae: 0.7476\n",
            "Epoch 83/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1763 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2812 - val_mse: 0.3970 - learning_rate: 0.0040 - val_custom_mse: 1.1965 - val_custom_mae: 0.7475\n",
            "Epoch 84/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1763 - mse: 0.1419 - val_loss: 0.3971 - val_mae: 0.2812 - val_mse: 0.3971 - learning_rate: 0.0040 - val_custom_mse: 1.1966 - val_custom_mae: 0.7476\n",
            "Epoch 85/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1763 - mse: 0.1419 - val_loss: 0.3971 - val_mae: 0.2812 - val_mse: 0.3971 - learning_rate: 0.0040 - val_custom_mse: 1.1967 - val_custom_mae: 0.7476\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3971 - val_mae: 0.2812 - val_mse: 0.3971 - learning_rate: 0.0040 - val_custom_mse: 1.1968 - val_custom_mae: 0.7476\n",
            "Epoch 87/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2812 - val_mse: 0.3970 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1965 - val_custom_mae: 0.7475\n",
            "Epoch 88/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1964 - val_custom_mae: 0.7475\n",
            "Epoch 89/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1964 - val_custom_mae: 0.7475\n",
            "Epoch 90/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1964 - val_custom_mae: 0.7475\n",
            "Epoch 91/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1964 - val_custom_mae: 0.7475\n",
            "Epoch 92/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1965 - val_custom_mae: 0.7475\n",
            "Epoch 93/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1964 - val_custom_mae: 0.7475\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1964 - val_custom_mae: 0.7475\n",
            "Epoch 95/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1965 - val_custom_mae: 0.7476\n",
            "Epoch 96/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1965 - val_custom_mae: 0.7476\n",
            "Epoch 97/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1965 - val_custom_mae: 0.7476\n",
            "Epoch 98/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1965 - val_custom_mae: 0.7476\n",
            "Epoch 99/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1966 - val_custom_mae: 0.7476\n",
            "Epoch 100/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1762 - mse: 0.1419 - val_loss: 0.3970 - val_mae: 0.2811 - val_mse: 0.3970 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1965 - val_custom_mae: 0.7476\n",
            "Running experiment: horizon=336, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_26', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228/228 - 6s - 27ms/step - loss: 0.5388 - mae: 0.5249 - mse: 0.5388 - val_loss: 1.1779 - val_mae: 0.7844 - val_mse: 1.1779 - learning_rate: 0.1000 - val_custom_mse: 1.4736 - val_custom_mae: 0.8842\n",
            "Epoch 2/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.5015 - mae: 0.5073 - mse: 0.5015 - val_loss: 1.1011 - val_mae: 0.7529 - val_mse: 1.1011 - learning_rate: 0.1000 - val_custom_mse: 1.4173 - val_custom_mae: 0.8600\n",
            "Epoch 3/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4693 - mae: 0.4896 - mse: 0.4693 - val_loss: 1.0077 - val_mae: 0.7177 - val_mse: 1.0077 - learning_rate: 0.1000 - val_custom_mse: 1.3737 - val_custom_mae: 0.8444\n",
            "Epoch 4/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4331 - mae: 0.4686 - mse: 0.4331 - val_loss: 0.9075 - val_mae: 0.6761 - val_mse: 0.9075 - learning_rate: 0.1000 - val_custom_mse: 1.3278 - val_custom_mae: 0.8263\n",
            "Epoch 5/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3924 - mae: 0.4435 - mse: 0.3924 - val_loss: 0.8019 - val_mae: 0.6277 - val_mse: 0.8019 - learning_rate: 0.1000 - val_custom_mse: 1.2850 - val_custom_mae: 0.8087\n",
            "Epoch 6/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3493 - mae: 0.4144 - mse: 0.3493 - val_loss: 0.7010 - val_mae: 0.5750 - val_mse: 0.7010 - learning_rate: 0.1000 - val_custom_mse: 1.2470 - val_custom_mae: 0.7911\n",
            "Epoch 7/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3080 - mae: 0.3835 - mse: 0.3080 - val_loss: 0.6109 - val_mae: 0.5222 - val_mse: 0.6109 - learning_rate: 0.1000 - val_custom_mse: 1.2185 - val_custom_mae: 0.7788\n",
            "Epoch 8/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2727 - mae: 0.3542 - mse: 0.2727 - val_loss: 0.5445 - val_mae: 0.4767 - val_mse: 0.5445 - learning_rate: 0.1000 - val_custom_mse: 1.1960 - val_custom_mae: 0.7672\n",
            "Epoch 9/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2460 - mae: 0.3296 - mse: 0.2460 - val_loss: 0.5014 - val_mae: 0.4424 - val_mse: 0.5014 - learning_rate: 0.1000 - val_custom_mse: 1.1841 - val_custom_mae: 0.7592\n",
            "Epoch 10/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2275 - mae: 0.3110 - mse: 0.2275 - val_loss: 0.4750 - val_mae: 0.4181 - val_mse: 0.4750 - learning_rate: 0.1000 - val_custom_mse: 1.1787 - val_custom_mae: 0.7542\n",
            "Epoch 11/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2152 - mae: 0.2977 - mse: 0.2152 - val_loss: 0.4577 - val_mae: 0.4004 - val_mse: 0.4577 - learning_rate: 0.1000 - val_custom_mse: 1.1741 - val_custom_mae: 0.7504\n",
            "Epoch 12/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2070 - mae: 0.2882 - mse: 0.2070 - val_loss: 0.4493 - val_mae: 0.3898 - val_mse: 0.4493 - learning_rate: 0.1000 - val_custom_mse: 1.1760 - val_custom_mae: 0.7486\n",
            "Epoch 13/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2014 - mae: 0.2812 - mse: 0.2014 - val_loss: 0.4425 - val_mae: 0.3810 - val_mse: 0.4425 - learning_rate: 0.1000 - val_custom_mse: 1.1773 - val_custom_mae: 0.7478\n",
            "Epoch 14/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1970 - mae: 0.2756 - mse: 0.1970 - val_loss: 0.4370 - val_mae: 0.3740 - val_mse: 0.4370 - learning_rate: 0.1000 - val_custom_mse: 1.1760 - val_custom_mae: 0.7462\n",
            "Epoch 15/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1937 - mae: 0.2710 - mse: 0.1937 - val_loss: 0.4328 - val_mae: 0.3681 - val_mse: 0.4328 - learning_rate: 0.1000 - val_custom_mse: 1.1749 - val_custom_mae: 0.7446\n",
            "Epoch 16/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1909 - mae: 0.2670 - mse: 0.1909 - val_loss: 0.4315 - val_mae: 0.3639 - val_mse: 0.4315 - learning_rate: 0.1000 - val_custom_mse: 1.1807 - val_custom_mae: 0.7455\n",
            "Epoch 17/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1885 - mae: 0.2635 - mse: 0.1885 - val_loss: 0.4307 - val_mae: 0.3603 - val_mse: 0.4307 - learning_rate: 0.1000 - val_custom_mse: 1.1863 - val_custom_mae: 0.7465\n",
            "Epoch 18/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1864 - mae: 0.2603 - mse: 0.1864 - val_loss: 0.4282 - val_mae: 0.3564 - val_mse: 0.4282 - learning_rate: 0.1000 - val_custom_mse: 1.1860 - val_custom_mae: 0.7459\n",
            "Epoch 19/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1846 - mae: 0.2575 - mse: 0.1846 - val_loss: 0.4263 - val_mae: 0.3524 - val_mse: 0.4263 - learning_rate: 0.1000 - val_custom_mse: 1.1876 - val_custom_mae: 0.7461\n",
            "Epoch 20/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1830 - mae: 0.2549 - mse: 0.1830 - val_loss: 0.4207 - val_mae: 0.3475 - val_mse: 0.4207 - learning_rate: 0.1000 - val_custom_mse: 1.1780 - val_custom_mae: 0.7431\n",
            "Epoch 21/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1815 - mae: 0.2525 - mse: 0.1815 - val_loss: 0.4198 - val_mae: 0.3443 - val_mse: 0.4198 - learning_rate: 0.1000 - val_custom_mse: 1.1812 - val_custom_mae: 0.7438\n",
            "Epoch 22/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1803 - mae: 0.2504 - mse: 0.1803 - val_loss: 0.4194 - val_mae: 0.3421 - val_mse: 0.4194 - learning_rate: 0.1000 - val_custom_mse: 1.1841 - val_custom_mae: 0.7445\n",
            "Epoch 23/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1791 - mae: 0.2484 - mse: 0.1791 - val_loss: 0.4161 - val_mae: 0.3386 - val_mse: 0.4161 - learning_rate: 0.1000 - val_custom_mse: 1.1797 - val_custom_mae: 0.7433\n",
            "Epoch 24/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1780 - mae: 0.2466 - mse: 0.1780 - val_loss: 0.4156 - val_mae: 0.3364 - val_mse: 0.4156 - learning_rate: 0.1000 - val_custom_mse: 1.1819 - val_custom_mae: 0.7437\n",
            "Epoch 25/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1770 - mae: 0.2449 - mse: 0.1770 - val_loss: 0.4145 - val_mae: 0.3341 - val_mse: 0.4145 - learning_rate: 0.1000 - val_custom_mse: 1.1825 - val_custom_mae: 0.7438\n",
            "Epoch 26/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1762 - mae: 0.2433 - mse: 0.1762 - val_loss: 0.4144 - val_mae: 0.3327 - val_mse: 0.4144 - learning_rate: 0.1000 - val_custom_mse: 1.1844 - val_custom_mae: 0.7439\n",
            "Epoch 27/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1754 - mae: 0.2420 - mse: 0.1754 - val_loss: 0.4108 - val_mae: 0.3300 - val_mse: 0.4108 - learning_rate: 0.1000 - val_custom_mse: 1.1773 - val_custom_mae: 0.7420\n",
            "Epoch 28/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1746 - mae: 0.2406 - mse: 0.1746 - val_loss: 0.4105 - val_mae: 0.3285 - val_mse: 0.4105 - learning_rate: 0.1000 - val_custom_mse: 1.1791 - val_custom_mae: 0.7424\n",
            "Epoch 29/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1740 - mae: 0.2394 - mse: 0.1740 - val_loss: 0.4112 - val_mae: 0.3269 - val_mse: 0.4112 - learning_rate: 0.1000 - val_custom_mse: 1.1844 - val_custom_mae: 0.7442\n",
            "Epoch 30/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1733 - mae: 0.2382 - mse: 0.1733 - val_loss: 0.4106 - val_mae: 0.3257 - val_mse: 0.4106 - learning_rate: 0.1000 - val_custom_mse: 1.1844 - val_custom_mae: 0.7439\n",
            "Epoch 31/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1727 - mae: 0.2372 - mse: 0.1727 - val_loss: 0.4085 - val_mae: 0.3238 - val_mse: 0.4085 - learning_rate: 0.1000 - val_custom_mse: 1.1805 - val_custom_mae: 0.7429\n",
            "Epoch 32/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1723 - mae: 0.2362 - mse: 0.1723 - val_loss: 0.4093 - val_mae: 0.3230 - val_mse: 0.4093 - learning_rate: 0.1000 - val_custom_mse: 1.1846 - val_custom_mae: 0.7437\n",
            "Epoch 33/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1717 - mae: 0.2352 - mse: 0.1717 - val_loss: 0.4096 - val_mae: 0.3220 - val_mse: 0.4096 - learning_rate: 0.1000 - val_custom_mse: 1.1877 - val_custom_mae: 0.7445\n",
            "Epoch 34/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1712 - mae: 0.2344 - mse: 0.1712 - val_loss: 0.4086 - val_mae: 0.3207 - val_mse: 0.4086 - learning_rate: 0.1000 - val_custom_mse: 1.1862 - val_custom_mae: 0.7440\n",
            "Epoch 35/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1708 - mae: 0.2336 - mse: 0.1708 - val_loss: 0.4062 - val_mae: 0.3194 - val_mse: 0.4062 - learning_rate: 0.1000 - val_custom_mse: 1.1808 - val_custom_mae: 0.7430\n",
            "Epoch 36/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1704 - mae: 0.2329 - mse: 0.1704 - val_loss: 0.4074 - val_mae: 0.3184 - val_mse: 0.4074 - learning_rate: 0.1000 - val_custom_mse: 1.1864 - val_custom_mae: 0.7445\n",
            "Epoch 37/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1701 - mae: 0.2322 - mse: 0.1701 - val_loss: 0.4067 - val_mae: 0.3175 - val_mse: 0.4067 - learning_rate: 0.1000 - val_custom_mse: 1.1854 - val_custom_mae: 0.7439\n",
            "Epoch 38/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1697 - mae: 0.2314 - mse: 0.1697 - val_loss: 0.4073 - val_mae: 0.3169 - val_mse: 0.4073 - learning_rate: 0.1000 - val_custom_mse: 1.1883 - val_custom_mae: 0.7447\n",
            "Epoch 39/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1693 - mae: 0.2308 - mse: 0.1693 - val_loss: 0.4061 - val_mae: 0.3159 - val_mse: 0.4061 - learning_rate: 0.1000 - val_custom_mse: 1.1860 - val_custom_mae: 0.7442\n",
            "Epoch 40/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1690 - mae: 0.2302 - mse: 0.1690 - val_loss: 0.4053 - val_mae: 0.3150 - val_mse: 0.4053 - learning_rate: 0.1000 - val_custom_mse: 1.1850 - val_custom_mae: 0.7442\n",
            "Epoch 41/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1688 - mae: 0.2297 - mse: 0.1688 - val_loss: 0.4067 - val_mae: 0.3146 - val_mse: 0.4067 - learning_rate: 0.1000 - val_custom_mse: 1.1903 - val_custom_mae: 0.7453\n",
            "Epoch 42/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1685 - mae: 0.2292 - mse: 0.1685 - val_loss: 0.4051 - val_mae: 0.3138 - val_mse: 0.4051 - learning_rate: 0.1000 - val_custom_mse: 1.1863 - val_custom_mae: 0.7443\n",
            "Epoch 43/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1683 - mae: 0.2287 - mse: 0.1683 - val_loss: 0.4053 - val_mae: 0.3132 - val_mse: 0.4053 - learning_rate: 0.1000 - val_custom_mse: 1.1877 - val_custom_mae: 0.7446\n",
            "Epoch 44/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1680 - mae: 0.2282 - mse: 0.1680 - val_loss: 0.4041 - val_mae: 0.3131 - val_mse: 0.4041 - learning_rate: 0.1000 - val_custom_mse: 1.1842 - val_custom_mae: 0.7439\n",
            "Epoch 45/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1677 - mae: 0.2278 - mse: 0.1677 - val_loss: 0.4058 - val_mae: 0.3125 - val_mse: 0.4058 - learning_rate: 0.1000 - val_custom_mse: 1.1909 - val_custom_mae: 0.7457\n",
            "Epoch 46/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1675 - mae: 0.2274 - mse: 0.1675 - val_loss: 0.4056 - val_mae: 0.3127 - val_mse: 0.4056 - learning_rate: 0.1000 - val_custom_mse: 1.1902 - val_custom_mae: 0.7454\n",
            "Epoch 47/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1673 - mae: 0.2271 - mse: 0.1673 - val_loss: 0.4038 - val_mae: 0.3115 - val_mse: 0.4038 - learning_rate: 0.1000 - val_custom_mse: 1.1859 - val_custom_mae: 0.7446\n",
            "Epoch 48/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1671 - mae: 0.2267 - mse: 0.1671 - val_loss: 0.4039 - val_mae: 0.3110 - val_mse: 0.4039 - learning_rate: 0.1000 - val_custom_mse: 1.1870 - val_custom_mae: 0.7447\n",
            "Epoch 49/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1670 - mae: 0.2265 - mse: 0.1670 - val_loss: 0.4048 - val_mae: 0.3105 - val_mse: 0.4048 - learning_rate: 0.1000 - val_custom_mse: 1.1904 - val_custom_mae: 0.7456\n",
            "Epoch 50/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1668 - mae: 0.2260 - mse: 0.1668 - val_loss: 0.4057 - val_mae: 0.3107 - val_mse: 0.4057 - learning_rate: 0.1000 - val_custom_mse: 1.1935 - val_custom_mae: 0.7464\n",
            "Epoch 51/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1666 - mae: 0.2257 - mse: 0.1666 - val_loss: 0.4065 - val_mae: 0.3110 - val_mse: 0.4065 - learning_rate: 0.1000 - val_custom_mse: 1.1959 - val_custom_mae: 0.7470\n",
            "Epoch 52/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1665 - mae: 0.2254 - mse: 0.1665 - val_loss: 0.4040 - val_mae: 0.3098 - val_mse: 0.4040 - learning_rate: 0.1000 - val_custom_mse: 1.1894 - val_custom_mae: 0.7454\n",
            "Epoch 53/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1664 - mae: 0.2252 - mse: 0.1664 - val_loss: 0.4028 - val_mae: 0.3094 - val_mse: 0.4028 - learning_rate: 0.1000 - val_custom_mse: 1.1862 - val_custom_mae: 0.7449\n",
            "Epoch 54/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1661 - mae: 0.2249 - mse: 0.1661 - val_loss: 0.4041 - val_mae: 0.3087 - val_mse: 0.4041 - learning_rate: 0.1000 - val_custom_mse: 1.1911 - val_custom_mae: 0.7460\n",
            "Epoch 55/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1661 - mae: 0.2247 - mse: 0.1661 - val_loss: 0.4033 - val_mae: 0.3085 - val_mse: 0.4033 - learning_rate: 0.1000 - val_custom_mse: 1.1890 - val_custom_mae: 0.7454\n",
            "Epoch 56/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1660 - mae: 0.2246 - mse: 0.1660 - val_loss: 0.4041 - val_mae: 0.3082 - val_mse: 0.4041 - learning_rate: 0.1000 - val_custom_mse: 1.1919 - val_custom_mae: 0.7465\n",
            "Epoch 57/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1658 - mae: 0.2243 - mse: 0.1658 - val_loss: 0.4026 - val_mae: 0.3087 - val_mse: 0.4026 - learning_rate: 0.1000 - val_custom_mse: 1.1870 - val_custom_mae: 0.7453\n",
            "Epoch 58/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1657 - mae: 0.2241 - mse: 0.1657 - val_loss: 0.4039 - val_mae: 0.3080 - val_mse: 0.4039 - learning_rate: 0.1000 - val_custom_mse: 1.1917 - val_custom_mae: 0.7462\n",
            "Epoch 59/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1655 - mae: 0.2239 - mse: 0.1655 - val_loss: 0.4028 - val_mae: 0.3082 - val_mse: 0.4028 - learning_rate: 0.1000 - val_custom_mse: 1.1884 - val_custom_mae: 0.7459\n",
            "Epoch 60/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1654 - mae: 0.2237 - mse: 0.1654 - val_loss: 0.4047 - val_mae: 0.3081 - val_mse: 0.4047 - learning_rate: 0.1000 - val_custom_mse: 1.1946 - val_custom_mae: 0.7471\n",
            "Epoch 61/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1653 - mae: 0.2235 - mse: 0.1653 - val_loss: 0.4027 - val_mae: 0.3079 - val_mse: 0.4027 - learning_rate: 0.1000 - val_custom_mse: 1.1883 - val_custom_mae: 0.7456\n",
            "Epoch 62/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1653 - mae: 0.2234 - mse: 0.1653 - val_loss: 0.4041 - val_mae: 0.3070 - val_mse: 0.4041 - learning_rate: 0.1000 - val_custom_mse: 1.1939 - val_custom_mae: 0.7471\n",
            "Epoch 63/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1652 - mae: 0.2232 - mse: 0.1652 - val_loss: 0.4020 - val_mae: 0.3074 - val_mse: 0.4020 - learning_rate: 0.1000 - val_custom_mse: 1.1870 - val_custom_mae: 0.7454\n",
            "Epoch 64/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1650 - mae: 0.2231 - mse: 0.1650 - val_loss: 0.4026 - val_mae: 0.3072 - val_mse: 0.4026 - learning_rate: 0.1000 - val_custom_mse: 1.1892 - val_custom_mae: 0.7463\n",
            "Epoch 65/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1650 - mae: 0.2230 - mse: 0.1650 - val_loss: 0.4041 - val_mae: 0.3077 - val_mse: 0.4041 - learning_rate: 0.1000 - val_custom_mse: 1.1934 - val_custom_mae: 0.7470\n",
            "Epoch 66/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1650 - mae: 0.2228 - mse: 0.1650 - val_loss: 0.4042 - val_mae: 0.3066 - val_mse: 0.4042 - learning_rate: 0.1000 - val_custom_mse: 1.1948 - val_custom_mae: 0.7474\n",
            "Epoch 67/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1648 - mae: 0.2226 - mse: 0.1648 - val_loss: 0.4055 - val_mae: 0.3072 - val_mse: 0.4055 - learning_rate: 0.1000 - val_custom_mse: 1.1986 - val_custom_mae: 0.7483\n",
            "Epoch 68/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1648 - mae: 0.2226 - mse: 0.1648 - val_loss: 0.4036 - val_mae: 0.3062 - val_mse: 0.4036 - learning_rate: 0.1000 - val_custom_mse: 1.1936 - val_custom_mae: 0.7472\n",
            "Epoch 69/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1647 - mae: 0.2225 - mse: 0.1647 - val_loss: 0.4048 - val_mae: 0.3065 - val_mse: 0.4048 - learning_rate: 0.1000 - val_custom_mse: 1.1973 - val_custom_mae: 0.7483\n",
            "Epoch 70/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1645 - mae: 0.2223 - mse: 0.1645 - val_loss: 0.4038 - val_mae: 0.3060 - val_mse: 0.4038 - learning_rate: 0.1000 - val_custom_mse: 1.1944 - val_custom_mae: 0.7476\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1645 - mae: 0.2222 - mse: 0.1645 - val_loss: 0.4027 - val_mae: 0.3061 - val_mse: 0.4027 - learning_rate: 0.1000 - val_custom_mse: 1.1910 - val_custom_mae: 0.7465\n",
            "Epoch 72/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1644 - mae: 0.2222 - mse: 0.1644 - val_loss: 0.4037 - val_mae: 0.3058 - val_mse: 0.4037 - learning_rate: 0.0200 - val_custom_mse: 1.1944 - val_custom_mae: 0.7476\n",
            "Epoch 73/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2220 - mse: 0.1643 - val_loss: 0.4041 - val_mae: 0.3058 - val_mse: 0.4041 - learning_rate: 0.0200 - val_custom_mse: 1.1957 - val_custom_mae: 0.7479\n",
            "Epoch 74/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2219 - mse: 0.1643 - val_loss: 0.4032 - val_mae: 0.3057 - val_mse: 0.4032 - learning_rate: 0.0200 - val_custom_mse: 1.1929 - val_custom_mae: 0.7472\n",
            "Epoch 75/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2220 - mse: 0.1643 - val_loss: 0.4041 - val_mae: 0.3058 - val_mse: 0.4041 - learning_rate: 0.0200 - val_custom_mse: 1.1958 - val_custom_mae: 0.7479\n",
            "Epoch 76/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2219 - mse: 0.1643 - val_loss: 0.4040 - val_mae: 0.3057 - val_mse: 0.4040 - learning_rate: 0.0200 - val_custom_mse: 1.1956 - val_custom_mae: 0.7479\n",
            "Epoch 77/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2219 - mse: 0.1643 - val_loss: 0.4034 - val_mae: 0.3056 - val_mse: 0.4034 - learning_rate: 0.0200 - val_custom_mse: 1.1937 - val_custom_mae: 0.7475\n",
            "Epoch 78/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2219 - mse: 0.1642 - val_loss: 0.4036 - val_mae: 0.3056 - val_mse: 0.4036 - learning_rate: 0.0200 - val_custom_mse: 1.1944 - val_custom_mae: 0.7477\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2219 - mse: 0.1642 - val_loss: 0.4034 - val_mae: 0.3056 - val_mse: 0.4034 - learning_rate: 0.0200 - val_custom_mse: 1.1936 - val_custom_mae: 0.7475\n",
            "Epoch 80/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2219 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 0.0040 - val_custom_mse: 1.1949 - val_custom_mae: 0.7478\n",
            "Epoch 81/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4038 - val_mae: 0.3055 - val_mse: 0.4038 - learning_rate: 0.0040 - val_custom_mse: 1.1952 - val_custom_mae: 0.7479\n",
            "Epoch 82/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2219 - mse: 0.1643 - val_loss: 0.4038 - val_mae: 0.3054 - val_mse: 0.4038 - learning_rate: 0.0040 - val_custom_mse: 1.1951 - val_custom_mae: 0.7479\n",
            "Epoch 83/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2219 - mse: 0.1643 - val_loss: 0.4038 - val_mae: 0.3054 - val_mse: 0.4038 - learning_rate: 0.0040 - val_custom_mse: 1.1953 - val_custom_mae: 0.7479\n",
            "Epoch 84/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2219 - mse: 0.1643 - val_loss: 0.4039 - val_mae: 0.3055 - val_mse: 0.4039 - learning_rate: 0.0040 - val_custom_mse: 1.1954 - val_custom_mae: 0.7479\n",
            "Epoch 85/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4038 - val_mae: 0.3054 - val_mse: 0.4038 - learning_rate: 0.0040 - val_custom_mse: 1.1951 - val_custom_mae: 0.7479\n",
            "Epoch 86/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 0.0040 - val_custom_mse: 1.1948 - val_custom_mae: 0.7478\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4038 - val_mae: 0.3054 - val_mse: 0.4038 - learning_rate: 0.0040 - val_custom_mse: 1.1951 - val_custom_mae: 0.7479\n",
            "Epoch 88/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2219 - mse: 0.1643 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1947 - val_custom_mae: 0.7478\n",
            "Epoch 89/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1948 - val_custom_mae: 0.7478\n",
            "Epoch 90/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1643 - mae: 0.2218 - mse: 0.1643 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1949 - val_custom_mae: 0.7478\n",
            "Epoch 91/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1949 - val_custom_mae: 0.7478\n",
            "Epoch 92/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4036 - val_mae: 0.3054 - val_mse: 0.4036 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1947 - val_custom_mae: 0.7478\n",
            "Epoch 93/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1948 - val_custom_mae: 0.7478\n",
            "Epoch 94/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1948 - val_custom_mae: 0.7478\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1948 - val_custom_mae: 0.7478\n",
            "Epoch 96/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1949 - val_custom_mae: 0.7478\n",
            "Epoch 97/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1949 - val_custom_mae: 0.7478\n",
            "Epoch 98/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1949 - val_custom_mae: 0.7478\n",
            "Epoch 99/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1641 - mae: 0.2218 - mse: 0.1641 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1949 - val_custom_mae: 0.7478\n",
            "Epoch 100/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1642 - mae: 0.2218 - mse: 0.1642 - val_loss: 0.4037 - val_mae: 0.3054 - val_mse: 0.4037 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1950 - val_custom_mae: 0.7479\n",
            "Running experiment: horizon=336, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_27', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228/228 - 6s - 27ms/step - loss: 0.5479 - mae: 0.5303 - mse: 0.5479 - val_loss: 1.1787 - val_mae: 0.7831 - val_mse: 1.1787 - learning_rate: 0.1000 - val_custom_mse: 1.4641 - val_custom_mae: 0.8793\n",
            "Epoch 2/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.5060 - mae: 0.5102 - mse: 0.5060 - val_loss: 1.0917 - val_mae: 0.7517 - val_mse: 1.0917 - learning_rate: 0.1000 - val_custom_mse: 1.4196 - val_custom_mae: 0.8635\n",
            "Epoch 3/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4719 - mae: 0.4916 - mse: 0.4719 - val_loss: 0.9991 - val_mae: 0.7152 - val_mse: 0.9991 - learning_rate: 0.1000 - val_custom_mse: 1.3694 - val_custom_mae: 0.8438\n",
            "Epoch 4/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4343 - mae: 0.4699 - mse: 0.4343 - val_loss: 0.8979 - val_mae: 0.6728 - val_mse: 0.8979 - learning_rate: 0.1000 - val_custom_mse: 1.3204 - val_custom_mae: 0.8243\n",
            "Epoch 5/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3928 - mae: 0.4442 - mse: 0.3928 - val_loss: 0.7946 - val_mae: 0.6239 - val_mse: 0.7946 - learning_rate: 0.1000 - val_custom_mse: 1.2740 - val_custom_mae: 0.8033\n",
            "Epoch 6/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3497 - mae: 0.4150 - mse: 0.3497 - val_loss: 0.6933 - val_mae: 0.5717 - val_mse: 0.6933 - learning_rate: 0.1000 - val_custom_mse: 1.2394 - val_custom_mae: 0.7885\n",
            "Epoch 7/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3095 - mae: 0.3848 - mse: 0.3095 - val_loss: 0.6076 - val_mae: 0.5211 - val_mse: 0.6076 - learning_rate: 0.1000 - val_custom_mse: 1.2118 - val_custom_mae: 0.7757\n",
            "Epoch 8/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2761 - mae: 0.3570 - mse: 0.2761 - val_loss: 0.5429 - val_mae: 0.4771 - val_mse: 0.5429 - learning_rate: 0.1000 - val_custom_mse: 1.1919 - val_custom_mae: 0.7661\n",
            "Epoch 9/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2516 - mae: 0.3346 - mse: 0.2516 - val_loss: 0.5014 - val_mae: 0.4439 - val_mse: 0.5014 - learning_rate: 0.1000 - val_custom_mse: 1.1803 - val_custom_mae: 0.7582\n",
            "Epoch 10/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2351 - mae: 0.3185 - mse: 0.2351 - val_loss: 0.4758 - val_mae: 0.4204 - val_mse: 0.4758 - learning_rate: 0.1000 - val_custom_mse: 1.1754 - val_custom_mae: 0.7538\n",
            "Epoch 11/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2245 - mae: 0.3074 - mse: 0.2245 - val_loss: 0.4605 - val_mae: 0.4046 - val_mse: 0.4605 - learning_rate: 0.1000 - val_custom_mse: 1.1723 - val_custom_mae: 0.7502\n",
            "Epoch 12/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2175 - mae: 0.2998 - mse: 0.2175 - val_loss: 0.4512 - val_mae: 0.3940 - val_mse: 0.4512 - learning_rate: 0.1000 - val_custom_mse: 1.1717 - val_custom_mae: 0.7480\n",
            "Epoch 13/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2127 - mae: 0.2943 - mse: 0.2127 - val_loss: 0.4449 - val_mae: 0.3858 - val_mse: 0.4449 - learning_rate: 0.1000 - val_custom_mse: 1.1730 - val_custom_mae: 0.7467\n",
            "Epoch 14/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2089 - mae: 0.2898 - mse: 0.2089 - val_loss: 0.4421 - val_mae: 0.3803 - val_mse: 0.4421 - learning_rate: 0.1000 - val_custom_mse: 1.1778 - val_custom_mae: 0.7468\n",
            "Epoch 15/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2060 - mae: 0.2862 - mse: 0.2060 - val_loss: 0.4383 - val_mae: 0.3750 - val_mse: 0.4383 - learning_rate: 0.1000 - val_custom_mse: 1.1781 - val_custom_mae: 0.7460\n",
            "Epoch 16/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2035 - mae: 0.2831 - mse: 0.2035 - val_loss: 0.4341 - val_mae: 0.3697 - val_mse: 0.4341 - learning_rate: 0.1000 - val_custom_mse: 1.1762 - val_custom_mae: 0.7450\n",
            "Epoch 17/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2016 - mae: 0.2805 - mse: 0.2016 - val_loss: 0.4324 - val_mae: 0.3662 - val_mse: 0.4324 - learning_rate: 0.1000 - val_custom_mse: 1.1794 - val_custom_mae: 0.7453\n",
            "Epoch 18/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1997 - mae: 0.2781 - mse: 0.1997 - val_loss: 0.4292 - val_mae: 0.3620 - val_mse: 0.4292 - learning_rate: 0.1000 - val_custom_mse: 1.1780 - val_custom_mae: 0.7450\n",
            "Epoch 19/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1980 - mae: 0.2759 - mse: 0.1980 - val_loss: 0.4286 - val_mae: 0.3595 - val_mse: 0.4286 - learning_rate: 0.1000 - val_custom_mse: 1.1812 - val_custom_mae: 0.7453\n",
            "Epoch 20/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1966 - mae: 0.2740 - mse: 0.1966 - val_loss: 0.4268 - val_mae: 0.3565 - val_mse: 0.4268 - learning_rate: 0.1000 - val_custom_mse: 1.1813 - val_custom_mae: 0.7450\n",
            "Epoch 21/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1953 - mae: 0.2722 - mse: 0.1953 - val_loss: 0.4238 - val_mae: 0.3533 - val_mse: 0.4238 - learning_rate: 0.1000 - val_custom_mse: 1.1779 - val_custom_mae: 0.7441\n",
            "Epoch 22/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1942 - mae: 0.2707 - mse: 0.1942 - val_loss: 0.4229 - val_mae: 0.3511 - val_mse: 0.4229 - learning_rate: 0.1000 - val_custom_mse: 1.1797 - val_custom_mae: 0.7442\n",
            "Epoch 23/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1932 - mae: 0.2693 - mse: 0.1932 - val_loss: 0.4209 - val_mae: 0.3486 - val_mse: 0.4209 - learning_rate: 0.1000 - val_custom_mse: 1.1780 - val_custom_mae: 0.7438\n",
            "Epoch 24/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1922 - mae: 0.2679 - mse: 0.1922 - val_loss: 0.4194 - val_mae: 0.3466 - val_mse: 0.4194 - learning_rate: 0.1000 - val_custom_mse: 1.1766 - val_custom_mae: 0.7431\n",
            "Epoch 25/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1913 - mae: 0.2667 - mse: 0.1913 - val_loss: 0.4193 - val_mae: 0.3451 - val_mse: 0.4193 - learning_rate: 0.1000 - val_custom_mse: 1.1795 - val_custom_mae: 0.7438\n",
            "Epoch 26/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1906 - mae: 0.2657 - mse: 0.1906 - val_loss: 0.4180 - val_mae: 0.3434 - val_mse: 0.4180 - learning_rate: 0.1000 - val_custom_mse: 1.1784 - val_custom_mae: 0.7436\n",
            "Epoch 27/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1898 - mae: 0.2647 - mse: 0.1898 - val_loss: 0.4185 - val_mae: 0.3424 - val_mse: 0.4185 - learning_rate: 0.1000 - val_custom_mse: 1.1823 - val_custom_mae: 0.7442\n",
            "Epoch 28/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1894 - mae: 0.2639 - mse: 0.1894 - val_loss: 0.4182 - val_mae: 0.3412 - val_mse: 0.4182 - learning_rate: 0.1000 - val_custom_mse: 1.1837 - val_custom_mae: 0.7446\n",
            "Epoch 29/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1886 - mae: 0.2629 - mse: 0.1886 - val_loss: 0.4156 - val_mae: 0.3395 - val_mse: 0.4156 - learning_rate: 0.1000 - val_custom_mse: 1.1785 - val_custom_mae: 0.7434\n",
            "Epoch 30/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1880 - mae: 0.2622 - mse: 0.1880 - val_loss: 0.4148 - val_mae: 0.3382 - val_mse: 0.4148 - learning_rate: 0.1000 - val_custom_mse: 1.1779 - val_custom_mae: 0.7434\n",
            "Epoch 31/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1875 - mae: 0.2614 - mse: 0.1875 - val_loss: 0.4154 - val_mae: 0.3374 - val_mse: 0.4154 - learning_rate: 0.1000 - val_custom_mse: 1.1818 - val_custom_mae: 0.7445\n",
            "Epoch 32/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1871 - mae: 0.2608 - mse: 0.1871 - val_loss: 0.4176 - val_mae: 0.3380 - val_mse: 0.4176 - learning_rate: 0.1000 - val_custom_mse: 1.1882 - val_custom_mae: 0.7457\n",
            "Epoch 33/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1866 - mae: 0.2602 - mse: 0.1866 - val_loss: 0.4138 - val_mae: 0.3352 - val_mse: 0.4138 - learning_rate: 0.1000 - val_custom_mse: 1.1803 - val_custom_mae: 0.7437\n",
            "Epoch 34/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1861 - mae: 0.2596 - mse: 0.1861 - val_loss: 0.4135 - val_mae: 0.3346 - val_mse: 0.4135 - learning_rate: 0.1000 - val_custom_mse: 1.1807 - val_custom_mae: 0.7439\n",
            "Epoch 35/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1858 - mae: 0.2592 - mse: 0.1858 - val_loss: 0.4133 - val_mae: 0.3342 - val_mse: 0.4133 - learning_rate: 0.1000 - val_custom_mse: 1.1807 - val_custom_mae: 0.7437\n",
            "Epoch 36/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1854 - mae: 0.2587 - mse: 0.1854 - val_loss: 0.4138 - val_mae: 0.3337 - val_mse: 0.4138 - learning_rate: 0.1000 - val_custom_mse: 1.1831 - val_custom_mae: 0.7444\n",
            "Epoch 37/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1851 - mae: 0.2583 - mse: 0.1851 - val_loss: 0.4142 - val_mae: 0.3335 - val_mse: 0.4142 - learning_rate: 0.1000 - val_custom_mse: 1.1852 - val_custom_mae: 0.7447\n",
            "Epoch 38/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1848 - mae: 0.2578 - mse: 0.1848 - val_loss: 0.4144 - val_mae: 0.3331 - val_mse: 0.4144 - learning_rate: 0.1000 - val_custom_mse: 1.1866 - val_custom_mae: 0.7453\n",
            "Epoch 39/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1844 - mae: 0.2574 - mse: 0.1844 - val_loss: 0.4137 - val_mae: 0.3321 - val_mse: 0.4137 - learning_rate: 0.1000 - val_custom_mse: 1.1861 - val_custom_mae: 0.7450\n",
            "Epoch 40/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1842 - mae: 0.2571 - mse: 0.1842 - val_loss: 0.4121 - val_mae: 0.3315 - val_mse: 0.4121 - learning_rate: 0.1000 - val_custom_mse: 1.1817 - val_custom_mae: 0.7442\n",
            "Epoch 41/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1840 - mae: 0.2568 - mse: 0.1840 - val_loss: 0.4135 - val_mae: 0.3316 - val_mse: 0.4135 - learning_rate: 0.1000 - val_custom_mse: 1.1863 - val_custom_mae: 0.7454\n",
            "Epoch 42/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1837 - mae: 0.2565 - mse: 0.1837 - val_loss: 0.4123 - val_mae: 0.3307 - val_mse: 0.4123 - learning_rate: 0.1000 - val_custom_mse: 1.1838 - val_custom_mae: 0.7447\n",
            "Epoch 43/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1835 - mae: 0.2562 - mse: 0.1835 - val_loss: 0.4122 - val_mae: 0.3304 - val_mse: 0.4122 - learning_rate: 0.1000 - val_custom_mse: 1.1842 - val_custom_mae: 0.7449\n",
            "Epoch 44/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1833 - mae: 0.2560 - mse: 0.1833 - val_loss: 0.4117 - val_mae: 0.3302 - val_mse: 0.4117 - learning_rate: 0.1000 - val_custom_mse: 1.1828 - val_custom_mae: 0.7445\n",
            "Epoch 45/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1831 - mae: 0.2557 - mse: 0.1831 - val_loss: 0.4125 - val_mae: 0.3296 - val_mse: 0.4125 - learning_rate: 0.1000 - val_custom_mse: 1.1865 - val_custom_mae: 0.7455\n",
            "Epoch 46/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1830 - mae: 0.2556 - mse: 0.1830 - val_loss: 0.4128 - val_mae: 0.3296 - val_mse: 0.4128 - learning_rate: 0.1000 - val_custom_mse: 1.1876 - val_custom_mae: 0.7458\n",
            "Epoch 47/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1826 - mae: 0.2552 - mse: 0.1826 - val_loss: 0.4116 - val_mae: 0.3292 - val_mse: 0.4116 - learning_rate: 0.1000 - val_custom_mse: 1.1843 - val_custom_mae: 0.7449\n",
            "Epoch 48/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1826 - mae: 0.2551 - mse: 0.1826 - val_loss: 0.4120 - val_mae: 0.3289 - val_mse: 0.4120 - learning_rate: 0.1000 - val_custom_mse: 1.1863 - val_custom_mae: 0.7455\n",
            "Epoch 49/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1822 - mae: 0.2549 - mse: 0.1822 - val_loss: 0.4121 - val_mae: 0.3291 - val_mse: 0.4121 - learning_rate: 0.1000 - val_custom_mse: 1.1862 - val_custom_mae: 0.7454\n",
            "Epoch 50/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1823 - mae: 0.2547 - mse: 0.1823 - val_loss: 0.4124 - val_mae: 0.3293 - val_mse: 0.4124 - learning_rate: 0.1000 - val_custom_mse: 1.1869 - val_custom_mae: 0.7457\n",
            "Epoch 51/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1821 - mae: 0.2545 - mse: 0.1821 - val_loss: 0.4112 - val_mae: 0.3291 - val_mse: 0.4112 - learning_rate: 0.1000 - val_custom_mse: 1.1832 - val_custom_mae: 0.7447\n",
            "Epoch 52/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1817 - mae: 0.2542 - mse: 0.1817 - val_loss: 0.4127 - val_mae: 0.3290 - val_mse: 0.4127 - learning_rate: 0.1000 - val_custom_mse: 1.1883 - val_custom_mae: 0.7459\n",
            "Epoch 53/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1818 - mae: 0.2542 - mse: 0.1818 - val_loss: 0.4124 - val_mae: 0.3290 - val_mse: 0.4124 - learning_rate: 0.1000 - val_custom_mse: 1.1875 - val_custom_mae: 0.7460\n",
            "Epoch 54/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1815 - mae: 0.2540 - mse: 0.1815 - val_loss: 0.4126 - val_mae: 0.3290 - val_mse: 0.4126 - learning_rate: 0.1000 - val_custom_mse: 1.1882 - val_custom_mae: 0.7459\n",
            "Epoch 55/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1814 - mae: 0.2539 - mse: 0.1814 - val_loss: 0.4116 - val_mae: 0.3284 - val_mse: 0.4116 - learning_rate: 0.1000 - val_custom_mse: 1.1858 - val_custom_mae: 0.7453\n",
            "Epoch 56/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1813 - mae: 0.2538 - mse: 0.1813 - val_loss: 0.4115 - val_mae: 0.3284 - val_mse: 0.4115 - learning_rate: 0.1000 - val_custom_mse: 1.1858 - val_custom_mae: 0.7456\n",
            "Epoch 57/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1811 - mae: 0.2536 - mse: 0.1811 - val_loss: 0.4107 - val_mae: 0.3282 - val_mse: 0.4107 - learning_rate: 0.1000 - val_custom_mse: 1.1833 - val_custom_mae: 0.7449\n",
            "Epoch 58/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1811 - mae: 0.2535 - mse: 0.1811 - val_loss: 0.4126 - val_mae: 0.3285 - val_mse: 0.4126 - learning_rate: 0.1000 - val_custom_mse: 1.1892 - val_custom_mae: 0.7464\n",
            "Epoch 59/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1809 - mae: 0.2535 - mse: 0.1809 - val_loss: 0.4124 - val_mae: 0.3283 - val_mse: 0.4124 - learning_rate: 0.1000 - val_custom_mse: 1.1889 - val_custom_mae: 0.7463\n",
            "Epoch 60/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1808 - mae: 0.2533 - mse: 0.1808 - val_loss: 0.4132 - val_mae: 0.3284 - val_mse: 0.4132 - learning_rate: 0.1000 - val_custom_mse: 1.1912 - val_custom_mae: 0.7469\n",
            "Epoch 61/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1806 - mae: 0.2532 - mse: 0.1806 - val_loss: 0.4113 - val_mae: 0.3279 - val_mse: 0.4113 - learning_rate: 0.1000 - val_custom_mse: 1.1856 - val_custom_mae: 0.7454\n",
            "Epoch 62/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1807 - mae: 0.2531 - mse: 0.1807 - val_loss: 0.4127 - val_mae: 0.3279 - val_mse: 0.4127 - learning_rate: 0.1000 - val_custom_mse: 1.1901 - val_custom_mae: 0.7463\n",
            "Epoch 63/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1806 - mae: 0.2530 - mse: 0.1806 - val_loss: 0.4120 - val_mae: 0.3279 - val_mse: 0.4120 - learning_rate: 0.1000 - val_custom_mse: 1.1883 - val_custom_mae: 0.7461\n",
            "Epoch 64/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1804 - mae: 0.2530 - mse: 0.1804 - val_loss: 0.4125 - val_mae: 0.3279 - val_mse: 0.4125 - learning_rate: 0.1000 - val_custom_mse: 1.1898 - val_custom_mae: 0.7465\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1803 - mae: 0.2528 - mse: 0.1803 - val_loss: 0.4120 - val_mae: 0.3284 - val_mse: 0.4120 - learning_rate: 0.1000 - val_custom_mse: 1.1875 - val_custom_mae: 0.7459\n",
            "Epoch 66/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1802 - mae: 0.2528 - mse: 0.1802 - val_loss: 0.4120 - val_mae: 0.3275 - val_mse: 0.4120 - learning_rate: 0.0200 - val_custom_mse: 1.1890 - val_custom_mae: 0.7466\n",
            "Epoch 67/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1802 - mae: 0.2527 - mse: 0.1802 - val_loss: 0.4123 - val_mae: 0.3276 - val_mse: 0.4123 - learning_rate: 0.0200 - val_custom_mse: 1.1899 - val_custom_mae: 0.7468\n",
            "Epoch 68/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1802 - mae: 0.2527 - mse: 0.1802 - val_loss: 0.4123 - val_mae: 0.3276 - val_mse: 0.4123 - learning_rate: 0.0200 - val_custom_mse: 1.1900 - val_custom_mae: 0.7468\n",
            "Epoch 69/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4122 - val_mae: 0.3275 - val_mse: 0.4122 - learning_rate: 0.0200 - val_custom_mse: 1.1896 - val_custom_mae: 0.7468\n",
            "Epoch 70/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4120 - val_mae: 0.3274 - val_mse: 0.4120 - learning_rate: 0.0200 - val_custom_mse: 1.1892 - val_custom_mae: 0.7466\n",
            "Epoch 71/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2526 - mse: 0.1800 - val_loss: 0.4121 - val_mae: 0.3276 - val_mse: 0.4121 - learning_rate: 0.0200 - val_custom_mse: 1.1893 - val_custom_mae: 0.7467\n",
            "Epoch 72/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4122 - val_mae: 0.3275 - val_mse: 0.4122 - learning_rate: 0.0200 - val_custom_mse: 1.1899 - val_custom_mae: 0.7468\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2526 - mse: 0.1800 - val_loss: 0.4121 - val_mae: 0.3274 - val_mse: 0.4121 - learning_rate: 0.0200 - val_custom_mse: 1.1895 - val_custom_mae: 0.7467\n",
            "Epoch 74/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2525 - mse: 0.1800 - val_loss: 0.4125 - val_mae: 0.3275 - val_mse: 0.4125 - learning_rate: 0.0040 - val_custom_mse: 1.1906 - val_custom_mae: 0.7470\n",
            "Epoch 75/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4125 - val_mae: 0.3275 - val_mse: 0.4125 - learning_rate: 0.0040 - val_custom_mse: 1.1906 - val_custom_mae: 0.7470\n",
            "Epoch 76/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 0.0040 - val_custom_mse: 1.1912 - val_custom_mae: 0.7472\n",
            "Epoch 77/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2526 - mse: 0.1800 - val_loss: 0.4126 - val_mae: 0.3276 - val_mse: 0.4126 - learning_rate: 0.0040 - val_custom_mse: 1.1911 - val_custom_mae: 0.7472\n",
            "Epoch 78/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2525 - mse: 0.1800 - val_loss: 0.4126 - val_mae: 0.3276 - val_mse: 0.4126 - learning_rate: 0.0040 - val_custom_mse: 1.1911 - val_custom_mae: 0.7472\n",
            "Epoch 79/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4126 - val_mae: 0.3275 - val_mse: 0.4126 - learning_rate: 0.0040 - val_custom_mse: 1.1911 - val_custom_mae: 0.7472\n",
            "Epoch 80/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4126 - val_mae: 0.3275 - val_mse: 0.4126 - learning_rate: 0.0040 - val_custom_mse: 1.1909 - val_custom_mae: 0.7471\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2525 - mse: 0.1800 - val_loss: 0.4126 - val_mae: 0.3275 - val_mse: 0.4126 - learning_rate: 0.0040 - val_custom_mse: 1.1909 - val_custom_mae: 0.7471\n",
            "Epoch 82/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1912 - val_custom_mae: 0.7472\n",
            "Epoch 83/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1799 - mae: 0.2525 - mse: 0.1799 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1912 - val_custom_mae: 0.7472\n",
            "Epoch 84/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2526 - mse: 0.1800 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 85/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1799 - mae: 0.2525 - mse: 0.1799 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1912 - val_custom_mae: 0.7471\n",
            "Epoch 86/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2526 - mse: 0.1800 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 87/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1799 - mae: 0.2525 - mse: 0.1799 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 88/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2525 - mse: 0.1800 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1912 - val_custom_mae: 0.7471\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2525 - mse: 0.1800 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 90/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1799 - mae: 0.2525 - mse: 0.1799 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 91/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1799 - mae: 0.2525 - mse: 0.1799 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 92/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2525 - mse: 0.1800 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 93/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 94/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 95/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1799 - mae: 0.2526 - mse: 0.1799 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 96/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1800 - mae: 0.2525 - mse: 0.1800 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1799 - mae: 0.2525 - mse: 0.1799 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 98/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1801 - mae: 0.2526 - mse: 0.1801 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 99/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1802 - mae: 0.2526 - mse: 0.1802 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Epoch 100/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1799 - mae: 0.2525 - mse: 0.1799 - val_loss: 0.4127 - val_mae: 0.3276 - val_mse: 0.4127 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1913 - val_custom_mae: 0.7472\n",
            "Running experiment: horizon=336, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_28', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228/228 - 6s - 27ms/step - loss: 0.5577 - mae: 0.5356 - mse: 0.5577 - val_loss: 1.1751 - val_mae: 0.7841 - val_mse: 1.1751 - learning_rate: 0.1000 - val_custom_mse: 1.4744 - val_custom_mae: 0.8854\n",
            "Epoch 2/100\n",
            "228/228 - 2s - 9ms/step - loss: 0.5106 - mae: 0.5133 - mse: 0.5106 - val_loss: 1.0870 - val_mae: 0.7512 - val_mse: 1.0870 - learning_rate: 0.1000 - val_custom_mse: 1.4238 - val_custom_mae: 0.8663\n",
            "Epoch 3/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4743 - mae: 0.4935 - mse: 0.4743 - val_loss: 0.9918 - val_mae: 0.7138 - val_mse: 0.9918 - learning_rate: 0.1000 - val_custom_mse: 1.3728 - val_custom_mae: 0.8468\n",
            "Epoch 4/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4345 - mae: 0.4706 - mse: 0.4345 - val_loss: 0.8886 - val_mae: 0.6687 - val_mse: 0.8886 - learning_rate: 0.1000 - val_custom_mse: 1.3159 - val_custom_mae: 0.8223\n",
            "Epoch 5/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3914 - mae: 0.4436 - mse: 0.3914 - val_loss: 0.7822 - val_mae: 0.6190 - val_mse: 0.7822 - learning_rate: 0.1000 - val_custom_mse: 1.2723 - val_custom_mae: 0.8042\n",
            "Epoch 6/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3482 - mae: 0.4139 - mse: 0.3482 - val_loss: 0.6835 - val_mae: 0.5661 - val_mse: 0.6835 - learning_rate: 0.1000 - val_custom_mse: 1.2357 - val_custom_mae: 0.7868\n",
            "Epoch 7/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3095 - mae: 0.3844 - mse: 0.3095 - val_loss: 0.6010 - val_mae: 0.5169 - val_mse: 0.6010 - learning_rate: 0.1000 - val_custom_mse: 1.2092 - val_custom_mae: 0.7748\n",
            "Epoch 8/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2788 - mae: 0.3587 - mse: 0.2788 - val_loss: 0.5431 - val_mae: 0.4766 - val_mse: 0.5431 - learning_rate: 0.1000 - val_custom_mse: 1.1928 - val_custom_mae: 0.7647\n",
            "Epoch 9/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2573 - mae: 0.3391 - mse: 0.2573 - val_loss: 0.5050 - val_mae: 0.4467 - val_mse: 0.5050 - learning_rate: 0.1000 - val_custom_mse: 1.1830 - val_custom_mae: 0.7581\n",
            "Epoch 10/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2433 - mae: 0.3258 - mse: 0.2433 - val_loss: 0.4799 - val_mae: 0.4245 - val_mse: 0.4799 - learning_rate: 0.1000 - val_custom_mse: 1.1771 - val_custom_mae: 0.7542\n",
            "Epoch 11/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2343 - mae: 0.3170 - mse: 0.2343 - val_loss: 0.4689 - val_mae: 0.4117 - val_mse: 0.4689 - learning_rate: 0.1000 - val_custom_mse: 1.1816 - val_custom_mae: 0.7524\n",
            "Epoch 12/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2284 - mae: 0.3110 - mse: 0.2284 - val_loss: 0.4572 - val_mae: 0.4003 - val_mse: 0.4572 - learning_rate: 0.1000 - val_custom_mse: 1.1754 - val_custom_mae: 0.7492\n",
            "Epoch 13/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2241 - mae: 0.3065 - mse: 0.2241 - val_loss: 0.4531 - val_mae: 0.3937 - val_mse: 0.4531 - learning_rate: 0.1000 - val_custom_mse: 1.1811 - val_custom_mae: 0.7494\n",
            "Epoch 14/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2209 - mae: 0.3031 - mse: 0.2209 - val_loss: 0.4470 - val_mae: 0.3872 - val_mse: 0.4470 - learning_rate: 0.1000 - val_custom_mse: 1.1777 - val_custom_mae: 0.7476\n",
            "Epoch 15/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2182 - mae: 0.3002 - mse: 0.2182 - val_loss: 0.4455 - val_mae: 0.3834 - val_mse: 0.4455 - learning_rate: 0.1000 - val_custom_mse: 1.1831 - val_custom_mae: 0.7481\n",
            "Epoch 16/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2161 - mae: 0.2978 - mse: 0.2161 - val_loss: 0.4400 - val_mae: 0.3780 - val_mse: 0.4400 - learning_rate: 0.1000 - val_custom_mse: 1.1780 - val_custom_mae: 0.7465\n",
            "Epoch 17/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2143 - mae: 0.2956 - mse: 0.2143 - val_loss: 0.4399 - val_mae: 0.3756 - val_mse: 0.4399 - learning_rate: 0.1000 - val_custom_mse: 1.1837 - val_custom_mae: 0.7472\n",
            "Epoch 18/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2125 - mae: 0.2937 - mse: 0.2125 - val_loss: 0.4358 - val_mae: 0.3715 - val_mse: 0.4358 - learning_rate: 0.1000 - val_custom_mse: 1.1795 - val_custom_mae: 0.7463\n",
            "Epoch 19/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2109 - mae: 0.2919 - mse: 0.2109 - val_loss: 0.4351 - val_mae: 0.3692 - val_mse: 0.4351 - learning_rate: 0.1000 - val_custom_mse: 1.1823 - val_custom_mae: 0.7465\n",
            "Epoch 20/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2096 - mae: 0.2904 - mse: 0.2096 - val_loss: 0.4336 - val_mae: 0.3668 - val_mse: 0.4336 - learning_rate: 0.1000 - val_custom_mse: 1.1831 - val_custom_mae: 0.7467\n",
            "Epoch 21/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2085 - mae: 0.2891 - mse: 0.2085 - val_loss: 0.4306 - val_mae: 0.3640 - val_mse: 0.4306 - learning_rate: 0.1000 - val_custom_mse: 1.1787 - val_custom_mae: 0.7451\n",
            "Epoch 22/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2075 - mae: 0.2879 - mse: 0.2075 - val_loss: 0.4301 - val_mae: 0.3624 - val_mse: 0.4301 - learning_rate: 0.1000 - val_custom_mse: 1.1807 - val_custom_mae: 0.7455\n",
            "Epoch 23/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2065 - mae: 0.2867 - mse: 0.2065 - val_loss: 0.4276 - val_mae: 0.3601 - val_mse: 0.4276 - learning_rate: 0.1000 - val_custom_mse: 1.1774 - val_custom_mae: 0.7445\n",
            "Epoch 24/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2056 - mae: 0.2857 - mse: 0.2056 - val_loss: 0.4276 - val_mae: 0.3591 - val_mse: 0.4276 - learning_rate: 0.1000 - val_custom_mse: 1.1796 - val_custom_mae: 0.7449\n",
            "Epoch 25/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2048 - mae: 0.2848 - mse: 0.2048 - val_loss: 0.4282 - val_mae: 0.3582 - val_mse: 0.4282 - learning_rate: 0.1000 - val_custom_mse: 1.1844 - val_custom_mae: 0.7462\n",
            "Epoch 26/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2039 - mae: 0.2839 - mse: 0.2039 - val_loss: 0.4259 - val_mae: 0.3562 - val_mse: 0.4259 - learning_rate: 0.1000 - val_custom_mse: 1.1802 - val_custom_mae: 0.7451\n",
            "Epoch 27/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2034 - mae: 0.2833 - mse: 0.2034 - val_loss: 0.4254 - val_mae: 0.3551 - val_mse: 0.4254 - learning_rate: 0.1000 - val_custom_mse: 1.1808 - val_custom_mae: 0.7449\n",
            "Epoch 28/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2027 - mae: 0.2825 - mse: 0.2027 - val_loss: 0.4254 - val_mae: 0.3542 - val_mse: 0.4254 - learning_rate: 0.1000 - val_custom_mse: 1.1828 - val_custom_mae: 0.7454\n",
            "Epoch 29/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2022 - mae: 0.2818 - mse: 0.2022 - val_loss: 0.4253 - val_mae: 0.3536 - val_mse: 0.4253 - learning_rate: 0.1000 - val_custom_mse: 1.1841 - val_custom_mae: 0.7456\n",
            "Epoch 30/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2017 - mae: 0.2813 - mse: 0.2017 - val_loss: 0.4228 - val_mae: 0.3521 - val_mse: 0.4228 - learning_rate: 0.1000 - val_custom_mse: 1.1782 - val_custom_mae: 0.7442\n",
            "Epoch 31/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2011 - mae: 0.2806 - mse: 0.2011 - val_loss: 0.4237 - val_mae: 0.3517 - val_mse: 0.4237 - learning_rate: 0.1000 - val_custom_mse: 1.1824 - val_custom_mae: 0.7451\n",
            "Epoch 32/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2009 - mae: 0.2803 - mse: 0.2009 - val_loss: 0.4238 - val_mae: 0.3510 - val_mse: 0.4238 - learning_rate: 0.1000 - val_custom_mse: 1.1839 - val_custom_mae: 0.7456\n",
            "Epoch 33/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2002 - mae: 0.2797 - mse: 0.2002 - val_loss: 0.4224 - val_mae: 0.3503 - val_mse: 0.4224 - learning_rate: 0.1000 - val_custom_mse: 1.1808 - val_custom_mae: 0.7451\n",
            "Epoch 34/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1999 - mae: 0.2793 - mse: 0.1999 - val_loss: 0.4232 - val_mae: 0.3500 - val_mse: 0.4232 - learning_rate: 0.1000 - val_custom_mse: 1.1840 - val_custom_mae: 0.7454\n",
            "Epoch 35/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1994 - mae: 0.2788 - mse: 0.1994 - val_loss: 0.4236 - val_mae: 0.3497 - val_mse: 0.4236 - learning_rate: 0.1000 - val_custom_mse: 1.1859 - val_custom_mae: 0.7457\n",
            "Epoch 36/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1990 - mae: 0.2785 - mse: 0.1990 - val_loss: 0.4213 - val_mae: 0.3483 - val_mse: 0.4213 - learning_rate: 0.1000 - val_custom_mse: 1.1810 - val_custom_mae: 0.7448\n",
            "Epoch 37/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1989 - mae: 0.2782 - mse: 0.1989 - val_loss: 0.4217 - val_mae: 0.3483 - val_mse: 0.4217 - learning_rate: 0.1000 - val_custom_mse: 1.1824 - val_custom_mae: 0.7452\n",
            "Epoch 38/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1984 - mae: 0.2779 - mse: 0.1984 - val_loss: 0.4207 - val_mae: 0.3477 - val_mse: 0.4207 - learning_rate: 0.1000 - val_custom_mse: 1.1801 - val_custom_mae: 0.7442\n",
            "Epoch 39/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1980 - mae: 0.2775 - mse: 0.1980 - val_loss: 0.4206 - val_mae: 0.3473 - val_mse: 0.4206 - learning_rate: 0.1000 - val_custom_mse: 1.1805 - val_custom_mae: 0.7445\n",
            "Epoch 40/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1979 - mae: 0.2773 - mse: 0.1979 - val_loss: 0.4219 - val_mae: 0.3474 - val_mse: 0.4219 - learning_rate: 0.1000 - val_custom_mse: 1.1847 - val_custom_mae: 0.7458\n",
            "Epoch 41/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1977 - mae: 0.2771 - mse: 0.1977 - val_loss: 0.4215 - val_mae: 0.3470 - val_mse: 0.4215 - learning_rate: 0.1000 - val_custom_mse: 1.1846 - val_custom_mae: 0.7458\n",
            "Epoch 42/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1974 - mae: 0.2767 - mse: 0.1974 - val_loss: 0.4219 - val_mae: 0.3469 - val_mse: 0.4219 - learning_rate: 0.1000 - val_custom_mse: 1.1860 - val_custom_mae: 0.7460\n",
            "Epoch 43/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1971 - mae: 0.2764 - mse: 0.1971 - val_loss: 0.4222 - val_mae: 0.3477 - val_mse: 0.4222 - learning_rate: 0.1000 - val_custom_mse: 1.1853 - val_custom_mae: 0.7461\n",
            "Epoch 44/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1968 - mae: 0.2761 - mse: 0.1968 - val_loss: 0.4215 - val_mae: 0.3469 - val_mse: 0.4215 - learning_rate: 0.1000 - val_custom_mse: 1.1840 - val_custom_mae: 0.7455\n",
            "Epoch 45/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1966 - mae: 0.2759 - mse: 0.1966 - val_loss: 0.4212 - val_mae: 0.3466 - val_mse: 0.4212 - learning_rate: 0.1000 - val_custom_mse: 1.1834 - val_custom_mae: 0.7453\n",
            "Epoch 46/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1963 - mae: 0.2757 - mse: 0.1963 - val_loss: 0.4230 - val_mae: 0.3471 - val_mse: 0.4230 - learning_rate: 0.1000 - val_custom_mse: 1.1884 - val_custom_mae: 0.7463\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1961 - mae: 0.2755 - mse: 0.1961 - val_loss: 0.4226 - val_mae: 0.3468 - val_mse: 0.4226 - learning_rate: 0.1000 - val_custom_mse: 1.1883 - val_custom_mae: 0.7467\n",
            "Epoch 48/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1961 - mae: 0.2754 - mse: 0.1961 - val_loss: 0.4208 - val_mae: 0.3456 - val_mse: 0.4208 - learning_rate: 0.0200 - val_custom_mse: 1.1843 - val_custom_mae: 0.7455\n",
            "Epoch 49/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1960 - mae: 0.2753 - mse: 0.1960 - val_loss: 0.4214 - val_mae: 0.3458 - val_mse: 0.4214 - learning_rate: 0.0200 - val_custom_mse: 1.1859 - val_custom_mae: 0.7459\n",
            "Epoch 50/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1960 - mae: 0.2753 - mse: 0.1960 - val_loss: 0.4214 - val_mae: 0.3457 - val_mse: 0.4214 - learning_rate: 0.0200 - val_custom_mse: 1.1860 - val_custom_mae: 0.7459\n",
            "Epoch 51/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1958 - mae: 0.2752 - mse: 0.1958 - val_loss: 0.4211 - val_mae: 0.3457 - val_mse: 0.4211 - learning_rate: 0.0200 - val_custom_mse: 1.1853 - val_custom_mae: 0.7459\n",
            "Epoch 52/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1959 - mae: 0.2752 - mse: 0.1959 - val_loss: 0.4209 - val_mae: 0.3455 - val_mse: 0.4209 - learning_rate: 0.0200 - val_custom_mse: 1.1845 - val_custom_mae: 0.7455\n",
            "Epoch 53/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1959 - mae: 0.2752 - mse: 0.1959 - val_loss: 0.4209 - val_mae: 0.3455 - val_mse: 0.4209 - learning_rate: 0.0200 - val_custom_mse: 1.1848 - val_custom_mae: 0.7457\n",
            "Epoch 54/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4210 - val_mae: 0.3454 - val_mse: 0.4210 - learning_rate: 0.0200 - val_custom_mse: 1.1851 - val_custom_mae: 0.7457\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2751 - mse: 0.1957 - val_loss: 0.4206 - val_mae: 0.3453 - val_mse: 0.4206 - learning_rate: 0.0200 - val_custom_mse: 1.1842 - val_custom_mae: 0.7454\n",
            "Epoch 56/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3454 - val_mse: 0.4209 - learning_rate: 0.0040 - val_custom_mse: 1.1852 - val_custom_mae: 0.7458\n",
            "Epoch 57/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1958 - mae: 0.2751 - mse: 0.1958 - val_loss: 0.4211 - val_mae: 0.3454 - val_mse: 0.4211 - learning_rate: 0.0040 - val_custom_mse: 1.1856 - val_custom_mae: 0.7459\n",
            "Epoch 58/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4209 - val_mae: 0.3453 - val_mse: 0.4209 - learning_rate: 0.0040 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 59/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3453 - val_mse: 0.4209 - learning_rate: 0.0040 - val_custom_mse: 1.1850 - val_custom_mae: 0.7457\n",
            "Epoch 60/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4210 - val_mae: 0.3453 - val_mse: 0.4210 - learning_rate: 0.0040 - val_custom_mse: 1.1855 - val_custom_mae: 0.7458\n",
            "Epoch 61/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4210 - val_mae: 0.3453 - val_mse: 0.4210 - learning_rate: 0.0040 - val_custom_mse: 1.1855 - val_custom_mae: 0.7458\n",
            "Epoch 62/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4209 - val_mae: 0.3453 - val_mse: 0.4209 - learning_rate: 0.0040 - val_custom_mse: 1.1853 - val_custom_mae: 0.7457\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2750 - mse: 0.1955 - val_loss: 0.4210 - val_mae: 0.3453 - val_mse: 0.4210 - learning_rate: 0.0040 - val_custom_mse: 1.1854 - val_custom_mae: 0.7458\n",
            "Epoch 64/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4210 - val_mae: 0.3453 - val_mse: 0.4210 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1855 - val_custom_mae: 0.7458\n",
            "Epoch 65/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4210 - val_mae: 0.3453 - val_mse: 0.4210 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1854 - val_custom_mae: 0.7458\n",
            "Epoch 66/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4210 - val_mae: 0.3453 - val_mse: 0.4210 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1854 - val_custom_mae: 0.7458\n",
            "Epoch 67/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2749 - mse: 0.1955 - val_loss: 0.4210 - val_mae: 0.3453 - val_mse: 0.4210 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1854 - val_custom_mae: 0.7458\n",
            "Epoch 68/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2749 - mse: 0.1955 - val_loss: 0.4210 - val_mae: 0.3453 - val_mse: 0.4210 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1855 - val_custom_mae: 0.7458\n",
            "Epoch 69/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4209 - val_mae: 0.3453 - val_mse: 0.4209 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1853 - val_custom_mae: 0.7457\n",
            "Epoch 70/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3453 - val_mse: 0.4209 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1853 - val_custom_mae: 0.7458\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2749 - mse: 0.1955 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 8.0000e-04 - val_custom_mse: 1.1854 - val_custom_mae: 0.7458\n",
            "Epoch 72/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1853 - val_custom_mae: 0.7457\n",
            "Epoch 73/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1958 - mae: 0.2750 - mse: 0.1958 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1853 - val_custom_mae: 0.7457\n",
            "Epoch 74/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2749 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 75/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 76/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 77/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 78/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2749 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.6000e-04 - val_custom_mse: 1.1853 - val_custom_mae: 0.7457\n",
            "Epoch 80/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2749 - mse: 0.1955 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 81/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2749 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 82/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2749 - mse: 0.1955 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 83/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 84/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2749 - mse: 0.1955 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 85/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 86/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2749 - mse: 0.1955 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2749 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 3.2000e-05 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 88/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 6.4000e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 89/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 6.4000e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 90/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2749 - mse: 0.1955 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 6.4000e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 91/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 6.4000e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 92/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2749 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 6.4000e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 93/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1954 - mae: 0.2750 - mse: 0.1954 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 6.4000e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 94/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 6.4000e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.2799998330592645e-06.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1955 - mae: 0.2749 - mse: 0.1955 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 6.4000e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 96/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.2800e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 97/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.2800e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 98/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.2800e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 99/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1956 - mae: 0.2750 - mse: 0.1956 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.2800e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Epoch 100/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1957 - mae: 0.2750 - mse: 0.1957 - val_loss: 0.4209 - val_mae: 0.3452 - val_mse: 0.4209 - learning_rate: 1.2800e-06 - val_custom_mse: 1.1852 - val_custom_mae: 0.7457\n",
            "Running experiment: horizon=720, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_29', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216/216 - 6s - 26ms/step - loss: 0.7053 - mae: 0.6159 - mse: 0.7053 - val_loss: 1.5174 - val_mae: 0.9091 - val_mse: 1.5174 - learning_rate: 0.1000 - val_custom_mse: 1.6657 - val_custom_mae: 0.9550\n",
            "Epoch 2/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6550 - mae: 0.5948 - mse: 0.6550 - val_loss: 1.4295 - val_mae: 0.8782 - val_mse: 1.4295 - learning_rate: 0.1000 - val_custom_mse: 1.5999 - val_custom_mae: 0.9322\n",
            "Epoch 3/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6217 - mae: 0.5773 - mse: 0.6217 - val_loss: 1.3398 - val_mae: 0.8445 - val_mse: 1.3398 - learning_rate: 0.1000 - val_custom_mse: 1.5280 - val_custom_mae: 0.9054\n",
            "Epoch 4/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5880 - mae: 0.5585 - mse: 0.5880 - val_loss: 1.2657 - val_mae: 0.8135 - val_mse: 1.2657 - learning_rate: 0.1000 - val_custom_mse: 1.4900 - val_custom_mae: 0.8896\n",
            "Epoch 5/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5537 - mae: 0.5382 - mse: 0.5537 - val_loss: 1.1856 - val_mae: 0.7772 - val_mse: 1.1856 - learning_rate: 0.1000 - val_custom_mse: 1.4387 - val_custom_mae: 0.8671\n",
            "Epoch 6/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5205 - mae: 0.5169 - mse: 0.5205 - val_loss: 1.1230 - val_mae: 0.7442 - val_mse: 1.1230 - learning_rate: 0.1000 - val_custom_mse: 1.4110 - val_custom_mae: 0.8530\n",
            "Epoch 7/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4907 - mae: 0.4961 - mse: 0.4907 - val_loss: 1.0727 - val_mae: 0.7131 - val_mse: 1.0727 - learning_rate: 0.1000 - val_custom_mse: 1.3896 - val_custom_mae: 0.8405\n",
            "Epoch 8/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4663 - mae: 0.4775 - mse: 0.4663 - val_loss: 1.0393 - val_mae: 0.6884 - val_mse: 1.0393 - learning_rate: 0.1000 - val_custom_mse: 1.3784 - val_custom_mae: 0.8320\n",
            "Epoch 9/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4480 - mae: 0.4623 - mse: 0.4480 - val_loss: 1.0173 - val_mae: 0.6713 - val_mse: 1.0173 - learning_rate: 0.1000 - val_custom_mse: 1.3691 - val_custom_mae: 0.8255\n",
            "Epoch 10/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4349 - mae: 0.4508 - mse: 0.4349 - val_loss: 1.0089 - val_mae: 0.6607 - val_mse: 1.0089 - learning_rate: 0.1000 - val_custom_mse: 1.3712 - val_custom_mae: 0.8232\n",
            "Epoch 11/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4258 - mae: 0.4423 - mse: 0.4258 - val_loss: 1.0039 - val_mae: 0.6533 - val_mse: 1.0039 - learning_rate: 0.1000 - val_custom_mse: 1.3734 - val_custom_mae: 0.8216\n",
            "Epoch 12/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4193 - mae: 0.4358 - mse: 0.4193 - val_loss: 0.9996 - val_mae: 0.6476 - val_mse: 0.9996 - learning_rate: 0.1000 - val_custom_mse: 1.3736 - val_custom_mae: 0.8199\n",
            "Epoch 13/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4143 - mae: 0.4306 - mse: 0.4143 - val_loss: 0.9980 - val_mae: 0.6440 - val_mse: 0.9980 - learning_rate: 0.1000 - val_custom_mse: 1.3754 - val_custom_mae: 0.8190\n",
            "Epoch 14/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4104 - mae: 0.4264 - mse: 0.4104 - val_loss: 0.9948 - val_mae: 0.6401 - val_mse: 0.9948 - learning_rate: 0.1000 - val_custom_mse: 1.3745 - val_custom_mae: 0.8173\n",
            "Epoch 15/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4072 - mae: 0.4228 - mse: 0.4072 - val_loss: 0.9934 - val_mae: 0.6364 - val_mse: 0.9934 - learning_rate: 0.1000 - val_custom_mse: 1.3765 - val_custom_mae: 0.8174\n",
            "Epoch 16/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4045 - mae: 0.4198 - mse: 0.4045 - val_loss: 0.9920 - val_mae: 0.6343 - val_mse: 0.9920 - learning_rate: 0.1000 - val_custom_mse: 1.3766 - val_custom_mae: 0.8169\n",
            "Epoch 17/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4022 - mae: 0.4171 - mse: 0.4022 - val_loss: 0.9913 - val_mae: 0.6313 - val_mse: 0.9913 - learning_rate: 0.1000 - val_custom_mse: 1.3793 - val_custom_mae: 0.8175\n",
            "Epoch 18/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4002 - mae: 0.4146 - mse: 0.4002 - val_loss: 0.9895 - val_mae: 0.6288 - val_mse: 0.9895 - learning_rate: 0.1000 - val_custom_mse: 1.3789 - val_custom_mae: 0.8169\n",
            "Epoch 19/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3983 - mae: 0.4123 - mse: 0.3983 - val_loss: 0.9859 - val_mae: 0.6264 - val_mse: 0.9859 - learning_rate: 0.1000 - val_custom_mse: 1.3748 - val_custom_mae: 0.8151\n",
            "Epoch 20/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4103 - mse: 0.3967 - val_loss: 0.9859 - val_mae: 0.6254 - val_mse: 0.9859 - learning_rate: 0.1000 - val_custom_mse: 1.3762 - val_custom_mae: 0.8154\n",
            "Epoch 21/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3951 - mae: 0.4085 - mse: 0.3951 - val_loss: 0.9857 - val_mae: 0.6226 - val_mse: 0.9857 - learning_rate: 0.1000 - val_custom_mse: 1.3790 - val_custom_mae: 0.8165\n",
            "Epoch 22/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3937 - mae: 0.4066 - mse: 0.3937 - val_loss: 0.9860 - val_mae: 0.6216 - val_mse: 0.9860 - learning_rate: 0.1000 - val_custom_mse: 1.3808 - val_custom_mae: 0.8174\n",
            "Epoch 23/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3926 - mae: 0.4052 - mse: 0.3926 - val_loss: 0.9813 - val_mae: 0.6185 - val_mse: 0.9813 - learning_rate: 0.1000 - val_custom_mse: 1.3754 - val_custom_mae: 0.8149\n",
            "Epoch 24/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3913 - mae: 0.4036 - mse: 0.3913 - val_loss: 0.9799 - val_mae: 0.6174 - val_mse: 0.9799 - learning_rate: 0.1000 - val_custom_mse: 1.3743 - val_custom_mae: 0.8147\n",
            "Epoch 25/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4021 - mse: 0.3903 - val_loss: 0.9806 - val_mae: 0.6156 - val_mse: 0.9806 - learning_rate: 0.1000 - val_custom_mse: 1.3770 - val_custom_mae: 0.8154\n",
            "Epoch 26/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3894 - mae: 0.4009 - mse: 0.3894 - val_loss: 0.9786 - val_mae: 0.6153 - val_mse: 0.9786 - learning_rate: 0.1000 - val_custom_mse: 1.3742 - val_custom_mae: 0.8145\n",
            "Epoch 27/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3884 - mae: 0.3996 - mse: 0.3884 - val_loss: 0.9787 - val_mae: 0.6133 - val_mse: 0.9787 - learning_rate: 0.1000 - val_custom_mse: 1.3759 - val_custom_mae: 0.8149\n",
            "Epoch 28/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3876 - mae: 0.3984 - mse: 0.3876 - val_loss: 0.9775 - val_mae: 0.6123 - val_mse: 0.9775 - learning_rate: 0.1000 - val_custom_mse: 1.3747 - val_custom_mae: 0.8141\n",
            "Epoch 29/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3868 - mae: 0.3973 - mse: 0.3868 - val_loss: 0.9785 - val_mae: 0.6115 - val_mse: 0.9785 - learning_rate: 0.1000 - val_custom_mse: 1.3773 - val_custom_mae: 0.8154\n",
            "Epoch 30/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3861 - mae: 0.3963 - mse: 0.3861 - val_loss: 0.9778 - val_mae: 0.6110 - val_mse: 0.9778 - learning_rate: 0.1000 - val_custom_mse: 1.3767 - val_custom_mae: 0.8150\n",
            "Epoch 31/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3854 - mae: 0.3953 - mse: 0.3854 - val_loss: 0.9751 - val_mae: 0.6087 - val_mse: 0.9751 - learning_rate: 0.1000 - val_custom_mse: 1.3737 - val_custom_mae: 0.8137\n",
            "Epoch 32/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3848 - mae: 0.3943 - mse: 0.3848 - val_loss: 0.9770 - val_mae: 0.6085 - val_mse: 0.9770 - learning_rate: 0.1000 - val_custom_mse: 1.3771 - val_custom_mae: 0.8148\n",
            "Epoch 33/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3841 - mae: 0.3934 - mse: 0.3841 - val_loss: 0.9749 - val_mae: 0.6074 - val_mse: 0.9749 - learning_rate: 0.1000 - val_custom_mse: 1.3743 - val_custom_mae: 0.8133\n",
            "Epoch 34/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3836 - mae: 0.3926 - mse: 0.3836 - val_loss: 0.9756 - val_mae: 0.6068 - val_mse: 0.9756 - learning_rate: 0.1000 - val_custom_mse: 1.3763 - val_custom_mae: 0.8148\n",
            "Epoch 35/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3831 - mae: 0.3918 - mse: 0.3831 - val_loss: 0.9721 - val_mae: 0.6044 - val_mse: 0.9721 - learning_rate: 0.1000 - val_custom_mse: 1.3718 - val_custom_mae: 0.8125\n",
            "Epoch 36/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3825 - mae: 0.3909 - mse: 0.3825 - val_loss: 0.9735 - val_mae: 0.6044 - val_mse: 0.9735 - learning_rate: 0.1000 - val_custom_mse: 1.3743 - val_custom_mae: 0.8135\n",
            "Epoch 37/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3821 - mae: 0.3903 - mse: 0.3821 - val_loss: 0.9731 - val_mae: 0.6034 - val_mse: 0.9731 - learning_rate: 0.1000 - val_custom_mse: 1.3741 - val_custom_mae: 0.8127\n",
            "Epoch 38/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3817 - mae: 0.3896 - mse: 0.3817 - val_loss: 0.9741 - val_mae: 0.6038 - val_mse: 0.9741 - learning_rate: 0.1000 - val_custom_mse: 1.3760 - val_custom_mae: 0.8146\n",
            "Epoch 39/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3812 - mae: 0.3889 - mse: 0.3812 - val_loss: 0.9735 - val_mae: 0.6023 - val_mse: 0.9735 - learning_rate: 0.1000 - val_custom_mse: 1.3758 - val_custom_mae: 0.8139\n",
            "Epoch 40/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3808 - mae: 0.3882 - mse: 0.3808 - val_loss: 0.9723 - val_mae: 0.6020 - val_mse: 0.9723 - learning_rate: 0.1000 - val_custom_mse: 1.3740 - val_custom_mae: 0.8128\n",
            "Epoch 41/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3804 - mae: 0.3876 - mse: 0.3804 - val_loss: 0.9714 - val_mae: 0.6004 - val_mse: 0.9714 - learning_rate: 0.1000 - val_custom_mse: 1.3734 - val_custom_mae: 0.8125\n",
            "Epoch 42/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3800 - mae: 0.3870 - mse: 0.3800 - val_loss: 0.9717 - val_mae: 0.6008 - val_mse: 0.9717 - learning_rate: 0.1000 - val_custom_mse: 1.3741 - val_custom_mae: 0.8133\n",
            "Epoch 43/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3798 - mae: 0.3865 - mse: 0.3798 - val_loss: 0.9700 - val_mae: 0.5994 - val_mse: 0.9700 - learning_rate: 0.1000 - val_custom_mse: 1.3718 - val_custom_mae: 0.8119\n",
            "Epoch 44/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3794 - mae: 0.3859 - mse: 0.3794 - val_loss: 0.9753 - val_mae: 0.6008 - val_mse: 0.9753 - learning_rate: 0.1000 - val_custom_mse: 1.3799 - val_custom_mae: 0.8156\n",
            "Epoch 45/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3790 - mae: 0.3854 - mse: 0.3790 - val_loss: 0.9729 - val_mae: 0.5992 - val_mse: 0.9729 - learning_rate: 0.1000 - val_custom_mse: 1.3769 - val_custom_mae: 0.8143\n",
            "Epoch 46/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3787 - mae: 0.3848 - mse: 0.3787 - val_loss: 0.9747 - val_mae: 0.5996 - val_mse: 0.9747 - learning_rate: 0.1000 - val_custom_mse: 1.3797 - val_custom_mae: 0.8155\n",
            "Epoch 47/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3785 - mae: 0.3845 - mse: 0.3785 - val_loss: 0.9712 - val_mae: 0.5973 - val_mse: 0.9712 - learning_rate: 0.1000 - val_custom_mse: 1.3749 - val_custom_mae: 0.8127\n",
            "Epoch 48/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3782 - mae: 0.3840 - mse: 0.3782 - val_loss: 0.9728 - val_mae: 0.5981 - val_mse: 0.9728 - learning_rate: 0.1000 - val_custom_mse: 1.3774 - val_custom_mae: 0.8143\n",
            "Epoch 49/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3779 - mae: 0.3834 - mse: 0.3779 - val_loss: 0.9716 - val_mae: 0.5976 - val_mse: 0.9716 - learning_rate: 0.1000 - val_custom_mse: 1.3756 - val_custom_mae: 0.8130\n",
            "Epoch 50/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3777 - mae: 0.3831 - mse: 0.3777 - val_loss: 0.9711 - val_mae: 0.5963 - val_mse: 0.9711 - learning_rate: 0.1000 - val_custom_mse: 1.3754 - val_custom_mae: 0.8132\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "216/216 - 2s - 8ms/step - loss: 0.3774 - mae: 0.3826 - mse: 0.3774 - val_loss: 0.9709 - val_mae: 0.5957 - val_mse: 0.9709 - learning_rate: 0.1000 - val_custom_mse: 1.3755 - val_custom_mae: 0.8130\n",
            "Epoch 52/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3771 - mae: 0.3824 - mse: 0.3771 - val_loss: 0.9738 - val_mae: 0.5969 - val_mse: 0.9738 - learning_rate: 0.0200 - val_custom_mse: 1.3797 - val_custom_mae: 0.8152\n",
            "Epoch 53/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3770 - mae: 0.3821 - mse: 0.3770 - val_loss: 0.9740 - val_mae: 0.5968 - val_mse: 0.9740 - learning_rate: 0.0200 - val_custom_mse: 1.3801 - val_custom_mae: 0.8153\n",
            "Epoch 54/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3770 - mae: 0.3820 - mse: 0.3770 - val_loss: 0.9746 - val_mae: 0.5970 - val_mse: 0.9746 - learning_rate: 0.0200 - val_custom_mse: 1.3809 - val_custom_mae: 0.8156\n",
            "Epoch 55/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3769 - mae: 0.3819 - mse: 0.3769 - val_loss: 0.9739 - val_mae: 0.5966 - val_mse: 0.9739 - learning_rate: 0.0200 - val_custom_mse: 1.3799 - val_custom_mae: 0.8151\n",
            "Epoch 56/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3769 - mae: 0.3819 - mse: 0.3769 - val_loss: 0.9743 - val_mae: 0.5967 - val_mse: 0.9743 - learning_rate: 0.0200 - val_custom_mse: 1.3805 - val_custom_mae: 0.8154\n",
            "Epoch 57/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3768 - mae: 0.3818 - mse: 0.3768 - val_loss: 0.9743 - val_mae: 0.5966 - val_mse: 0.9743 - learning_rate: 0.0200 - val_custom_mse: 1.3806 - val_custom_mae: 0.8155\n",
            "Epoch 58/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3768 - mae: 0.3817 - mse: 0.3768 - val_loss: 0.9751 - val_mae: 0.5969 - val_mse: 0.9751 - learning_rate: 0.0200 - val_custom_mse: 1.3818 - val_custom_mae: 0.8161\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3768 - mae: 0.3816 - mse: 0.3768 - val_loss: 0.9738 - val_mae: 0.5963 - val_mse: 0.9738 - learning_rate: 0.0200 - val_custom_mse: 1.3800 - val_custom_mae: 0.8152\n",
            "Epoch 60/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3767 - mae: 0.3816 - mse: 0.3767 - val_loss: 0.9739 - val_mae: 0.5964 - val_mse: 0.9739 - learning_rate: 0.0040 - val_custom_mse: 1.3801 - val_custom_mae: 0.8151\n",
            "Epoch 61/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3767 - mae: 0.3815 - mse: 0.3767 - val_loss: 0.9740 - val_mae: 0.5964 - val_mse: 0.9740 - learning_rate: 0.0040 - val_custom_mse: 1.3802 - val_custom_mae: 0.8151\n",
            "Epoch 62/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3767 - mae: 0.3815 - mse: 0.3767 - val_loss: 0.9740 - val_mae: 0.5963 - val_mse: 0.9740 - learning_rate: 0.0040 - val_custom_mse: 1.3802 - val_custom_mae: 0.8151\n",
            "Epoch 63/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3815 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5962 - val_mse: 0.9740 - learning_rate: 0.0040 - val_custom_mse: 1.3802 - val_custom_mae: 0.8150\n",
            "Epoch 64/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3815 - mse: 0.3766 - val_loss: 0.9739 - val_mae: 0.5962 - val_mse: 0.9739 - learning_rate: 0.0040 - val_custom_mse: 1.3801 - val_custom_mae: 0.8150\n",
            "Epoch 65/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5962 - val_mse: 0.9740 - learning_rate: 0.0040 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 66/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9739 - val_mae: 0.5961 - val_mse: 0.9739 - learning_rate: 0.0040 - val_custom_mse: 1.3802 - val_custom_mae: 0.8150\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9741 - val_mae: 0.5962 - val_mse: 0.9741 - learning_rate: 0.0040 - val_custom_mse: 1.3804 - val_custom_mae: 0.8151\n",
            "Epoch 68/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9741 - val_mae: 0.5962 - val_mse: 0.9741 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3804 - val_custom_mae: 0.8151\n",
            "Epoch 69/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9741 - val_mae: 0.5962 - val_mse: 0.9741 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3804 - val_custom_mae: 0.8151\n",
            "Epoch 70/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9741 - val_mae: 0.5962 - val_mse: 0.9741 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3804 - val_custom_mae: 0.8151\n",
            "Epoch 71/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9741 - val_mae: 0.5962 - val_mse: 0.9741 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3804 - val_custom_mae: 0.8151\n",
            "Epoch 72/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9741 - val_mae: 0.5962 - val_mse: 0.9741 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3804 - val_custom_mae: 0.8151\n",
            "Epoch 73/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5962 - val_mse: 0.9740 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 74/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 75: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 76/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 77/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 78/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 79/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 80/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3804 - val_custom_mae: 0.8150\n",
            "Epoch 81/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 82/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3804 - val_custom_mae: 0.8150\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "216/216 - 2s - 8ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 84/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 85/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 86/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 87/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 88/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 89/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 90/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "216/216 - 2s - 8ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 92/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 93/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 94/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 95/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 96/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 97/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 98/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.2799998330592645e-06.\n",
            "216/216 - 2s - 8ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Epoch 100/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3766 - mae: 0.3814 - mse: 0.3766 - val_loss: 0.9740 - val_mae: 0.5961 - val_mse: 0.9740 - learning_rate: 1.2800e-06 - val_custom_mse: 1.3803 - val_custom_mae: 0.8150\n",
            "Running experiment: horizon=720, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_30', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216/216 - 6s - 28ms/step - loss: 0.7128 - mae: 0.6186 - mse: 0.7128 - val_loss: 1.5169 - val_mae: 0.9099 - val_mse: 1.5169 - learning_rate: 0.1000 - val_custom_mse: 1.6649 - val_custom_mae: 0.9558\n",
            "Epoch 2/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6576 - mae: 0.5956 - mse: 0.6576 - val_loss: 1.4321 - val_mae: 0.8788 - val_mse: 1.4321 - learning_rate: 0.1000 - val_custom_mse: 1.6077 - val_custom_mae: 0.9346\n",
            "Epoch 3/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6221 - mae: 0.5771 - mse: 0.6221 - val_loss: 1.3526 - val_mae: 0.8481 - val_mse: 1.3526 - learning_rate: 0.1000 - val_custom_mse: 1.5564 - val_custom_mae: 0.9150\n",
            "Epoch 4/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5865 - mae: 0.5575 - mse: 0.5865 - val_loss: 1.2621 - val_mae: 0.8116 - val_mse: 1.2621 - learning_rate: 0.1000 - val_custom_mse: 1.4906 - val_custom_mae: 0.8894\n",
            "Epoch 5/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5513 - mae: 0.5366 - mse: 0.5513 - val_loss: 1.1845 - val_mae: 0.7758 - val_mse: 1.1845 - learning_rate: 0.1000 - val_custom_mse: 1.4442 - val_custom_mae: 0.8686\n",
            "Epoch 6/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5181 - mae: 0.5152 - mse: 0.5181 - val_loss: 1.1271 - val_mae: 0.7445 - val_mse: 1.1271 - learning_rate: 0.1000 - val_custom_mse: 1.4207 - val_custom_mae: 0.8558\n",
            "Epoch 7/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4893 - mae: 0.4949 - mse: 0.4893 - val_loss: 1.0773 - val_mae: 0.7142 - val_mse: 1.0773 - learning_rate: 0.1000 - val_custom_mse: 1.3978 - val_custom_mae: 0.8429\n",
            "Epoch 8/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4663 - mae: 0.4774 - mse: 0.4663 - val_loss: 1.0391 - val_mae: 0.6886 - val_mse: 1.0391 - learning_rate: 0.1000 - val_custom_mse: 1.3774 - val_custom_mae: 0.8312\n",
            "Epoch 9/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4496 - mae: 0.4636 - mse: 0.4496 - val_loss: 1.0205 - val_mae: 0.6723 - val_mse: 1.0205 - learning_rate: 0.1000 - val_custom_mse: 1.3721 - val_custom_mae: 0.8253\n",
            "Epoch 10/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4380 - mae: 0.4534 - mse: 0.4380 - val_loss: 1.0130 - val_mae: 0.6625 - val_mse: 1.0130 - learning_rate: 0.1000 - val_custom_mse: 1.3753 - val_custom_mae: 0.8238\n",
            "Epoch 11/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4299 - mae: 0.4460 - mse: 0.4299 - val_loss: 1.0071 - val_mae: 0.6551 - val_mse: 1.0071 - learning_rate: 0.1000 - val_custom_mse: 1.3759 - val_custom_mae: 0.8218\n",
            "Epoch 12/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4244 - mae: 0.4407 - mse: 0.4244 - val_loss: 1.0048 - val_mae: 0.6506 - val_mse: 1.0048 - learning_rate: 0.1000 - val_custom_mse: 1.3785 - val_custom_mae: 0.8211\n",
            "Epoch 13/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4202 - mae: 0.4364 - mse: 0.4202 - val_loss: 1.0059 - val_mae: 0.6480 - val_mse: 1.0059 - learning_rate: 0.1000 - val_custom_mse: 1.3850 - val_custom_mae: 0.8226\n",
            "Epoch 14/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4169 - mae: 0.4331 - mse: 0.4169 - val_loss: 1.0060 - val_mae: 0.6462 - val_mse: 1.0060 - learning_rate: 0.1000 - val_custom_mse: 1.3884 - val_custom_mae: 0.8234\n",
            "Epoch 15/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4142 - mae: 0.4302 - mse: 0.4142 - val_loss: 0.9973 - val_mae: 0.6396 - val_mse: 0.9973 - learning_rate: 0.1000 - val_custom_mse: 1.3799 - val_custom_mae: 0.8189\n",
            "Epoch 16/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4117 - mae: 0.4276 - mse: 0.4117 - val_loss: 0.9978 - val_mae: 0.6380 - val_mse: 0.9978 - learning_rate: 0.1000 - val_custom_mse: 1.3833 - val_custom_mae: 0.8198\n",
            "Epoch 17/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4099 - mae: 0.4255 - mse: 0.4099 - val_loss: 0.9969 - val_mae: 0.6357 - val_mse: 0.9969 - learning_rate: 0.1000 - val_custom_mse: 1.3846 - val_custom_mae: 0.8200\n",
            "Epoch 18/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4081 - mae: 0.4236 - mse: 0.4081 - val_loss: 0.9952 - val_mae: 0.6339 - val_mse: 0.9952 - learning_rate: 0.1000 - val_custom_mse: 1.3841 - val_custom_mae: 0.8198\n",
            "Epoch 19/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4066 - mae: 0.4219 - mse: 0.4066 - val_loss: 0.9923 - val_mae: 0.6315 - val_mse: 0.9923 - learning_rate: 0.1000 - val_custom_mse: 1.3818 - val_custom_mae: 0.8187\n",
            "Epoch 20/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4053 - mae: 0.4204 - mse: 0.4053 - val_loss: 0.9940 - val_mae: 0.6297 - val_mse: 0.9940 - learning_rate: 0.1000 - val_custom_mse: 1.3865 - val_custom_mae: 0.8196\n",
            "Epoch 21/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4039 - mae: 0.4189 - mse: 0.4039 - val_loss: 0.9909 - val_mae: 0.6280 - val_mse: 0.9909 - learning_rate: 0.1000 - val_custom_mse: 1.3832 - val_custom_mae: 0.8188\n",
            "Epoch 22/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4028 - mae: 0.4177 - mse: 0.4028 - val_loss: 0.9911 - val_mae: 0.6274 - val_mse: 0.9911 - learning_rate: 0.1000 - val_custom_mse: 1.3846 - val_custom_mae: 0.8195\n",
            "Epoch 23/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4017 - mae: 0.4164 - mse: 0.4017 - val_loss: 0.9834 - val_mae: 0.6227 - val_mse: 0.9834 - learning_rate: 0.1000 - val_custom_mse: 1.3750 - val_custom_mae: 0.8148\n",
            "Epoch 24/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4007 - mae: 0.4152 - mse: 0.4007 - val_loss: 0.9862 - val_mae: 0.6227 - val_mse: 0.9862 - learning_rate: 0.1000 - val_custom_mse: 1.3805 - val_custom_mae: 0.8174\n",
            "Epoch 25/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4000 - mae: 0.4143 - mse: 0.4000 - val_loss: 0.9870 - val_mae: 0.6225 - val_mse: 0.9870 - learning_rate: 0.1000 - val_custom_mse: 1.3825 - val_custom_mae: 0.8184\n",
            "Epoch 26/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3991 - mae: 0.4133 - mse: 0.3991 - val_loss: 0.9836 - val_mae: 0.6202 - val_mse: 0.9836 - learning_rate: 0.1000 - val_custom_mse: 1.3786 - val_custom_mae: 0.8166\n",
            "Epoch 27/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3984 - mae: 0.4124 - mse: 0.3984 - val_loss: 0.9867 - val_mae: 0.6208 - val_mse: 0.9867 - learning_rate: 0.1000 - val_custom_mse: 1.3838 - val_custom_mae: 0.8187\n",
            "Epoch 28/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3978 - mae: 0.4117 - mse: 0.3978 - val_loss: 0.9881 - val_mae: 0.6209 - val_mse: 0.9881 - learning_rate: 0.1000 - val_custom_mse: 1.3863 - val_custom_mae: 0.8196\n",
            "Epoch 29/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3971 - mae: 0.4108 - mse: 0.3971 - val_loss: 0.9824 - val_mae: 0.6179 - val_mse: 0.9824 - learning_rate: 0.1000 - val_custom_mse: 1.3790 - val_custom_mae: 0.8167\n",
            "Epoch 30/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3965 - mae: 0.4102 - mse: 0.3965 - val_loss: 0.9827 - val_mae: 0.6170 - val_mse: 0.9827 - learning_rate: 0.1000 - val_custom_mse: 1.3803 - val_custom_mae: 0.8169\n",
            "Epoch 31/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3958 - mae: 0.4094 - mse: 0.3958 - val_loss: 0.9813 - val_mae: 0.6171 - val_mse: 0.9813 - learning_rate: 0.1000 - val_custom_mse: 1.3782 - val_custom_mae: 0.8166\n",
            "Epoch 32/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3954 - mae: 0.4089 - mse: 0.3954 - val_loss: 0.9800 - val_mae: 0.6151 - val_mse: 0.9800 - learning_rate: 0.1000 - val_custom_mse: 1.3773 - val_custom_mae: 0.8157\n",
            "Epoch 33/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3949 - mae: 0.4082 - mse: 0.3949 - val_loss: 0.9771 - val_mae: 0.6131 - val_mse: 0.9771 - learning_rate: 0.1000 - val_custom_mse: 1.3739 - val_custom_mae: 0.8143\n",
            "Epoch 34/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3943 - mae: 0.4076 - mse: 0.3943 - val_loss: 0.9791 - val_mae: 0.6132 - val_mse: 0.9791 - learning_rate: 0.1000 - val_custom_mse: 1.3771 - val_custom_mae: 0.8151\n",
            "Epoch 35/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3939 - mae: 0.4071 - mse: 0.3939 - val_loss: 0.9791 - val_mae: 0.6126 - val_mse: 0.9791 - learning_rate: 0.1000 - val_custom_mse: 1.3776 - val_custom_mae: 0.8152\n",
            "Epoch 36/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3935 - mae: 0.4066 - mse: 0.3935 - val_loss: 0.9812 - val_mae: 0.6130 - val_mse: 0.9812 - learning_rate: 0.1000 - val_custom_mse: 1.3813 - val_custom_mae: 0.8170\n",
            "Epoch 37/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3932 - mae: 0.4061 - mse: 0.3932 - val_loss: 0.9803 - val_mae: 0.6121 - val_mse: 0.9803 - learning_rate: 0.1000 - val_custom_mse: 1.3803 - val_custom_mae: 0.8165\n",
            "Epoch 38/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3931 - mae: 0.4058 - mse: 0.3931 - val_loss: 0.9798 - val_mae: 0.6117 - val_mse: 0.9798 - learning_rate: 0.1000 - val_custom_mse: 1.3800 - val_custom_mae: 0.8164\n",
            "Epoch 39/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3926 - mae: 0.4052 - mse: 0.3926 - val_loss: 0.9798 - val_mae: 0.6117 - val_mse: 0.9798 - learning_rate: 0.1000 - val_custom_mse: 1.3801 - val_custom_mae: 0.8165\n",
            "Epoch 40/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3921 - mae: 0.4049 - mse: 0.3921 - val_loss: 0.9807 - val_mae: 0.6119 - val_mse: 0.9807 - learning_rate: 0.1000 - val_custom_mse: 1.3815 - val_custom_mae: 0.8170\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3919 - mae: 0.4046 - mse: 0.3919 - val_loss: 0.9801 - val_mae: 0.6111 - val_mse: 0.9801 - learning_rate: 0.1000 - val_custom_mse: 1.3812 - val_custom_mae: 0.8170\n",
            "Epoch 42/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3915 - mae: 0.4043 - mse: 0.3915 - val_loss: 0.9769 - val_mae: 0.6088 - val_mse: 0.9769 - learning_rate: 0.0200 - val_custom_mse: 1.3769 - val_custom_mae: 0.8148\n",
            "Epoch 43/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3914 - mae: 0.4041 - mse: 0.3914 - val_loss: 0.9760 - val_mae: 0.6083 - val_mse: 0.9760 - learning_rate: 0.0200 - val_custom_mse: 1.3756 - val_custom_mae: 0.8140\n",
            "Epoch 44/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3913 - mae: 0.4039 - mse: 0.3913 - val_loss: 0.9771 - val_mae: 0.6086 - val_mse: 0.9771 - learning_rate: 0.0200 - val_custom_mse: 1.3773 - val_custom_mae: 0.8149\n",
            "Epoch 45/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3914 - mae: 0.4039 - mse: 0.3914 - val_loss: 0.9757 - val_mae: 0.6079 - val_mse: 0.9757 - learning_rate: 0.0200 - val_custom_mse: 1.3754 - val_custom_mae: 0.8138\n",
            "Epoch 46/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3912 - mae: 0.4038 - mse: 0.3912 - val_loss: 0.9776 - val_mae: 0.6089 - val_mse: 0.9776 - learning_rate: 0.0200 - val_custom_mse: 1.3782 - val_custom_mae: 0.8153\n",
            "Epoch 47/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3911 - mae: 0.4038 - mse: 0.3911 - val_loss: 0.9769 - val_mae: 0.6086 - val_mse: 0.9769 - learning_rate: 0.0200 - val_custom_mse: 1.3773 - val_custom_mae: 0.8149\n",
            "Epoch 48/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3912 - mae: 0.4037 - mse: 0.3912 - val_loss: 0.9767 - val_mae: 0.6082 - val_mse: 0.9767 - learning_rate: 0.0200 - val_custom_mse: 1.3771 - val_custom_mae: 0.8147\n",
            "Epoch 49/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3911 - mae: 0.4036 - mse: 0.3911 - val_loss: 0.9758 - val_mae: 0.6078 - val_mse: 0.9758 - learning_rate: 0.0200 - val_custom_mse: 1.3758 - val_custom_mae: 0.8140\n",
            "Epoch 50/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3912 - mae: 0.4037 - mse: 0.3912 - val_loss: 0.9770 - val_mae: 0.6085 - val_mse: 0.9770 - learning_rate: 0.0200 - val_custom_mse: 1.3775 - val_custom_mae: 0.8150\n",
            "Epoch 51/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3910 - mae: 0.4035 - mse: 0.3910 - val_loss: 0.9767 - val_mae: 0.6080 - val_mse: 0.9767 - learning_rate: 0.0200 - val_custom_mse: 1.3771 - val_custom_mae: 0.8147\n",
            "Epoch 52/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3910 - mae: 0.4035 - mse: 0.3910 - val_loss: 0.9766 - val_mae: 0.6079 - val_mse: 0.9766 - learning_rate: 0.0200 - val_custom_mse: 1.3770 - val_custom_mae: 0.8145\n",
            "Epoch 53/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3909 - mae: 0.4033 - mse: 0.3909 - val_loss: 0.9755 - val_mae: 0.6074 - val_mse: 0.9755 - learning_rate: 0.0200 - val_custom_mse: 1.3755 - val_custom_mae: 0.8138\n",
            "Epoch 54/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3908 - mae: 0.4033 - mse: 0.3908 - val_loss: 0.9776 - val_mae: 0.6084 - val_mse: 0.9776 - learning_rate: 0.0200 - val_custom_mse: 1.3786 - val_custom_mae: 0.8154\n",
            "Epoch 55/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3907 - mae: 0.4032 - mse: 0.3907 - val_loss: 0.9765 - val_mae: 0.6077 - val_mse: 0.9765 - learning_rate: 0.0200 - val_custom_mse: 1.3770 - val_custom_mae: 0.8145\n",
            "Epoch 56/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3908 - mae: 0.4032 - mse: 0.3908 - val_loss: 0.9763 - val_mae: 0.6077 - val_mse: 0.9763 - learning_rate: 0.0200 - val_custom_mse: 1.3767 - val_custom_mae: 0.8145\n",
            "Epoch 57/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3908 - mae: 0.4032 - mse: 0.3908 - val_loss: 0.9765 - val_mae: 0.6077 - val_mse: 0.9765 - learning_rate: 0.0200 - val_custom_mse: 1.3771 - val_custom_mae: 0.8146\n",
            "Epoch 58/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3907 - mae: 0.4031 - mse: 0.3907 - val_loss: 0.9772 - val_mae: 0.6079 - val_mse: 0.9772 - learning_rate: 0.0200 - val_custom_mse: 1.3782 - val_custom_mae: 0.8151\n",
            "Epoch 59/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3906 - mae: 0.4030 - mse: 0.3906 - val_loss: 0.9775 - val_mae: 0.6080 - val_mse: 0.9775 - learning_rate: 0.0200 - val_custom_mse: 1.3786 - val_custom_mae: 0.8153\n",
            "Epoch 60/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3906 - mae: 0.4030 - mse: 0.3906 - val_loss: 0.9773 - val_mae: 0.6079 - val_mse: 0.9773 - learning_rate: 0.0200 - val_custom_mse: 1.3784 - val_custom_mae: 0.8153\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3905 - mae: 0.4029 - mse: 0.3905 - val_loss: 0.9764 - val_mae: 0.6074 - val_mse: 0.9764 - learning_rate: 0.0200 - val_custom_mse: 1.3771 - val_custom_mae: 0.8146\n",
            "Epoch 62/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3905 - mae: 0.4031 - mse: 0.3905 - val_loss: 0.9768 - val_mae: 0.6076 - val_mse: 0.9768 - learning_rate: 0.0040 - val_custom_mse: 1.3778 - val_custom_mae: 0.8149\n",
            "Epoch 63/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4029 - mse: 0.3904 - val_loss: 0.9767 - val_mae: 0.6075 - val_mse: 0.9767 - learning_rate: 0.0040 - val_custom_mse: 1.3777 - val_custom_mae: 0.8149\n",
            "Epoch 64/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4028 - mse: 0.3904 - val_loss: 0.9767 - val_mae: 0.6074 - val_mse: 0.9767 - learning_rate: 0.0040 - val_custom_mse: 1.3777 - val_custom_mae: 0.8148\n",
            "Epoch 65/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4028 - mse: 0.3904 - val_loss: 0.9767 - val_mae: 0.6074 - val_mse: 0.9767 - learning_rate: 0.0040 - val_custom_mse: 1.3777 - val_custom_mae: 0.8148\n",
            "Epoch 66/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4028 - mse: 0.3904 - val_loss: 0.9766 - val_mae: 0.6073 - val_mse: 0.9766 - learning_rate: 0.0040 - val_custom_mse: 1.3775 - val_custom_mae: 0.8148\n",
            "Epoch 67/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4027 - mse: 0.3904 - val_loss: 0.9766 - val_mae: 0.6073 - val_mse: 0.9766 - learning_rate: 0.0040 - val_custom_mse: 1.3776 - val_custom_mae: 0.8148\n",
            "Epoch 68/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3902 - mae: 0.4027 - mse: 0.3902 - val_loss: 0.9765 - val_mae: 0.6072 - val_mse: 0.9765 - learning_rate: 0.0040 - val_custom_mse: 1.3775 - val_custom_mae: 0.8147\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "216/216 - 2s - 8ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6072 - val_mse: 0.9765 - learning_rate: 0.0040 - val_custom_mse: 1.3775 - val_custom_mae: 0.8148\n",
            "Epoch 70/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3902 - mae: 0.4027 - mse: 0.3902 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3773 - val_custom_mae: 0.8146\n",
            "Epoch 71/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4028 - mse: 0.3904 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3772 - val_custom_mae: 0.8146\n",
            "Epoch 72/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4027 - mse: 0.3904 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3773 - val_custom_mae: 0.8146\n",
            "Epoch 73/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3905 - mae: 0.4028 - mse: 0.3905 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3773 - val_custom_mae: 0.8146\n",
            "Epoch 74/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4027 - mse: 0.3904 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 75/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3905 - mae: 0.4028 - mse: 0.3905 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3773 - val_custom_mae: 0.8146\n",
            "Epoch 76/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 78/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4028 - mse: 0.3904 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8147\n",
            "Epoch 79/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3902 - mae: 0.4027 - mse: 0.3902 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 80/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 81/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3902 - mae: 0.4027 - mse: 0.3902 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 82/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3902 - mae: 0.4027 - mse: 0.3902 - val_loss: 0.9764 - val_mae: 0.6071 - val_mse: 0.9764 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 83/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 84/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4027 - mse: 0.3904 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "216/216 - 2s - 8ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 86/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4027 - mse: 0.3904 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 87/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4027 - mse: 0.3904 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 88/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 89/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 90/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 91/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4027 - mse: 0.3904 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 92/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3902 - mae: 0.4027 - mse: 0.3902 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 94/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4027 - mse: 0.3904 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 95/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4027 - mse: 0.3904 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 96/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3904 - mae: 0.4028 - mse: 0.3904 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 97/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3902 - mae: 0.4026 - mse: 0.3902 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 98/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3902 - mae: 0.4027 - mse: 0.3902 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 99/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Epoch 100/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3903 - mae: 0.4027 - mse: 0.3903 - val_loss: 0.9765 - val_mae: 0.6071 - val_mse: 0.9765 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3774 - val_custom_mae: 0.8146\n",
            "Running experiment: horizon=720, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_31', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216/216 - 6s - 28ms/step - loss: 0.7204 - mae: 0.6213 - mse: 0.7204 - val_loss: 1.5267 - val_mae: 0.9129 - val_mse: 1.5267 - learning_rate: 0.1000 - val_custom_mse: 1.6848 - val_custom_mae: 0.9624\n",
            "Epoch 2/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6606 - mae: 0.5968 - mse: 0.6606 - val_loss: 1.4285 - val_mae: 0.8777 - val_mse: 1.4285 - learning_rate: 0.1000 - val_custom_mse: 1.6051 - val_custom_mae: 0.9339\n",
            "Epoch 3/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6230 - mae: 0.5775 - mse: 0.6230 - val_loss: 1.3524 - val_mae: 0.8477 - val_mse: 1.3524 - learning_rate: 0.1000 - val_custom_mse: 1.5583 - val_custom_mae: 0.9155\n",
            "Epoch 4/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5863 - mae: 0.5573 - mse: 0.5863 - val_loss: 1.2568 - val_mae: 0.8094 - val_mse: 1.2568 - learning_rate: 0.1000 - val_custom_mse: 1.4864 - val_custom_mae: 0.8877\n",
            "Epoch 5/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5504 - mae: 0.5359 - mse: 0.5504 - val_loss: 1.1785 - val_mae: 0.7734 - val_mse: 1.1785 - learning_rate: 0.1000 - val_custom_mse: 1.4390 - val_custom_mae: 0.8668\n",
            "Epoch 6/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5175 - mae: 0.5146 - mse: 0.5175 - val_loss: 1.1212 - val_mae: 0.7419 - val_mse: 1.1212 - learning_rate: 0.1000 - val_custom_mse: 1.4148 - val_custom_mae: 0.8534\n",
            "Epoch 7/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4897 - mae: 0.4950 - mse: 0.4897 - val_loss: 1.0716 - val_mae: 0.7125 - val_mse: 1.0716 - learning_rate: 0.1000 - val_custom_mse: 1.3899 - val_custom_mae: 0.8403\n",
            "Epoch 8/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4679 - mae: 0.4784 - mse: 0.4679 - val_loss: 1.0378 - val_mae: 0.6889 - val_mse: 1.0378 - learning_rate: 0.1000 - val_custom_mse: 1.3742 - val_custom_mae: 0.8304\n",
            "Epoch 9/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4525 - mae: 0.4658 - mse: 0.4525 - val_loss: 1.0234 - val_mae: 0.6742 - val_mse: 1.0234 - learning_rate: 0.1000 - val_custom_mse: 1.3751 - val_custom_mae: 0.8271\n",
            "Epoch 10/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4421 - mae: 0.4568 - mse: 0.4421 - val_loss: 1.0130 - val_mae: 0.6647 - val_mse: 1.0130 - learning_rate: 0.1000 - val_custom_mse: 1.3726 - val_custom_mae: 0.8242\n",
            "Epoch 11/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4351 - mae: 0.4506 - mse: 0.4351 - val_loss: 1.0060 - val_mae: 0.6577 - val_mse: 1.0060 - learning_rate: 0.1000 - val_custom_mse: 1.3704 - val_custom_mae: 0.8213\n",
            "Epoch 12/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4302 - mae: 0.4460 - mse: 0.4302 - val_loss: 1.0057 - val_mae: 0.6535 - val_mse: 1.0057 - learning_rate: 0.1000 - val_custom_mse: 1.3764 - val_custom_mae: 0.8218\n",
            "Epoch 13/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4264 - mae: 0.4425 - mse: 0.4264 - val_loss: 1.0010 - val_mae: 0.6496 - val_mse: 1.0010 - learning_rate: 0.1000 - val_custom_mse: 1.3739 - val_custom_mae: 0.8203\n",
            "Epoch 14/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4236 - mae: 0.4396 - mse: 0.4236 - val_loss: 1.0022 - val_mae: 0.6469 - val_mse: 1.0022 - learning_rate: 0.1000 - val_custom_mse: 1.3802 - val_custom_mae: 0.8216\n",
            "Epoch 15/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4211 - mae: 0.4374 - mse: 0.4211 - val_loss: 0.9982 - val_mae: 0.6436 - val_mse: 0.9982 - learning_rate: 0.1000 - val_custom_mse: 1.3773 - val_custom_mae: 0.8201\n",
            "Epoch 16/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4190 - mae: 0.4353 - mse: 0.4190 - val_loss: 0.9951 - val_mae: 0.6407 - val_mse: 0.9951 - learning_rate: 0.1000 - val_custom_mse: 1.3756 - val_custom_mae: 0.8190\n",
            "Epoch 17/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4171 - mae: 0.4334 - mse: 0.4171 - val_loss: 0.9939 - val_mae: 0.6381 - val_mse: 0.9939 - learning_rate: 0.1000 - val_custom_mse: 1.3763 - val_custom_mae: 0.8182\n",
            "Epoch 18/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4159 - mae: 0.4321 - mse: 0.4159 - val_loss: 0.9869 - val_mae: 0.6344 - val_mse: 0.9869 - learning_rate: 0.1000 - val_custom_mse: 1.3678 - val_custom_mae: 0.8147\n",
            "Epoch 19/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4144 - mae: 0.4308 - mse: 0.4144 - val_loss: 0.9867 - val_mae: 0.6335 - val_mse: 0.9867 - learning_rate: 0.1000 - val_custom_mse: 1.3698 - val_custom_mae: 0.8161\n",
            "Epoch 20/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4130 - mae: 0.4294 - mse: 0.4130 - val_loss: 0.9878 - val_mae: 0.6321 - val_mse: 0.9878 - learning_rate: 0.1000 - val_custom_mse: 1.3731 - val_custom_mae: 0.8167\n",
            "Epoch 21/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4119 - mae: 0.4283 - mse: 0.4119 - val_loss: 0.9859 - val_mae: 0.6313 - val_mse: 0.9859 - learning_rate: 0.1000 - val_custom_mse: 1.3712 - val_custom_mae: 0.8165\n",
            "Epoch 22/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4109 - mae: 0.4274 - mse: 0.4109 - val_loss: 0.9837 - val_mae: 0.6292 - val_mse: 0.9837 - learning_rate: 0.1000 - val_custom_mse: 1.3696 - val_custom_mae: 0.8154\n",
            "Epoch 23/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4099 - mae: 0.4264 - mse: 0.4099 - val_loss: 0.9823 - val_mae: 0.6279 - val_mse: 0.9823 - learning_rate: 0.1000 - val_custom_mse: 1.3686 - val_custom_mae: 0.8150\n",
            "Epoch 24/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4092 - mae: 0.4256 - mse: 0.4092 - val_loss: 0.9817 - val_mae: 0.6269 - val_mse: 0.9817 - learning_rate: 0.1000 - val_custom_mse: 1.3687 - val_custom_mae: 0.8147\n",
            "Epoch 25/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4082 - mae: 0.4248 - mse: 0.4082 - val_loss: 0.9815 - val_mae: 0.6263 - val_mse: 0.9815 - learning_rate: 0.1000 - val_custom_mse: 1.3692 - val_custom_mae: 0.8150\n",
            "Epoch 26/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4074 - mae: 0.4240 - mse: 0.4074 - val_loss: 0.9833 - val_mae: 0.6260 - val_mse: 0.9833 - learning_rate: 0.1000 - val_custom_mse: 1.3728 - val_custom_mae: 0.8164\n",
            "Epoch 27/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4068 - mae: 0.4234 - mse: 0.4068 - val_loss: 0.9782 - val_mae: 0.6228 - val_mse: 0.9782 - learning_rate: 0.1000 - val_custom_mse: 1.3665 - val_custom_mae: 0.8133\n",
            "Epoch 28/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4063 - mae: 0.4228 - mse: 0.4063 - val_loss: 0.9793 - val_mae: 0.6229 - val_mse: 0.9793 - learning_rate: 0.1000 - val_custom_mse: 1.3687 - val_custom_mae: 0.8142\n",
            "Epoch 29/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4058 - mae: 0.4223 - mse: 0.4058 - val_loss: 0.9789 - val_mae: 0.6225 - val_mse: 0.9789 - learning_rate: 0.1000 - val_custom_mse: 1.3685 - val_custom_mae: 0.8142\n",
            "Epoch 30/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4052 - mae: 0.4218 - mse: 0.4052 - val_loss: 0.9779 - val_mae: 0.6217 - val_mse: 0.9779 - learning_rate: 0.1000 - val_custom_mse: 1.3678 - val_custom_mae: 0.8141\n",
            "Epoch 31/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4047 - mae: 0.4213 - mse: 0.4047 - val_loss: 0.9769 - val_mae: 0.6207 - val_mse: 0.9769 - learning_rate: 0.1000 - val_custom_mse: 1.3669 - val_custom_mae: 0.8134\n",
            "Epoch 32/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4041 - mae: 0.4208 - mse: 0.4041 - val_loss: 0.9784 - val_mae: 0.6208 - val_mse: 0.9784 - learning_rate: 0.1000 - val_custom_mse: 1.3695 - val_custom_mae: 0.8144\n",
            "Epoch 33/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4035 - mae: 0.4203 - mse: 0.4035 - val_loss: 0.9799 - val_mae: 0.6216 - val_mse: 0.9799 - learning_rate: 0.1000 - val_custom_mse: 1.3718 - val_custom_mae: 0.8156\n",
            "Epoch 34/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4034 - mae: 0.4200 - mse: 0.4034 - val_loss: 0.9789 - val_mae: 0.6212 - val_mse: 0.9789 - learning_rate: 0.1000 - val_custom_mse: 1.3704 - val_custom_mae: 0.8151\n",
            "Epoch 35/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4028 - mae: 0.4196 - mse: 0.4028 - val_loss: 0.9795 - val_mae: 0.6204 - val_mse: 0.9795 - learning_rate: 0.1000 - val_custom_mse: 1.3721 - val_custom_mae: 0.8155\n",
            "Epoch 36/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4026 - mae: 0.4192 - mse: 0.4026 - val_loss: 0.9782 - val_mae: 0.6202 - val_mse: 0.9782 - learning_rate: 0.1000 - val_custom_mse: 1.3703 - val_custom_mae: 0.8151\n",
            "Epoch 37/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4023 - mae: 0.4189 - mse: 0.4023 - val_loss: 0.9730 - val_mae: 0.6168 - val_mse: 0.9730 - learning_rate: 0.1000 - val_custom_mse: 1.3633 - val_custom_mae: 0.8111\n",
            "Epoch 38/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4017 - mae: 0.4186 - mse: 0.4017 - val_loss: 0.9772 - val_mae: 0.6188 - val_mse: 0.9772 - learning_rate: 0.1000 - val_custom_mse: 1.3698 - val_custom_mae: 0.8147\n",
            "Epoch 39/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4015 - mae: 0.4183 - mse: 0.4015 - val_loss: 0.9734 - val_mae: 0.6166 - val_mse: 0.9734 - learning_rate: 0.1000 - val_custom_mse: 1.3647 - val_custom_mae: 0.8120\n",
            "Epoch 40/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4012 - mae: 0.4180 - mse: 0.4012 - val_loss: 0.9765 - val_mae: 0.6180 - val_mse: 0.9765 - learning_rate: 0.1000 - val_custom_mse: 1.3692 - val_custom_mae: 0.8142\n",
            "Epoch 41/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4010 - mae: 0.4178 - mse: 0.4010 - val_loss: 0.9756 - val_mae: 0.6167 - val_mse: 0.9756 - learning_rate: 0.1000 - val_custom_mse: 1.3684 - val_custom_mae: 0.8132\n",
            "Epoch 42/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4007 - mae: 0.4175 - mse: 0.4007 - val_loss: 0.9761 - val_mae: 0.6166 - val_mse: 0.9761 - learning_rate: 0.1000 - val_custom_mse: 1.3693 - val_custom_mae: 0.8136\n",
            "Epoch 43/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4005 - mae: 0.4173 - mse: 0.4005 - val_loss: 0.9759 - val_mae: 0.6173 - val_mse: 0.9759 - learning_rate: 0.1000 - val_custom_mse: 1.3690 - val_custom_mae: 0.8143\n",
            "Epoch 44/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4000 - mae: 0.4169 - mse: 0.4000 - val_loss: 0.9724 - val_mae: 0.6145 - val_mse: 0.9724 - learning_rate: 0.1000 - val_custom_mse: 1.3645 - val_custom_mae: 0.8113\n",
            "Epoch 45/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4000 - mae: 0.4169 - mse: 0.4000 - val_loss: 0.9749 - val_mae: 0.6161 - val_mse: 0.9749 - learning_rate: 0.1000 - val_custom_mse: 1.3681 - val_custom_mae: 0.8135\n",
            "Epoch 46/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3995 - mae: 0.4165 - mse: 0.3995 - val_loss: 0.9744 - val_mae: 0.6151 - val_mse: 0.9744 - learning_rate: 0.1000 - val_custom_mse: 1.3678 - val_custom_mae: 0.8129\n",
            "Epoch 47/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3992 - mae: 0.4163 - mse: 0.3992 - val_loss: 0.9768 - val_mae: 0.6164 - val_mse: 0.9768 - learning_rate: 0.1000 - val_custom_mse: 1.3710 - val_custom_mae: 0.8144\n",
            "Epoch 48/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3992 - mae: 0.4162 - mse: 0.3992 - val_loss: 0.9736 - val_mae: 0.6141 - val_mse: 0.9736 - learning_rate: 0.1000 - val_custom_mse: 1.3668 - val_custom_mae: 0.8121\n",
            "Epoch 49/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3991 - mae: 0.4161 - mse: 0.3991 - val_loss: 0.9761 - val_mae: 0.6152 - val_mse: 0.9761 - learning_rate: 0.1000 - val_custom_mse: 1.3705 - val_custom_mae: 0.8138\n",
            "Epoch 50/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3987 - mae: 0.4159 - mse: 0.3987 - val_loss: 0.9751 - val_mae: 0.6148 - val_mse: 0.9751 - learning_rate: 0.1000 - val_custom_mse: 1.3694 - val_custom_mae: 0.8136\n",
            "Epoch 51/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3986 - mae: 0.4157 - mse: 0.3986 - val_loss: 0.9713 - val_mae: 0.6126 - val_mse: 0.9713 - learning_rate: 0.1000 - val_custom_mse: 1.3641 - val_custom_mae: 0.8110\n",
            "Epoch 52/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3986 - mae: 0.4156 - mse: 0.3986 - val_loss: 0.9726 - val_mae: 0.6132 - val_mse: 0.9726 - learning_rate: 0.1000 - val_custom_mse: 1.3659 - val_custom_mae: 0.8116\n",
            "Epoch 53/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3981 - mae: 0.4153 - mse: 0.3981 - val_loss: 0.9744 - val_mae: 0.6139 - val_mse: 0.9744 - learning_rate: 0.1000 - val_custom_mse: 1.3685 - val_custom_mae: 0.8128\n",
            "Epoch 54/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3980 - mae: 0.4152 - mse: 0.3980 - val_loss: 0.9730 - val_mae: 0.6131 - val_mse: 0.9730 - learning_rate: 0.1000 - val_custom_mse: 1.3668 - val_custom_mae: 0.8120\n",
            "Epoch 55/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3979 - mae: 0.4151 - mse: 0.3979 - val_loss: 0.9713 - val_mae: 0.6121 - val_mse: 0.9713 - learning_rate: 0.1000 - val_custom_mse: 1.3643 - val_custom_mae: 0.8106\n",
            "Epoch 56/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3978 - mae: 0.4151 - mse: 0.3978 - val_loss: 0.9724 - val_mae: 0.6125 - val_mse: 0.9724 - learning_rate: 0.1000 - val_custom_mse: 1.3662 - val_custom_mae: 0.8116\n",
            "Epoch 57/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3974 - mae: 0.4148 - mse: 0.3974 - val_loss: 0.9737 - val_mae: 0.6127 - val_mse: 0.9737 - learning_rate: 0.1000 - val_custom_mse: 1.3681 - val_custom_mae: 0.8123\n",
            "Epoch 58/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3974 - mae: 0.4148 - mse: 0.3974 - val_loss: 0.9719 - val_mae: 0.6119 - val_mse: 0.9719 - learning_rate: 0.1000 - val_custom_mse: 1.3656 - val_custom_mae: 0.8111\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "216/216 - 2s - 8ms/step - loss: 0.3971 - mae: 0.4146 - mse: 0.3971 - val_loss: 0.9716 - val_mae: 0.6118 - val_mse: 0.9716 - learning_rate: 0.1000 - val_custom_mse: 1.3650 - val_custom_mae: 0.8108\n",
            "Epoch 60/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3973 - mae: 0.4145 - mse: 0.3973 - val_loss: 0.9778 - val_mae: 0.6140 - val_mse: 0.9778 - learning_rate: 0.0200 - val_custom_mse: 1.3742 - val_custom_mae: 0.8146\n",
            "Epoch 61/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3969 - mae: 0.4144 - mse: 0.3969 - val_loss: 0.9771 - val_mae: 0.6132 - val_mse: 0.9771 - learning_rate: 0.0200 - val_custom_mse: 1.3733 - val_custom_mae: 0.8138\n",
            "Epoch 62/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3969 - mae: 0.4144 - mse: 0.3969 - val_loss: 0.9775 - val_mae: 0.6133 - val_mse: 0.9775 - learning_rate: 0.0200 - val_custom_mse: 1.3738 - val_custom_mae: 0.8139\n",
            "Epoch 63/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3969 - mae: 0.4144 - mse: 0.3969 - val_loss: 0.9789 - val_mae: 0.6142 - val_mse: 0.9789 - learning_rate: 0.0200 - val_custom_mse: 1.3758 - val_custom_mae: 0.8151\n",
            "Epoch 64/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4143 - mse: 0.3967 - val_loss: 0.9785 - val_mae: 0.6140 - val_mse: 0.9785 - learning_rate: 0.0200 - val_custom_mse: 1.3752 - val_custom_mae: 0.8147\n",
            "Epoch 65/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4143 - mse: 0.3967 - val_loss: 0.9782 - val_mae: 0.6136 - val_mse: 0.9782 - learning_rate: 0.0200 - val_custom_mse: 1.3748 - val_custom_mae: 0.8144\n",
            "Epoch 66/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3969 - mae: 0.4143 - mse: 0.3969 - val_loss: 0.9780 - val_mae: 0.6136 - val_mse: 0.9780 - learning_rate: 0.0200 - val_custom_mse: 1.3746 - val_custom_mae: 0.8144\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9778 - val_mae: 0.6134 - val_mse: 0.9778 - learning_rate: 0.0200 - val_custom_mse: 1.3743 - val_custom_mae: 0.8142\n",
            "Epoch 68/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3968 - mae: 0.4143 - mse: 0.3968 - val_loss: 0.9783 - val_mae: 0.6137 - val_mse: 0.9783 - learning_rate: 0.0040 - val_custom_mse: 1.3751 - val_custom_mae: 0.8146\n",
            "Epoch 69/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4143 - mse: 0.3966 - val_loss: 0.9788 - val_mae: 0.6138 - val_mse: 0.9788 - learning_rate: 0.0040 - val_custom_mse: 1.3758 - val_custom_mae: 0.8148\n",
            "Epoch 70/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9788 - val_mae: 0.6137 - val_mse: 0.9788 - learning_rate: 0.0040 - val_custom_mse: 1.3758 - val_custom_mae: 0.8147\n",
            "Epoch 71/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9791 - val_mae: 0.6137 - val_mse: 0.9791 - learning_rate: 0.0040 - val_custom_mse: 1.3762 - val_custom_mae: 0.8148\n",
            "Epoch 72/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3968 - mae: 0.4142 - mse: 0.3968 - val_loss: 0.9791 - val_mae: 0.6138 - val_mse: 0.9791 - learning_rate: 0.0040 - val_custom_mse: 1.3762 - val_custom_mae: 0.8149\n",
            "Epoch 73/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3965 - mae: 0.4141 - mse: 0.3965 - val_loss: 0.9793 - val_mae: 0.6138 - val_mse: 0.9793 - learning_rate: 0.0040 - val_custom_mse: 1.3765 - val_custom_mae: 0.8150\n",
            "Epoch 74/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9793 - val_mae: 0.6138 - val_mse: 0.9793 - learning_rate: 0.0040 - val_custom_mse: 1.3765 - val_custom_mae: 0.8149\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 75: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9793 - val_mae: 0.6138 - val_mse: 0.9793 - learning_rate: 0.0040 - val_custom_mse: 1.3765 - val_custom_mae: 0.8150\n",
            "Epoch 76/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3964 - mae: 0.4141 - mse: 0.3964 - val_loss: 0.9789 - val_mae: 0.6136 - val_mse: 0.9789 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3759 - val_custom_mae: 0.8147\n",
            "Epoch 77/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9789 - val_mae: 0.6136 - val_mse: 0.9789 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3759 - val_custom_mae: 0.8147\n",
            "Epoch 78/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9790 - val_mae: 0.6136 - val_mse: 0.9790 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3760 - val_custom_mae: 0.8147\n",
            "Epoch 79/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9790 - val_mae: 0.6136 - val_mse: 0.9790 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3761 - val_custom_mae: 0.8147\n",
            "Epoch 80/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9790 - val_mae: 0.6136 - val_mse: 0.9790 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3761 - val_custom_mae: 0.8147\n",
            "Epoch 81/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8148\n",
            "Epoch 82/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9791 - val_mae: 0.6137 - val_mse: 0.9791 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8148\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9791 - val_mae: 0.6137 - val_mse: 0.9791 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3763 - val_custom_mae: 0.8148\n",
            "Epoch 84/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9791 - val_mae: 0.6137 - val_mse: 0.9791 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8148\n",
            "Epoch 85/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3968 - mae: 0.4143 - mse: 0.3968 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8148\n",
            "Epoch 86/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9790 - val_mae: 0.6136 - val_mse: 0.9790 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 87/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4141 - mse: 0.3966 - val_loss: 0.9790 - val_mae: 0.6136 - val_mse: 0.9790 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 88/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9790 - val_mae: 0.6136 - val_mse: 0.9790 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 89/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 90/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3969 - mae: 0.4143 - mse: 0.3969 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8148\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3762 - val_custom_mae: 0.8148\n",
            "Epoch 92/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3762 - val_custom_mae: 0.8148\n",
            "Epoch 93/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 94/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 95/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 96/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3964 - mae: 0.4141 - mse: 0.3964 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 97/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 98/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3967 - mae: 0.4142 - mse: 0.3967 - val_loss: 0.9791 - val_mae: 0.6136 - val_mse: 0.9791 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.3965 - mae: 0.4141 - mse: 0.3965 - val_loss: 0.9790 - val_mae: 0.6136 - val_mse: 0.9790 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Epoch 100/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.3966 - mae: 0.4142 - mse: 0.3966 - val_loss: 0.9790 - val_mae: 0.6136 - val_mse: 0.9790 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3762 - val_custom_mae: 0.8147\n",
            "Running experiment: horizon=720, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_32', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216/216 - 6s - 28ms/step - loss: 0.7273 - mae: 0.6241 - mse: 0.7273 - val_loss: 1.5220 - val_mae: 0.9120 - val_mse: 1.5220 - learning_rate: 0.1000 - val_custom_mse: 1.6767 - val_custom_mae: 0.9607\n",
            "Epoch 2/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6637 - mae: 0.5980 - mse: 0.6637 - val_loss: 1.4291 - val_mae: 0.8780 - val_mse: 1.4291 - learning_rate: 0.1000 - val_custom_mse: 1.6053 - val_custom_mae: 0.9341\n",
            "Epoch 3/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6241 - mae: 0.5781 - mse: 0.6241 - val_loss: 1.3508 - val_mae: 0.8476 - val_mse: 1.3508 - learning_rate: 0.1000 - val_custom_mse: 1.5557 - val_custom_mae: 0.9151\n",
            "Epoch 4/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5864 - mae: 0.5574 - mse: 0.5864 - val_loss: 1.2565 - val_mae: 0.8089 - val_mse: 1.2565 - learning_rate: 0.1000 - val_custom_mse: 1.4856 - val_custom_mae: 0.8871\n",
            "Epoch 5/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5503 - mae: 0.5358 - mse: 0.5503 - val_loss: 1.1874 - val_mae: 0.7759 - val_mse: 1.1874 - learning_rate: 0.1000 - val_custom_mse: 1.4495 - val_custom_mae: 0.8699\n",
            "Epoch 6/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5179 - mae: 0.5149 - mse: 0.5179 - val_loss: 1.1166 - val_mae: 0.7389 - val_mse: 1.1166 - learning_rate: 0.1000 - val_custom_mse: 1.4060 - val_custom_mae: 0.8486\n",
            "Epoch 7/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4909 - mae: 0.4960 - mse: 0.4909 - val_loss: 1.0760 - val_mae: 0.7137 - val_mse: 1.0760 - learning_rate: 0.1000 - val_custom_mse: 1.3924 - val_custom_mae: 0.8402\n",
            "Epoch 8/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4708 - mae: 0.4807 - mse: 0.4708 - val_loss: 1.0509 - val_mae: 0.6947 - val_mse: 1.0509 - learning_rate: 0.1000 - val_custom_mse: 1.3868 - val_custom_mae: 0.8343\n",
            "Epoch 9/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4570 - mae: 0.4694 - mse: 0.4570 - val_loss: 1.0282 - val_mae: 0.6770 - val_mse: 1.0282 - learning_rate: 0.1000 - val_custom_mse: 1.3764 - val_custom_mae: 0.8266\n",
            "Epoch 10/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4475 - mae: 0.4614 - mse: 0.4475 - val_loss: 1.0177 - val_mae: 0.6673 - val_mse: 1.0177 - learning_rate: 0.1000 - val_custom_mse: 1.3740 - val_custom_mae: 0.8236\n",
            "Epoch 11/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4412 - mae: 0.4559 - mse: 0.4412 - val_loss: 1.0117 - val_mae: 0.6596 - val_mse: 1.0117 - learning_rate: 0.1000 - val_custom_mse: 1.3750 - val_custom_mae: 0.8214\n",
            "Epoch 12/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4365 - mae: 0.4518 - mse: 0.4365 - val_loss: 1.0126 - val_mae: 0.6573 - val_mse: 1.0126 - learning_rate: 0.1000 - val_custom_mse: 1.3818 - val_custom_mae: 0.8231\n",
            "Epoch 13/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4332 - mae: 0.4488 - mse: 0.4332 - val_loss: 1.0053 - val_mae: 0.6507 - val_mse: 1.0053 - learning_rate: 0.1000 - val_custom_mse: 1.3772 - val_custom_mae: 0.8196\n",
            "Epoch 14/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4303 - mae: 0.4463 - mse: 0.4303 - val_loss: 1.0044 - val_mae: 0.6490 - val_mse: 1.0044 - learning_rate: 0.1000 - val_custom_mse: 1.3784 - val_custom_mae: 0.8195\n",
            "Epoch 15/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4282 - mae: 0.4443 - mse: 0.4282 - val_loss: 1.0047 - val_mae: 0.6470 - val_mse: 1.0047 - learning_rate: 0.1000 - val_custom_mse: 1.3828 - val_custom_mae: 0.8210\n",
            "Epoch 16/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4265 - mae: 0.4427 - mse: 0.4265 - val_loss: 1.0037 - val_mae: 0.6447 - val_mse: 1.0037 - learning_rate: 0.1000 - val_custom_mse: 1.3841 - val_custom_mae: 0.8206\n",
            "Epoch 17/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4248 - mae: 0.4411 - mse: 0.4248 - val_loss: 0.9989 - val_mae: 0.6415 - val_mse: 0.9989 - learning_rate: 0.1000 - val_custom_mse: 1.3794 - val_custom_mae: 0.8187\n",
            "Epoch 18/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4234 - mae: 0.4398 - mse: 0.4234 - val_loss: 0.9957 - val_mae: 0.6393 - val_mse: 0.9957 - learning_rate: 0.1000 - val_custom_mse: 1.3767 - val_custom_mae: 0.8177\n",
            "Epoch 19/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4218 - mae: 0.4386 - mse: 0.4218 - val_loss: 0.9948 - val_mae: 0.6383 - val_mse: 0.9948 - learning_rate: 0.1000 - val_custom_mse: 1.3764 - val_custom_mae: 0.8173\n",
            "Epoch 20/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4204 - mae: 0.4374 - mse: 0.4204 - val_loss: 0.9908 - val_mae: 0.6359 - val_mse: 0.9908 - learning_rate: 0.1000 - val_custom_mse: 1.3722 - val_custom_mae: 0.8158\n",
            "Epoch 21/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4194 - mae: 0.4366 - mse: 0.4194 - val_loss: 0.9906 - val_mae: 0.6340 - val_mse: 0.9906 - learning_rate: 0.1000 - val_custom_mse: 1.3736 - val_custom_mae: 0.8155\n",
            "Epoch 22/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4186 - mae: 0.4357 - mse: 0.4186 - val_loss: 0.9894 - val_mae: 0.6333 - val_mse: 0.9894 - learning_rate: 0.1000 - val_custom_mse: 1.3731 - val_custom_mae: 0.8157\n",
            "Epoch 23/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4176 - mae: 0.4349 - mse: 0.4176 - val_loss: 0.9924 - val_mae: 0.6339 - val_mse: 0.9924 - learning_rate: 0.1000 - val_custom_mse: 1.3788 - val_custom_mae: 0.8183\n",
            "Epoch 24/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4173 - mae: 0.4344 - mse: 0.4173 - val_loss: 0.9851 - val_mae: 0.6298 - val_mse: 0.9851 - learning_rate: 0.1000 - val_custom_mse: 1.3689 - val_custom_mae: 0.8133\n",
            "Epoch 25/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4161 - mae: 0.4336 - mse: 0.4161 - val_loss: 0.9863 - val_mae: 0.6308 - val_mse: 0.9863 - learning_rate: 0.1000 - val_custom_mse: 1.3714 - val_custom_mae: 0.8155\n",
            "Epoch 26/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4154 - mae: 0.4330 - mse: 0.4154 - val_loss: 0.9857 - val_mae: 0.6293 - val_mse: 0.9857 - learning_rate: 0.1000 - val_custom_mse: 1.3716 - val_custom_mae: 0.8150\n",
            "Epoch 27/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4147 - mae: 0.4324 - mse: 0.4147 - val_loss: 0.9855 - val_mae: 0.6292 - val_mse: 0.9855 - learning_rate: 0.1000 - val_custom_mse: 1.3716 - val_custom_mae: 0.8149\n",
            "Epoch 28/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4141 - mae: 0.4319 - mse: 0.4141 - val_loss: 0.9836 - val_mae: 0.6271 - val_mse: 0.9836 - learning_rate: 0.1000 - val_custom_mse: 1.3697 - val_custom_mae: 0.8133\n",
            "Epoch 29/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4134 - mae: 0.4313 - mse: 0.4134 - val_loss: 0.9850 - val_mae: 0.6277 - val_mse: 0.9850 - learning_rate: 0.1000 - val_custom_mse: 1.3723 - val_custom_mae: 0.8149\n",
            "Epoch 30/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4128 - mae: 0.4308 - mse: 0.4128 - val_loss: 0.9868 - val_mae: 0.6290 - val_mse: 0.9868 - learning_rate: 0.1000 - val_custom_mse: 1.3749 - val_custom_mae: 0.8165\n",
            "Epoch 31/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4123 - mae: 0.4304 - mse: 0.4123 - val_loss: 0.9832 - val_mae: 0.6263 - val_mse: 0.9832 - learning_rate: 0.1000 - val_custom_mse: 1.3705 - val_custom_mae: 0.8140\n",
            "Epoch 32/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4121 - mae: 0.4301 - mse: 0.4121 - val_loss: 0.9835 - val_mae: 0.6257 - val_mse: 0.9835 - learning_rate: 0.1000 - val_custom_mse: 1.3718 - val_custom_mae: 0.8145\n",
            "Epoch 33/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4117 - mae: 0.4298 - mse: 0.4117 - val_loss: 0.9805 - val_mae: 0.6243 - val_mse: 0.9805 - learning_rate: 0.1000 - val_custom_mse: 1.3677 - val_custom_mae: 0.8127\n",
            "Epoch 34/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4112 - mae: 0.4294 - mse: 0.4112 - val_loss: 0.9830 - val_mae: 0.6251 - val_mse: 0.9830 - learning_rate: 0.1000 - val_custom_mse: 1.3716 - val_custom_mae: 0.8143\n",
            "Epoch 35/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4110 - mae: 0.4292 - mse: 0.4110 - val_loss: 0.9829 - val_mae: 0.6257 - val_mse: 0.9829 - learning_rate: 0.1000 - val_custom_mse: 1.3712 - val_custom_mae: 0.8147\n",
            "Epoch 36/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4102 - mae: 0.4287 - mse: 0.4102 - val_loss: 0.9815 - val_mae: 0.6241 - val_mse: 0.9815 - learning_rate: 0.1000 - val_custom_mse: 1.3700 - val_custom_mae: 0.8136\n",
            "Epoch 37/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4101 - mae: 0.4284 - mse: 0.4101 - val_loss: 0.9813 - val_mae: 0.6238 - val_mse: 0.9813 - learning_rate: 0.1000 - val_custom_mse: 1.3699 - val_custom_mae: 0.8135\n",
            "Epoch 38/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4097 - mae: 0.4282 - mse: 0.4097 - val_loss: 0.9821 - val_mae: 0.6233 - val_mse: 0.9821 - learning_rate: 0.1000 - val_custom_mse: 1.3714 - val_custom_mae: 0.8137\n",
            "Epoch 39/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4097 - mae: 0.4280 - mse: 0.4097 - val_loss: 0.9804 - val_mae: 0.6225 - val_mse: 0.9804 - learning_rate: 0.1000 - val_custom_mse: 1.3691 - val_custom_mae: 0.8128\n",
            "Epoch 40/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4092 - mae: 0.4277 - mse: 0.4092 - val_loss: 0.9830 - val_mae: 0.6241 - val_mse: 0.9830 - learning_rate: 0.1000 - val_custom_mse: 1.3729 - val_custom_mae: 0.8149\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "216/216 - 2s - 7ms/step - loss: 0.4088 - mae: 0.4274 - mse: 0.4088 - val_loss: 0.9820 - val_mae: 0.6228 - val_mse: 0.9820 - learning_rate: 0.1000 - val_custom_mse: 1.3719 - val_custom_mae: 0.8138\n",
            "Epoch 42/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4084 - mae: 0.4270 - mse: 0.4084 - val_loss: 0.9836 - val_mae: 0.6240 - val_mse: 0.9836 - learning_rate: 0.0200 - val_custom_mse: 1.3740 - val_custom_mae: 0.8150\n",
            "Epoch 43/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4087 - mae: 0.4273 - mse: 0.4087 - val_loss: 0.9835 - val_mae: 0.6241 - val_mse: 0.9835 - learning_rate: 0.0200 - val_custom_mse: 1.3737 - val_custom_mae: 0.8151\n",
            "Epoch 44/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4083 - mae: 0.4270 - mse: 0.4083 - val_loss: 0.9835 - val_mae: 0.6241 - val_mse: 0.9835 - learning_rate: 0.0200 - val_custom_mse: 1.3737 - val_custom_mae: 0.8151\n",
            "Epoch 45/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4082 - mae: 0.4270 - mse: 0.4082 - val_loss: 0.9835 - val_mae: 0.6239 - val_mse: 0.9835 - learning_rate: 0.0200 - val_custom_mse: 1.3739 - val_custom_mae: 0.8151\n",
            "Epoch 46/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4085 - mae: 0.4271 - mse: 0.4085 - val_loss: 0.9836 - val_mae: 0.6238 - val_mse: 0.9836 - learning_rate: 0.0200 - val_custom_mse: 1.3741 - val_custom_mae: 0.8151\n",
            "Epoch 47/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4085 - mae: 0.4271 - mse: 0.4085 - val_loss: 0.9836 - val_mae: 0.6240 - val_mse: 0.9836 - learning_rate: 0.0200 - val_custom_mse: 1.3741 - val_custom_mae: 0.8152\n",
            "Epoch 48/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4084 - mae: 0.4270 - mse: 0.4084 - val_loss: 0.9832 - val_mae: 0.6235 - val_mse: 0.9832 - learning_rate: 0.0200 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "216/216 - 2s - 8ms/step - loss: 0.4082 - mae: 0.4269 - mse: 0.4082 - val_loss: 0.9819 - val_mae: 0.6228 - val_mse: 0.9819 - learning_rate: 0.0200 - val_custom_mse: 1.3717 - val_custom_mae: 0.8139\n",
            "Epoch 50/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4082 - mae: 0.4268 - mse: 0.4082 - val_loss: 0.9828 - val_mae: 0.6233 - val_mse: 0.9828 - learning_rate: 0.0040 - val_custom_mse: 1.3731 - val_custom_mae: 0.8146\n",
            "Epoch 51/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4268 - mse: 0.4081 - val_loss: 0.9834 - val_mae: 0.6235 - val_mse: 0.9834 - learning_rate: 0.0040 - val_custom_mse: 1.3739 - val_custom_mae: 0.8149\n",
            "Epoch 52/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4268 - mse: 0.4081 - val_loss: 0.9835 - val_mae: 0.6236 - val_mse: 0.9835 - learning_rate: 0.0040 - val_custom_mse: 1.3742 - val_custom_mae: 0.8151\n",
            "Epoch 53/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4268 - mse: 0.4081 - val_loss: 0.9832 - val_mae: 0.6235 - val_mse: 0.9832 - learning_rate: 0.0040 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 54/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4267 - mse: 0.4079 - val_loss: 0.9835 - val_mae: 0.6237 - val_mse: 0.9835 - learning_rate: 0.0040 - val_custom_mse: 1.3741 - val_custom_mae: 0.8151\n",
            "Epoch 55/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4080 - mae: 0.4268 - mse: 0.4080 - val_loss: 0.9832 - val_mae: 0.6235 - val_mse: 0.9832 - learning_rate: 0.0040 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 56/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4080 - mae: 0.4268 - mse: 0.4080 - val_loss: 0.9835 - val_mae: 0.6237 - val_mse: 0.9835 - learning_rate: 0.0040 - val_custom_mse: 1.3741 - val_custom_mae: 0.8151\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4268 - mse: 0.4081 - val_loss: 0.9835 - val_mae: 0.6236 - val_mse: 0.9835 - learning_rate: 0.0040 - val_custom_mse: 1.3742 - val_custom_mae: 0.8151\n",
            "Epoch 58/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4268 - mse: 0.4079 - val_loss: 0.9833 - val_mae: 0.6235 - val_mse: 0.9833 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3738 - val_custom_mae: 0.8149\n",
            "Epoch 59/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4080 - mae: 0.4268 - mse: 0.4080 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 60/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4268 - mse: 0.4081 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 61/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4078 - mae: 0.4267 - mse: 0.4078 - val_loss: 0.9833 - val_mae: 0.6235 - val_mse: 0.9833 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3738 - val_custom_mae: 0.8149\n",
            "Epoch 62/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4077 - mae: 0.4266 - mse: 0.4077 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 63/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4269 - mse: 0.4081 - val_loss: 0.9833 - val_mae: 0.6235 - val_mse: 0.9833 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3738 - val_custom_mae: 0.8149\n",
            "Epoch 64/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4078 - mae: 0.4267 - mse: 0.4078 - val_loss: 0.9833 - val_mae: 0.6235 - val_mse: 0.9833 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3738 - val_custom_mae: 0.8149\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4267 - mse: 0.4079 - val_loss: 0.9833 - val_mae: 0.6235 - val_mse: 0.9833 - learning_rate: 8.0000e-04 - val_custom_mse: 1.3738 - val_custom_mae: 0.8149\n",
            "Epoch 66/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4078 - mae: 0.4267 - mse: 0.4078 - val_loss: 0.9833 - val_mae: 0.6235 - val_mse: 0.9833 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3738 - val_custom_mae: 0.8149\n",
            "Epoch 67/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4268 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3738 - val_custom_mae: 0.8149\n",
            "Epoch 68/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4080 - mae: 0.4268 - mse: 0.4080 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3738 - val_custom_mae: 0.8149\n",
            "Epoch 69/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4267 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3738 - val_custom_mae: 0.8149\n",
            "Epoch 70/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4081 - mae: 0.4268 - mse: 0.4081 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 71/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4081 - mae: 0.4268 - mse: 0.4081 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 72/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4268 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4269 - mse: 0.4081 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.6000e-04 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 74/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4080 - mae: 0.4268 - mse: 0.4080 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 75/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4082 - mae: 0.4269 - mse: 0.4082 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 76/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4078 - mae: 0.4267 - mse: 0.4078 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 77/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4268 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 78/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4080 - mae: 0.4268 - mse: 0.4080 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 79/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4269 - mse: 0.4081 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 80/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4078 - mae: 0.4267 - mse: 0.4078 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.4080 - mae: 0.4268 - mse: 0.4080 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 3.2000e-05 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 82/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4080 - mae: 0.4267 - mse: 0.4080 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 83/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4078 - mae: 0.4267 - mse: 0.4078 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 84/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4075 - mae: 0.4266 - mse: 0.4075 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 85/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4268 - mse: 0.4081 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 86/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4079 - mae: 0.4268 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 87/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4078 - mae: 0.4266 - mse: 0.4078 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 88/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4081 - mae: 0.4269 - mse: 0.4081 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.2799998330592645e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4267 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 6.4000e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 90/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4077 - mae: 0.4266 - mse: 0.4077 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.2800e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 91/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4268 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.2800e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 92/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4268 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.2800e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 93/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4077 - mae: 0.4267 - mse: 0.4077 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.2800e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8148\n",
            "Epoch 94/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.4082 - mae: 0.4269 - mse: 0.4082 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.2800e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 95/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4267 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.2800e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 96/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4078 - mae: 0.4267 - mse: 0.4078 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.2800e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 2.559999757067999e-07.\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4267 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 1.2800e-06 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 98/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4267 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 2.5600e-07 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 99/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4080 - mae: 0.4268 - mse: 0.4080 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 2.5600e-07 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "Epoch 100/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.4079 - mae: 0.4268 - mse: 0.4079 - val_loss: 0.9832 - val_mae: 0.6234 - val_mse: 0.9832 - learning_rate: 2.5600e-07 - val_custom_mse: 1.3737 - val_custom_mae: 0.8149\n",
            "\n",
            " ETTh1 Final Results:\n",
            "\n",
            "MSE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%  DR=20%  DR=30%\n",
            "Horizon                                \n",
            "96       0.3583  0.3582  0.3585  0.3595\n",
            "192      0.3948  0.3941  0.3937  0.3942\n",
            "336      0.4212  0.4196  0.4183  0.4179\n",
            "720      0.4703  0.4676  0.4660  0.4647\n",
            "\n",
            "MAE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%  DR=20%  DR=30%\n",
            "Horizon                                \n",
            "96       0.3891  0.3897  0.3903  0.3914\n",
            "192      0.4112  0.4116  0.4120  0.4128\n",
            "336      0.4296  0.4292  0.4293  0.4299\n",
            "720      0.4768  0.4758  0.4756  0.4753\n",
            "\n",
            "Results saved to: ./flowmixer_results/ETTh1_experiment_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Run the experiments\n",
        "data_name='ETTh1'\n",
        "results = run_experiments(data_name,horizons=[96,192,336,720], dropout_rates=[0.0, 0.1, 0.2, 0.3], revin=1, seq_len_=1024, learning_rate=1e-1, mopt='sgd')\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\n {data_name} Final Results:\")\n",
        "df = pd.DataFrame(results)\n",
        "# Assuming results is a list of dictionaries with horizon, dropout, MSE, and MAE values\n",
        "display_results_tables(results[0])\n",
        "\n",
        "\n",
        "# The results are already saved in CSV format after each experiment\n",
        "print(f\"\\nResults saved to: ./flowmixer_results/{data_name}_experiment_results.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETTh2"
      ],
      "metadata": {
        "id": "sRvz2gelw7pM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J5pqyAA9Mip",
        "outputId": "a9ef7e26-0af6-4124-c0d7-b6f800778743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: horizon=96, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_33', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "236/236 - 5s - 22ms/step - loss: 0.4728 - mae: 0.4123 - mse: 0.4728 - val_loss: 0.2741 - val_mae: 0.3782 - val_mse: 0.2741 - learning_rate: 0.1000 - val_custom_mse: 0.3736 - val_custom_mae: 0.4494\n",
            "Epoch 2/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.4247 - mae: 0.3925 - mse: 0.4247 - val_loss: 0.2386 - val_mae: 0.3545 - val_mse: 0.2386 - learning_rate: 0.1000 - val_custom_mse: 0.3510 - val_custom_mae: 0.4352\n",
            "Epoch 3/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3723 - mae: 0.3689 - mse: 0.3723 - val_loss: 0.2079 - val_mae: 0.3292 - val_mse: 0.2079 - learning_rate: 0.1000 - val_custom_mse: 0.3381 - val_custom_mae: 0.4228\n",
            "Epoch 4/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3179 - mae: 0.3402 - mse: 0.3179 - val_loss: 0.1724 - val_mae: 0.2990 - val_mse: 0.1724 - learning_rate: 0.1000 - val_custom_mse: 0.3141 - val_custom_mae: 0.4042\n",
            "Epoch 5/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2600 - mae: 0.3092 - mse: 0.2600 - val_loss: 0.1399 - val_mae: 0.2679 - val_mse: 0.1399 - learning_rate: 0.1000 - val_custom_mse: 0.2877 - val_custom_mae: 0.3838\n",
            "Epoch 6/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2109 - mae: 0.2802 - mse: 0.2109 - val_loss: 0.1170 - val_mae: 0.2435 - val_mse: 0.1170 - learning_rate: 0.1000 - val_custom_mse: 0.2737 - val_custom_mae: 0.3727\n",
            "Epoch 7/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1739 - mae: 0.2559 - mse: 0.1739 - val_loss: 0.1015 - val_mae: 0.2250 - val_mse: 0.1015 - learning_rate: 0.1000 - val_custom_mse: 0.2649 - val_custom_mae: 0.3656\n",
            "Epoch 8/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1484 - mae: 0.2370 - mse: 0.1484 - val_loss: 0.0904 - val_mae: 0.2108 - val_mse: 0.0904 - learning_rate: 0.1000 - val_custom_mse: 0.2516 - val_custom_mae: 0.3547\n",
            "Epoch 9/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1317 - mae: 0.2229 - mse: 0.1317 - val_loss: 0.0838 - val_mae: 0.2014 - val_mse: 0.0838 - learning_rate: 0.1000 - val_custom_mse: 0.2490 - val_custom_mae: 0.3527\n",
            "Epoch 10/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1203 - mae: 0.2118 - mse: 0.1203 - val_loss: 0.0779 - val_mae: 0.1930 - val_mse: 0.0779 - learning_rate: 0.1000 - val_custom_mse: 0.2384 - val_custom_mae: 0.3436\n",
            "Epoch 11/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1121 - mae: 0.2028 - mse: 0.1121 - val_loss: 0.0736 - val_mae: 0.1860 - val_mse: 0.0736 - learning_rate: 0.1000 - val_custom_mse: 0.2348 - val_custom_mae: 0.3404\n",
            "Epoch 12/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1056 - mae: 0.1948 - mse: 0.1056 - val_loss: 0.0698 - val_mae: 0.1798 - val_mse: 0.0698 - learning_rate: 0.1000 - val_custom_mse: 0.2318 - val_custom_mae: 0.3378\n",
            "Epoch 13/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1001 - mae: 0.1876 - mse: 0.1001 - val_loss: 0.0666 - val_mae: 0.1739 - val_mse: 0.0666 - learning_rate: 0.1000 - val_custom_mse: 0.2293 - val_custom_mae: 0.3355\n",
            "Epoch 14/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0955 - mae: 0.1810 - mse: 0.0955 - val_loss: 0.0637 - val_mae: 0.1685 - val_mse: 0.0637 - learning_rate: 0.1000 - val_custom_mse: 0.2291 - val_custom_mae: 0.3355\n",
            "Epoch 15/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0914 - mae: 0.1750 - mse: 0.0914 - val_loss: 0.0611 - val_mae: 0.1635 - val_mse: 0.0611 - learning_rate: 0.1000 - val_custom_mse: 0.2278 - val_custom_mae: 0.3344\n",
            "Epoch 16/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0879 - mae: 0.1694 - mse: 0.0879 - val_loss: 0.0588 - val_mae: 0.1589 - val_mse: 0.0588 - learning_rate: 0.1000 - val_custom_mse: 0.2270 - val_custom_mae: 0.3337\n",
            "Epoch 17/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0847 - mae: 0.1643 - mse: 0.0847 - val_loss: 0.0565 - val_mae: 0.1545 - val_mse: 0.0565 - learning_rate: 0.1000 - val_custom_mse: 0.2243 - val_custom_mae: 0.3311\n",
            "Epoch 18/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0818 - mae: 0.1595 - mse: 0.0818 - val_loss: 0.0546 - val_mae: 0.1503 - val_mse: 0.0546 - learning_rate: 0.1000 - val_custom_mse: 0.2238 - val_custom_mae: 0.3306\n",
            "Epoch 19/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0793 - mae: 0.1551 - mse: 0.0793 - val_loss: 0.0536 - val_mae: 0.1474 - val_mse: 0.0536 - learning_rate: 0.1000 - val_custom_mse: 0.2274 - val_custom_mae: 0.3343\n",
            "Epoch 20/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0770 - mae: 0.1511 - mse: 0.0770 - val_loss: 0.0512 - val_mae: 0.1430 - val_mse: 0.0512 - learning_rate: 0.1000 - val_custom_mse: 0.2223 - val_custom_mae: 0.3293\n",
            "Epoch 21/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0748 - mae: 0.1471 - mse: 0.0748 - val_loss: 0.0498 - val_mae: 0.1396 - val_mse: 0.0498 - learning_rate: 0.1000 - val_custom_mse: 0.2222 - val_custom_mae: 0.3292\n",
            "Epoch 22/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0729 - mae: 0.1435 - mse: 0.0729 - val_loss: 0.0482 - val_mae: 0.1365 - val_mse: 0.0482 - learning_rate: 0.1000 - val_custom_mse: 0.2198 - val_custom_mae: 0.3268\n",
            "Epoch 23/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0711 - mae: 0.1402 - mse: 0.0711 - val_loss: 0.0469 - val_mae: 0.1336 - val_mse: 0.0469 - learning_rate: 0.1000 - val_custom_mse: 0.2193 - val_custom_mae: 0.3263\n",
            "Epoch 24/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0695 - mae: 0.1370 - mse: 0.0695 - val_loss: 0.0457 - val_mae: 0.1308 - val_mse: 0.0457 - learning_rate: 0.1000 - val_custom_mse: 0.2185 - val_custom_mae: 0.3255\n",
            "Epoch 25/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0680 - mae: 0.1340 - mse: 0.0680 - val_loss: 0.0446 - val_mae: 0.1280 - val_mse: 0.0446 - learning_rate: 0.1000 - val_custom_mse: 0.2187 - val_custom_mae: 0.3256\n",
            "Epoch 26/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0666 - mae: 0.1312 - mse: 0.0666 - val_loss: 0.0436 - val_mae: 0.1254 - val_mse: 0.0436 - learning_rate: 0.1000 - val_custom_mse: 0.2186 - val_custom_mae: 0.3256\n",
            "Epoch 27/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0653 - mae: 0.1286 - mse: 0.0653 - val_loss: 0.0425 - val_mae: 0.1232 - val_mse: 0.0425 - learning_rate: 0.1000 - val_custom_mse: 0.2172 - val_custom_mae: 0.3241\n",
            "Epoch 28/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0641 - mae: 0.1261 - mse: 0.0641 - val_loss: 0.0418 - val_mae: 0.1207 - val_mse: 0.0418 - learning_rate: 0.1000 - val_custom_mse: 0.2187 - val_custom_mae: 0.3258\n",
            "Epoch 29/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0629 - mae: 0.1237 - mse: 0.0629 - val_loss: 0.0408 - val_mae: 0.1186 - val_mse: 0.0408 - learning_rate: 0.1000 - val_custom_mse: 0.2172 - val_custom_mae: 0.3240\n",
            "Epoch 30/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0618 - mae: 0.1214 - mse: 0.0618 - val_loss: 0.0400 - val_mae: 0.1163 - val_mse: 0.0400 - learning_rate: 0.1000 - val_custom_mse: 0.2176 - val_custom_mae: 0.3245\n",
            "Epoch 31/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0608 - mae: 0.1192 - mse: 0.0608 - val_loss: 0.0392 - val_mae: 0.1143 - val_mse: 0.0392 - learning_rate: 0.1000 - val_custom_mse: 0.2176 - val_custom_mae: 0.3246\n",
            "Epoch 32/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0599 - mae: 0.1172 - mse: 0.0599 - val_loss: 0.0383 - val_mae: 0.1128 - val_mse: 0.0383 - learning_rate: 0.1000 - val_custom_mse: 0.2150 - val_custom_mae: 0.3218\n",
            "Epoch 33/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0590 - mae: 0.1152 - mse: 0.0590 - val_loss: 0.0376 - val_mae: 0.1107 - val_mse: 0.0376 - learning_rate: 0.1000 - val_custom_mse: 0.2154 - val_custom_mae: 0.3222\n",
            "Epoch 34/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0581 - mae: 0.1133 - mse: 0.0581 - val_loss: 0.0370 - val_mae: 0.1088 - val_mse: 0.0370 - learning_rate: 0.1000 - val_custom_mse: 0.2163 - val_custom_mae: 0.3232\n",
            "Epoch 35/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0573 - mae: 0.1114 - mse: 0.0573 - val_loss: 0.0364 - val_mae: 0.1069 - val_mse: 0.0364 - learning_rate: 0.1000 - val_custom_mse: 0.2163 - val_custom_mae: 0.3232\n",
            "Epoch 36/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0565 - mae: 0.1096 - mse: 0.0565 - val_loss: 0.0357 - val_mae: 0.1054 - val_mse: 0.0357 - learning_rate: 0.1000 - val_custom_mse: 0.2149 - val_custom_mae: 0.3217\n",
            "Epoch 37/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0558 - mae: 0.1080 - mse: 0.0558 - val_loss: 0.0352 - val_mae: 0.1036 - val_mse: 0.0352 - learning_rate: 0.1000 - val_custom_mse: 0.2161 - val_custom_mae: 0.3229\n",
            "Epoch 38/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0551 - mae: 0.1063 - mse: 0.0551 - val_loss: 0.0346 - val_mae: 0.1021 - val_mse: 0.0346 - learning_rate: 0.1000 - val_custom_mse: 0.2156 - val_custom_mae: 0.3224\n",
            "Epoch 39/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0544 - mae: 0.1047 - mse: 0.0544 - val_loss: 0.0341 - val_mae: 0.1006 - val_mse: 0.0341 - learning_rate: 0.1000 - val_custom_mse: 0.2160 - val_custom_mae: 0.3229\n",
            "Epoch 40/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0538 - mae: 0.1032 - mse: 0.0538 - val_loss: 0.0335 - val_mae: 0.0991 - val_mse: 0.0335 - learning_rate: 0.1000 - val_custom_mse: 0.2154 - val_custom_mae: 0.3223\n",
            "Epoch 41/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0532 - mae: 0.1017 - mse: 0.0532 - val_loss: 0.0330 - val_mae: 0.0977 - val_mse: 0.0330 - learning_rate: 0.1000 - val_custom_mse: 0.2145 - val_custom_mae: 0.3212\n",
            "Epoch 42/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0526 - mae: 0.1003 - mse: 0.0526 - val_loss: 0.0325 - val_mae: 0.0963 - val_mse: 0.0325 - learning_rate: 0.1000 - val_custom_mse: 0.2151 - val_custom_mae: 0.3219\n",
            "Epoch 43/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0520 - mae: 0.0989 - mse: 0.0520 - val_loss: 0.0320 - val_mae: 0.0951 - val_mse: 0.0320 - learning_rate: 0.1000 - val_custom_mse: 0.2136 - val_custom_mae: 0.3203\n",
            "Epoch 44/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0515 - mae: 0.0976 - mse: 0.0515 - val_loss: 0.0315 - val_mae: 0.0937 - val_mse: 0.0315 - learning_rate: 0.1000 - val_custom_mse: 0.2138 - val_custom_mae: 0.3206\n",
            "Epoch 45/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0510 - mae: 0.0962 - mse: 0.0510 - val_loss: 0.0312 - val_mae: 0.0936 - val_mse: 0.0312 - learning_rate: 0.1000 - val_custom_mse: 0.2121 - val_custom_mae: 0.3189\n",
            "Epoch 46/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0505 - mae: 0.0950 - mse: 0.0505 - val_loss: 0.0307 - val_mae: 0.0916 - val_mse: 0.0307 - learning_rate: 0.1000 - val_custom_mse: 0.2127 - val_custom_mae: 0.3195\n",
            "Epoch 47/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0500 - mae: 0.0938 - mse: 0.0500 - val_loss: 0.0303 - val_mae: 0.0900 - val_mse: 0.0303 - learning_rate: 0.1000 - val_custom_mse: 0.2135 - val_custom_mae: 0.3203\n",
            "Epoch 48/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0496 - mae: 0.0925 - mse: 0.0496 - val_loss: 0.0300 - val_mae: 0.0887 - val_mse: 0.0300 - learning_rate: 0.1000 - val_custom_mse: 0.2140 - val_custom_mae: 0.3207\n",
            "Epoch 49/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0492 - mae: 0.0914 - mse: 0.0492 - val_loss: 0.0297 - val_mae: 0.0877 - val_mse: 0.0297 - learning_rate: 0.1000 - val_custom_mse: 0.2139 - val_custom_mae: 0.3208\n",
            "Epoch 50/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0487 - mae: 0.0902 - mse: 0.0487 - val_loss: 0.0293 - val_mae: 0.0866 - val_mse: 0.0293 - learning_rate: 0.1000 - val_custom_mse: 0.2138 - val_custom_mae: 0.3207\n",
            "Epoch 51/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0483 - mae: 0.0892 - mse: 0.0483 - val_loss: 0.0290 - val_mae: 0.0853 - val_mse: 0.0290 - learning_rate: 0.1000 - val_custom_mse: 0.2139 - val_custom_mae: 0.3206\n",
            "Epoch 52/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0479 - mae: 0.0881 - mse: 0.0479 - val_loss: 0.0286 - val_mae: 0.0847 - val_mse: 0.0286 - learning_rate: 0.1000 - val_custom_mse: 0.2123 - val_custom_mae: 0.3191\n",
            "Epoch 53/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0476 - mae: 0.0870 - mse: 0.0476 - val_loss: 0.0282 - val_mae: 0.0835 - val_mse: 0.0282 - learning_rate: 0.1000 - val_custom_mse: 0.2124 - val_custom_mae: 0.3191\n",
            "Epoch 54/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0472 - mae: 0.0860 - mse: 0.0472 - val_loss: 0.0279 - val_mae: 0.0828 - val_mse: 0.0279 - learning_rate: 0.1000 - val_custom_mse: 0.2119 - val_custom_mae: 0.3187\n",
            "Epoch 55/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0469 - mae: 0.0850 - mse: 0.0469 - val_loss: 0.0277 - val_mae: 0.0819 - val_mse: 0.0277 - learning_rate: 0.1000 - val_custom_mse: 0.2120 - val_custom_mae: 0.3187\n",
            "Epoch 56/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0466 - mae: 0.0840 - mse: 0.0466 - val_loss: 0.0274 - val_mae: 0.0809 - val_mse: 0.0274 - learning_rate: 0.1000 - val_custom_mse: 0.2117 - val_custom_mae: 0.3184\n",
            "Epoch 57/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0463 - mae: 0.0832 - mse: 0.0463 - val_loss: 0.0272 - val_mae: 0.0802 - val_mse: 0.0272 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3183\n",
            "Epoch 58/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0459 - mae: 0.0822 - mse: 0.0459 - val_loss: 0.0269 - val_mae: 0.0785 - val_mse: 0.0269 - learning_rate: 0.1000 - val_custom_mse: 0.2126 - val_custom_mae: 0.3195\n",
            "Epoch 59/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0456 - mae: 0.0813 - mse: 0.0456 - val_loss: 0.0267 - val_mae: 0.0777 - val_mse: 0.0267 - learning_rate: 0.1000 - val_custom_mse: 0.2126 - val_custom_mae: 0.3194\n",
            "Epoch 60/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0454 - mae: 0.0806 - mse: 0.0454 - val_loss: 0.0266 - val_mae: 0.0770 - val_mse: 0.0266 - learning_rate: 0.1000 - val_custom_mse: 0.2135 - val_custom_mae: 0.3204\n",
            "Epoch 61/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0451 - mae: 0.0796 - mse: 0.0451 - val_loss: 0.0262 - val_mae: 0.0766 - val_mse: 0.0262 - learning_rate: 0.1000 - val_custom_mse: 0.2118 - val_custom_mae: 0.3185\n",
            "Epoch 62/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0448 - mae: 0.0788 - mse: 0.0448 - val_loss: 0.0260 - val_mae: 0.0753 - val_mse: 0.0260 - learning_rate: 0.1000 - val_custom_mse: 0.2123 - val_custom_mae: 0.3191\n",
            "Epoch 63/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0446 - mae: 0.0781 - mse: 0.0446 - val_loss: 0.0257 - val_mae: 0.0747 - val_mse: 0.0257 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3183\n",
            "Epoch 64/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0444 - mae: 0.0774 - mse: 0.0444 - val_loss: 0.0255 - val_mae: 0.0738 - val_mse: 0.0255 - learning_rate: 0.1000 - val_custom_mse: 0.2117 - val_custom_mae: 0.3185\n",
            "Epoch 65/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0441 - mae: 0.0765 - mse: 0.0441 - val_loss: 0.0254 - val_mae: 0.0732 - val_mse: 0.0254 - learning_rate: 0.1000 - val_custom_mse: 0.2117 - val_custom_mae: 0.3184\n",
            "Epoch 66/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0439 - mae: 0.0758 - mse: 0.0439 - val_loss: 0.0252 - val_mae: 0.0721 - val_mse: 0.0252 - learning_rate: 0.1000 - val_custom_mse: 0.2120 - val_custom_mae: 0.3188\n",
            "Epoch 67/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0437 - mae: 0.0750 - mse: 0.0437 - val_loss: 0.0250 - val_mae: 0.0714 - val_mse: 0.0250 - learning_rate: 0.1000 - val_custom_mse: 0.2117 - val_custom_mae: 0.3185\n",
            "Epoch 68/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0435 - mae: 0.0744 - mse: 0.0435 - val_loss: 0.0248 - val_mae: 0.0710 - val_mse: 0.0248 - learning_rate: 0.1000 - val_custom_mse: 0.2113 - val_custom_mae: 0.3181\n",
            "Epoch 69/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0433 - mae: 0.0736 - mse: 0.0433 - val_loss: 0.0247 - val_mae: 0.0702 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.2119 - val_custom_mae: 0.3188\n",
            "Epoch 70/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0431 - mae: 0.0730 - mse: 0.0431 - val_loss: 0.0245 - val_mae: 0.0693 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.2120 - val_custom_mae: 0.3187\n",
            "Epoch 71/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0429 - mae: 0.0724 - mse: 0.0429 - val_loss: 0.0243 - val_mae: 0.0689 - val_mse: 0.0243 - learning_rate: 0.1000 - val_custom_mse: 0.2112 - val_custom_mae: 0.3180\n",
            "Epoch 72/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0427 - mae: 0.0717 - mse: 0.0427 - val_loss: 0.0245 - val_mae: 0.0689 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.2136 - val_custom_mae: 0.3205\n",
            "Epoch 73/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0426 - mae: 0.0713 - mse: 0.0426 - val_loss: 0.0242 - val_mae: 0.0678 - val_mse: 0.0242 - learning_rate: 0.1000 - val_custom_mse: 0.2128 - val_custom_mae: 0.3195\n",
            "Epoch 74/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0424 - mae: 0.0704 - mse: 0.0424 - val_loss: 0.0239 - val_mae: 0.0669 - val_mse: 0.0239 - learning_rate: 0.1000 - val_custom_mse: 0.2112 - val_custom_mae: 0.3179\n",
            "Epoch 75/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0422 - mae: 0.0699 - mse: 0.0422 - val_loss: 0.0237 - val_mae: 0.0665 - val_mse: 0.0237 - learning_rate: 0.1000 - val_custom_mse: 0.2110 - val_custom_mae: 0.3178\n",
            "Epoch 76/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0421 - mae: 0.0693 - mse: 0.0421 - val_loss: 0.0237 - val_mae: 0.0655 - val_mse: 0.0237 - learning_rate: 0.1000 - val_custom_mse: 0.2120 - val_custom_mae: 0.3186\n",
            "Epoch 77/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0419 - mae: 0.0686 - mse: 0.0419 - val_loss: 0.0237 - val_mae: 0.0653 - val_mse: 0.0237 - learning_rate: 0.1000 - val_custom_mse: 0.2129 - val_custom_mae: 0.3198\n",
            "Epoch 78/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0418 - mae: 0.0682 - mse: 0.0418 - val_loss: 0.0234 - val_mae: 0.0648 - val_mse: 0.0234 - learning_rate: 0.1000 - val_custom_mse: 0.2111 - val_custom_mae: 0.3180\n",
            "Epoch 79/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0417 - mae: 0.0676 - mse: 0.0417 - val_loss: 0.0234 - val_mae: 0.0640 - val_mse: 0.0234 - learning_rate: 0.1000 - val_custom_mse: 0.2123 - val_custom_mae: 0.3191\n",
            "Epoch 80/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0415 - mae: 0.0671 - mse: 0.0415 - val_loss: 0.0232 - val_mae: 0.0636 - val_mse: 0.0232 - learning_rate: 0.1000 - val_custom_mse: 0.2113 - val_custom_mae: 0.3181\n",
            "Epoch 81/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0414 - mae: 0.0668 - mse: 0.0414 - val_loss: 0.0231 - val_mae: 0.0630 - val_mse: 0.0231 - learning_rate: 0.1000 - val_custom_mse: 0.2114 - val_custom_mae: 0.3180\n",
            "Epoch 82/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0413 - mae: 0.0661 - mse: 0.0413 - val_loss: 0.0230 - val_mae: 0.0625 - val_mse: 0.0230 - learning_rate: 0.1000 - val_custom_mse: 0.2115 - val_custom_mae: 0.3182\n",
            "Epoch 83/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0412 - mae: 0.0656 - mse: 0.0412 - val_loss: 0.0229 - val_mae: 0.0619 - val_mse: 0.0229 - learning_rate: 0.1000 - val_custom_mse: 0.2114 - val_custom_mae: 0.3180\n",
            "Epoch 84/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0410 - mae: 0.0651 - mse: 0.0410 - val_loss: 0.0228 - val_mae: 0.0613 - val_mse: 0.0228 - learning_rate: 0.1000 - val_custom_mse: 0.2119 - val_custom_mae: 0.3186\n",
            "Epoch 85/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0410 - mae: 0.0648 - mse: 0.0410 - val_loss: 0.0227 - val_mae: 0.0608 - val_mse: 0.0227 - learning_rate: 0.1000 - val_custom_mse: 0.2121 - val_custom_mae: 0.3188\n",
            "Epoch 86/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0408 - mae: 0.0643 - mse: 0.0408 - val_loss: 0.0226 - val_mae: 0.0606 - val_mse: 0.0226 - learning_rate: 0.1000 - val_custom_mse: 0.2112 - val_custom_mae: 0.3180\n",
            "Epoch 87/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0407 - mae: 0.0638 - mse: 0.0407 - val_loss: 0.0225 - val_mae: 0.0600 - val_mse: 0.0225 - learning_rate: 0.1000 - val_custom_mse: 0.2117 - val_custom_mae: 0.3184\n",
            "Epoch 88/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0406 - mae: 0.0634 - mse: 0.0406 - val_loss: 0.0225 - val_mae: 0.0608 - val_mse: 0.0225 - learning_rate: 0.1000 - val_custom_mse: 0.2104 - val_custom_mae: 0.3171\n",
            "Epoch 89/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0405 - mae: 0.0630 - mse: 0.0405 - val_loss: 0.0223 - val_mae: 0.0592 - val_mse: 0.0223 - learning_rate: 0.1000 - val_custom_mse: 0.2112 - val_custom_mae: 0.3179\n",
            "Epoch 90/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0404 - mae: 0.0626 - mse: 0.0404 - val_loss: 0.0223 - val_mae: 0.0591 - val_mse: 0.0223 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3186\n",
            "Epoch 91/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0404 - mae: 0.0622 - mse: 0.0404 - val_loss: 0.0222 - val_mae: 0.0585 - val_mse: 0.0222 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3186\n",
            "Epoch 92/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0403 - mae: 0.0618 - mse: 0.0403 - val_loss: 0.0221 - val_mae: 0.0584 - val_mse: 0.0221 - learning_rate: 0.1000 - val_custom_mse: 0.2111 - val_custom_mae: 0.3179\n",
            "Epoch 93/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0402 - mae: 0.0614 - mse: 0.0402 - val_loss: 0.0221 - val_mae: 0.0588 - val_mse: 0.0221 - learning_rate: 0.1000 - val_custom_mse: 0.2103 - val_custom_mae: 0.3171\n",
            "Epoch 94/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0401 - mae: 0.0610 - mse: 0.0401 - val_loss: 0.0220 - val_mae: 0.0572 - val_mse: 0.0220 - learning_rate: 0.1000 - val_custom_mse: 0.2112 - val_custom_mae: 0.3180\n",
            "Epoch 95/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0400 - mae: 0.0606 - mse: 0.0400 - val_loss: 0.0219 - val_mae: 0.0572 - val_mse: 0.0219 - learning_rate: 0.1000 - val_custom_mse: 0.2108 - val_custom_mae: 0.3176\n",
            "Epoch 96/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0399 - mae: 0.0602 - mse: 0.0399 - val_loss: 0.0219 - val_mae: 0.0570 - val_mse: 0.0219 - learning_rate: 0.1000 - val_custom_mse: 0.2106 - val_custom_mae: 0.3173\n",
            "Epoch 97/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0399 - mae: 0.0599 - mse: 0.0399 - val_loss: 0.0218 - val_mae: 0.0563 - val_mse: 0.0218 - learning_rate: 0.1000 - val_custom_mse: 0.2110 - val_custom_mae: 0.3178\n",
            "Epoch 98/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0398 - mae: 0.0596 - mse: 0.0398 - val_loss: 0.0217 - val_mae: 0.0557 - val_mse: 0.0217 - learning_rate: 0.1000 - val_custom_mse: 0.2112 - val_custom_mae: 0.3180\n",
            "Epoch 99/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0397 - mae: 0.0593 - mse: 0.0397 - val_loss: 0.0217 - val_mae: 0.0557 - val_mse: 0.0217 - learning_rate: 0.1000 - val_custom_mse: 0.2108 - val_custom_mae: 0.3177\n",
            "Epoch 100/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0397 - mae: 0.0590 - mse: 0.0397 - val_loss: 0.0217 - val_mae: 0.0551 - val_mse: 0.0217 - learning_rate: 0.1000 - val_custom_mse: 0.2115 - val_custom_mae: 0.3184\n",
            "Running experiment: horizon=96, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_34', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "236/236 - 6s - 25ms/step - loss: 0.4793 - mae: 0.4169 - mse: 0.4793 - val_loss: 0.2760 - val_mae: 0.3800 - val_mse: 0.2760 - learning_rate: 0.1000 - val_custom_mse: 0.3903 - val_custom_mae: 0.4583\n",
            "Epoch 2/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.4266 - mae: 0.3946 - mse: 0.4266 - val_loss: 0.2369 - val_mae: 0.3522 - val_mse: 0.2369 - learning_rate: 0.1000 - val_custom_mse: 0.3495 - val_custom_mae: 0.4321\n",
            "Epoch 3/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3735 - mae: 0.3698 - mse: 0.3735 - val_loss: 0.2012 - val_mae: 0.3229 - val_mse: 0.2012 - learning_rate: 0.1000 - val_custom_mse: 0.3181 - val_custom_mae: 0.4079\n",
            "Epoch 4/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3162 - mae: 0.3418 - mse: 0.3162 - val_loss: 0.1686 - val_mae: 0.2953 - val_mse: 0.1686 - learning_rate: 0.1000 - val_custom_mse: 0.3036 - val_custom_mae: 0.3965\n",
            "Epoch 5/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2600 - mae: 0.3105 - mse: 0.2600 - val_loss: 0.1391 - val_mae: 0.2673 - val_mse: 0.1391 - learning_rate: 0.1000 - val_custom_mse: 0.2843 - val_custom_mae: 0.3815\n",
            "Epoch 6/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2123 - mae: 0.2823 - mse: 0.2123 - val_loss: 0.1168 - val_mae: 0.2435 - val_mse: 0.1168 - learning_rate: 0.1000 - val_custom_mse: 0.2692 - val_custom_mae: 0.3694\n",
            "Epoch 7/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1767 - mae: 0.2588 - mse: 0.1767 - val_loss: 0.1011 - val_mae: 0.2251 - val_mse: 0.1011 - learning_rate: 0.1000 - val_custom_mse: 0.2559 - val_custom_mae: 0.3586\n",
            "Epoch 8/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1524 - mae: 0.2408 - mse: 0.1524 - val_loss: 0.0910 - val_mae: 0.2121 - val_mse: 0.0910 - learning_rate: 0.1000 - val_custom_mse: 0.2465 - val_custom_mae: 0.3507\n",
            "Epoch 9/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1367 - mae: 0.2277 - mse: 0.1367 - val_loss: 0.0841 - val_mae: 0.2026 - val_mse: 0.0841 - learning_rate: 0.1000 - val_custom_mse: 0.2400 - val_custom_mae: 0.3451\n",
            "Epoch 10/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1263 - mae: 0.2178 - mse: 0.1263 - val_loss: 0.0790 - val_mae: 0.1950 - val_mse: 0.0790 - learning_rate: 0.1000 - val_custom_mse: 0.2378 - val_custom_mae: 0.3434\n",
            "Epoch 11/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1188 - mae: 0.2098 - mse: 0.1188 - val_loss: 0.0748 - val_mae: 0.1884 - val_mse: 0.0748 - learning_rate: 0.1000 - val_custom_mse: 0.2343 - val_custom_mae: 0.3403\n",
            "Epoch 12/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1129 - mae: 0.2029 - mse: 0.1129 - val_loss: 0.0712 - val_mae: 0.1824 - val_mse: 0.0712 - learning_rate: 0.1000 - val_custom_mse: 0.2316 - val_custom_mae: 0.3379\n",
            "Epoch 13/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1080 - mae: 0.1968 - mse: 0.1080 - val_loss: 0.0680 - val_mae: 0.1769 - val_mse: 0.0680 - learning_rate: 0.1000 - val_custom_mse: 0.2286 - val_custom_mae: 0.3351\n",
            "Epoch 14/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1039 - mae: 0.1914 - mse: 0.1039 - val_loss: 0.0653 - val_mae: 0.1719 - val_mse: 0.0653 - learning_rate: 0.1000 - val_custom_mse: 0.2283 - val_custom_mae: 0.3350\n",
            "Epoch 15/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1004 - mae: 0.1864 - mse: 0.1004 - val_loss: 0.0629 - val_mae: 0.1674 - val_mse: 0.0629 - learning_rate: 0.1000 - val_custom_mse: 0.2282 - val_custom_mae: 0.3350\n",
            "Epoch 16/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0974 - mae: 0.1820 - mse: 0.0974 - val_loss: 0.0604 - val_mae: 0.1628 - val_mse: 0.0604 - learning_rate: 0.1000 - val_custom_mse: 0.2250 - val_custom_mae: 0.3318\n",
            "Epoch 17/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0946 - mae: 0.1779 - mse: 0.0946 - val_loss: 0.0584 - val_mae: 0.1589 - val_mse: 0.0584 - learning_rate: 0.1000 - val_custom_mse: 0.2241 - val_custom_mae: 0.3310\n",
            "Epoch 18/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0922 - mae: 0.1742 - mse: 0.0922 - val_loss: 0.0565 - val_mae: 0.1553 - val_mse: 0.0565 - learning_rate: 0.1000 - val_custom_mse: 0.2221 - val_custom_mae: 0.3290\n",
            "Epoch 19/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0902 - mae: 0.1708 - mse: 0.0902 - val_loss: 0.0549 - val_mae: 0.1519 - val_mse: 0.0549 - learning_rate: 0.1000 - val_custom_mse: 0.2216 - val_custom_mae: 0.3285\n",
            "Epoch 20/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0883 - mae: 0.1678 - mse: 0.0883 - val_loss: 0.0533 - val_mae: 0.1488 - val_mse: 0.0533 - learning_rate: 0.1000 - val_custom_mse: 0.2207 - val_custom_mae: 0.3278\n",
            "Epoch 21/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0866 - mae: 0.1649 - mse: 0.0866 - val_loss: 0.0519 - val_mae: 0.1459 - val_mse: 0.0519 - learning_rate: 0.1000 - val_custom_mse: 0.2199 - val_custom_mae: 0.3268\n",
            "Epoch 22/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0851 - mae: 0.1623 - mse: 0.0851 - val_loss: 0.0508 - val_mae: 0.1432 - val_mse: 0.0508 - learning_rate: 0.1000 - val_custom_mse: 0.2203 - val_custom_mae: 0.3272\n",
            "Epoch 23/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0837 - mae: 0.1600 - mse: 0.0837 - val_loss: 0.0497 - val_mae: 0.1408 - val_mse: 0.0497 - learning_rate: 0.1000 - val_custom_mse: 0.2206 - val_custom_mae: 0.3276\n",
            "Epoch 24/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0826 - mae: 0.1578 - mse: 0.0826 - val_loss: 0.0485 - val_mae: 0.1383 - val_mse: 0.0485 - learning_rate: 0.1000 - val_custom_mse: 0.2194 - val_custom_mae: 0.3263\n",
            "Epoch 25/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0814 - mae: 0.1557 - mse: 0.0814 - val_loss: 0.0475 - val_mae: 0.1362 - val_mse: 0.0475 - learning_rate: 0.1000 - val_custom_mse: 0.2189 - val_custom_mae: 0.3259\n",
            "Epoch 26/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0804 - mae: 0.1538 - mse: 0.0804 - val_loss: 0.0465 - val_mae: 0.1342 - val_mse: 0.0465 - learning_rate: 0.1000 - val_custom_mse: 0.2178 - val_custom_mae: 0.3248\n",
            "Epoch 27/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0795 - mae: 0.1521 - mse: 0.0795 - val_loss: 0.0456 - val_mae: 0.1323 - val_mse: 0.0456 - learning_rate: 0.1000 - val_custom_mse: 0.2168 - val_custom_mae: 0.3237\n",
            "Epoch 28/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0786 - mae: 0.1504 - mse: 0.0786 - val_loss: 0.0448 - val_mae: 0.1304 - val_mse: 0.0448 - learning_rate: 0.1000 - val_custom_mse: 0.2173 - val_custom_mae: 0.3242\n",
            "Epoch 29/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0777 - mae: 0.1489 - mse: 0.0777 - val_loss: 0.0440 - val_mae: 0.1287 - val_mse: 0.0440 - learning_rate: 0.1000 - val_custom_mse: 0.2166 - val_custom_mae: 0.3234\n",
            "Epoch 30/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0770 - mae: 0.1474 - mse: 0.0770 - val_loss: 0.0433 - val_mae: 0.1271 - val_mse: 0.0433 - learning_rate: 0.1000 - val_custom_mse: 0.2161 - val_custom_mae: 0.3229\n",
            "Epoch 31/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0763 - mae: 0.1460 - mse: 0.0763 - val_loss: 0.0426 - val_mae: 0.1256 - val_mse: 0.0426 - learning_rate: 0.1000 - val_custom_mse: 0.2160 - val_custom_mae: 0.3229\n",
            "Epoch 32/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0757 - mae: 0.1448 - mse: 0.0757 - val_loss: 0.0420 - val_mae: 0.1242 - val_mse: 0.0420 - learning_rate: 0.1000 - val_custom_mse: 0.2162 - val_custom_mae: 0.3230\n",
            "Epoch 33/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0751 - mae: 0.1435 - mse: 0.0751 - val_loss: 0.0414 - val_mae: 0.1228 - val_mse: 0.0414 - learning_rate: 0.1000 - val_custom_mse: 0.2154 - val_custom_mae: 0.3221\n",
            "Epoch 34/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0744 - mae: 0.1423 - mse: 0.0744 - val_loss: 0.0408 - val_mae: 0.1215 - val_mse: 0.0408 - learning_rate: 0.1000 - val_custom_mse: 0.2151 - val_custom_mae: 0.3218\n",
            "Epoch 35/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0741 - mae: 0.1413 - mse: 0.0741 - val_loss: 0.0403 - val_mae: 0.1203 - val_mse: 0.0403 - learning_rate: 0.1000 - val_custom_mse: 0.2153 - val_custom_mae: 0.3222\n",
            "Epoch 36/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0736 - mae: 0.1403 - mse: 0.0736 - val_loss: 0.0397 - val_mae: 0.1192 - val_mse: 0.0397 - learning_rate: 0.1000 - val_custom_mse: 0.2147 - val_custom_mae: 0.3216\n",
            "Epoch 37/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0731 - mae: 0.1393 - mse: 0.0731 - val_loss: 0.0393 - val_mae: 0.1180 - val_mse: 0.0393 - learning_rate: 0.1000 - val_custom_mse: 0.2156 - val_custom_mae: 0.3224\n",
            "Epoch 38/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0726 - mae: 0.1383 - mse: 0.0726 - val_loss: 0.0388 - val_mae: 0.1170 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.2148 - val_custom_mae: 0.3216\n",
            "Epoch 39/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0722 - mae: 0.1375 - mse: 0.0722 - val_loss: 0.0384 - val_mae: 0.1161 - val_mse: 0.0384 - learning_rate: 0.1000 - val_custom_mse: 0.2148 - val_custom_mae: 0.3217\n",
            "Epoch 40/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0719 - mae: 0.1366 - mse: 0.0719 - val_loss: 0.0379 - val_mae: 0.1150 - val_mse: 0.0379 - learning_rate: 0.1000 - val_custom_mse: 0.2142 - val_custom_mae: 0.3210\n",
            "Epoch 41/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0716 - mae: 0.1359 - mse: 0.0716 - val_loss: 0.0376 - val_mae: 0.1142 - val_mse: 0.0376 - learning_rate: 0.1000 - val_custom_mse: 0.2148 - val_custom_mae: 0.3216\n",
            "Epoch 42/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0711 - mae: 0.1351 - mse: 0.0711 - val_loss: 0.0372 - val_mae: 0.1132 - val_mse: 0.0372 - learning_rate: 0.1000 - val_custom_mse: 0.2139 - val_custom_mae: 0.3207\n",
            "Epoch 43/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0708 - mae: 0.1344 - mse: 0.0708 - val_loss: 0.0368 - val_mae: 0.1124 - val_mse: 0.0368 - learning_rate: 0.1000 - val_custom_mse: 0.2136 - val_custom_mae: 0.3204\n",
            "Epoch 44/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0706 - mae: 0.1337 - mse: 0.0706 - val_loss: 0.0366 - val_mae: 0.1117 - val_mse: 0.0366 - learning_rate: 0.1000 - val_custom_mse: 0.2145 - val_custom_mae: 0.3213\n",
            "Epoch 45/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0703 - mae: 0.1331 - mse: 0.0703 - val_loss: 0.0363 - val_mae: 0.1110 - val_mse: 0.0363 - learning_rate: 0.1000 - val_custom_mse: 0.2143 - val_custom_mae: 0.3210\n",
            "Epoch 46/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0701 - mae: 0.1324 - mse: 0.0701 - val_loss: 0.0358 - val_mae: 0.1102 - val_mse: 0.0358 - learning_rate: 0.1000 - val_custom_mse: 0.2133 - val_custom_mae: 0.3201\n",
            "Epoch 47/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0697 - mae: 0.1318 - mse: 0.0697 - val_loss: 0.0357 - val_mae: 0.1097 - val_mse: 0.0357 - learning_rate: 0.1000 - val_custom_mse: 0.2144 - val_custom_mae: 0.3212\n",
            "Epoch 48/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0696 - mae: 0.1312 - mse: 0.0696 - val_loss: 0.0354 - val_mae: 0.1092 - val_mse: 0.0354 - learning_rate: 0.1000 - val_custom_mse: 0.2132 - val_custom_mae: 0.3201\n",
            "Epoch 49/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0692 - mae: 0.1306 - mse: 0.0692 - val_loss: 0.0351 - val_mae: 0.1085 - val_mse: 0.0351 - learning_rate: 0.1000 - val_custom_mse: 0.2129 - val_custom_mae: 0.3197\n",
            "Epoch 50/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0690 - mae: 0.1301 - mse: 0.0690 - val_loss: 0.0348 - val_mae: 0.1078 - val_mse: 0.0348 - learning_rate: 0.1000 - val_custom_mse: 0.2135 - val_custom_mae: 0.3200\n",
            "Epoch 51/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0688 - mae: 0.1297 - mse: 0.0688 - val_loss: 0.0358 - val_mae: 0.1100 - val_mse: 0.0358 - learning_rate: 0.1000 - val_custom_mse: 0.2185 - val_custom_mae: 0.3253\n",
            "Epoch 52/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0689 - mae: 0.1296 - mse: 0.0689 - val_loss: 0.0343 - val_mae: 0.1068 - val_mse: 0.0343 - learning_rate: 0.1000 - val_custom_mse: 0.2126 - val_custom_mae: 0.3194\n",
            "Epoch 53/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0684 - mae: 0.1288 - mse: 0.0684 - val_loss: 0.0341 - val_mae: 0.1061 - val_mse: 0.0341 - learning_rate: 0.1000 - val_custom_mse: 0.2132 - val_custom_mae: 0.3199\n",
            "Epoch 54/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0683 - mae: 0.1283 - mse: 0.0683 - val_loss: 0.0339 - val_mae: 0.1057 - val_mse: 0.0339 - learning_rate: 0.1000 - val_custom_mse: 0.2126 - val_custom_mae: 0.3193\n",
            "Epoch 55/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0681 - mae: 0.1279 - mse: 0.0681 - val_loss: 0.0337 - val_mae: 0.1052 - val_mse: 0.0337 - learning_rate: 0.1000 - val_custom_mse: 0.2130 - val_custom_mae: 0.3197\n",
            "Epoch 56/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0680 - mae: 0.1275 - mse: 0.0680 - val_loss: 0.0335 - val_mae: 0.1047 - val_mse: 0.0335 - learning_rate: 0.1000 - val_custom_mse: 0.2126 - val_custom_mae: 0.3193\n",
            "Epoch 57/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0678 - mae: 0.1271 - mse: 0.0678 - val_loss: 0.0334 - val_mae: 0.1043 - val_mse: 0.0334 - learning_rate: 0.1000 - val_custom_mse: 0.2132 - val_custom_mae: 0.3199\n",
            "Epoch 58/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0677 - mae: 0.1267 - mse: 0.0677 - val_loss: 0.0332 - val_mae: 0.1039 - val_mse: 0.0332 - learning_rate: 0.1000 - val_custom_mse: 0.2126 - val_custom_mae: 0.3194\n",
            "Epoch 59/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0674 - mae: 0.1263 - mse: 0.0674 - val_loss: 0.0330 - val_mae: 0.1035 - val_mse: 0.0330 - learning_rate: 0.1000 - val_custom_mse: 0.2123 - val_custom_mae: 0.3189\n",
            "Epoch 60/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0674 - mae: 0.1259 - mse: 0.0674 - val_loss: 0.0328 - val_mae: 0.1032 - val_mse: 0.0328 - learning_rate: 0.1000 - val_custom_mse: 0.2117 - val_custom_mae: 0.3183\n",
            "Epoch 61/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0672 - mae: 0.1256 - mse: 0.0672 - val_loss: 0.0327 - val_mae: 0.1026 - val_mse: 0.0327 - learning_rate: 0.1000 - val_custom_mse: 0.2124 - val_custom_mae: 0.3191\n",
            "Epoch 62/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0672 - mae: 0.1253 - mse: 0.0672 - val_loss: 0.0326 - val_mae: 0.1025 - val_mse: 0.0326 - learning_rate: 0.1000 - val_custom_mse: 0.2128 - val_custom_mae: 0.3197\n",
            "Epoch 63/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0669 - mae: 0.1250 - mse: 0.0669 - val_loss: 0.0324 - val_mae: 0.1020 - val_mse: 0.0324 - learning_rate: 0.1000 - val_custom_mse: 0.2118 - val_custom_mae: 0.3185\n",
            "Epoch 64/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0670 - mae: 0.1248 - mse: 0.0670 - val_loss: 0.0323 - val_mae: 0.1018 - val_mse: 0.0323 - learning_rate: 0.1000 - val_custom_mse: 0.2124 - val_custom_mae: 0.3191\n",
            "Epoch 65/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0667 - mae: 0.1244 - mse: 0.0667 - val_loss: 0.0322 - val_mae: 0.1014 - val_mse: 0.0322 - learning_rate: 0.1000 - val_custom_mse: 0.2121 - val_custom_mae: 0.3187\n",
            "Epoch 66/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0668 - mae: 0.1242 - mse: 0.0668 - val_loss: 0.0320 - val_mae: 0.1011 - val_mse: 0.0320 - learning_rate: 0.1000 - val_custom_mse: 0.2118 - val_custom_mae: 0.3184\n",
            "Epoch 67/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0666 - mae: 0.1239 - mse: 0.0666 - val_loss: 0.0318 - val_mae: 0.1010 - val_mse: 0.0318 - learning_rate: 0.1000 - val_custom_mse: 0.2105 - val_custom_mae: 0.3173\n",
            "Epoch 68/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0666 - mae: 0.1237 - mse: 0.0666 - val_loss: 0.0318 - val_mae: 0.1008 - val_mse: 0.0318 - learning_rate: 0.1000 - val_custom_mse: 0.2110 - val_custom_mae: 0.3176\n",
            "Epoch 69/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0666 - mae: 0.1235 - mse: 0.0666 - val_loss: 0.0317 - val_mae: 0.1003 - val_mse: 0.0317 - learning_rate: 0.1000 - val_custom_mse: 0.2114 - val_custom_mae: 0.3179\n",
            "Epoch 70/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0664 - mae: 0.1232 - mse: 0.0664 - val_loss: 0.0316 - val_mae: 0.1001 - val_mse: 0.0316 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3182\n",
            "Epoch 71/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0663 - mae: 0.1230 - mse: 0.0663 - val_loss: 0.0316 - val_mae: 0.0997 - val_mse: 0.0316 - learning_rate: 0.1000 - val_custom_mse: 0.2124 - val_custom_mae: 0.3187\n",
            "Epoch 72/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0662 - mae: 0.1228 - mse: 0.0662 - val_loss: 0.0314 - val_mae: 0.0996 - val_mse: 0.0314 - learning_rate: 0.1000 - val_custom_mse: 0.2112 - val_custom_mae: 0.3178\n",
            "Epoch 73/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0661 - mae: 0.1226 - mse: 0.0661 - val_loss: 0.0313 - val_mae: 0.0994 - val_mse: 0.0313 - learning_rate: 0.1000 - val_custom_mse: 0.2111 - val_custom_mae: 0.3175\n",
            "Epoch 74/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0662 - mae: 0.1224 - mse: 0.0662 - val_loss: 0.0324 - val_mae: 0.1015 - val_mse: 0.0324 - learning_rate: 0.1000 - val_custom_mse: 0.2172 - val_custom_mae: 0.3235\n",
            "Epoch 75/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0663 - mae: 0.1227 - mse: 0.0663 - val_loss: 0.0312 - val_mae: 0.0989 - val_mse: 0.0312 - learning_rate: 0.1000 - val_custom_mse: 0.2118 - val_custom_mae: 0.3184\n",
            "Epoch 76/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0660 - mae: 0.1221 - mse: 0.0660 - val_loss: 0.0312 - val_mae: 0.0988 - val_mse: 0.0312 - learning_rate: 0.1000 - val_custom_mse: 0.2118 - val_custom_mae: 0.3185\n",
            "Epoch 77/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0659 - mae: 0.1220 - mse: 0.0659 - val_loss: 0.0311 - val_mae: 0.0985 - val_mse: 0.0311 - learning_rate: 0.1000 - val_custom_mse: 0.2114 - val_custom_mae: 0.3181\n",
            "Epoch 78/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0659 - mae: 0.1218 - mse: 0.0659 - val_loss: 0.0312 - val_mae: 0.0986 - val_mse: 0.0312 - learning_rate: 0.1000 - val_custom_mse: 0.2126 - val_custom_mae: 0.3194\n",
            "Epoch 79/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0659 - mae: 0.1216 - mse: 0.0659 - val_loss: 0.0309 - val_mae: 0.0982 - val_mse: 0.0309 - learning_rate: 0.1000 - val_custom_mse: 0.2112 - val_custom_mae: 0.3177\n",
            "Epoch 80/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0658 - mae: 0.1215 - mse: 0.0658 - val_loss: 0.0309 - val_mae: 0.0981 - val_mse: 0.0309 - learning_rate: 0.1000 - val_custom_mse: 0.2115 - val_custom_mae: 0.3181\n",
            "Epoch 81/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0658 - mae: 0.1214 - mse: 0.0658 - val_loss: 0.0309 - val_mae: 0.0979 - val_mse: 0.0309 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3183\n",
            "Epoch 82/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0657 - mae: 0.1212 - mse: 0.0657 - val_loss: 0.0308 - val_mae: 0.0978 - val_mse: 0.0308 - learning_rate: 0.1000 - val_custom_mse: 0.2112 - val_custom_mae: 0.3179\n",
            "Epoch 83/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0657 - mae: 0.1211 - mse: 0.0657 - val_loss: 0.0308 - val_mae: 0.0976 - val_mse: 0.0308 - learning_rate: 0.1000 - val_custom_mse: 0.2117 - val_custom_mae: 0.3182\n",
            "Epoch 84/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0656 - mae: 0.1209 - mse: 0.0656 - val_loss: 0.0307 - val_mae: 0.0974 - val_mse: 0.0307 - learning_rate: 0.1000 - val_custom_mse: 0.2119 - val_custom_mae: 0.3183\n",
            "Epoch 85/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0656 - mae: 0.1208 - mse: 0.0656 - val_loss: 0.0307 - val_mae: 0.0975 - val_mse: 0.0307 - learning_rate: 0.1000 - val_custom_mse: 0.2119 - val_custom_mae: 0.3186\n",
            "Epoch 86/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0656 - mae: 0.1207 - mse: 0.0656 - val_loss: 0.0307 - val_mae: 0.0973 - val_mse: 0.0307 - learning_rate: 0.1000 - val_custom_mse: 0.2119 - val_custom_mae: 0.3185\n",
            "Epoch 87/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0654 - mae: 0.1205 - mse: 0.0654 - val_loss: 0.0306 - val_mae: 0.0971 - val_mse: 0.0306 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3180\n",
            "Epoch 88/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0654 - mae: 0.1204 - mse: 0.0654 - val_loss: 0.0305 - val_mae: 0.0970 - val_mse: 0.0305 - learning_rate: 0.1000 - val_custom_mse: 0.2111 - val_custom_mae: 0.3179\n",
            "Epoch 89/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0654 - mae: 0.1203 - mse: 0.0654 - val_loss: 0.0305 - val_mae: 0.0969 - val_mse: 0.0305 - learning_rate: 0.1000 - val_custom_mse: 0.2115 - val_custom_mae: 0.3182\n",
            "Epoch 90/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0654 - mae: 0.1202 - mse: 0.0654 - val_loss: 0.0305 - val_mae: 0.0970 - val_mse: 0.0305 - learning_rate: 0.1000 - val_custom_mse: 0.2114 - val_custom_mae: 0.3182\n",
            "Epoch 91/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0653 - mae: 0.1202 - mse: 0.0653 - val_loss: 0.0305 - val_mae: 0.0967 - val_mse: 0.0305 - learning_rate: 0.1000 - val_custom_mse: 0.2123 - val_custom_mae: 0.3188\n",
            "Epoch 92/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0652 - mae: 0.1201 - mse: 0.0652 - val_loss: 0.0303 - val_mae: 0.0966 - val_mse: 0.0303 - learning_rate: 0.1000 - val_custom_mse: 0.2107 - val_custom_mae: 0.3172\n",
            "Epoch 93/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0652 - mae: 0.1199 - mse: 0.0652 - val_loss: 0.0304 - val_mae: 0.0965 - val_mse: 0.0304 - learning_rate: 0.1000 - val_custom_mse: 0.2115 - val_custom_mae: 0.3182\n",
            "Epoch 94/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0653 - mae: 0.1199 - mse: 0.0653 - val_loss: 0.0304 - val_mae: 0.0965 - val_mse: 0.0304 - learning_rate: 0.1000 - val_custom_mse: 0.2115 - val_custom_mae: 0.3183\n",
            "Epoch 95/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0652 - mae: 0.1198 - mse: 0.0652 - val_loss: 0.0304 - val_mae: 0.0964 - val_mse: 0.0304 - learning_rate: 0.1000 - val_custom_mse: 0.2121 - val_custom_mae: 0.3187\n",
            "Epoch 96/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0651 - mae: 0.1197 - mse: 0.0651 - val_loss: 0.0302 - val_mae: 0.0963 - val_mse: 0.0302 - learning_rate: 0.1000 - val_custom_mse: 0.2107 - val_custom_mae: 0.3172\n",
            "Epoch 97/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0651 - mae: 0.1196 - mse: 0.0651 - val_loss: 0.0303 - val_mae: 0.0961 - val_mse: 0.0303 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3182\n",
            "Epoch 98/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0651 - mae: 0.1196 - mse: 0.0651 - val_loss: 0.0302 - val_mae: 0.0962 - val_mse: 0.0302 - learning_rate: 0.1000 - val_custom_mse: 0.2111 - val_custom_mae: 0.3175\n",
            "Epoch 99/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0650 - mae: 0.1195 - mse: 0.0650 - val_loss: 0.0316 - val_mae: 0.0997 - val_mse: 0.0316 - learning_rate: 0.1000 - val_custom_mse: 0.2177 - val_custom_mae: 0.3241\n",
            "Epoch 100/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0654 - mae: 0.1201 - mse: 0.0654 - val_loss: 0.0302 - val_mae: 0.0961 - val_mse: 0.0302 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3182\n",
            "Running experiment: horizon=96, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_35', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "236/236 - 6s - 26ms/step - loss: 0.4857 - mae: 0.4211 - mse: 0.4857 - val_loss: 0.2734 - val_mae: 0.3783 - val_mse: 0.2734 - learning_rate: 0.1000 - val_custom_mse: 0.3858 - val_custom_mae: 0.4541\n",
            "Epoch 2/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.4246 - mae: 0.3944 - mse: 0.4246 - val_loss: 0.2358 - val_mae: 0.3511 - val_mse: 0.2358 - learning_rate: 0.1000 - val_custom_mse: 0.3524 - val_custom_mae: 0.4318\n",
            "Epoch 3/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3682 - mae: 0.3681 - mse: 0.3682 - val_loss: 0.2005 - val_mae: 0.3238 - val_mse: 0.2005 - learning_rate: 0.1000 - val_custom_mse: 0.3298 - val_custom_mae: 0.4156\n",
            "Epoch 4/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.3093 - mae: 0.3386 - mse: 0.3093 - val_loss: 0.1635 - val_mae: 0.2911 - val_mse: 0.1635 - learning_rate: 0.1000 - val_custom_mse: 0.2961 - val_custom_mae: 0.3905\n",
            "Epoch 5/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.2542 - mae: 0.3084 - mse: 0.2542 - val_loss: 0.1358 - val_mae: 0.2644 - val_mse: 0.1358 - learning_rate: 0.1000 - val_custom_mse: 0.2798 - val_custom_mae: 0.3777\n",
            "Epoch 6/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.2091 - mae: 0.2813 - mse: 0.2091 - val_loss: 0.1148 - val_mae: 0.2417 - val_mse: 0.1148 - learning_rate: 0.1000 - val_custom_mse: 0.2654 - val_custom_mae: 0.3661\n",
            "Epoch 7/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1764 - mae: 0.2593 - mse: 0.1764 - val_loss: 0.1003 - val_mae: 0.2243 - val_mse: 0.1003 - learning_rate: 0.1000 - val_custom_mse: 0.2528 - val_custom_mae: 0.3557\n",
            "Epoch 8/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1549 - mae: 0.2432 - mse: 0.1549 - val_loss: 0.0911 - val_mae: 0.2125 - val_mse: 0.0911 - learning_rate: 0.1000 - val_custom_mse: 0.2465 - val_custom_mae: 0.3505\n",
            "Epoch 9/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1410 - mae: 0.2317 - mse: 0.1410 - val_loss: 0.0847 - val_mae: 0.2037 - val_mse: 0.0847 - learning_rate: 0.1000 - val_custom_mse: 0.2403 - val_custom_mae: 0.3453\n",
            "Epoch 10/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1319 - mae: 0.2231 - mse: 0.1319 - val_loss: 0.0798 - val_mae: 0.1965 - val_mse: 0.0798 - learning_rate: 0.1000 - val_custom_mse: 0.2365 - val_custom_mae: 0.3420\n",
            "Epoch 11/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1253 - mae: 0.2162 - mse: 0.1253 - val_loss: 0.0759 - val_mae: 0.1903 - val_mse: 0.0759 - learning_rate: 0.1000 - val_custom_mse: 0.2336 - val_custom_mae: 0.3394\n",
            "Epoch 12/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1202 - mae: 0.2105 - mse: 0.1202 - val_loss: 0.0726 - val_mae: 0.1848 - val_mse: 0.0726 - learning_rate: 0.1000 - val_custom_mse: 0.2329 - val_custom_mae: 0.3389\n",
            "Epoch 13/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1159 - mae: 0.2053 - mse: 0.1159 - val_loss: 0.0694 - val_mae: 0.1797 - val_mse: 0.0694 - learning_rate: 0.1000 - val_custom_mse: 0.2290 - val_custom_mae: 0.3353\n",
            "Epoch 14/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1124 - mae: 0.2008 - mse: 0.1124 - val_loss: 0.0679 - val_mae: 0.1763 - val_mse: 0.0679 - learning_rate: 0.1000 - val_custom_mse: 0.2341 - val_custom_mae: 0.3404\n",
            "Epoch 15/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1094 - mae: 0.1969 - mse: 0.1094 - val_loss: 0.0646 - val_mae: 0.1709 - val_mse: 0.0646 - learning_rate: 0.1000 - val_custom_mse: 0.2274 - val_custom_mae: 0.3340\n",
            "Epoch 16/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1065 - mae: 0.1930 - mse: 0.1065 - val_loss: 0.0624 - val_mae: 0.1670 - val_mse: 0.0624 - learning_rate: 0.1000 - val_custom_mse: 0.2256 - val_custom_mae: 0.3324\n",
            "Epoch 17/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1042 - mae: 0.1898 - mse: 0.1042 - val_loss: 0.0605 - val_mae: 0.1636 - val_mse: 0.0605 - learning_rate: 0.1000 - val_custom_mse: 0.2225 - val_custom_mae: 0.3292\n",
            "Epoch 18/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1022 - mae: 0.1869 - mse: 0.1022 - val_loss: 0.0589 - val_mae: 0.1603 - val_mse: 0.0589 - learning_rate: 0.1000 - val_custom_mse: 0.2231 - val_custom_mae: 0.3298\n",
            "Epoch 19/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1004 - mae: 0.1843 - mse: 0.1004 - val_loss: 0.0573 - val_mae: 0.1575 - val_mse: 0.0573 - learning_rate: 0.1000 - val_custom_mse: 0.2214 - val_custom_mae: 0.3282\n",
            "Epoch 20/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0988 - mae: 0.1819 - mse: 0.0988 - val_loss: 0.0560 - val_mae: 0.1547 - val_mse: 0.0560 - learning_rate: 0.1000 - val_custom_mse: 0.2214 - val_custom_mae: 0.3283\n",
            "Epoch 21/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0974 - mae: 0.1797 - mse: 0.0974 - val_loss: 0.0547 - val_mae: 0.1522 - val_mse: 0.0547 - learning_rate: 0.1000 - val_custom_mse: 0.2206 - val_custom_mae: 0.3274\n",
            "Epoch 22/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0962 - mae: 0.1778 - mse: 0.0962 - val_loss: 0.0536 - val_mae: 0.1500 - val_mse: 0.0536 - learning_rate: 0.1000 - val_custom_mse: 0.2195 - val_custom_mae: 0.3262\n",
            "Epoch 23/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0950 - mae: 0.1760 - mse: 0.0950 - val_loss: 0.0526 - val_mae: 0.1480 - val_mse: 0.0526 - learning_rate: 0.1000 - val_custom_mse: 0.2194 - val_custom_mae: 0.3263\n",
            "Epoch 24/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0940 - mae: 0.1744 - mse: 0.0940 - val_loss: 0.0517 - val_mae: 0.1460 - val_mse: 0.0517 - learning_rate: 0.1000 - val_custom_mse: 0.2192 - val_custom_mae: 0.3261\n",
            "Epoch 25/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0932 - mae: 0.1730 - mse: 0.0932 - val_loss: 0.0508 - val_mae: 0.1442 - val_mse: 0.0508 - learning_rate: 0.1000 - val_custom_mse: 0.2188 - val_custom_mae: 0.3256\n",
            "Epoch 26/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0922 - mae: 0.1716 - mse: 0.0922 - val_loss: 0.0500 - val_mae: 0.1427 - val_mse: 0.0500 - learning_rate: 0.1000 - val_custom_mse: 0.2179 - val_custom_mae: 0.3247\n",
            "Epoch 27/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0916 - mae: 0.1704 - mse: 0.0916 - val_loss: 0.0494 - val_mae: 0.1413 - val_mse: 0.0494 - learning_rate: 0.1000 - val_custom_mse: 0.2187 - val_custom_mae: 0.3256\n",
            "Epoch 28/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0907 - mae: 0.1691 - mse: 0.0907 - val_loss: 0.0487 - val_mae: 0.1399 - val_mse: 0.0487 - learning_rate: 0.1000 - val_custom_mse: 0.2183 - val_custom_mae: 0.3252\n",
            "Epoch 29/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0900 - mae: 0.1681 - mse: 0.0900 - val_loss: 0.0484 - val_mae: 0.1390 - val_mse: 0.0484 - learning_rate: 0.1000 - val_custom_mse: 0.2197 - val_custom_mae: 0.3268\n",
            "Epoch 30/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0894 - mae: 0.1671 - mse: 0.0894 - val_loss: 0.0474 - val_mae: 0.1373 - val_mse: 0.0474 - learning_rate: 0.1000 - val_custom_mse: 0.2170 - val_custom_mae: 0.3239\n",
            "Epoch 31/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0890 - mae: 0.1662 - mse: 0.0890 - val_loss: 0.0468 - val_mae: 0.1362 - val_mse: 0.0468 - learning_rate: 0.1000 - val_custom_mse: 0.2165 - val_custom_mae: 0.3233\n",
            "Epoch 32/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0884 - mae: 0.1653 - mse: 0.0884 - val_loss: 0.0464 - val_mae: 0.1352 - val_mse: 0.0464 - learning_rate: 0.1000 - val_custom_mse: 0.2165 - val_custom_mae: 0.3234\n",
            "Epoch 33/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0881 - mae: 0.1646 - mse: 0.0881 - val_loss: 0.0460 - val_mae: 0.1342 - val_mse: 0.0460 - learning_rate: 0.1000 - val_custom_mse: 0.2167 - val_custom_mae: 0.3236\n",
            "Epoch 34/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0875 - mae: 0.1638 - mse: 0.0875 - val_loss: 0.0456 - val_mae: 0.1334 - val_mse: 0.0456 - learning_rate: 0.1000 - val_custom_mse: 0.2171 - val_custom_mae: 0.3240\n",
            "Epoch 35/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0871 - mae: 0.1631 - mse: 0.0871 - val_loss: 0.0453 - val_mae: 0.1327 - val_mse: 0.0453 - learning_rate: 0.1000 - val_custom_mse: 0.2176 - val_custom_mae: 0.3245\n",
            "Epoch 36/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0866 - mae: 0.1625 - mse: 0.0866 - val_loss: 0.0447 - val_mae: 0.1317 - val_mse: 0.0447 - learning_rate: 0.1000 - val_custom_mse: 0.2157 - val_custom_mae: 0.3226\n",
            "Epoch 37/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0864 - mae: 0.1619 - mse: 0.0864 - val_loss: 0.0444 - val_mae: 0.1310 - val_mse: 0.0444 - learning_rate: 0.1000 - val_custom_mse: 0.2161 - val_custom_mae: 0.3231\n",
            "Epoch 38/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0861 - mae: 0.1613 - mse: 0.0861 - val_loss: 0.0441 - val_mae: 0.1303 - val_mse: 0.0441 - learning_rate: 0.1000 - val_custom_mse: 0.2160 - val_custom_mae: 0.3229\n",
            "Epoch 39/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0857 - mae: 0.1608 - mse: 0.0857 - val_loss: 0.0439 - val_mae: 0.1297 - val_mse: 0.0439 - learning_rate: 0.1000 - val_custom_mse: 0.2163 - val_custom_mae: 0.3232\n",
            "Epoch 40/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0854 - mae: 0.1603 - mse: 0.0854 - val_loss: 0.0434 - val_mae: 0.1290 - val_mse: 0.0434 - learning_rate: 0.1000 - val_custom_mse: 0.2145 - val_custom_mae: 0.3211\n",
            "Epoch 41/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0851 - mae: 0.1598 - mse: 0.0851 - val_loss: 0.0431 - val_mae: 0.1285 - val_mse: 0.0431 - learning_rate: 0.1000 - val_custom_mse: 0.2145 - val_custom_mae: 0.3214\n",
            "Epoch 42/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0850 - mae: 0.1594 - mse: 0.0850 - val_loss: 0.0428 - val_mae: 0.1279 - val_mse: 0.0428 - learning_rate: 0.1000 - val_custom_mse: 0.2142 - val_custom_mae: 0.3210\n",
            "Epoch 43/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0847 - mae: 0.1590 - mse: 0.0847 - val_loss: 0.0426 - val_mae: 0.1273 - val_mse: 0.0426 - learning_rate: 0.1000 - val_custom_mse: 0.2144 - val_custom_mae: 0.3211\n",
            "Epoch 44/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0844 - mae: 0.1586 - mse: 0.0844 - val_loss: 0.0424 - val_mae: 0.1269 - val_mse: 0.0424 - learning_rate: 0.1000 - val_custom_mse: 0.2141 - val_custom_mae: 0.3209\n",
            "Epoch 45/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0842 - mae: 0.1582 - mse: 0.0842 - val_loss: 0.0422 - val_mae: 0.1264 - val_mse: 0.0422 - learning_rate: 0.1000 - val_custom_mse: 0.2141 - val_custom_mae: 0.3207\n",
            "Epoch 46/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0839 - mae: 0.1577 - mse: 0.0839 - val_loss: 0.0421 - val_mae: 0.1265 - val_mse: 0.0421 - learning_rate: 0.1000 - val_custom_mse: 0.2140 - val_custom_mae: 0.3206\n",
            "Epoch 47/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0836 - mae: 0.1572 - mse: 0.0836 - val_loss: 0.0419 - val_mae: 0.1260 - val_mse: 0.0419 - learning_rate: 0.1000 - val_custom_mse: 0.2144 - val_custom_mae: 0.3212\n",
            "Epoch 48/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0834 - mae: 0.1570 - mse: 0.0834 - val_loss: 0.0420 - val_mae: 0.1259 - val_mse: 0.0420 - learning_rate: 0.1000 - val_custom_mse: 0.2160 - val_custom_mae: 0.3228\n",
            "Epoch 49/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0832 - mae: 0.1567 - mse: 0.0832 - val_loss: 0.0416 - val_mae: 0.1252 - val_mse: 0.0416 - learning_rate: 0.1000 - val_custom_mse: 0.2151 - val_custom_mae: 0.3219\n",
            "Epoch 50/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0830 - mae: 0.1564 - mse: 0.0830 - val_loss: 0.0413 - val_mae: 0.1247 - val_mse: 0.0413 - learning_rate: 0.1000 - val_custom_mse: 0.2139 - val_custom_mae: 0.3206\n",
            "Epoch 51/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0829 - mae: 0.1562 - mse: 0.0829 - val_loss: 0.0411 - val_mae: 0.1244 - val_mse: 0.0411 - learning_rate: 0.1000 - val_custom_mse: 0.2136 - val_custom_mae: 0.3203\n",
            "Epoch 52/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0826 - mae: 0.1559 - mse: 0.0826 - val_loss: 0.0410 - val_mae: 0.1241 - val_mse: 0.0410 - learning_rate: 0.1000 - val_custom_mse: 0.2138 - val_custom_mae: 0.3205\n",
            "Epoch 53/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0825 - mae: 0.1556 - mse: 0.0825 - val_loss: 0.0407 - val_mae: 0.1238 - val_mse: 0.0407 - learning_rate: 0.1000 - val_custom_mse: 0.2133 - val_custom_mae: 0.3200\n",
            "Epoch 54/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0825 - mae: 0.1555 - mse: 0.0825 - val_loss: 0.0406 - val_mae: 0.1234 - val_mse: 0.0406 - learning_rate: 0.1000 - val_custom_mse: 0.2135 - val_custom_mae: 0.3203\n",
            "Epoch 55/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0822 - mae: 0.1552 - mse: 0.0822 - val_loss: 0.0406 - val_mae: 0.1232 - val_mse: 0.0406 - learning_rate: 0.1000 - val_custom_mse: 0.2140 - val_custom_mae: 0.3208\n",
            "Epoch 56/100\n",
            "236/236 - 3s - 11ms/step - loss: 0.0822 - mae: 0.1550 - mse: 0.0822 - val_loss: 0.0404 - val_mae: 0.1230 - val_mse: 0.0404 - learning_rate: 0.1000 - val_custom_mse: 0.2135 - val_custom_mae: 0.3202\n",
            "Epoch 57/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1547 - mse: 0.0819 - val_loss: 0.0402 - val_mae: 0.1226 - val_mse: 0.0402 - learning_rate: 0.1000 - val_custom_mse: 0.2127 - val_custom_mae: 0.3194\n",
            "Epoch 58/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0819 - mae: 0.1546 - mse: 0.0819 - val_loss: 0.0403 - val_mae: 0.1225 - val_mse: 0.0403 - learning_rate: 0.1000 - val_custom_mse: 0.2140 - val_custom_mae: 0.3207\n",
            "Epoch 59/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0818 - mae: 0.1543 - mse: 0.0818 - val_loss: 0.0400 - val_mae: 0.1221 - val_mse: 0.0400 - learning_rate: 0.1000 - val_custom_mse: 0.2134 - val_custom_mae: 0.3201\n",
            "Epoch 60/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0816 - mae: 0.1542 - mse: 0.0816 - val_loss: 0.0400 - val_mae: 0.1220 - val_mse: 0.0400 - learning_rate: 0.1000 - val_custom_mse: 0.2133 - val_custom_mae: 0.3200\n",
            "Epoch 61/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0816 - mae: 0.1540 - mse: 0.0816 - val_loss: 0.0402 - val_mae: 0.1225 - val_mse: 0.0402 - learning_rate: 0.1000 - val_custom_mse: 0.2144 - val_custom_mae: 0.3213\n",
            "Epoch 62/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0814 - mae: 0.1538 - mse: 0.0814 - val_loss: 0.0398 - val_mae: 0.1217 - val_mse: 0.0398 - learning_rate: 0.1000 - val_custom_mse: 0.2135 - val_custom_mae: 0.3203\n",
            "Epoch 63/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0813 - mae: 0.1537 - mse: 0.0813 - val_loss: 0.0397 - val_mae: 0.1215 - val_mse: 0.0397 - learning_rate: 0.1000 - val_custom_mse: 0.2130 - val_custom_mae: 0.3195\n",
            "Epoch 64/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0812 - mae: 0.1535 - mse: 0.0812 - val_loss: 0.0396 - val_mae: 0.1213 - val_mse: 0.0396 - learning_rate: 0.1000 - val_custom_mse: 0.2130 - val_custom_mae: 0.3197\n",
            "Epoch 65/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0812 - mae: 0.1534 - mse: 0.0812 - val_loss: 0.0396 - val_mae: 0.1212 - val_mse: 0.0396 - learning_rate: 0.1000 - val_custom_mse: 0.2131 - val_custom_mae: 0.3197\n",
            "Epoch 66/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0810 - mae: 0.1532 - mse: 0.0810 - val_loss: 0.0396 - val_mae: 0.1211 - val_mse: 0.0396 - learning_rate: 0.1000 - val_custom_mse: 0.2137 - val_custom_mae: 0.3203\n",
            "Epoch 67/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0810 - mae: 0.1531 - mse: 0.0810 - val_loss: 0.0393 - val_mae: 0.1208 - val_mse: 0.0393 - learning_rate: 0.1000 - val_custom_mse: 0.2119 - val_custom_mae: 0.3187\n",
            "Epoch 68/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0810 - mae: 0.1530 - mse: 0.0810 - val_loss: 0.0394 - val_mae: 0.1206 - val_mse: 0.0394 - learning_rate: 0.1000 - val_custom_mse: 0.2130 - val_custom_mae: 0.3194\n",
            "Epoch 69/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0808 - mae: 0.1529 - mse: 0.0808 - val_loss: 0.0393 - val_mae: 0.1207 - val_mse: 0.0393 - learning_rate: 0.1000 - val_custom_mse: 0.2126 - val_custom_mae: 0.3193\n",
            "Epoch 70/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0807 - mae: 0.1527 - mse: 0.0807 - val_loss: 0.0391 - val_mae: 0.1203 - val_mse: 0.0391 - learning_rate: 0.1000 - val_custom_mse: 0.2117 - val_custom_mae: 0.3182\n",
            "Epoch 71/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0807 - mae: 0.1526 - mse: 0.0807 - val_loss: 0.0391 - val_mae: 0.1203 - val_mse: 0.0391 - learning_rate: 0.1000 - val_custom_mse: 0.2121 - val_custom_mae: 0.3186\n",
            "Epoch 72/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0806 - mae: 0.1525 - mse: 0.0806 - val_loss: 0.0391 - val_mae: 0.1201 - val_mse: 0.0391 - learning_rate: 0.1000 - val_custom_mse: 0.2123 - val_custom_mae: 0.3189\n",
            "Epoch 73/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0804 - mae: 0.1523 - mse: 0.0804 - val_loss: 0.0391 - val_mae: 0.1202 - val_mse: 0.0391 - learning_rate: 0.1000 - val_custom_mse: 0.2129 - val_custom_mae: 0.3196\n",
            "Epoch 74/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0805 - mae: 0.1523 - mse: 0.0805 - val_loss: 0.0391 - val_mae: 0.1201 - val_mse: 0.0391 - learning_rate: 0.1000 - val_custom_mse: 0.2128 - val_custom_mae: 0.3194\n",
            "Epoch 75/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0804 - mae: 0.1522 - mse: 0.0804 - val_loss: 0.0390 - val_mae: 0.1200 - val_mse: 0.0390 - learning_rate: 0.1000 - val_custom_mse: 0.2121 - val_custom_mae: 0.3188\n",
            "Epoch 76/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0803 - mae: 0.1521 - mse: 0.0803 - val_loss: 0.0391 - val_mae: 0.1201 - val_mse: 0.0391 - learning_rate: 0.1000 - val_custom_mse: 0.2129 - val_custom_mae: 0.3196\n",
            "Epoch 77/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0802 - mae: 0.1520 - mse: 0.0802 - val_loss: 0.0390 - val_mae: 0.1198 - val_mse: 0.0390 - learning_rate: 0.1000 - val_custom_mse: 0.2129 - val_custom_mae: 0.3196\n",
            "Epoch 78/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0802 - mae: 0.1520 - mse: 0.0802 - val_loss: 0.0388 - val_mae: 0.1197 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.2120 - val_custom_mae: 0.3187\n",
            "Epoch 79/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0802 - mae: 0.1519 - mse: 0.0802 - val_loss: 0.0391 - val_mae: 0.1200 - val_mse: 0.0391 - learning_rate: 0.1000 - val_custom_mse: 0.2134 - val_custom_mae: 0.3201\n",
            "Epoch 80/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0802 - mae: 0.1518 - mse: 0.0802 - val_loss: 0.0388 - val_mae: 0.1194 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.2123 - val_custom_mae: 0.3189\n",
            "Epoch 81/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0800 - mae: 0.1517 - mse: 0.0800 - val_loss: 0.0388 - val_mae: 0.1195 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.2120 - val_custom_mae: 0.3185\n",
            "Epoch 82/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0800 - mae: 0.1517 - mse: 0.0800 - val_loss: 0.0388 - val_mae: 0.1194 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.2121 - val_custom_mae: 0.3187\n",
            "Epoch 83/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0799 - mae: 0.1516 - mse: 0.0799 - val_loss: 0.0388 - val_mae: 0.1194 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.2124 - val_custom_mae: 0.3190\n",
            "Epoch 84/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0798 - mae: 0.1515 - mse: 0.0798 - val_loss: 0.0388 - val_mae: 0.1193 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.2124 - val_custom_mae: 0.3190\n",
            "Epoch 85/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0798 - mae: 0.1514 - mse: 0.0798 - val_loss: 0.0386 - val_mae: 0.1191 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.2120 - val_custom_mae: 0.3186\n",
            "Epoch 86/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0797 - mae: 0.1513 - mse: 0.0797 - val_loss: 0.0385 - val_mae: 0.1191 - val_mse: 0.0385 - learning_rate: 0.1000 - val_custom_mse: 0.2113 - val_custom_mae: 0.3179\n",
            "Epoch 87/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0797 - mae: 0.1513 - mse: 0.0797 - val_loss: 0.0387 - val_mae: 0.1192 - val_mse: 0.0387 - learning_rate: 0.1000 - val_custom_mse: 0.2123 - val_custom_mae: 0.3190\n",
            "Epoch 88/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0797 - mae: 0.1512 - mse: 0.0797 - val_loss: 0.0387 - val_mae: 0.1191 - val_mse: 0.0387 - learning_rate: 0.1000 - val_custom_mse: 0.2124 - val_custom_mae: 0.3189\n",
            "Epoch 89/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0796 - mae: 0.1511 - mse: 0.0796 - val_loss: 0.0386 - val_mae: 0.1190 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.2125 - val_custom_mae: 0.3191\n",
            "Epoch 90/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0796 - mae: 0.1511 - mse: 0.0796 - val_loss: 0.0386 - val_mae: 0.1189 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.2120 - val_custom_mae: 0.3185\n",
            "Epoch 91/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0796 - mae: 0.1510 - mse: 0.0796 - val_loss: 0.0385 - val_mae: 0.1189 - val_mse: 0.0385 - learning_rate: 0.1000 - val_custom_mse: 0.2116 - val_custom_mae: 0.3183\n",
            "Epoch 92/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0796 - mae: 0.1510 - mse: 0.0796 - val_loss: 0.0385 - val_mae: 0.1188 - val_mse: 0.0385 - learning_rate: 0.1000 - val_custom_mse: 0.2121 - val_custom_mae: 0.3186\n",
            "Epoch 93/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0795 - mae: 0.1509 - mse: 0.0795 - val_loss: 0.0386 - val_mae: 0.1190 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.2124 - val_custom_mae: 0.3190\n",
            "Epoch 94/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0795 - mae: 0.1509 - mse: 0.0795 - val_loss: 0.0386 - val_mae: 0.1190 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.2124 - val_custom_mae: 0.3191\n",
            "Epoch 95/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0794 - mae: 0.1508 - mse: 0.0794 - val_loss: 0.0384 - val_mae: 0.1187 - val_mse: 0.0384 - learning_rate: 0.1000 - val_custom_mse: 0.2111 - val_custom_mae: 0.3175\n",
            "Epoch 96/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0793 - mae: 0.1507 - mse: 0.0793 - val_loss: 0.0385 - val_mae: 0.1189 - val_mse: 0.0385 - learning_rate: 0.1000 - val_custom_mse: 0.2121 - val_custom_mae: 0.3188\n",
            "Epoch 97/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0794 - mae: 0.1508 - mse: 0.0794 - val_loss: 0.0387 - val_mae: 0.1191 - val_mse: 0.0387 - learning_rate: 0.1000 - val_custom_mse: 0.2127 - val_custom_mae: 0.3195\n",
            "Epoch 98/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0794 - mae: 0.1507 - mse: 0.0794 - val_loss: 0.0385 - val_mae: 0.1187 - val_mse: 0.0385 - learning_rate: 0.1000 - val_custom_mse: 0.2122 - val_custom_mae: 0.3188\n",
            "Epoch 99/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0793 - mae: 0.1506 - mse: 0.0793 - val_loss: 0.0386 - val_mae: 0.1189 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.2126 - val_custom_mae: 0.3194\n",
            "Epoch 100/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0792 - mae: 0.1506 - mse: 0.0792 - val_loss: 0.0384 - val_mae: 0.1186 - val_mse: 0.0384 - learning_rate: 0.1000 - val_custom_mse: 0.2118 - val_custom_mae: 0.3185\n",
            "Running experiment: horizon=96, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_36', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "236/236 - 6s - 25ms/step - loss: 0.4916 - mae: 0.4253 - mse: 0.4916 - val_loss: 0.2724 - val_mae: 0.3778 - val_mse: 0.2724 - learning_rate: 0.1000 - val_custom_mse: 0.3872 - val_custom_mae: 0.4549\n",
            "Epoch 2/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.4268 - mae: 0.3972 - mse: 0.4268 - val_loss: 0.2355 - val_mae: 0.3512 - val_mse: 0.2355 - learning_rate: 0.1000 - val_custom_mse: 0.3551 - val_custom_mae: 0.4335\n",
            "Epoch 3/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3688 - mae: 0.3700 - mse: 0.3688 - val_loss: 0.2015 - val_mae: 0.3250 - val_mse: 0.2015 - learning_rate: 0.1000 - val_custom_mse: 0.3357 - val_custom_mae: 0.4196\n",
            "Epoch 4/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.3080 - mae: 0.3393 - mse: 0.3080 - val_loss: 0.1627 - val_mae: 0.2908 - val_mse: 0.1627 - learning_rate: 0.1000 - val_custom_mse: 0.2980 - val_custom_mae: 0.3919\n",
            "Epoch 5/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2534 - mae: 0.3090 - mse: 0.2534 - val_loss: 0.1346 - val_mae: 0.2633 - val_mse: 0.1346 - learning_rate: 0.1000 - val_custom_mse: 0.2773 - val_custom_mae: 0.3757\n",
            "Epoch 6/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.2098 - mae: 0.2824 - mse: 0.2098 - val_loss: 0.1147 - val_mae: 0.2418 - val_mse: 0.1147 - learning_rate: 0.1000 - val_custom_mse: 0.2658 - val_custom_mae: 0.3665\n",
            "Epoch 7/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1792 - mae: 0.2617 - mse: 0.1792 - val_loss: 0.1006 - val_mae: 0.2248 - val_mse: 0.1006 - learning_rate: 0.1000 - val_custom_mse: 0.2536 - val_custom_mae: 0.3566\n",
            "Epoch 8/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1592 - mae: 0.2469 - mse: 0.1592 - val_loss: 0.0915 - val_mae: 0.2132 - val_mse: 0.0915 - learning_rate: 0.1000 - val_custom_mse: 0.2449 - val_custom_mae: 0.3493\n",
            "Epoch 9/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1467 - mae: 0.2366 - mse: 0.1467 - val_loss: 0.0856 - val_mae: 0.2048 - val_mse: 0.0856 - learning_rate: 0.1000 - val_custom_mse: 0.2424 - val_custom_mae: 0.3473\n",
            "Epoch 10/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1385 - mae: 0.2291 - mse: 0.1385 - val_loss: 0.0807 - val_mae: 0.1980 - val_mse: 0.0807 - learning_rate: 0.1000 - val_custom_mse: 0.2359 - val_custom_mae: 0.3415\n",
            "Epoch 11/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1326 - mae: 0.2232 - mse: 0.1326 - val_loss: 0.0770 - val_mae: 0.1920 - val_mse: 0.0770 - learning_rate: 0.1000 - val_custom_mse: 0.2354 - val_custom_mae: 0.3413\n",
            "Epoch 12/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1280 - mae: 0.2182 - mse: 0.1280 - val_loss: 0.0737 - val_mae: 0.1869 - val_mse: 0.0737 - learning_rate: 0.1000 - val_custom_mse: 0.2321 - val_custom_mae: 0.3382\n",
            "Epoch 13/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1242 - mae: 0.2138 - mse: 0.1242 - val_loss: 0.0709 - val_mae: 0.1822 - val_mse: 0.0709 - learning_rate: 0.1000 - val_custom_mse: 0.2296 - val_custom_mae: 0.3359\n",
            "Epoch 14/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1209 - mae: 0.2098 - mse: 0.1209 - val_loss: 0.0685 - val_mae: 0.1780 - val_mse: 0.0685 - learning_rate: 0.1000 - val_custom_mse: 0.2291 - val_custom_mae: 0.3355\n",
            "Epoch 15/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1183 - mae: 0.2065 - mse: 0.1183 - val_loss: 0.0663 - val_mae: 0.1742 - val_mse: 0.0663 - learning_rate: 0.1000 - val_custom_mse: 0.2274 - val_custom_mae: 0.3339\n",
            "Epoch 16/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1159 - mae: 0.2035 - mse: 0.1159 - val_loss: 0.0644 - val_mae: 0.1707 - val_mse: 0.0644 - learning_rate: 0.1000 - val_custom_mse: 0.2262 - val_custom_mae: 0.3328\n",
            "Epoch 17/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1138 - mae: 0.2007 - mse: 0.1138 - val_loss: 0.0628 - val_mae: 0.1676 - val_mse: 0.0628 - learning_rate: 0.1000 - val_custom_mse: 0.2259 - val_custom_mae: 0.3325\n",
            "Epoch 18/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1118 - mae: 0.1982 - mse: 0.1118 - val_loss: 0.0615 - val_mae: 0.1653 - val_mse: 0.0615 - learning_rate: 0.1000 - val_custom_mse: 0.2267 - val_custom_mae: 0.3335\n",
            "Epoch 19/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1103 - mae: 0.1961 - mse: 0.1103 - val_loss: 0.0600 - val_mae: 0.1626 - val_mse: 0.0600 - learning_rate: 0.1000 - val_custom_mse: 0.2247 - val_custom_mae: 0.3316\n",
            "Epoch 20/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1090 - mae: 0.1942 - mse: 0.1090 - val_loss: 0.0586 - val_mae: 0.1600 - val_mse: 0.0586 - learning_rate: 0.1000 - val_custom_mse: 0.2230 - val_custom_mae: 0.3298\n",
            "Epoch 21/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1076 - mae: 0.1924 - mse: 0.1076 - val_loss: 0.0578 - val_mae: 0.1581 - val_mse: 0.0578 - learning_rate: 0.1000 - val_custom_mse: 0.2243 - val_custom_mae: 0.3311\n",
            "Epoch 22/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1065 - mae: 0.1908 - mse: 0.1065 - val_loss: 0.0568 - val_mae: 0.1563 - val_mse: 0.0568 - learning_rate: 0.1000 - val_custom_mse: 0.2234 - val_custom_mae: 0.3302\n",
            "Epoch 23/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1055 - mae: 0.1894 - mse: 0.1055 - val_loss: 0.0556 - val_mae: 0.1542 - val_mse: 0.0556 - learning_rate: 0.1000 - val_custom_mse: 0.2208 - val_custom_mae: 0.3275\n",
            "Epoch 24/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1045 - mae: 0.1880 - mse: 0.1045 - val_loss: 0.0549 - val_mae: 0.1527 - val_mse: 0.0549 - learning_rate: 0.1000 - val_custom_mse: 0.2212 - val_custom_mae: 0.3279\n",
            "Epoch 25/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1037 - mae: 0.1869 - mse: 0.1037 - val_loss: 0.0540 - val_mae: 0.1511 - val_mse: 0.0540 - learning_rate: 0.1000 - val_custom_mse: 0.2197 - val_custom_mae: 0.3264\n",
            "Epoch 26/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1028 - mae: 0.1857 - mse: 0.1028 - val_loss: 0.0536 - val_mae: 0.1501 - val_mse: 0.0536 - learning_rate: 0.1000 - val_custom_mse: 0.2211 - val_custom_mae: 0.3281\n",
            "Epoch 27/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1021 - mae: 0.1847 - mse: 0.1021 - val_loss: 0.0528 - val_mae: 0.1486 - val_mse: 0.0528 - learning_rate: 0.1000 - val_custom_mse: 0.2197 - val_custom_mae: 0.3264\n",
            "Epoch 28/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1015 - mae: 0.1838 - mse: 0.1015 - val_loss: 0.0523 - val_mae: 0.1476 - val_mse: 0.0523 - learning_rate: 0.1000 - val_custom_mse: 0.2201 - val_custom_mae: 0.3268\n",
            "Epoch 29/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1010 - mae: 0.1830 - mse: 0.1010 - val_loss: 0.0519 - val_mae: 0.1467 - val_mse: 0.0519 - learning_rate: 0.1000 - val_custom_mse: 0.2201 - val_custom_mae: 0.3269\n",
            "Epoch 30/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1004 - mae: 0.1822 - mse: 0.1004 - val_loss: 0.0515 - val_mae: 0.1458 - val_mse: 0.0515 - learning_rate: 0.1000 - val_custom_mse: 0.2203 - val_custom_mae: 0.3270\n",
            "Epoch 31/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.1000 - mae: 0.1815 - mse: 0.1000 - val_loss: 0.0510 - val_mae: 0.1449 - val_mse: 0.0510 - learning_rate: 0.1000 - val_custom_mse: 0.2198 - val_custom_mae: 0.3265\n",
            "Epoch 32/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0995 - mae: 0.1808 - mse: 0.0995 - val_loss: 0.0505 - val_mae: 0.1441 - val_mse: 0.0505 - learning_rate: 0.1000 - val_custom_mse: 0.2188 - val_custom_mae: 0.3256\n",
            "Epoch 33/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0990 - mae: 0.1802 - mse: 0.0990 - val_loss: 0.0503 - val_mae: 0.1436 - val_mse: 0.0503 - learning_rate: 0.1000 - val_custom_mse: 0.2195 - val_custom_mae: 0.3262\n",
            "Epoch 34/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0986 - mae: 0.1796 - mse: 0.0986 - val_loss: 0.0499 - val_mae: 0.1429 - val_mse: 0.0499 - learning_rate: 0.1000 - val_custom_mse: 0.2193 - val_custom_mae: 0.3261\n",
            "Epoch 35/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0982 - mae: 0.1791 - mse: 0.0982 - val_loss: 0.0494 - val_mae: 0.1420 - val_mse: 0.0494 - learning_rate: 0.1000 - val_custom_mse: 0.2181 - val_custom_mae: 0.3249\n",
            "Epoch 36/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0978 - mae: 0.1786 - mse: 0.0978 - val_loss: 0.0490 - val_mae: 0.1412 - val_mse: 0.0490 - learning_rate: 0.1000 - val_custom_mse: 0.2172 - val_custom_mae: 0.3237\n",
            "Epoch 37/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0976 - mae: 0.1781 - mse: 0.0976 - val_loss: 0.0489 - val_mae: 0.1408 - val_mse: 0.0489 - learning_rate: 0.1000 - val_custom_mse: 0.2182 - val_custom_mae: 0.3247\n",
            "Epoch 38/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0972 - mae: 0.1777 - mse: 0.0972 - val_loss: 0.0485 - val_mae: 0.1402 - val_mse: 0.0485 - learning_rate: 0.1000 - val_custom_mse: 0.2172 - val_custom_mae: 0.3238\n",
            "Epoch 39/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0968 - mae: 0.1772 - mse: 0.0968 - val_loss: 0.0488 - val_mae: 0.1406 - val_mse: 0.0488 - learning_rate: 0.1000 - val_custom_mse: 0.2187 - val_custom_mae: 0.3252\n",
            "Epoch 40/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0966 - mae: 0.1769 - mse: 0.0966 - val_loss: 0.0504 - val_mae: 0.1440 - val_mse: 0.0504 - learning_rate: 0.1000 - val_custom_mse: 0.2253 - val_custom_mae: 0.3320\n",
            "Epoch 41/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0967 - mae: 0.1771 - mse: 0.0967 - val_loss: 0.0482 - val_mae: 0.1394 - val_mse: 0.0482 - learning_rate: 0.1000 - val_custom_mse: 0.2193 - val_custom_mae: 0.3259\n",
            "Epoch 42/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0961 - mae: 0.1762 - mse: 0.0961 - val_loss: 0.0478 - val_mae: 0.1388 - val_mse: 0.0478 - learning_rate: 0.1000 - val_custom_mse: 0.2174 - val_custom_mae: 0.3238\n",
            "Epoch 43/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0958 - mae: 0.1759 - mse: 0.0958 - val_loss: 0.0478 - val_mae: 0.1387 - val_mse: 0.0478 - learning_rate: 0.1000 - val_custom_mse: 0.2180 - val_custom_mae: 0.3248\n",
            "Epoch 44/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0956 - mae: 0.1756 - mse: 0.0956 - val_loss: 0.0475 - val_mae: 0.1382 - val_mse: 0.0475 - learning_rate: 0.1000 - val_custom_mse: 0.2174 - val_custom_mae: 0.3239\n",
            "Epoch 45/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0954 - mae: 0.1753 - mse: 0.0954 - val_loss: 0.0475 - val_mae: 0.1380 - val_mse: 0.0475 - learning_rate: 0.1000 - val_custom_mse: 0.2181 - val_custom_mae: 0.3246\n",
            "Epoch 46/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0951 - mae: 0.1750 - mse: 0.0951 - val_loss: 0.0470 - val_mae: 0.1372 - val_mse: 0.0470 - learning_rate: 0.1000 - val_custom_mse: 0.2158 - val_custom_mae: 0.3223\n",
            "Epoch 47/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0949 - mae: 0.1747 - mse: 0.0949 - val_loss: 0.0470 - val_mae: 0.1370 - val_mse: 0.0470 - learning_rate: 0.1000 - val_custom_mse: 0.2166 - val_custom_mae: 0.3230\n",
            "Epoch 48/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0948 - mae: 0.1746 - mse: 0.0948 - val_loss: 0.0471 - val_mae: 0.1371 - val_mse: 0.0471 - learning_rate: 0.1000 - val_custom_mse: 0.2180 - val_custom_mae: 0.3244\n",
            "Epoch 49/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0946 - mae: 0.1743 - mse: 0.0946 - val_loss: 0.0469 - val_mae: 0.1369 - val_mse: 0.0469 - learning_rate: 0.1000 - val_custom_mse: 0.2176 - val_custom_mae: 0.3240\n",
            "Epoch 50/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0944 - mae: 0.1741 - mse: 0.0944 - val_loss: 0.0465 - val_mae: 0.1362 - val_mse: 0.0465 - learning_rate: 0.1000 - val_custom_mse: 0.2159 - val_custom_mae: 0.3223\n",
            "Epoch 51/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0943 - mae: 0.1739 - mse: 0.0943 - val_loss: 0.0466 - val_mae: 0.1362 - val_mse: 0.0466 - learning_rate: 0.1000 - val_custom_mse: 0.2168 - val_custom_mae: 0.3232\n",
            "Epoch 52/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0942 - mae: 0.1738 - mse: 0.0942 - val_loss: 0.0463 - val_mae: 0.1359 - val_mse: 0.0463 - learning_rate: 0.1000 - val_custom_mse: 0.2156 - val_custom_mae: 0.3220\n",
            "Epoch 53/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0941 - mae: 0.1736 - mse: 0.0941 - val_loss: 0.0463 - val_mae: 0.1360 - val_mse: 0.0463 - learning_rate: 0.1000 - val_custom_mse: 0.2159 - val_custom_mae: 0.3225\n",
            "Epoch 54/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0939 - mae: 0.1734 - mse: 0.0939 - val_loss: 0.0481 - val_mae: 0.1395 - val_mse: 0.0481 - learning_rate: 0.1000 - val_custom_mse: 0.2227 - val_custom_mae: 0.3291\n",
            "Epoch 55/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0940 - mae: 0.1736 - mse: 0.0940 - val_loss: 0.0465 - val_mae: 0.1361 - val_mse: 0.0465 - learning_rate: 0.1000 - val_custom_mse: 0.2174 - val_custom_mae: 0.3239\n",
            "Epoch 56/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0937 - mae: 0.1731 - mse: 0.0937 - val_loss: 0.0461 - val_mae: 0.1354 - val_mse: 0.0461 - learning_rate: 0.1000 - val_custom_mse: 0.2159 - val_custom_mae: 0.3223\n",
            "Epoch 57/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0935 - mae: 0.1729 - mse: 0.0935 - val_loss: 0.0462 - val_mae: 0.1356 - val_mse: 0.0462 - learning_rate: 0.1000 - val_custom_mse: 0.2168 - val_custom_mae: 0.3232\n",
            "Epoch 58/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0934 - mae: 0.1728 - mse: 0.0934 - val_loss: 0.0459 - val_mae: 0.1350 - val_mse: 0.0459 - learning_rate: 0.1000 - val_custom_mse: 0.2150 - val_custom_mae: 0.3212\n",
            "Epoch 59/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0933 - mae: 0.1727 - mse: 0.0933 - val_loss: 0.0459 - val_mae: 0.1350 - val_mse: 0.0459 - learning_rate: 0.1000 - val_custom_mse: 0.2152 - val_custom_mae: 0.3217\n",
            "Epoch 60/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0931 - mae: 0.1724 - mse: 0.0931 - val_loss: 0.0461 - val_mae: 0.1352 - val_mse: 0.0461 - learning_rate: 0.1000 - val_custom_mse: 0.2167 - val_custom_mae: 0.3231\n",
            "Epoch 61/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0932 - mae: 0.1724 - mse: 0.0932 - val_loss: 0.0459 - val_mae: 0.1349 - val_mse: 0.0459 - learning_rate: 0.1000 - val_custom_mse: 0.2163 - val_custom_mae: 0.3226\n",
            "Epoch 62/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0931 - mae: 0.1723 - mse: 0.0931 - val_loss: 0.0457 - val_mae: 0.1348 - val_mse: 0.0457 - learning_rate: 0.1000 - val_custom_mse: 0.2148 - val_custom_mae: 0.3213\n",
            "Epoch 63/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0929 - mae: 0.1722 - mse: 0.0929 - val_loss: 0.0457 - val_mae: 0.1347 - val_mse: 0.0457 - learning_rate: 0.1000 - val_custom_mse: 0.2157 - val_custom_mae: 0.3222\n",
            "Epoch 64/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0929 - mae: 0.1721 - mse: 0.0929 - val_loss: 0.0457 - val_mae: 0.1346 - val_mse: 0.0457 - learning_rate: 0.1000 - val_custom_mse: 0.2158 - val_custom_mae: 0.3223\n",
            "Epoch 65/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0925 - mae: 0.1718 - mse: 0.0925 - val_loss: 0.0458 - val_mae: 0.1346 - val_mse: 0.0458 - learning_rate: 0.1000 - val_custom_mse: 0.2161 - val_custom_mae: 0.3223\n",
            "Epoch 66/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0926 - mae: 0.1718 - mse: 0.0926 - val_loss: 0.0458 - val_mae: 0.1347 - val_mse: 0.0458 - learning_rate: 0.1000 - val_custom_mse: 0.2160 - val_custom_mae: 0.3224\n",
            "Epoch 67/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0924 - mae: 0.1717 - mse: 0.0924 - val_loss: 0.0454 - val_mae: 0.1342 - val_mse: 0.0454 - learning_rate: 0.1000 - val_custom_mse: 0.2145 - val_custom_mae: 0.3209\n",
            "Epoch 68/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0925 - mae: 0.1716 - mse: 0.0925 - val_loss: 0.0454 - val_mae: 0.1340 - val_mse: 0.0454 - learning_rate: 0.1000 - val_custom_mse: 0.2146 - val_custom_mae: 0.3209\n",
            "Epoch 69/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0923 - mae: 0.1715 - mse: 0.0923 - val_loss: 0.0457 - val_mae: 0.1345 - val_mse: 0.0457 - learning_rate: 0.1000 - val_custom_mse: 0.2161 - val_custom_mae: 0.3225\n",
            "Epoch 70/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0924 - mae: 0.1714 - mse: 0.0924 - val_loss: 0.0454 - val_mae: 0.1341 - val_mse: 0.0454 - learning_rate: 0.1000 - val_custom_mse: 0.2151 - val_custom_mae: 0.3215\n",
            "Epoch 71/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0923 - mae: 0.1713 - mse: 0.0923 - val_loss: 0.0454 - val_mae: 0.1340 - val_mse: 0.0454 - learning_rate: 0.1000 - val_custom_mse: 0.2154 - val_custom_mae: 0.3217\n",
            "Epoch 72/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0921 - mae: 0.1712 - mse: 0.0921 - val_loss: 0.0455 - val_mae: 0.1340 - val_mse: 0.0455 - learning_rate: 0.1000 - val_custom_mse: 0.2158 - val_custom_mae: 0.3221\n",
            "Epoch 73/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0920 - mae: 0.1711 - mse: 0.0920 - val_loss: 0.0453 - val_mae: 0.1337 - val_mse: 0.0453 - learning_rate: 0.1000 - val_custom_mse: 0.2150 - val_custom_mae: 0.3212\n",
            "Epoch 74/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0920 - mae: 0.1710 - mse: 0.0920 - val_loss: 0.0455 - val_mae: 0.1341 - val_mse: 0.0455 - learning_rate: 0.1000 - val_custom_mse: 0.2160 - val_custom_mae: 0.3224\n",
            "Epoch 75/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0919 - mae: 0.1710 - mse: 0.0919 - val_loss: 0.0451 - val_mae: 0.1336 - val_mse: 0.0451 - learning_rate: 0.1000 - val_custom_mse: 0.2140 - val_custom_mae: 0.3201\n",
            "Epoch 76/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0917 - mae: 0.1709 - mse: 0.0917 - val_loss: 0.0455 - val_mae: 0.1342 - val_mse: 0.0455 - learning_rate: 0.1000 - val_custom_mse: 0.2163 - val_custom_mae: 0.3226\n",
            "Epoch 77/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0917 - mae: 0.1708 - mse: 0.0917 - val_loss: 0.0454 - val_mae: 0.1338 - val_mse: 0.0454 - learning_rate: 0.1000 - val_custom_mse: 0.2161 - val_custom_mae: 0.3222\n",
            "Epoch 78/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0916 - mae: 0.1707 - mse: 0.0916 - val_loss: 0.0454 - val_mae: 0.1338 - val_mse: 0.0454 - learning_rate: 0.1000 - val_custom_mse: 0.2162 - val_custom_mae: 0.3224\n",
            "Epoch 79/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0915 - mae: 0.1706 - mse: 0.0915 - val_loss: 0.0451 - val_mae: 0.1334 - val_mse: 0.0451 - learning_rate: 0.1000 - val_custom_mse: 0.2148 - val_custom_mae: 0.3212\n",
            "Epoch 80/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0917 - mae: 0.1706 - mse: 0.0917 - val_loss: 0.0451 - val_mae: 0.1333 - val_mse: 0.0451 - learning_rate: 0.1000 - val_custom_mse: 0.2143 - val_custom_mae: 0.3204\n",
            "Epoch 81/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0915 - mae: 0.1705 - mse: 0.0915 - val_loss: 0.0449 - val_mae: 0.1331 - val_mse: 0.0449 - learning_rate: 0.1000 - val_custom_mse: 0.2133 - val_custom_mae: 0.3195\n",
            "Epoch 82/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0916 - mae: 0.1705 - mse: 0.0916 - val_loss: 0.0451 - val_mae: 0.1334 - val_mse: 0.0451 - learning_rate: 0.1000 - val_custom_mse: 0.2149 - val_custom_mae: 0.3211\n",
            "Epoch 83/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0915 - mae: 0.1704 - mse: 0.0915 - val_loss: 0.0450 - val_mae: 0.1332 - val_mse: 0.0450 - learning_rate: 0.1000 - val_custom_mse: 0.2142 - val_custom_mae: 0.3206\n",
            "Epoch 84/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0913 - mae: 0.1703 - mse: 0.0913 - val_loss: 0.0449 - val_mae: 0.1331 - val_mse: 0.0449 - learning_rate: 0.1000 - val_custom_mse: 0.2136 - val_custom_mae: 0.3198\n",
            "Epoch 85/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0912 - mae: 0.1703 - mse: 0.0912 - val_loss: 0.0449 - val_mae: 0.1330 - val_mse: 0.0449 - learning_rate: 0.1000 - val_custom_mse: 0.2140 - val_custom_mae: 0.3202\n",
            "Epoch 86/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0912 - mae: 0.1701 - mse: 0.0912 - val_loss: 0.0454 - val_mae: 0.1339 - val_mse: 0.0454 - learning_rate: 0.1000 - val_custom_mse: 0.2162 - val_custom_mae: 0.3224\n",
            "Epoch 87/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0913 - mae: 0.1702 - mse: 0.0913 - val_loss: 0.0450 - val_mae: 0.1333 - val_mse: 0.0450 - learning_rate: 0.1000 - val_custom_mse: 0.2147 - val_custom_mae: 0.3211\n",
            "Epoch 88/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0912 - mae: 0.1701 - mse: 0.0912 - val_loss: 0.0452 - val_mae: 0.1334 - val_mse: 0.0452 - learning_rate: 0.1000 - val_custom_mse: 0.2154 - val_custom_mae: 0.3215\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "236/236 - 2s - 8ms/step - loss: 0.0910 - mae: 0.1700 - mse: 0.0910 - val_loss: 0.0451 - val_mae: 0.1334 - val_mse: 0.0451 - learning_rate: 0.1000 - val_custom_mse: 0.2148 - val_custom_mae: 0.3210\n",
            "Epoch 90/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0911 - mae: 0.1700 - mse: 0.0911 - val_loss: 0.0450 - val_mae: 0.1332 - val_mse: 0.0450 - learning_rate: 0.0200 - val_custom_mse: 0.2142 - val_custom_mae: 0.3206\n",
            "Epoch 91/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0911 - mae: 0.1700 - mse: 0.0911 - val_loss: 0.0450 - val_mae: 0.1332 - val_mse: 0.0450 - learning_rate: 0.0200 - val_custom_mse: 0.2141 - val_custom_mae: 0.3205\n",
            "Epoch 92/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0910 - mae: 0.1699 - mse: 0.0910 - val_loss: 0.0450 - val_mae: 0.1332 - val_mse: 0.0450 - learning_rate: 0.0200 - val_custom_mse: 0.2141 - val_custom_mae: 0.3206\n",
            "Epoch 93/100\n",
            "236/236 - 2s - 9ms/step - loss: 0.0909 - mae: 0.1699 - mse: 0.0909 - val_loss: 0.0450 - val_mae: 0.1333 - val_mse: 0.0450 - learning_rate: 0.0200 - val_custom_mse: 0.2143 - val_custom_mae: 0.3208\n",
            "Epoch 94/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0910 - mae: 0.1699 - mse: 0.0910 - val_loss: 0.0451 - val_mae: 0.1334 - val_mse: 0.0451 - learning_rate: 0.0200 - val_custom_mse: 0.2147 - val_custom_mae: 0.3211\n",
            "Epoch 95/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0909 - mae: 0.1699 - mse: 0.0909 - val_loss: 0.0450 - val_mae: 0.1331 - val_mse: 0.0450 - learning_rate: 0.0200 - val_custom_mse: 0.2142 - val_custom_mae: 0.3206\n",
            "Epoch 96/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0911 - mae: 0.1699 - mse: 0.0911 - val_loss: 0.0450 - val_mae: 0.1332 - val_mse: 0.0450 - learning_rate: 0.0200 - val_custom_mse: 0.2145 - val_custom_mae: 0.3209\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "236/236 - 2s - 8ms/step - loss: 0.0910 - mae: 0.1699 - mse: 0.0910 - val_loss: 0.0450 - val_mae: 0.1332 - val_mse: 0.0450 - learning_rate: 0.0200 - val_custom_mse: 0.2143 - val_custom_mae: 0.3207\n",
            "Epoch 98/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0911 - mae: 0.1700 - mse: 0.0911 - val_loss: 0.0449 - val_mae: 0.1331 - val_mse: 0.0449 - learning_rate: 0.0040 - val_custom_mse: 0.2140 - val_custom_mae: 0.3204\n",
            "Epoch 99/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0908 - mae: 0.1698 - mse: 0.0908 - val_loss: 0.0449 - val_mae: 0.1331 - val_mse: 0.0449 - learning_rate: 0.0040 - val_custom_mse: 0.2140 - val_custom_mae: 0.3205\n",
            "Epoch 100/100\n",
            "236/236 - 2s - 8ms/step - loss: 0.0908 - mae: 0.1698 - mse: 0.0908 - val_loss: 0.0449 - val_mae: 0.1331 - val_mse: 0.0449 - learning_rate: 0.0040 - val_custom_mse: 0.2140 - val_custom_mae: 0.3204\n",
            "Running experiment: horizon=192, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_37', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "233/233 - 5s - 23ms/step - loss: 0.5305 - mae: 0.4491 - mse: 0.5305 - val_loss: 1.7417 - val_mae: 0.7004 - val_mse: 1.7417 - learning_rate: 0.1000 - val_custom_mse: 1.7848 - val_custom_mae: 0.7846\n",
            "Epoch 2/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.7764 - mae: 0.5011 - mse: 0.7764 - val_loss: 0.3726 - val_mae: 0.4389 - val_mse: 0.3726 - learning_rate: 0.1000 - val_custom_mse: 0.5578 - val_custom_mae: 0.5483\n",
            "Epoch 3/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.5505 - mae: 0.4427 - mse: 0.5505 - val_loss: 0.3181 - val_mae: 0.4046 - val_mse: 0.3181 - learning_rate: 0.1000 - val_custom_mse: 0.5023 - val_custom_mae: 0.5187\n",
            "Epoch 4/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.5060 - mae: 0.4226 - mse: 0.5060 - val_loss: 0.2807 - val_mae: 0.3782 - val_mse: 0.2807 - learning_rate: 0.1000 - val_custom_mse: 0.4630 - val_custom_mae: 0.4963\n",
            "Epoch 5/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4640 - mae: 0.4011 - mse: 0.4640 - val_loss: 0.2439 - val_mae: 0.3486 - val_mse: 0.2439 - learning_rate: 0.1000 - val_custom_mse: 0.4158 - val_custom_mae: 0.4667\n",
            "Epoch 6/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4233 - mae: 0.3781 - mse: 0.4233 - val_loss: 0.2171 - val_mae: 0.3249 - val_mse: 0.2171 - learning_rate: 0.1000 - val_custom_mse: 0.3888 - val_custom_mae: 0.4483\n",
            "Epoch 7/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3876 - mae: 0.3560 - mse: 0.3876 - val_loss: 0.1979 - val_mae: 0.3065 - val_mse: 0.1979 - learning_rate: 0.1000 - val_custom_mse: 0.3735 - val_custom_mae: 0.4374\n",
            "Epoch 8/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3585 - mae: 0.3364 - mse: 0.3585 - val_loss: 0.1812 - val_mae: 0.2894 - val_mse: 0.1812 - learning_rate: 0.1000 - val_custom_mse: 0.3529 - val_custom_mae: 0.4219\n",
            "Epoch 9/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3363 - mae: 0.3200 - mse: 0.3363 - val_loss: 0.1686 - val_mae: 0.2758 - val_mse: 0.1686 - learning_rate: 0.1000 - val_custom_mse: 0.3347 - val_custom_mae: 0.4076\n",
            "Epoch 10/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3200 - mae: 0.3071 - mse: 0.3200 - val_loss: 0.1617 - val_mae: 0.2676 - val_mse: 0.1617 - learning_rate: 0.1000 - val_custom_mse: 0.3312 - val_custom_mae: 0.4051\n",
            "Epoch 11/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3079 - mae: 0.2972 - mse: 0.3079 - val_loss: 0.1559 - val_mae: 0.2606 - val_mse: 0.1559 - learning_rate: 0.1000 - val_custom_mse: 0.3256 - val_custom_mae: 0.4007\n",
            "Epoch 12/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2986 - mae: 0.2893 - mse: 0.2986 - val_loss: 0.1510 - val_mae: 0.2545 - val_mse: 0.1510 - learning_rate: 0.1000 - val_custom_mse: 0.3201 - val_custom_mae: 0.3963\n",
            "Epoch 13/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2911 - mae: 0.2827 - mse: 0.2911 - val_loss: 0.1467 - val_mae: 0.2489 - val_mse: 0.1467 - learning_rate: 0.1000 - val_custom_mse: 0.3149 - val_custom_mae: 0.3919\n",
            "Epoch 14/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2846 - mae: 0.2768 - mse: 0.2846 - val_loss: 0.1435 - val_mae: 0.2445 - val_mse: 0.1435 - learning_rate: 0.1000 - val_custom_mse: 0.3132 - val_custom_mae: 0.3907\n",
            "Epoch 15/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2788 - mae: 0.2714 - mse: 0.2788 - val_loss: 0.1404 - val_mae: 0.2402 - val_mse: 0.1404 - learning_rate: 0.1000 - val_custom_mse: 0.3107 - val_custom_mae: 0.3886\n",
            "Epoch 16/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2736 - mae: 0.2664 - mse: 0.2736 - val_loss: 0.1378 - val_mae: 0.2364 - val_mse: 0.1378 - learning_rate: 0.1000 - val_custom_mse: 0.3098 - val_custom_mae: 0.3880\n",
            "Epoch 17/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2688 - mae: 0.2618 - mse: 0.2688 - val_loss: 0.1351 - val_mae: 0.2325 - val_mse: 0.1351 - learning_rate: 0.1000 - val_custom_mse: 0.3079 - val_custom_mae: 0.3864\n",
            "Epoch 18/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2643 - mae: 0.2574 - mse: 0.2643 - val_loss: 0.1326 - val_mae: 0.2288 - val_mse: 0.1326 - learning_rate: 0.1000 - val_custom_mse: 0.3062 - val_custom_mae: 0.3849\n",
            "Epoch 19/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2602 - mae: 0.2534 - mse: 0.2602 - val_loss: 0.1303 - val_mae: 0.2254 - val_mse: 0.1303 - learning_rate: 0.1000 - val_custom_mse: 0.3050 - val_custom_mae: 0.3840\n",
            "Epoch 20/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2563 - mae: 0.2496 - mse: 0.2563 - val_loss: 0.1283 - val_mae: 0.2223 - val_mse: 0.1283 - learning_rate: 0.1000 - val_custom_mse: 0.3044 - val_custom_mae: 0.3836\n",
            "Epoch 21/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2526 - mae: 0.2461 - mse: 0.2526 - val_loss: 0.1263 - val_mae: 0.2192 - val_mse: 0.1263 - learning_rate: 0.1000 - val_custom_mse: 0.3033 - val_custom_mae: 0.3827\n",
            "Epoch 22/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2491 - mae: 0.2427 - mse: 0.2491 - val_loss: 0.1243 - val_mae: 0.2162 - val_mse: 0.1243 - learning_rate: 0.1000 - val_custom_mse: 0.3021 - val_custom_mae: 0.3817\n",
            "Epoch 23/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2458 - mae: 0.2395 - mse: 0.2458 - val_loss: 0.1228 - val_mae: 0.2138 - val_mse: 0.1228 - learning_rate: 0.1000 - val_custom_mse: 0.3028 - val_custom_mae: 0.3824\n",
            "Epoch 24/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2426 - mae: 0.2365 - mse: 0.2426 - val_loss: 0.1210 - val_mae: 0.2109 - val_mse: 0.1210 - learning_rate: 0.1000 - val_custom_mse: 0.3016 - val_custom_mae: 0.3815\n",
            "Epoch 25/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2396 - mae: 0.2336 - mse: 0.2396 - val_loss: 0.1193 - val_mae: 0.2083 - val_mse: 0.1193 - learning_rate: 0.1000 - val_custom_mse: 0.3009 - val_custom_mae: 0.3809\n",
            "Epoch 26/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2366 - mae: 0.2308 - mse: 0.2366 - val_loss: 0.1180 - val_mae: 0.2062 - val_mse: 0.1180 - learning_rate: 0.1000 - val_custom_mse: 0.3011 - val_custom_mae: 0.3812\n",
            "Epoch 27/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2338 - mae: 0.2282 - mse: 0.2338 - val_loss: 0.1165 - val_mae: 0.2038 - val_mse: 0.1165 - learning_rate: 0.1000 - val_custom_mse: 0.3007 - val_custom_mae: 0.3809\n",
            "Epoch 28/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2312 - mae: 0.2257 - mse: 0.2312 - val_loss: 0.1150 - val_mae: 0.2014 - val_mse: 0.1150 - learning_rate: 0.1000 - val_custom_mse: 0.2998 - val_custom_mae: 0.3801\n",
            "Epoch 29/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2285 - mae: 0.2233 - mse: 0.2285 - val_loss: 0.1136 - val_mae: 0.1992 - val_mse: 0.1136 - learning_rate: 0.1000 - val_custom_mse: 0.2993 - val_custom_mae: 0.3797\n",
            "Epoch 30/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2260 - mae: 0.2210 - mse: 0.2260 - val_loss: 0.1123 - val_mae: 0.1971 - val_mse: 0.1123 - learning_rate: 0.1000 - val_custom_mse: 0.2989 - val_custom_mae: 0.3794\n",
            "Epoch 31/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2236 - mae: 0.2188 - mse: 0.2236 - val_loss: 0.1110 - val_mae: 0.1951 - val_mse: 0.1110 - learning_rate: 0.1000 - val_custom_mse: 0.2984 - val_custom_mae: 0.3791\n",
            "Epoch 32/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2212 - mae: 0.2166 - mse: 0.2212 - val_loss: 0.1097 - val_mae: 0.1930 - val_mse: 0.1097 - learning_rate: 0.1000 - val_custom_mse: 0.2978 - val_custom_mae: 0.3786\n",
            "Epoch 33/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2189 - mae: 0.2146 - mse: 0.2189 - val_loss: 0.1088 - val_mae: 0.1916 - val_mse: 0.1088 - learning_rate: 0.1000 - val_custom_mse: 0.2984 - val_custom_mae: 0.3792\n",
            "Epoch 34/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2167 - mae: 0.2127 - mse: 0.2167 - val_loss: 0.1078 - val_mae: 0.1900 - val_mse: 0.1078 - learning_rate: 0.1000 - val_custom_mse: 0.2987 - val_custom_mae: 0.3795\n",
            "Epoch 35/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2145 - mae: 0.2107 - mse: 0.2145 - val_loss: 0.1061 - val_mae: 0.1874 - val_mse: 0.1061 - learning_rate: 0.1000 - val_custom_mse: 0.2962 - val_custom_mae: 0.3773\n",
            "Epoch 36/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2124 - mae: 0.2089 - mse: 0.2124 - val_loss: 0.1051 - val_mae: 0.1857 - val_mse: 0.1051 - learning_rate: 0.1000 - val_custom_mse: 0.2958 - val_custom_mae: 0.3770\n",
            "Epoch 37/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2104 - mae: 0.2072 - mse: 0.2104 - val_loss: 0.1042 - val_mae: 0.1843 - val_mse: 0.1042 - learning_rate: 0.1000 - val_custom_mse: 0.2964 - val_custom_mae: 0.3777\n",
            "Epoch 38/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2084 - mae: 0.2054 - mse: 0.2084 - val_loss: 0.1034 - val_mae: 0.1830 - val_mse: 0.1034 - learning_rate: 0.1000 - val_custom_mse: 0.2970 - val_custom_mae: 0.3782\n",
            "Epoch 39/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2065 - mae: 0.2038 - mse: 0.2065 - val_loss: 0.1022 - val_mae: 0.1811 - val_mse: 0.1022 - learning_rate: 0.1000 - val_custom_mse: 0.2959 - val_custom_mae: 0.3773\n",
            "Epoch 40/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2046 - mae: 0.2021 - mse: 0.2046 - val_loss: 0.1012 - val_mae: 0.1796 - val_mse: 0.1012 - learning_rate: 0.1000 - val_custom_mse: 0.2955 - val_custom_mae: 0.3770\n",
            "Epoch 41/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2027 - mae: 0.2005 - mse: 0.2027 - val_loss: 0.1004 - val_mae: 0.1782 - val_mse: 0.1004 - learning_rate: 0.1000 - val_custom_mse: 0.2955 - val_custom_mae: 0.3770\n",
            "Epoch 42/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2009 - mae: 0.1990 - mse: 0.2009 - val_loss: 0.0999 - val_mae: 0.1776 - val_mse: 0.0999 - learning_rate: 0.1000 - val_custom_mse: 0.2971 - val_custom_mae: 0.3784\n",
            "Epoch 43/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1992 - mae: 0.1975 - mse: 0.1992 - val_loss: 0.0988 - val_mae: 0.1757 - val_mse: 0.0988 - learning_rate: 0.1000 - val_custom_mse: 0.2960 - val_custom_mae: 0.3775\n",
            "Epoch 44/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1974 - mae: 0.1961 - mse: 0.1974 - val_loss: 0.0978 - val_mae: 0.1741 - val_mse: 0.0978 - learning_rate: 0.1000 - val_custom_mse: 0.2954 - val_custom_mae: 0.3769\n",
            "Epoch 45/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1957 - mae: 0.1946 - mse: 0.1957 - val_loss: 0.0971 - val_mae: 0.1730 - val_mse: 0.0971 - learning_rate: 0.1000 - val_custom_mse: 0.2956 - val_custom_mae: 0.3772\n",
            "Epoch 46/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1940 - mae: 0.1932 - mse: 0.1940 - val_loss: 0.0964 - val_mae: 0.1720 - val_mse: 0.0964 - learning_rate: 0.1000 - val_custom_mse: 0.2961 - val_custom_mae: 0.3776\n",
            "Epoch 47/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1924 - mae: 0.1919 - mse: 0.1924 - val_loss: 0.0955 - val_mae: 0.1705 - val_mse: 0.0955 - learning_rate: 0.1000 - val_custom_mse: 0.2955 - val_custom_mae: 0.3772\n",
            "Epoch 48/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1906 - mae: 0.1905 - mse: 0.1906 - val_loss: 0.0948 - val_mae: 0.1696 - val_mse: 0.0948 - learning_rate: 0.1000 - val_custom_mse: 0.2959 - val_custom_mae: 0.3775\n",
            "Epoch 49/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1889 - mae: 0.1893 - mse: 0.1889 - val_loss: 0.0939 - val_mae: 0.1681 - val_mse: 0.0939 - learning_rate: 0.1000 - val_custom_mse: 0.2955 - val_custom_mae: 0.3772\n",
            "Epoch 50/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1872 - mae: 0.1883 - mse: 0.1872 - val_loss: 0.0940 - val_mae: 0.1693 - val_mse: 0.0940 - learning_rate: 0.1000 - val_custom_mse: 0.2984 - val_custom_mae: 0.3797\n",
            "Epoch 51/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1855 - mae: 0.1873 - mse: 0.1855 - val_loss: 0.0926 - val_mae: 0.1667 - val_mse: 0.0926 - learning_rate: 0.1000 - val_custom_mse: 0.2962 - val_custom_mae: 0.3778\n",
            "Epoch 52/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1838 - mae: 0.1861 - mse: 0.1838 - val_loss: 0.0917 - val_mae: 0.1652 - val_mse: 0.0917 - learning_rate: 0.1000 - val_custom_mse: 0.2954 - val_custom_mae: 0.3772\n",
            "Epoch 53/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1820 - mae: 0.1849 - mse: 0.1820 - val_loss: 0.0905 - val_mae: 0.1634 - val_mse: 0.0905 - learning_rate: 0.1000 - val_custom_mse: 0.2939 - val_custom_mae: 0.3759\n",
            "Epoch 54/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1800 - mae: 0.1836 - mse: 0.1800 - val_loss: 0.0899 - val_mae: 0.1630 - val_mse: 0.0899 - learning_rate: 0.1000 - val_custom_mse: 0.2950 - val_custom_mae: 0.3769\n",
            "Epoch 55/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1774 - mae: 0.1821 - mse: 0.1774 - val_loss: 0.0884 - val_mae: 0.1612 - val_mse: 0.0884 - learning_rate: 0.1000 - val_custom_mse: 0.2940 - val_custom_mae: 0.3761\n",
            "Epoch 56/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1726 - mae: 0.1797 - mse: 0.1726 - val_loss: 0.0852 - val_mae: 0.1579 - val_mse: 0.0852 - learning_rate: 0.1000 - val_custom_mse: 0.2923 - val_custom_mae: 0.3748\n",
            "Epoch 57/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1344 - mae: 0.1564 - mse: 0.1344 - val_loss: 0.0622 - val_mae: 0.1254 - val_mse: 0.0622 - learning_rate: 0.1000 - val_custom_mse: 0.2802 - val_custom_mae: 0.3651\n",
            "Epoch 58/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1071 - mae: 0.1325 - mse: 0.1071 - val_loss: 0.0621 - val_mae: 0.1226 - val_mse: 0.0621 - learning_rate: 0.1000 - val_custom_mse: 0.2833 - val_custom_mae: 0.3678\n",
            "Epoch 59/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1062 - mae: 0.1301 - mse: 0.1062 - val_loss: 0.0614 - val_mae: 0.1214 - val_mse: 0.0614 - learning_rate: 0.1000 - val_custom_mse: 0.2812 - val_custom_mae: 0.3659\n",
            "Epoch 60/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1057 - mae: 0.1287 - mse: 0.1057 - val_loss: 0.0615 - val_mae: 0.1201 - val_mse: 0.0615 - learning_rate: 0.1000 - val_custom_mse: 0.2838 - val_custom_mae: 0.3683\n",
            "Epoch 61/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1053 - mae: 0.1276 - mse: 0.1053 - val_loss: 0.0619 - val_mae: 0.1203 - val_mse: 0.0619 - learning_rate: 0.1000 - val_custom_mse: 0.2867 - val_custom_mae: 0.3708\n",
            "Epoch 62/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1050 - mae: 0.1268 - mse: 0.1050 - val_loss: 0.0609 - val_mae: 0.1182 - val_mse: 0.0609 - learning_rate: 0.1000 - val_custom_mse: 0.2833 - val_custom_mae: 0.3678\n",
            "Epoch 63/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1046 - mae: 0.1257 - mse: 0.1046 - val_loss: 0.0605 - val_mae: 0.1173 - val_mse: 0.0605 - learning_rate: 0.1000 - val_custom_mse: 0.2825 - val_custom_mae: 0.3672\n",
            "Epoch 64/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1042 - mae: 0.1248 - mse: 0.1042 - val_loss: 0.0601 - val_mae: 0.1164 - val_mse: 0.0601 - learning_rate: 0.1000 - val_custom_mse: 0.2819 - val_custom_mae: 0.3665\n",
            "Epoch 65/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1039 - mae: 0.1239 - mse: 0.1039 - val_loss: 0.0599 - val_mae: 0.1155 - val_mse: 0.0599 - learning_rate: 0.1000 - val_custom_mse: 0.2823 - val_custom_mae: 0.3669\n",
            "Epoch 66/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1036 - mae: 0.1231 - mse: 0.1036 - val_loss: 0.0600 - val_mae: 0.1149 - val_mse: 0.0600 - learning_rate: 0.1000 - val_custom_mse: 0.2839 - val_custom_mae: 0.3684\n",
            "Epoch 67/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1033 - mae: 0.1222 - mse: 0.1033 - val_loss: 0.0594 - val_mae: 0.1138 - val_mse: 0.0594 - learning_rate: 0.1000 - val_custom_mse: 0.2820 - val_custom_mae: 0.3667\n",
            "Epoch 68/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1030 - mae: 0.1214 - mse: 0.1030 - val_loss: 0.0591 - val_mae: 0.1130 - val_mse: 0.0591 - learning_rate: 0.1000 - val_custom_mse: 0.2817 - val_custom_mae: 0.3664\n",
            "Epoch 69/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1027 - mae: 0.1205 - mse: 0.1027 - val_loss: 0.0591 - val_mae: 0.1122 - val_mse: 0.0591 - learning_rate: 0.1000 - val_custom_mse: 0.2824 - val_custom_mae: 0.3671\n",
            "Epoch 70/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1025 - mae: 0.1198 - mse: 0.1025 - val_loss: 0.0586 - val_mae: 0.1115 - val_mse: 0.0586 - learning_rate: 0.1000 - val_custom_mse: 0.2809 - val_custom_mae: 0.3657\n",
            "Epoch 71/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1022 - mae: 0.1190 - mse: 0.1022 - val_loss: 0.0583 - val_mae: 0.1109 - val_mse: 0.0583 - learning_rate: 0.1000 - val_custom_mse: 0.2802 - val_custom_mae: 0.3651\n",
            "Epoch 72/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1020 - mae: 0.1184 - mse: 0.1020 - val_loss: 0.0581 - val_mae: 0.1101 - val_mse: 0.0581 - learning_rate: 0.1000 - val_custom_mse: 0.2802 - val_custom_mae: 0.3651\n",
            "Epoch 73/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1017 - mae: 0.1175 - mse: 0.1017 - val_loss: 0.0581 - val_mae: 0.1092 - val_mse: 0.0581 - learning_rate: 0.1000 - val_custom_mse: 0.2813 - val_custom_mae: 0.3660\n",
            "Epoch 74/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1015 - mae: 0.1168 - mse: 0.1015 - val_loss: 0.0579 - val_mae: 0.1085 - val_mse: 0.0579 - learning_rate: 0.1000 - val_custom_mse: 0.2813 - val_custom_mae: 0.3661\n",
            "Epoch 75/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1012 - mae: 0.1161 - mse: 0.1012 - val_loss: 0.0580 - val_mae: 0.1080 - val_mse: 0.0580 - learning_rate: 0.1000 - val_custom_mse: 0.2824 - val_custom_mae: 0.3671\n",
            "Epoch 76/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1010 - mae: 0.1155 - mse: 0.1010 - val_loss: 0.0575 - val_mae: 0.1071 - val_mse: 0.0575 - learning_rate: 0.1000 - val_custom_mse: 0.2805 - val_custom_mae: 0.3654\n",
            "Epoch 77/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1008 - mae: 0.1148 - mse: 0.1008 - val_loss: 0.0572 - val_mae: 0.1069 - val_mse: 0.0572 - learning_rate: 0.1000 - val_custom_mse: 0.2794 - val_custom_mae: 0.3643\n",
            "Epoch 78/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1006 - mae: 0.1142 - mse: 0.1006 - val_loss: 0.0569 - val_mae: 0.1063 - val_mse: 0.0569 - learning_rate: 0.1000 - val_custom_mse: 0.2790 - val_custom_mae: 0.3640\n",
            "Epoch 79/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1005 - mae: 0.1137 - mse: 0.1005 - val_loss: 0.0569 - val_mae: 0.1053 - val_mse: 0.0569 - learning_rate: 0.1000 - val_custom_mse: 0.2797 - val_custom_mae: 0.3646\n",
            "Epoch 80/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1002 - mae: 0.1129 - mse: 0.1002 - val_loss: 0.0570 - val_mae: 0.1046 - val_mse: 0.0570 - learning_rate: 0.1000 - val_custom_mse: 0.2811 - val_custom_mae: 0.3659\n",
            "Epoch 81/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1001 - mae: 0.1125 - mse: 0.1001 - val_loss: 0.0569 - val_mae: 0.1042 - val_mse: 0.0569 - learning_rate: 0.1000 - val_custom_mse: 0.2816 - val_custom_mae: 0.3664\n",
            "Epoch 82/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0999 - mae: 0.1119 - mse: 0.0999 - val_loss: 0.0566 - val_mae: 0.1034 - val_mse: 0.0566 - learning_rate: 0.1000 - val_custom_mse: 0.2805 - val_custom_mae: 0.3654\n",
            "Epoch 83/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0998 - mae: 0.1113 - mse: 0.0998 - val_loss: 0.0566 - val_mae: 0.1030 - val_mse: 0.0566 - learning_rate: 0.1000 - val_custom_mse: 0.2813 - val_custom_mae: 0.3661\n",
            "Epoch 84/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0996 - mae: 0.1108 - mse: 0.0996 - val_loss: 0.0562 - val_mae: 0.1025 - val_mse: 0.0562 - learning_rate: 0.1000 - val_custom_mse: 0.2793 - val_custom_mae: 0.3642\n",
            "Epoch 85/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0994 - mae: 0.1101 - mse: 0.0994 - val_loss: 0.0561 - val_mae: 0.1018 - val_mse: 0.0561 - learning_rate: 0.1000 - val_custom_mse: 0.2799 - val_custom_mae: 0.3648\n",
            "Epoch 86/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0993 - mae: 0.1098 - mse: 0.0993 - val_loss: 0.0559 - val_mae: 0.1016 - val_mse: 0.0559 - learning_rate: 0.1000 - val_custom_mse: 0.2792 - val_custom_mae: 0.3641\n",
            "Epoch 87/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0991 - mae: 0.1092 - mse: 0.0991 - val_loss: 0.0560 - val_mae: 0.1007 - val_mse: 0.0560 - learning_rate: 0.1000 - val_custom_mse: 0.2803 - val_custom_mae: 0.3651\n",
            "Epoch 88/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0990 - mae: 0.1085 - mse: 0.0990 - val_loss: 0.0557 - val_mae: 0.1003 - val_mse: 0.0557 - learning_rate: 0.1000 - val_custom_mse: 0.2792 - val_custom_mae: 0.3641\n",
            "Epoch 89/100\n",
            "233/233 - 2s - 9ms/step - loss: 0.0989 - mae: 0.1082 - mse: 0.0989 - val_loss: 0.0554 - val_mae: 0.1007 - val_mse: 0.0554 - learning_rate: 0.1000 - val_custom_mse: 0.2777 - val_custom_mae: 0.3628\n",
            "Epoch 90/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0987 - mae: 0.1077 - mse: 0.0987 - val_loss: 0.0557 - val_mae: 0.0993 - val_mse: 0.0557 - learning_rate: 0.1000 - val_custom_mse: 0.2805 - val_custom_mae: 0.3655\n",
            "Epoch 91/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0986 - mae: 0.1074 - mse: 0.0986 - val_loss: 0.0560 - val_mae: 0.0997 - val_mse: 0.0560 - learning_rate: 0.1000 - val_custom_mse: 0.2823 - val_custom_mae: 0.3671\n",
            "Epoch 92/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0985 - mae: 0.1069 - mse: 0.0985 - val_loss: 0.0554 - val_mae: 0.0984 - val_mse: 0.0554 - learning_rate: 0.1000 - val_custom_mse: 0.2795 - val_custom_mae: 0.3645\n",
            "Epoch 93/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0984 - mae: 0.1064 - mse: 0.0984 - val_loss: 0.0553 - val_mae: 0.0980 - val_mse: 0.0553 - learning_rate: 0.1000 - val_custom_mse: 0.2794 - val_custom_mae: 0.3643\n",
            "Epoch 94/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0983 - mae: 0.1059 - mse: 0.0983 - val_loss: 0.0555 - val_mae: 0.0978 - val_mse: 0.0555 - learning_rate: 0.1000 - val_custom_mse: 0.2810 - val_custom_mae: 0.3659\n",
            "Epoch 95/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0982 - mae: 0.1058 - mse: 0.0982 - val_loss: 0.0553 - val_mae: 0.0971 - val_mse: 0.0553 - learning_rate: 0.1000 - val_custom_mse: 0.2803 - val_custom_mae: 0.3653\n",
            "Epoch 96/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0980 - mae: 0.1051 - mse: 0.0980 - val_loss: 0.0549 - val_mae: 0.0972 - val_mse: 0.0549 - learning_rate: 0.1000 - val_custom_mse: 0.2783 - val_custom_mae: 0.3634\n",
            "Epoch 97/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0979 - mae: 0.1047 - mse: 0.0979 - val_loss: 0.0556 - val_mae: 0.0974 - val_mse: 0.0556 - learning_rate: 0.1000 - val_custom_mse: 0.2824 - val_custom_mae: 0.3672\n",
            "Epoch 98/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0979 - mae: 0.1044 - mse: 0.0979 - val_loss: 0.0547 - val_mae: 0.0962 - val_mse: 0.0547 - learning_rate: 0.1000 - val_custom_mse: 0.2785 - val_custom_mae: 0.3635\n",
            "Epoch 99/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0978 - mae: 0.1039 - mse: 0.0978 - val_loss: 0.0547 - val_mae: 0.0957 - val_mse: 0.0547 - learning_rate: 0.1000 - val_custom_mse: 0.2787 - val_custom_mae: 0.3638\n",
            "Epoch 100/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.0977 - mae: 0.1036 - mse: 0.0977 - val_loss: 0.0546 - val_mae: 0.0955 - val_mse: 0.0546 - learning_rate: 0.1000 - val_custom_mse: 0.2784 - val_custom_mae: 0.3635\n",
            "Running experiment: horizon=192, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_38', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "233/233 - 6s - 26ms/step - loss: 0.5358 - mae: 0.4520 - mse: 0.5358 - val_loss: 0.3287 - val_mae: 0.4128 - val_mse: 0.3287 - learning_rate: 0.1000 - val_custom_mse: 0.5024 - val_custom_mae: 0.5184\n",
            "Epoch 2/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4698 - mae: 0.4248 - mse: 0.4698 - val_loss: 0.2742 - val_mae: 0.3784 - val_mse: 0.2742 - learning_rate: 0.1000 - val_custom_mse: 0.4426 - val_custom_mae: 0.4838\n",
            "Epoch 3/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4156 - mae: 0.3959 - mse: 0.4156 - val_loss: 0.2347 - val_mae: 0.3487 - val_mse: 0.2347 - learning_rate: 0.1000 - val_custom_mse: 0.4127 - val_custom_mae: 0.4646\n",
            "Epoch 4/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3534 - mae: 0.3633 - mse: 0.3534 - val_loss: 0.1964 - val_mae: 0.3157 - val_mse: 0.1964 - learning_rate: 0.1000 - val_custom_mse: 0.3791 - val_custom_mae: 0.4411\n",
            "Epoch 5/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2981 - mae: 0.3312 - mse: 0.2981 - val_loss: 0.1638 - val_mae: 0.2842 - val_mse: 0.1638 - learning_rate: 0.1000 - val_custom_mse: 0.3477 - val_custom_mae: 0.4178\n",
            "Epoch 6/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2539 - mae: 0.3030 - mse: 0.2539 - val_loss: 0.1427 - val_mae: 0.2619 - val_mse: 0.1427 - learning_rate: 0.1000 - val_custom_mse: 0.3327 - val_custom_mae: 0.4067\n",
            "Epoch 7/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2222 - mae: 0.2806 - mse: 0.2222 - val_loss: 0.1293 - val_mae: 0.2462 - val_mse: 0.1293 - learning_rate: 0.1000 - val_custom_mse: 0.3252 - val_custom_mae: 0.4011\n",
            "Epoch 8/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2012 - mae: 0.2642 - mse: 0.2012 - val_loss: 0.1181 - val_mae: 0.2329 - val_mse: 0.1181 - learning_rate: 0.1000 - val_custom_mse: 0.3091 - val_custom_mae: 0.3882\n",
            "Epoch 9/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1875 - mae: 0.2525 - mse: 0.1875 - val_loss: 0.1120 - val_mae: 0.2246 - val_mse: 0.1120 - learning_rate: 0.1000 - val_custom_mse: 0.3056 - val_custom_mae: 0.3855\n",
            "Epoch 10/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1781 - mae: 0.2435 - mse: 0.1781 - val_loss: 0.1074 - val_mae: 0.2179 - val_mse: 0.1074 - learning_rate: 0.1000 - val_custom_mse: 0.3025 - val_custom_mae: 0.3832\n",
            "Epoch 11/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1714 - mae: 0.2363 - mse: 0.1714 - val_loss: 0.1037 - val_mae: 0.2120 - val_mse: 0.1037 - learning_rate: 0.1000 - val_custom_mse: 0.3007 - val_custom_mae: 0.3818\n",
            "Epoch 12/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1660 - mae: 0.2301 - mse: 0.1660 - val_loss: 0.1004 - val_mae: 0.2065 - val_mse: 0.1004 - learning_rate: 0.1000 - val_custom_mse: 0.2993 - val_custom_mae: 0.3806\n",
            "Epoch 13/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1614 - mae: 0.2244 - mse: 0.1614 - val_loss: 0.0977 - val_mae: 0.2020 - val_mse: 0.0977 - learning_rate: 0.1000 - val_custom_mse: 0.2985 - val_custom_mae: 0.3802\n",
            "Epoch 14/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1575 - mae: 0.2195 - mse: 0.1575 - val_loss: 0.0949 - val_mae: 0.1972 - val_mse: 0.0949 - learning_rate: 0.1000 - val_custom_mse: 0.2964 - val_custom_mae: 0.3785\n",
            "Epoch 15/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1542 - mae: 0.2151 - mse: 0.1542 - val_loss: 0.0918 - val_mae: 0.1924 - val_mse: 0.0918 - learning_rate: 0.1000 - val_custom_mse: 0.2905 - val_custom_mae: 0.3731\n",
            "Epoch 16/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1513 - mae: 0.2110 - mse: 0.1513 - val_loss: 0.0905 - val_mae: 0.1891 - val_mse: 0.0905 - learning_rate: 0.1000 - val_custom_mse: 0.2941 - val_custom_mae: 0.3766\n",
            "Epoch 17/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1488 - mae: 0.2074 - mse: 0.1488 - val_loss: 0.0884 - val_mae: 0.1854 - val_mse: 0.0884 - learning_rate: 0.1000 - val_custom_mse: 0.2925 - val_custom_mae: 0.3752\n",
            "Epoch 18/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1465 - mae: 0.2040 - mse: 0.1465 - val_loss: 0.0870 - val_mae: 0.1823 - val_mse: 0.0870 - learning_rate: 0.1000 - val_custom_mse: 0.2927 - val_custom_mae: 0.3755\n",
            "Epoch 19/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1445 - mae: 0.2010 - mse: 0.1445 - val_loss: 0.0849 - val_mae: 0.1789 - val_mse: 0.0849 - learning_rate: 0.1000 - val_custom_mse: 0.2892 - val_custom_mae: 0.3724\n",
            "Epoch 20/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1427 - mae: 0.1982 - mse: 0.1427 - val_loss: 0.0835 - val_mae: 0.1759 - val_mse: 0.0835 - learning_rate: 0.1000 - val_custom_mse: 0.2884 - val_custom_mae: 0.3716\n",
            "Epoch 21/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1410 - mae: 0.1957 - mse: 0.1410 - val_loss: 0.0829 - val_mae: 0.1741 - val_mse: 0.0829 - learning_rate: 0.1000 - val_custom_mse: 0.2911 - val_custom_mae: 0.3743\n",
            "Epoch 22/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1396 - mae: 0.1933 - mse: 0.1396 - val_loss: 0.0811 - val_mae: 0.1711 - val_mse: 0.0811 - learning_rate: 0.1000 - val_custom_mse: 0.2880 - val_custom_mae: 0.3714\n",
            "Epoch 23/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1385 - mae: 0.1913 - mse: 0.1385 - val_loss: 0.0802 - val_mae: 0.1689 - val_mse: 0.0802 - learning_rate: 0.1000 - val_custom_mse: 0.2881 - val_custom_mae: 0.3714\n",
            "Epoch 24/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1372 - mae: 0.1893 - mse: 0.1372 - val_loss: 0.0788 - val_mae: 0.1667 - val_mse: 0.0788 - learning_rate: 0.1000 - val_custom_mse: 0.2858 - val_custom_mae: 0.3694\n",
            "Epoch 25/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1361 - mae: 0.1874 - mse: 0.1361 - val_loss: 0.0781 - val_mae: 0.1648 - val_mse: 0.0781 - learning_rate: 0.1000 - val_custom_mse: 0.2864 - val_custom_mae: 0.3700\n",
            "Epoch 26/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1351 - mae: 0.1856 - mse: 0.1351 - val_loss: 0.0774 - val_mae: 0.1631 - val_mse: 0.0774 - learning_rate: 0.1000 - val_custom_mse: 0.2872 - val_custom_mae: 0.3708\n",
            "Epoch 27/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1343 - mae: 0.1841 - mse: 0.1343 - val_loss: 0.0765 - val_mae: 0.1613 - val_mse: 0.0765 - learning_rate: 0.1000 - val_custom_mse: 0.2859 - val_custom_mae: 0.3697\n",
            "Epoch 28/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1334 - mae: 0.1826 - mse: 0.1334 - val_loss: 0.0760 - val_mae: 0.1600 - val_mse: 0.0760 - learning_rate: 0.1000 - val_custom_mse: 0.2872 - val_custom_mae: 0.3709\n",
            "Epoch 29/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1326 - mae: 0.1812 - mse: 0.1326 - val_loss: 0.0748 - val_mae: 0.1582 - val_mse: 0.0748 - learning_rate: 0.1000 - val_custom_mse: 0.2841 - val_custom_mae: 0.3681\n",
            "Epoch 30/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1320 - mae: 0.1799 - mse: 0.1320 - val_loss: 0.0745 - val_mae: 0.1569 - val_mse: 0.0745 - learning_rate: 0.1000 - val_custom_mse: 0.2858 - val_custom_mae: 0.3696\n",
            "Epoch 31/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1312 - mae: 0.1786 - mse: 0.1312 - val_loss: 0.0739 - val_mae: 0.1556 - val_mse: 0.0739 - learning_rate: 0.1000 - val_custom_mse: 0.2857 - val_custom_mae: 0.3695\n",
            "Epoch 32/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1307 - mae: 0.1775 - mse: 0.1307 - val_loss: 0.0734 - val_mae: 0.1543 - val_mse: 0.0734 - learning_rate: 0.1000 - val_custom_mse: 0.2858 - val_custom_mae: 0.3696\n",
            "Epoch 33/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1300 - mae: 0.1764 - mse: 0.1300 - val_loss: 0.0728 - val_mae: 0.1533 - val_mse: 0.0728 - learning_rate: 0.1000 - val_custom_mse: 0.2852 - val_custom_mae: 0.3692\n",
            "Epoch 34/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1296 - mae: 0.1754 - mse: 0.1296 - val_loss: 0.0730 - val_mae: 0.1528 - val_mse: 0.0730 - learning_rate: 0.1000 - val_custom_mse: 0.2882 - val_custom_mae: 0.3718\n",
            "Epoch 35/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1292 - mae: 0.1745 - mse: 0.1292 - val_loss: 0.0723 - val_mae: 0.1517 - val_mse: 0.0723 - learning_rate: 0.1000 - val_custom_mse: 0.2871 - val_custom_mae: 0.3710\n",
            "Epoch 36/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1287 - mae: 0.1736 - mse: 0.1287 - val_loss: 0.0712 - val_mae: 0.1497 - val_mse: 0.0712 - learning_rate: 0.1000 - val_custom_mse: 0.2841 - val_custom_mae: 0.3680\n",
            "Epoch 37/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1281 - mae: 0.1726 - mse: 0.1281 - val_loss: 0.0711 - val_mae: 0.1493 - val_mse: 0.0711 - learning_rate: 0.1000 - val_custom_mse: 0.2853 - val_custom_mae: 0.3695\n",
            "Epoch 38/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1277 - mae: 0.1718 - mse: 0.1277 - val_loss: 0.0705 - val_mae: 0.1480 - val_mse: 0.0705 - learning_rate: 0.1000 - val_custom_mse: 0.2846 - val_custom_mae: 0.3686\n",
            "Epoch 39/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1274 - mae: 0.1711 - mse: 0.1274 - val_loss: 0.0702 - val_mae: 0.1472 - val_mse: 0.0702 - learning_rate: 0.1000 - val_custom_mse: 0.2849 - val_custom_mae: 0.3689\n",
            "Epoch 40/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1270 - mae: 0.1702 - mse: 0.1270 - val_loss: 0.0698 - val_mae: 0.1465 - val_mse: 0.0698 - learning_rate: 0.1000 - val_custom_mse: 0.2846 - val_custom_mae: 0.3687\n",
            "Epoch 41/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1266 - mae: 0.1696 - mse: 0.1266 - val_loss: 0.0691 - val_mae: 0.1453 - val_mse: 0.0691 - learning_rate: 0.1000 - val_custom_mse: 0.2824 - val_custom_mae: 0.3665\n",
            "Epoch 42/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1264 - mae: 0.1688 - mse: 0.1264 - val_loss: 0.0692 - val_mae: 0.1449 - val_mse: 0.0692 - learning_rate: 0.1000 - val_custom_mse: 0.2845 - val_custom_mae: 0.3686\n",
            "Epoch 43/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1261 - mae: 0.1682 - mse: 0.1261 - val_loss: 0.0685 - val_mae: 0.1439 - val_mse: 0.0685 - learning_rate: 0.1000 - val_custom_mse: 0.2828 - val_custom_mae: 0.3670\n",
            "Epoch 44/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1258 - mae: 0.1676 - mse: 0.1258 - val_loss: 0.0685 - val_mae: 0.1435 - val_mse: 0.0685 - learning_rate: 0.1000 - val_custom_mse: 0.2840 - val_custom_mae: 0.3681\n",
            "Epoch 45/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1255 - mae: 0.1670 - mse: 0.1255 - val_loss: 0.0682 - val_mae: 0.1427 - val_mse: 0.0682 - learning_rate: 0.1000 - val_custom_mse: 0.2838 - val_custom_mae: 0.3679\n",
            "Epoch 46/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1252 - mae: 0.1664 - mse: 0.1252 - val_loss: 0.0677 - val_mae: 0.1420 - val_mse: 0.0677 - learning_rate: 0.1000 - val_custom_mse: 0.2829 - val_custom_mae: 0.3671\n",
            "Epoch 47/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1250 - mae: 0.1659 - mse: 0.1250 - val_loss: 0.0675 - val_mae: 0.1414 - val_mse: 0.0675 - learning_rate: 0.1000 - val_custom_mse: 0.2831 - val_custom_mae: 0.3673\n",
            "Epoch 48/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1248 - mae: 0.1653 - mse: 0.1248 - val_loss: 0.0669 - val_mae: 0.1408 - val_mse: 0.0669 - learning_rate: 0.1000 - val_custom_mse: 0.2808 - val_custom_mae: 0.3651\n",
            "Epoch 49/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1246 - mae: 0.1649 - mse: 0.1246 - val_loss: 0.0669 - val_mae: 0.1402 - val_mse: 0.0669 - learning_rate: 0.1000 - val_custom_mse: 0.2821 - val_custom_mae: 0.3663\n",
            "Epoch 50/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1244 - mae: 0.1644 - mse: 0.1244 - val_loss: 0.0667 - val_mae: 0.1397 - val_mse: 0.0667 - learning_rate: 0.1000 - val_custom_mse: 0.2823 - val_custom_mae: 0.3666\n",
            "Epoch 51/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1243 - mae: 0.1640 - mse: 0.1243 - val_loss: 0.0667 - val_mae: 0.1395 - val_mse: 0.0667 - learning_rate: 0.1000 - val_custom_mse: 0.2829 - val_custom_mae: 0.3672\n",
            "Epoch 52/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1241 - mae: 0.1637 - mse: 0.1241 - val_loss: 0.0662 - val_mae: 0.1387 - val_mse: 0.0662 - learning_rate: 0.1000 - val_custom_mse: 0.2816 - val_custom_mae: 0.3659\n",
            "Epoch 53/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1238 - mae: 0.1632 - mse: 0.1238 - val_loss: 0.0659 - val_mae: 0.1382 - val_mse: 0.0659 - learning_rate: 0.1000 - val_custom_mse: 0.2809 - val_custom_mae: 0.3652\n",
            "Epoch 54/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1236 - mae: 0.1627 - mse: 0.1236 - val_loss: 0.0656 - val_mae: 0.1379 - val_mse: 0.0656 - learning_rate: 0.1000 - val_custom_mse: 0.2799 - val_custom_mae: 0.3644\n",
            "Epoch 55/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1235 - mae: 0.1625 - mse: 0.1235 - val_loss: 0.0657 - val_mae: 0.1374 - val_mse: 0.0657 - learning_rate: 0.1000 - val_custom_mse: 0.2817 - val_custom_mae: 0.3659\n",
            "Epoch 56/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1233 - mae: 0.1620 - mse: 0.1233 - val_loss: 0.0655 - val_mae: 0.1369 - val_mse: 0.0655 - learning_rate: 0.1000 - val_custom_mse: 0.2812 - val_custom_mae: 0.3655\n",
            "Epoch 57/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1232 - mae: 0.1617 - mse: 0.1232 - val_loss: 0.0653 - val_mae: 0.1365 - val_mse: 0.0653 - learning_rate: 0.1000 - val_custom_mse: 0.2812 - val_custom_mae: 0.3654\n",
            "Epoch 58/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1231 - mae: 0.1614 - mse: 0.1231 - val_loss: 0.0653 - val_mae: 0.1362 - val_mse: 0.0653 - learning_rate: 0.1000 - val_custom_mse: 0.2818 - val_custom_mae: 0.3659\n",
            "Epoch 59/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1230 - mae: 0.1611 - mse: 0.1230 - val_loss: 0.0648 - val_mae: 0.1360 - val_mse: 0.0648 - learning_rate: 0.1000 - val_custom_mse: 0.2794 - val_custom_mae: 0.3638\n",
            "Epoch 60/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1228 - mae: 0.1607 - mse: 0.1228 - val_loss: 0.0646 - val_mae: 0.1358 - val_mse: 0.0646 - learning_rate: 0.1000 - val_custom_mse: 0.2791 - val_custom_mae: 0.3637\n",
            "Epoch 61/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1226 - mae: 0.1605 - mse: 0.1226 - val_loss: 0.0648 - val_mae: 0.1352 - val_mse: 0.0648 - learning_rate: 0.1000 - val_custom_mse: 0.2809 - val_custom_mae: 0.3652\n",
            "Epoch 62/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1226 - mae: 0.1602 - mse: 0.1226 - val_loss: 0.0647 - val_mae: 0.1349 - val_mse: 0.0647 - learning_rate: 0.1000 - val_custom_mse: 0.2811 - val_custom_mae: 0.3652\n",
            "Epoch 63/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1225 - mae: 0.1599 - mse: 0.1225 - val_loss: 0.0644 - val_mae: 0.1346 - val_mse: 0.0644 - learning_rate: 0.1000 - val_custom_mse: 0.2802 - val_custom_mae: 0.3648\n",
            "Epoch 64/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1222 - mae: 0.1596 - mse: 0.1222 - val_loss: 0.0644 - val_mae: 0.1343 - val_mse: 0.0644 - learning_rate: 0.1000 - val_custom_mse: 0.2809 - val_custom_mae: 0.3652\n",
            "Epoch 65/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1222 - mae: 0.1593 - mse: 0.1222 - val_loss: 0.0644 - val_mae: 0.1340 - val_mse: 0.0644 - learning_rate: 0.1000 - val_custom_mse: 0.2817 - val_custom_mae: 0.3659\n",
            "Epoch 66/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1222 - mae: 0.1591 - mse: 0.1222 - val_loss: 0.0642 - val_mae: 0.1338 - val_mse: 0.0642 - learning_rate: 0.1000 - val_custom_mse: 0.2809 - val_custom_mae: 0.3653\n",
            "Epoch 67/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1222 - mae: 0.1589 - mse: 0.1222 - val_loss: 0.0642 - val_mae: 0.1335 - val_mse: 0.0642 - learning_rate: 0.1000 - val_custom_mse: 0.2812 - val_custom_mae: 0.3656\n",
            "Epoch 68/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1218 - mae: 0.1586 - mse: 0.1218 - val_loss: 0.0640 - val_mae: 0.1333 - val_mse: 0.0640 - learning_rate: 0.1000 - val_custom_mse: 0.2810 - val_custom_mae: 0.3653\n",
            "Epoch 69/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1218 - mae: 0.1584 - mse: 0.1218 - val_loss: 0.0638 - val_mae: 0.1330 - val_mse: 0.0638 - learning_rate: 0.1000 - val_custom_mse: 0.2798 - val_custom_mae: 0.3643\n",
            "Epoch 70/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1218 - mae: 0.1583 - mse: 0.1218 - val_loss: 0.0636 - val_mae: 0.1330 - val_mse: 0.0636 - learning_rate: 0.1000 - val_custom_mse: 0.2794 - val_custom_mae: 0.3638\n",
            "Epoch 71/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1215 - mae: 0.1580 - mse: 0.1215 - val_loss: 0.0638 - val_mae: 0.1326 - val_mse: 0.0638 - learning_rate: 0.1000 - val_custom_mse: 0.2810 - val_custom_mae: 0.3655\n",
            "Epoch 72/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1215 - mae: 0.1578 - mse: 0.1215 - val_loss: 0.0637 - val_mae: 0.1324 - val_mse: 0.0637 - learning_rate: 0.1000 - val_custom_mse: 0.2807 - val_custom_mae: 0.3652\n",
            "Epoch 73/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1215 - mae: 0.1577 - mse: 0.1215 - val_loss: 0.0639 - val_mae: 0.1325 - val_mse: 0.0639 - learning_rate: 0.1000 - val_custom_mse: 0.2821 - val_custom_mae: 0.3665\n",
            "Epoch 74/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1213 - mae: 0.1575 - mse: 0.1213 - val_loss: 0.0636 - val_mae: 0.1320 - val_mse: 0.0636 - learning_rate: 0.1000 - val_custom_mse: 0.2813 - val_custom_mae: 0.3656\n",
            "Epoch 75/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1212 - mae: 0.1572 - mse: 0.1212 - val_loss: 0.0635 - val_mae: 0.1318 - val_mse: 0.0635 - learning_rate: 0.1000 - val_custom_mse: 0.2809 - val_custom_mae: 0.3655\n",
            "Epoch 76/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1212 - mae: 0.1571 - mse: 0.1212 - val_loss: 0.0633 - val_mae: 0.1315 - val_mse: 0.0633 - learning_rate: 0.1000 - val_custom_mse: 0.2801 - val_custom_mae: 0.3646\n",
            "Epoch 77/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1212 - mae: 0.1570 - mse: 0.1212 - val_loss: 0.0635 - val_mae: 0.1316 - val_mse: 0.0635 - learning_rate: 0.1000 - val_custom_mse: 0.2818 - val_custom_mae: 0.3662\n",
            "Epoch 78/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1211 - mae: 0.1568 - mse: 0.1211 - val_loss: 0.0637 - val_mae: 0.1317 - val_mse: 0.0637 - learning_rate: 0.1000 - val_custom_mse: 0.2829 - val_custom_mae: 0.3672\n",
            "Epoch 79/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1208 - mae: 0.1566 - mse: 0.1208 - val_loss: 0.0635 - val_mae: 0.1314 - val_mse: 0.0635 - learning_rate: 0.1000 - val_custom_mse: 0.2820 - val_custom_mae: 0.3664\n",
            "Epoch 80/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1210 - mae: 0.1567 - mse: 0.1210 - val_loss: 0.0633 - val_mae: 0.1312 - val_mse: 0.0633 - learning_rate: 0.1000 - val_custom_mse: 0.2816 - val_custom_mae: 0.3661\n",
            "Epoch 81/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1209 - mae: 0.1564 - mse: 0.1209 - val_loss: 0.0630 - val_mae: 0.1308 - val_mse: 0.0630 - learning_rate: 0.1000 - val_custom_mse: 0.2804 - val_custom_mae: 0.3647\n",
            "Epoch 82/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1208 - mae: 0.1562 - mse: 0.1208 - val_loss: 0.0628 - val_mae: 0.1306 - val_mse: 0.0628 - learning_rate: 0.1000 - val_custom_mse: 0.2792 - val_custom_mae: 0.3640\n",
            "Epoch 83/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1208 - mae: 0.1562 - mse: 0.1208 - val_loss: 0.0626 - val_mae: 0.1308 - val_mse: 0.0626 - learning_rate: 0.1000 - val_custom_mse: 0.2783 - val_custom_mae: 0.3629\n",
            "Epoch 84/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1208 - mae: 0.1562 - mse: 0.1208 - val_loss: 0.0630 - val_mae: 0.1304 - val_mse: 0.0630 - learning_rate: 0.1000 - val_custom_mse: 0.2809 - val_custom_mae: 0.3652\n",
            "Epoch 85/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1207 - mae: 0.1559 - mse: 0.1207 - val_loss: 0.0627 - val_mae: 0.1302 - val_mse: 0.0627 - learning_rate: 0.1000 - val_custom_mse: 0.2797 - val_custom_mae: 0.3641\n",
            "Epoch 86/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1204 - mae: 0.1557 - mse: 0.1204 - val_loss: 0.0628 - val_mae: 0.1302 - val_mse: 0.0628 - learning_rate: 0.1000 - val_custom_mse: 0.2802 - val_custom_mae: 0.3651\n",
            "Epoch 87/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1205 - mae: 0.1556 - mse: 0.1205 - val_loss: 0.0626 - val_mae: 0.1300 - val_mse: 0.0626 - learning_rate: 0.1000 - val_custom_mse: 0.2798 - val_custom_mae: 0.3644\n",
            "Epoch 88/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1204 - mae: 0.1555 - mse: 0.1204 - val_loss: 0.0628 - val_mae: 0.1300 - val_mse: 0.0628 - learning_rate: 0.1000 - val_custom_mse: 0.2810 - val_custom_mae: 0.3655\n",
            "Epoch 89/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1204 - mae: 0.1554 - mse: 0.1204 - val_loss: 0.0633 - val_mae: 0.1307 - val_mse: 0.0633 - learning_rate: 0.1000 - val_custom_mse: 0.2833 - val_custom_mae: 0.3676\n",
            "Epoch 90/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1203 - mae: 0.1552 - mse: 0.1203 - val_loss: 0.0625 - val_mae: 0.1296 - val_mse: 0.0625 - learning_rate: 0.1000 - val_custom_mse: 0.2800 - val_custom_mae: 0.3646\n",
            "Epoch 91/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1204 - mae: 0.1551 - mse: 0.1204 - val_loss: 0.0623 - val_mae: 0.1295 - val_mse: 0.0623 - learning_rate: 0.1000 - val_custom_mse: 0.2791 - val_custom_mae: 0.3639\n",
            "Epoch 92/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1203 - mae: 0.1551 - mse: 0.1203 - val_loss: 0.0624 - val_mae: 0.1294 - val_mse: 0.0624 - learning_rate: 0.1000 - val_custom_mse: 0.2796 - val_custom_mae: 0.3643\n",
            "Epoch 93/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1201 - mae: 0.1549 - mse: 0.1201 - val_loss: 0.0622 - val_mae: 0.1293 - val_mse: 0.0622 - learning_rate: 0.1000 - val_custom_mse: 0.2786 - val_custom_mae: 0.3634\n",
            "Epoch 94/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1202 - mae: 0.1549 - mse: 0.1202 - val_loss: 0.0629 - val_mae: 0.1297 - val_mse: 0.0629 - learning_rate: 0.1000 - val_custom_mse: 0.2825 - val_custom_mae: 0.3668\n",
            "Epoch 95/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1201 - mae: 0.1548 - mse: 0.1201 - val_loss: 0.0641 - val_mae: 0.1321 - val_mse: 0.0641 - learning_rate: 0.1000 - val_custom_mse: 0.2867 - val_custom_mae: 0.3707\n",
            "Epoch 96/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1204 - mae: 0.1552 - mse: 0.1204 - val_loss: 0.0624 - val_mae: 0.1292 - val_mse: 0.0624 - learning_rate: 0.1000 - val_custom_mse: 0.2801 - val_custom_mae: 0.3647\n",
            "Epoch 97/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1201 - mae: 0.1549 - mse: 0.1201 - val_loss: 0.0635 - val_mae: 0.1307 - val_mse: 0.0635 - learning_rate: 0.1000 - val_custom_mse: 0.2849 - val_custom_mae: 0.3690\n",
            "Epoch 98/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1201 - mae: 0.1547 - mse: 0.1201 - val_loss: 0.0623 - val_mae: 0.1289 - val_mse: 0.0623 - learning_rate: 0.1000 - val_custom_mse: 0.2800 - val_custom_mae: 0.3647\n",
            "Epoch 99/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1199 - mae: 0.1545 - mse: 0.1199 - val_loss: 0.0622 - val_mae: 0.1288 - val_mse: 0.0622 - learning_rate: 0.1000 - val_custom_mse: 0.2797 - val_custom_mae: 0.3642\n",
            "Epoch 100/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1198 - mae: 0.1544 - mse: 0.1198 - val_loss: 0.0623 - val_mae: 0.1288 - val_mse: 0.0623 - learning_rate: 0.1000 - val_custom_mse: 0.2806 - val_custom_mae: 0.3652\n",
            "Running experiment: horizon=192, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_39', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "233/233 - 6s - 25ms/step - loss: 0.5398 - mae: 0.4534 - mse: 0.5398 - val_loss: 0.3183 - val_mae: 0.4080 - val_mse: 0.3183 - learning_rate: 0.1000 - val_custom_mse: 0.4923 - val_custom_mae: 0.5121\n",
            "Epoch 2/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4732 - mae: 0.4241 - mse: 0.4732 - val_loss: 0.2737 - val_mae: 0.3783 - val_mse: 0.2737 - learning_rate: 0.1000 - val_custom_mse: 0.4479 - val_custom_mae: 0.4868\n",
            "Epoch 3/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4110 - mae: 0.3941 - mse: 0.4110 - val_loss: 0.2290 - val_mae: 0.3437 - val_mse: 0.2290 - learning_rate: 0.1000 - val_custom_mse: 0.3995 - val_custom_mae: 0.4553\n",
            "Epoch 4/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3495 - mae: 0.3614 - mse: 0.3495 - val_loss: 0.1929 - val_mae: 0.3121 - val_mse: 0.1929 - learning_rate: 0.1000 - val_custom_mse: 0.3702 - val_custom_mae: 0.4343\n",
            "Epoch 5/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2961 - mae: 0.3303 - mse: 0.2961 - val_loss: 0.1638 - val_mae: 0.2843 - val_mse: 0.1638 - learning_rate: 0.1000 - val_custom_mse: 0.3480 - val_custom_mae: 0.4182\n",
            "Epoch 6/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2539 - mae: 0.3032 - mse: 0.2539 - val_loss: 0.1413 - val_mae: 0.2608 - val_mse: 0.1413 - learning_rate: 0.1000 - val_custom_mse: 0.3287 - val_custom_mae: 0.4038\n",
            "Epoch 7/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2237 - mae: 0.2818 - mse: 0.2237 - val_loss: 0.1276 - val_mae: 0.2448 - val_mse: 0.1276 - learning_rate: 0.1000 - val_custom_mse: 0.3194 - val_custom_mae: 0.3969\n",
            "Epoch 8/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2039 - mae: 0.2664 - mse: 0.2039 - val_loss: 0.1192 - val_mae: 0.2341 - val_mse: 0.1192 - learning_rate: 0.1000 - val_custom_mse: 0.3150 - val_custom_mae: 0.3937\n",
            "Epoch 9/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1912 - mae: 0.2556 - mse: 0.1912 - val_loss: 0.1120 - val_mae: 0.2248 - val_mse: 0.1120 - learning_rate: 0.1000 - val_custom_mse: 0.3057 - val_custom_mae: 0.3860\n",
            "Epoch 10/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1828 - mae: 0.2475 - mse: 0.1828 - val_loss: 0.1073 - val_mae: 0.2181 - val_mse: 0.1073 - learning_rate: 0.1000 - val_custom_mse: 0.3019 - val_custom_mae: 0.3830\n",
            "Epoch 11/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1766 - mae: 0.2411 - mse: 0.1766 - val_loss: 0.1042 - val_mae: 0.2131 - val_mse: 0.1042 - learning_rate: 0.1000 - val_custom_mse: 0.3021 - val_custom_mae: 0.3834\n",
            "Epoch 12/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1718 - mae: 0.2358 - mse: 0.1718 - val_loss: 0.1007 - val_mae: 0.2076 - val_mse: 0.1007 - learning_rate: 0.1000 - val_custom_mse: 0.2989 - val_custom_mae: 0.3807\n",
            "Epoch 13/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1679 - mae: 0.2311 - mse: 0.1679 - val_loss: 0.0976 - val_mae: 0.2025 - val_mse: 0.0976 - learning_rate: 0.1000 - val_custom_mse: 0.2960 - val_custom_mae: 0.3781\n",
            "Epoch 14/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1645 - mae: 0.2269 - mse: 0.1645 - val_loss: 0.0951 - val_mae: 0.1984 - val_mse: 0.0951 - learning_rate: 0.1000 - val_custom_mse: 0.2944 - val_custom_mae: 0.3768\n",
            "Epoch 15/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1616 - mae: 0.2232 - mse: 0.1616 - val_loss: 0.0934 - val_mae: 0.1950 - val_mse: 0.0934 - learning_rate: 0.1000 - val_custom_mse: 0.2949 - val_custom_mae: 0.3775\n",
            "Epoch 16/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1592 - mae: 0.2200 - mse: 0.1592 - val_loss: 0.0926 - val_mae: 0.1930 - val_mse: 0.0926 - learning_rate: 0.1000 - val_custom_mse: 0.2980 - val_custom_mae: 0.3805\n",
            "Epoch 17/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1570 - mae: 0.2171 - mse: 0.1570 - val_loss: 0.0893 - val_mae: 0.1879 - val_mse: 0.0893 - learning_rate: 0.1000 - val_custom_mse: 0.2913 - val_custom_mae: 0.3743\n",
            "Epoch 18/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1550 - mae: 0.2143 - mse: 0.1550 - val_loss: 0.0882 - val_mae: 0.1855 - val_mse: 0.0882 - learning_rate: 0.1000 - val_custom_mse: 0.2924 - val_custom_mae: 0.3755\n",
            "Epoch 19/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1532 - mae: 0.2119 - mse: 0.1532 - val_loss: 0.0863 - val_mae: 0.1824 - val_mse: 0.0863 - learning_rate: 0.1000 - val_custom_mse: 0.2891 - val_custom_mae: 0.3724\n",
            "Epoch 20/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1519 - mae: 0.2099 - mse: 0.1519 - val_loss: 0.0848 - val_mae: 0.1798 - val_mse: 0.0848 - learning_rate: 0.1000 - val_custom_mse: 0.2868 - val_custom_mae: 0.3703\n",
            "Epoch 21/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1506 - mae: 0.2079 - mse: 0.1506 - val_loss: 0.0842 - val_mae: 0.1780 - val_mse: 0.0842 - learning_rate: 0.1000 - val_custom_mse: 0.2892 - val_custom_mae: 0.3727\n",
            "Epoch 22/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1492 - mae: 0.2062 - mse: 0.1492 - val_loss: 0.0829 - val_mae: 0.1757 - val_mse: 0.0829 - learning_rate: 0.1000 - val_custom_mse: 0.2876 - val_custom_mae: 0.3711\n",
            "Epoch 23/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1480 - mae: 0.2045 - mse: 0.1480 - val_loss: 0.0823 - val_mae: 0.1743 - val_mse: 0.0823 - learning_rate: 0.1000 - val_custom_mse: 0.2881 - val_custom_mae: 0.3717\n",
            "Epoch 24/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1472 - mae: 0.2031 - mse: 0.1472 - val_loss: 0.0813 - val_mae: 0.1723 - val_mse: 0.0813 - learning_rate: 0.1000 - val_custom_mse: 0.2874 - val_custom_mae: 0.3709\n",
            "Epoch 25/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1462 - mae: 0.2018 - mse: 0.1462 - val_loss: 0.0804 - val_mae: 0.1708 - val_mse: 0.0804 - learning_rate: 0.1000 - val_custom_mse: 0.2860 - val_custom_mae: 0.3699\n",
            "Epoch 26/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1456 - mae: 0.2006 - mse: 0.1456 - val_loss: 0.0796 - val_mae: 0.1691 - val_mse: 0.0796 - learning_rate: 0.1000 - val_custom_mse: 0.2853 - val_custom_mae: 0.3691\n",
            "Epoch 27/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1447 - mae: 0.1994 - mse: 0.1447 - val_loss: 0.0790 - val_mae: 0.1679 - val_mse: 0.0790 - learning_rate: 0.1000 - val_custom_mse: 0.2855 - val_custom_mae: 0.3693\n",
            "Epoch 28/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1440 - mae: 0.1984 - mse: 0.1440 - val_loss: 0.0781 - val_mae: 0.1666 - val_mse: 0.0781 - learning_rate: 0.1000 - val_custom_mse: 0.2836 - val_custom_mae: 0.3675\n",
            "Epoch 29/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1436 - mae: 0.1975 - mse: 0.1436 - val_loss: 0.0780 - val_mae: 0.1658 - val_mse: 0.0780 - learning_rate: 0.1000 - val_custom_mse: 0.2858 - val_custom_mae: 0.3695\n",
            "Epoch 30/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1428 - mae: 0.1965 - mse: 0.1428 - val_loss: 0.0783 - val_mae: 0.1658 - val_mse: 0.0783 - learning_rate: 0.1000 - val_custom_mse: 0.2886 - val_custom_mae: 0.3723\n",
            "Epoch 31/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1422 - mae: 0.1958 - mse: 0.1422 - val_loss: 0.0782 - val_mae: 0.1653 - val_mse: 0.0782 - learning_rate: 0.1000 - val_custom_mse: 0.2897 - val_custom_mae: 0.3734\n",
            "Epoch 32/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1419 - mae: 0.1950 - mse: 0.1419 - val_loss: 0.0765 - val_mae: 0.1629 - val_mse: 0.0765 - learning_rate: 0.1000 - val_custom_mse: 0.2841 - val_custom_mae: 0.3682\n",
            "Epoch 33/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1413 - mae: 0.1942 - mse: 0.1413 - val_loss: 0.0761 - val_mae: 0.1619 - val_mse: 0.0761 - learning_rate: 0.1000 - val_custom_mse: 0.2845 - val_custom_mae: 0.3683\n",
            "Epoch 34/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1409 - mae: 0.1936 - mse: 0.1409 - val_loss: 0.0755 - val_mae: 0.1612 - val_mse: 0.0755 - learning_rate: 0.1000 - val_custom_mse: 0.2832 - val_custom_mae: 0.3673\n",
            "Epoch 35/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1404 - mae: 0.1929 - mse: 0.1404 - val_loss: 0.0751 - val_mae: 0.1602 - val_mse: 0.0751 - learning_rate: 0.1000 - val_custom_mse: 0.2831 - val_custom_mae: 0.3672\n",
            "Epoch 36/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1402 - mae: 0.1924 - mse: 0.1402 - val_loss: 0.0750 - val_mae: 0.1598 - val_mse: 0.0750 - learning_rate: 0.1000 - val_custom_mse: 0.2844 - val_custom_mae: 0.3684\n",
            "Epoch 37/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1399 - mae: 0.1919 - mse: 0.1399 - val_loss: 0.0751 - val_mae: 0.1596 - val_mse: 0.0751 - learning_rate: 0.1000 - val_custom_mse: 0.2859 - val_custom_mae: 0.3698\n",
            "Epoch 38/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1392 - mae: 0.1913 - mse: 0.1392 - val_loss: 0.0746 - val_mae: 0.1587 - val_mse: 0.0746 - learning_rate: 0.1000 - val_custom_mse: 0.2850 - val_custom_mae: 0.3689\n",
            "Epoch 39/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1393 - mae: 0.1910 - mse: 0.1393 - val_loss: 0.0743 - val_mae: 0.1582 - val_mse: 0.0743 - learning_rate: 0.1000 - val_custom_mse: 0.2847 - val_custom_mae: 0.3688\n",
            "Epoch 40/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1388 - mae: 0.1904 - mse: 0.1388 - val_loss: 0.0741 - val_mae: 0.1577 - val_mse: 0.0741 - learning_rate: 0.1000 - val_custom_mse: 0.2850 - val_custom_mae: 0.3690\n",
            "Epoch 41/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1385 - mae: 0.1900 - mse: 0.1385 - val_loss: 0.0736 - val_mae: 0.1568 - val_mse: 0.0736 - learning_rate: 0.1000 - val_custom_mse: 0.2835 - val_custom_mae: 0.3675\n",
            "Epoch 42/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1383 - mae: 0.1895 - mse: 0.1383 - val_loss: 0.0737 - val_mae: 0.1566 - val_mse: 0.0737 - learning_rate: 0.1000 - val_custom_mse: 0.2848 - val_custom_mae: 0.3687\n",
            "Epoch 43/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1380 - mae: 0.1892 - mse: 0.1380 - val_loss: 0.0734 - val_mae: 0.1564 - val_mse: 0.0734 - learning_rate: 0.1000 - val_custom_mse: 0.2844 - val_custom_mae: 0.3686\n",
            "Epoch 44/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1379 - mae: 0.1889 - mse: 0.1379 - val_loss: 0.0728 - val_mae: 0.1555 - val_mse: 0.0728 - learning_rate: 0.1000 - val_custom_mse: 0.2826 - val_custom_mae: 0.3668\n",
            "Epoch 45/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1376 - mae: 0.1885 - mse: 0.1376 - val_loss: 0.0729 - val_mae: 0.1553 - val_mse: 0.0729 - learning_rate: 0.1000 - val_custom_mse: 0.2837 - val_custom_mae: 0.3678\n",
            "Epoch 46/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1373 - mae: 0.1881 - mse: 0.1373 - val_loss: 0.0728 - val_mae: 0.1550 - val_mse: 0.0728 - learning_rate: 0.1000 - val_custom_mse: 0.2841 - val_custom_mae: 0.3681\n",
            "Epoch 47/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1372 - mae: 0.1878 - mse: 0.1372 - val_loss: 0.0727 - val_mae: 0.1547 - val_mse: 0.0727 - learning_rate: 0.1000 - val_custom_mse: 0.2844 - val_custom_mae: 0.3685\n",
            "Epoch 48/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1370 - mae: 0.1877 - mse: 0.1370 - val_loss: 0.0721 - val_mae: 0.1541 - val_mse: 0.0721 - learning_rate: 0.1000 - val_custom_mse: 0.2821 - val_custom_mae: 0.3663\n",
            "Epoch 49/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1369 - mae: 0.1873 - mse: 0.1369 - val_loss: 0.0733 - val_mae: 0.1555 - val_mse: 0.0733 - learning_rate: 0.1000 - val_custom_mse: 0.2879 - val_custom_mae: 0.3715\n",
            "Epoch 50/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1367 - mae: 0.1872 - mse: 0.1367 - val_loss: 0.0722 - val_mae: 0.1538 - val_mse: 0.0722 - learning_rate: 0.1000 - val_custom_mse: 0.2841 - val_custom_mae: 0.3681\n",
            "Epoch 51/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1364 - mae: 0.1868 - mse: 0.1364 - val_loss: 0.0717 - val_mae: 0.1531 - val_mse: 0.0717 - learning_rate: 0.1000 - val_custom_mse: 0.2823 - val_custom_mae: 0.3662\n",
            "Epoch 52/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1364 - mae: 0.1866 - mse: 0.1364 - val_loss: 0.0717 - val_mae: 0.1529 - val_mse: 0.0717 - learning_rate: 0.1000 - val_custom_mse: 0.2832 - val_custom_mae: 0.3672\n",
            "Epoch 53/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1361 - mae: 0.1863 - mse: 0.1361 - val_loss: 0.0719 - val_mae: 0.1531 - val_mse: 0.0719 - learning_rate: 0.1000 - val_custom_mse: 0.2844 - val_custom_mae: 0.3684\n",
            "Epoch 54/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1360 - mae: 0.1861 - mse: 0.1360 - val_loss: 0.0718 - val_mae: 0.1527 - val_mse: 0.0718 - learning_rate: 0.1000 - val_custom_mse: 0.2842 - val_custom_mae: 0.3681\n",
            "Epoch 55/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1357 - mae: 0.1858 - mse: 0.1357 - val_loss: 0.0715 - val_mae: 0.1525 - val_mse: 0.0715 - learning_rate: 0.1000 - val_custom_mse: 0.2833 - val_custom_mae: 0.3675\n",
            "Epoch 56/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1355 - mae: 0.1856 - mse: 0.1355 - val_loss: 0.0712 - val_mae: 0.1520 - val_mse: 0.0712 - learning_rate: 0.1000 - val_custom_mse: 0.2825 - val_custom_mae: 0.3666\n",
            "Epoch 57/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1357 - mae: 0.1855 - mse: 0.1357 - val_loss: 0.0713 - val_mae: 0.1520 - val_mse: 0.0713 - learning_rate: 0.1000 - val_custom_mse: 0.2832 - val_custom_mae: 0.3673\n",
            "Epoch 58/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1355 - mae: 0.1853 - mse: 0.1355 - val_loss: 0.0711 - val_mae: 0.1519 - val_mse: 0.0711 - learning_rate: 0.1000 - val_custom_mse: 0.2827 - val_custom_mae: 0.3669\n",
            "Epoch 59/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1354 - mae: 0.1853 - mse: 0.1354 - val_loss: 0.0707 - val_mae: 0.1513 - val_mse: 0.0707 - learning_rate: 0.1000 - val_custom_mse: 0.2812 - val_custom_mae: 0.3655\n",
            "Epoch 60/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1352 - mae: 0.1850 - mse: 0.1352 - val_loss: 0.0709 - val_mae: 0.1512 - val_mse: 0.0709 - learning_rate: 0.1000 - val_custom_mse: 0.2824 - val_custom_mae: 0.3666\n",
            "Epoch 61/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1352 - mae: 0.1848 - mse: 0.1352 - val_loss: 0.0708 - val_mae: 0.1512 - val_mse: 0.0708 - learning_rate: 0.1000 - val_custom_mse: 0.2821 - val_custom_mae: 0.3663\n",
            "Epoch 62/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1352 - mae: 0.1847 - mse: 0.1352 - val_loss: 0.0709 - val_mae: 0.1511 - val_mse: 0.0709 - learning_rate: 0.1000 - val_custom_mse: 0.2831 - val_custom_mae: 0.3671\n",
            "Epoch 63/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1350 - mae: 0.1845 - mse: 0.1350 - val_loss: 0.0704 - val_mae: 0.1506 - val_mse: 0.0704 - learning_rate: 0.1000 - val_custom_mse: 0.2811 - val_custom_mae: 0.3654\n",
            "Epoch 64/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1349 - mae: 0.1844 - mse: 0.1349 - val_loss: 0.0703 - val_mae: 0.1505 - val_mse: 0.0703 - learning_rate: 0.1000 - val_custom_mse: 0.2809 - val_custom_mae: 0.3653\n",
            "Epoch 65/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1347 - mae: 0.1843 - mse: 0.1347 - val_loss: 0.0706 - val_mae: 0.1508 - val_mse: 0.0706 - learning_rate: 0.1000 - val_custom_mse: 0.2825 - val_custom_mae: 0.3668\n",
            "Epoch 66/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1347 - mae: 0.1842 - mse: 0.1347 - val_loss: 0.0702 - val_mae: 0.1502 - val_mse: 0.0702 - learning_rate: 0.1000 - val_custom_mse: 0.2805 - val_custom_mae: 0.3650\n",
            "Epoch 67/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1347 - mae: 0.1840 - mse: 0.1347 - val_loss: 0.0708 - val_mae: 0.1510 - val_mse: 0.0708 - learning_rate: 0.1000 - val_custom_mse: 0.2835 - val_custom_mae: 0.3680\n",
            "Epoch 68/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1344 - mae: 0.1839 - mse: 0.1344 - val_loss: 0.0703 - val_mae: 0.1502 - val_mse: 0.0703 - learning_rate: 0.1000 - val_custom_mse: 0.2816 - val_custom_mae: 0.3659\n",
            "Epoch 69/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1346 - mae: 0.1838 - mse: 0.1346 - val_loss: 0.0703 - val_mae: 0.1501 - val_mse: 0.0703 - learning_rate: 0.1000 - val_custom_mse: 0.2822 - val_custom_mae: 0.3663\n",
            "Epoch 70/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1345 - mae: 0.1838 - mse: 0.1345 - val_loss: 0.0701 - val_mae: 0.1500 - val_mse: 0.0701 - learning_rate: 0.1000 - val_custom_mse: 0.2813 - val_custom_mae: 0.3655\n",
            "Epoch 71/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1342 - mae: 0.1836 - mse: 0.1342 - val_loss: 0.0702 - val_mae: 0.1501 - val_mse: 0.0702 - learning_rate: 0.1000 - val_custom_mse: 0.2817 - val_custom_mae: 0.3662\n",
            "Epoch 72/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1343 - mae: 0.1836 - mse: 0.1343 - val_loss: 0.0699 - val_mae: 0.1498 - val_mse: 0.0699 - learning_rate: 0.1000 - val_custom_mse: 0.2804 - val_custom_mae: 0.3649\n",
            "Epoch 73/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1342 - mae: 0.1835 - mse: 0.1342 - val_loss: 0.0702 - val_mae: 0.1498 - val_mse: 0.0702 - learning_rate: 0.1000 - val_custom_mse: 0.2821 - val_custom_mae: 0.3664\n",
            "Epoch 74/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1341 - mae: 0.1834 - mse: 0.1341 - val_loss: 0.0699 - val_mae: 0.1497 - val_mse: 0.0699 - learning_rate: 0.1000 - val_custom_mse: 0.2808 - val_custom_mae: 0.3653\n",
            "Epoch 75/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1340 - mae: 0.1833 - mse: 0.1340 - val_loss: 0.0699 - val_mae: 0.1494 - val_mse: 0.0699 - learning_rate: 0.1000 - val_custom_mse: 0.2811 - val_custom_mae: 0.3656\n",
            "Epoch 76/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1338 - mae: 0.1831 - mse: 0.1338 - val_loss: 0.0698 - val_mae: 0.1494 - val_mse: 0.0698 - learning_rate: 0.1000 - val_custom_mse: 0.2809 - val_custom_mae: 0.3652\n",
            "Epoch 77/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1337 - mae: 0.1830 - mse: 0.1337 - val_loss: 0.0695 - val_mae: 0.1493 - val_mse: 0.0695 - learning_rate: 0.1000 - val_custom_mse: 0.2792 - val_custom_mae: 0.3637\n",
            "Epoch 78/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1337 - mae: 0.1830 - mse: 0.1337 - val_loss: 0.0709 - val_mae: 0.1506 - val_mse: 0.0709 - learning_rate: 0.1000 - val_custom_mse: 0.2856 - val_custom_mae: 0.3694\n",
            "Epoch 79/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1339 - mae: 0.1831 - mse: 0.1339 - val_loss: 0.0700 - val_mae: 0.1494 - val_mse: 0.0700 - learning_rate: 0.1000 - val_custom_mse: 0.2824 - val_custom_mae: 0.3665\n",
            "Epoch 80/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1336 - mae: 0.1829 - mse: 0.1336 - val_loss: 0.0697 - val_mae: 0.1492 - val_mse: 0.0697 - learning_rate: 0.1000 - val_custom_mse: 0.2810 - val_custom_mae: 0.3654\n",
            "Epoch 81/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1336 - mae: 0.1827 - mse: 0.1336 - val_loss: 0.0696 - val_mae: 0.1489 - val_mse: 0.0696 - learning_rate: 0.1000 - val_custom_mse: 0.2808 - val_custom_mae: 0.3652\n",
            "Epoch 82/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1335 - mae: 0.1827 - mse: 0.1335 - val_loss: 0.0701 - val_mae: 0.1494 - val_mse: 0.0701 - learning_rate: 0.1000 - val_custom_mse: 0.2827 - val_custom_mae: 0.3671\n",
            "Epoch 83/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1333 - mae: 0.1826 - mse: 0.1333 - val_loss: 0.0698 - val_mae: 0.1490 - val_mse: 0.0698 - learning_rate: 0.1000 - val_custom_mse: 0.2816 - val_custom_mae: 0.3658\n",
            "Epoch 84/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1333 - mae: 0.1825 - mse: 0.1333 - val_loss: 0.0696 - val_mae: 0.1489 - val_mse: 0.0696 - learning_rate: 0.1000 - val_custom_mse: 0.2810 - val_custom_mae: 0.3654\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1333 - mae: 0.1825 - mse: 0.1333 - val_loss: 0.0698 - val_mae: 0.1489 - val_mse: 0.0698 - learning_rate: 0.1000 - val_custom_mse: 0.2822 - val_custom_mae: 0.3664\n",
            "Epoch 86/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1334 - mae: 0.1823 - mse: 0.1334 - val_loss: 0.0697 - val_mae: 0.1489 - val_mse: 0.0697 - learning_rate: 0.0200 - val_custom_mse: 0.2814 - val_custom_mae: 0.3658\n",
            "Epoch 87/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1333 - mae: 0.1823 - mse: 0.1333 - val_loss: 0.0698 - val_mae: 0.1489 - val_mse: 0.0698 - learning_rate: 0.0200 - val_custom_mse: 0.2818 - val_custom_mae: 0.3661\n",
            "Epoch 88/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1335 - mae: 0.1823 - mse: 0.1335 - val_loss: 0.0697 - val_mae: 0.1489 - val_mse: 0.0697 - learning_rate: 0.0200 - val_custom_mse: 0.2815 - val_custom_mae: 0.3658\n",
            "Epoch 89/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1334 - mae: 0.1823 - mse: 0.1334 - val_loss: 0.0697 - val_mae: 0.1488 - val_mse: 0.0697 - learning_rate: 0.0200 - val_custom_mse: 0.2813 - val_custom_mae: 0.3657\n",
            "Epoch 90/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1331 - mae: 0.1822 - mse: 0.1331 - val_loss: 0.0697 - val_mae: 0.1489 - val_mse: 0.0697 - learning_rate: 0.0200 - val_custom_mse: 0.2815 - val_custom_mae: 0.3659\n",
            "Epoch 91/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1333 - mae: 0.1823 - mse: 0.1333 - val_loss: 0.0697 - val_mae: 0.1488 - val_mse: 0.0697 - learning_rate: 0.0200 - val_custom_mse: 0.2813 - val_custom_mae: 0.3656\n",
            "Epoch 92/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1332 - mae: 0.1823 - mse: 0.1332 - val_loss: 0.0697 - val_mae: 0.1489 - val_mse: 0.0697 - learning_rate: 0.0200 - val_custom_mse: 0.2817 - val_custom_mae: 0.3661\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1332 - mae: 0.1823 - mse: 0.1332 - val_loss: 0.0696 - val_mae: 0.1488 - val_mse: 0.0696 - learning_rate: 0.0200 - val_custom_mse: 0.2812 - val_custom_mae: 0.3656\n",
            "Epoch 94/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1335 - mae: 0.1823 - mse: 0.1335 - val_loss: 0.0697 - val_mae: 0.1488 - val_mse: 0.0697 - learning_rate: 0.0040 - val_custom_mse: 0.2816 - val_custom_mae: 0.3659\n",
            "Epoch 95/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1331 - mae: 0.1822 - mse: 0.1331 - val_loss: 0.0697 - val_mae: 0.1489 - val_mse: 0.0697 - learning_rate: 0.0040 - val_custom_mse: 0.2816 - val_custom_mae: 0.3660\n",
            "Epoch 96/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1331 - mae: 0.1822 - mse: 0.1331 - val_loss: 0.0697 - val_mae: 0.1489 - val_mse: 0.0697 - learning_rate: 0.0040 - val_custom_mse: 0.2815 - val_custom_mae: 0.3659\n",
            "Epoch 97/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1333 - mae: 0.1823 - mse: 0.1333 - val_loss: 0.0697 - val_mae: 0.1489 - val_mse: 0.0697 - learning_rate: 0.0040 - val_custom_mse: 0.2816 - val_custom_mae: 0.3660\n",
            "Epoch 98/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1332 - mae: 0.1822 - mse: 0.1332 - val_loss: 0.0697 - val_mae: 0.1489 - val_mse: 0.0697 - learning_rate: 0.0040 - val_custom_mse: 0.2815 - val_custom_mae: 0.3659\n",
            "Epoch 99/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1331 - mae: 0.1822 - mse: 0.1331 - val_loss: 0.0697 - val_mae: 0.1489 - val_mse: 0.0697 - learning_rate: 0.0040 - val_custom_mse: 0.2815 - val_custom_mae: 0.3659\n",
            "Epoch 100/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1333 - mae: 0.1823 - mse: 0.1333 - val_loss: 0.0697 - val_mae: 0.1488 - val_mse: 0.0697 - learning_rate: 0.0040 - val_custom_mse: 0.2813 - val_custom_mae: 0.3658\n",
            "Running experiment: horizon=192, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_40', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "233/233 - 6s - 26ms/step - loss: 0.5476 - mae: 0.4571 - mse: 0.5476 - val_loss: 0.3272 - val_mae: 0.4116 - val_mse: 0.3272 - learning_rate: 0.1000 - val_custom_mse: 0.5075 - val_custom_mae: 0.5202\n",
            "Epoch 2/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4733 - mae: 0.4246 - mse: 0.4733 - val_loss: 0.2803 - val_mae: 0.3814 - val_mse: 0.2803 - learning_rate: 0.1000 - val_custom_mse: 0.4561 - val_custom_mae: 0.4888\n",
            "Epoch 3/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.4115 - mae: 0.3954 - mse: 0.4115 - val_loss: 0.2336 - val_mae: 0.3469 - val_mse: 0.2336 - learning_rate: 0.1000 - val_custom_mse: 0.4017 - val_custom_mae: 0.4554\n",
            "Epoch 4/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.3527 - mae: 0.3639 - mse: 0.3527 - val_loss: 0.1968 - val_mae: 0.3162 - val_mse: 0.1968 - learning_rate: 0.1000 - val_custom_mse: 0.3741 - val_custom_mae: 0.4371\n",
            "Epoch 5/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2999 - mae: 0.3331 - mse: 0.2999 - val_loss: 0.1646 - val_mae: 0.2855 - val_mse: 0.1646 - learning_rate: 0.1000 - val_custom_mse: 0.3457 - val_custom_mae: 0.4165\n",
            "Epoch 6/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2576 - mae: 0.3059 - mse: 0.2576 - val_loss: 0.1428 - val_mae: 0.2626 - val_mse: 0.1428 - learning_rate: 0.1000 - val_custom_mse: 0.3303 - val_custom_mae: 0.4051\n",
            "Epoch 7/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2285 - mae: 0.2852 - mse: 0.2285 - val_loss: 0.1300 - val_mae: 0.2476 - val_mse: 0.1300 - learning_rate: 0.1000 - val_custom_mse: 0.3244 - val_custom_mae: 0.4009\n",
            "Epoch 8/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.2093 - mae: 0.2706 - mse: 0.2093 - val_loss: 0.1195 - val_mae: 0.2347 - val_mse: 0.1195 - learning_rate: 0.1000 - val_custom_mse: 0.3123 - val_custom_mae: 0.3911\n",
            "Epoch 9/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1975 - mae: 0.2607 - mse: 0.1975 - val_loss: 0.1136 - val_mae: 0.2268 - val_mse: 0.1136 - learning_rate: 0.1000 - val_custom_mse: 0.3082 - val_custom_mae: 0.3878\n",
            "Epoch 10/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1898 - mae: 0.2537 - mse: 0.1898 - val_loss: 0.1096 - val_mae: 0.2210 - val_mse: 0.1096 - learning_rate: 0.1000 - val_custom_mse: 0.3066 - val_custom_mae: 0.3867\n",
            "Epoch 11/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1842 - mae: 0.2482 - mse: 0.1842 - val_loss: 0.1054 - val_mae: 0.2150 - val_mse: 0.1054 - learning_rate: 0.1000 - val_custom_mse: 0.3016 - val_custom_mae: 0.3826\n",
            "Epoch 12/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1798 - mae: 0.2436 - mse: 0.1798 - val_loss: 0.1025 - val_mae: 0.2103 - val_mse: 0.1025 - learning_rate: 0.1000 - val_custom_mse: 0.2999 - val_custom_mae: 0.3811\n",
            "Epoch 13/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1764 - mae: 0.2396 - mse: 0.1764 - val_loss: 0.1001 - val_mae: 0.2064 - val_mse: 0.1001 - learning_rate: 0.1000 - val_custom_mse: 0.2995 - val_custom_mae: 0.3810\n",
            "Epoch 14/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1733 - mae: 0.2361 - mse: 0.1733 - val_loss: 0.0984 - val_mae: 0.2033 - val_mse: 0.0984 - learning_rate: 0.1000 - val_custom_mse: 0.3002 - val_custom_mae: 0.3818\n",
            "Epoch 15/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1706 - mae: 0.2329 - mse: 0.1706 - val_loss: 0.0959 - val_mae: 0.1992 - val_mse: 0.0959 - learning_rate: 0.1000 - val_custom_mse: 0.2969 - val_custom_mae: 0.3790\n",
            "Epoch 16/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1683 - mae: 0.2301 - mse: 0.1683 - val_loss: 0.0939 - val_mae: 0.1958 - val_mse: 0.0939 - learning_rate: 0.1000 - val_custom_mse: 0.2949 - val_custom_mae: 0.3772\n",
            "Epoch 17/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1662 - mae: 0.2277 - mse: 0.1662 - val_loss: 0.0924 - val_mae: 0.1932 - val_mse: 0.0924 - learning_rate: 0.1000 - val_custom_mse: 0.2943 - val_custom_mae: 0.3768\n",
            "Epoch 18/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1647 - mae: 0.2255 - mse: 0.1647 - val_loss: 0.0907 - val_mae: 0.1903 - val_mse: 0.0907 - learning_rate: 0.1000 - val_custom_mse: 0.2925 - val_custom_mae: 0.3751\n",
            "Epoch 19/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1628 - mae: 0.2235 - mse: 0.1628 - val_loss: 0.0899 - val_mae: 0.1884 - val_mse: 0.0899 - learning_rate: 0.1000 - val_custom_mse: 0.2933 - val_custom_mae: 0.3760\n",
            "Epoch 20/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1617 - mae: 0.2217 - mse: 0.1617 - val_loss: 0.0889 - val_mae: 0.1866 - val_mse: 0.0889 - learning_rate: 0.1000 - val_custom_mse: 0.2934 - val_custom_mae: 0.3760\n",
            "Epoch 21/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1603 - mae: 0.2201 - mse: 0.1603 - val_loss: 0.0875 - val_mae: 0.1843 - val_mse: 0.0875 - learning_rate: 0.1000 - val_custom_mse: 0.2912 - val_custom_mae: 0.3741\n",
            "Epoch 22/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1593 - mae: 0.2186 - mse: 0.1593 - val_loss: 0.0865 - val_mae: 0.1824 - val_mse: 0.0865 - learning_rate: 0.1000 - val_custom_mse: 0.2902 - val_custom_mae: 0.3731\n",
            "Epoch 23/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1583 - mae: 0.2174 - mse: 0.1583 - val_loss: 0.0855 - val_mae: 0.1806 - val_mse: 0.0855 - learning_rate: 0.1000 - val_custom_mse: 0.2889 - val_custom_mae: 0.3720\n",
            "Epoch 24/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1573 - mae: 0.2161 - mse: 0.1573 - val_loss: 0.0848 - val_mae: 0.1793 - val_mse: 0.0848 - learning_rate: 0.1000 - val_custom_mse: 0.2889 - val_custom_mae: 0.3720\n",
            "Epoch 25/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1565 - mae: 0.2150 - mse: 0.1565 - val_loss: 0.0841 - val_mae: 0.1781 - val_mse: 0.0841 - learning_rate: 0.1000 - val_custom_mse: 0.2884 - val_custom_mae: 0.3717\n",
            "Epoch 26/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1555 - mae: 0.2140 - mse: 0.1555 - val_loss: 0.0834 - val_mae: 0.1767 - val_mse: 0.0834 - learning_rate: 0.1000 - val_custom_mse: 0.2878 - val_custom_mae: 0.3711\n",
            "Epoch 27/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1551 - mae: 0.2130 - mse: 0.1551 - val_loss: 0.0830 - val_mae: 0.1757 - val_mse: 0.0830 - learning_rate: 0.1000 - val_custom_mse: 0.2880 - val_custom_mae: 0.3712\n",
            "Epoch 28/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1543 - mae: 0.2122 - mse: 0.1543 - val_loss: 0.0826 - val_mae: 0.1750 - val_mse: 0.0826 - learning_rate: 0.1000 - val_custom_mse: 0.2883 - val_custom_mae: 0.3716\n",
            "Epoch 29/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1540 - mae: 0.2115 - mse: 0.1540 - val_loss: 0.0820 - val_mae: 0.1739 - val_mse: 0.0820 - learning_rate: 0.1000 - val_custom_mse: 0.2876 - val_custom_mae: 0.3710\n",
            "Epoch 30/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1534 - mae: 0.2107 - mse: 0.1534 - val_loss: 0.0817 - val_mae: 0.1732 - val_mse: 0.0817 - learning_rate: 0.1000 - val_custom_mse: 0.2883 - val_custom_mae: 0.3716\n",
            "Epoch 31/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1529 - mae: 0.2101 - mse: 0.1529 - val_loss: 0.0815 - val_mae: 0.1726 - val_mse: 0.0815 - learning_rate: 0.1000 - val_custom_mse: 0.2883 - val_custom_mae: 0.3715\n",
            "Epoch 32/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1522 - mae: 0.2094 - mse: 0.1522 - val_loss: 0.0811 - val_mae: 0.1720 - val_mse: 0.0811 - learning_rate: 0.1000 - val_custom_mse: 0.2880 - val_custom_mae: 0.3715\n",
            "Epoch 33/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1518 - mae: 0.2088 - mse: 0.1518 - val_loss: 0.0808 - val_mae: 0.1713 - val_mse: 0.0808 - learning_rate: 0.1000 - val_custom_mse: 0.2882 - val_custom_mae: 0.3715\n",
            "Epoch 34/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1514 - mae: 0.2083 - mse: 0.1514 - val_loss: 0.0806 - val_mae: 0.1709 - val_mse: 0.0806 - learning_rate: 0.1000 - val_custom_mse: 0.2886 - val_custom_mae: 0.3720\n",
            "Epoch 35/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1511 - mae: 0.2078 - mse: 0.1511 - val_loss: 0.0801 - val_mae: 0.1702 - val_mse: 0.0801 - learning_rate: 0.1000 - val_custom_mse: 0.2870 - val_custom_mae: 0.3704\n",
            "Epoch 36/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1505 - mae: 0.2073 - mse: 0.1505 - val_loss: 0.0800 - val_mae: 0.1698 - val_mse: 0.0800 - learning_rate: 0.1000 - val_custom_mse: 0.2880 - val_custom_mae: 0.3713\n",
            "Epoch 37/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1502 - mae: 0.2069 - mse: 0.1502 - val_loss: 0.0797 - val_mae: 0.1691 - val_mse: 0.0797 - learning_rate: 0.1000 - val_custom_mse: 0.2877 - val_custom_mae: 0.3710\n",
            "Epoch 38/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1496 - mae: 0.2064 - mse: 0.1496 - val_loss: 0.0798 - val_mae: 0.1691 - val_mse: 0.0798 - learning_rate: 0.1000 - val_custom_mse: 0.2892 - val_custom_mae: 0.3724\n",
            "Epoch 39/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1496 - mae: 0.2061 - mse: 0.1496 - val_loss: 0.0792 - val_mae: 0.1683 - val_mse: 0.0792 - learning_rate: 0.1000 - val_custom_mse: 0.2873 - val_custom_mae: 0.3709\n",
            "Epoch 40/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1494 - mae: 0.2058 - mse: 0.1494 - val_loss: 0.0801 - val_mae: 0.1694 - val_mse: 0.0801 - learning_rate: 0.1000 - val_custom_mse: 0.2915 - val_custom_mae: 0.3744\n",
            "Epoch 41/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1492 - mae: 0.2056 - mse: 0.1492 - val_loss: 0.0783 - val_mae: 0.1668 - val_mse: 0.0783 - learning_rate: 0.1000 - val_custom_mse: 0.2853 - val_custom_mae: 0.3688\n",
            "Epoch 42/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1487 - mae: 0.2051 - mse: 0.1487 - val_loss: 0.0787 - val_mae: 0.1672 - val_mse: 0.0787 - learning_rate: 0.1000 - val_custom_mse: 0.2874 - val_custom_mae: 0.3706\n",
            "Epoch 43/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1486 - mae: 0.2049 - mse: 0.1486 - val_loss: 0.0792 - val_mae: 0.1682 - val_mse: 0.0792 - learning_rate: 0.1000 - val_custom_mse: 0.2891 - val_custom_mae: 0.3727\n",
            "Epoch 44/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1483 - mae: 0.2048 - mse: 0.1483 - val_loss: 0.0784 - val_mae: 0.1664 - val_mse: 0.0784 - learning_rate: 0.1000 - val_custom_mse: 0.2875 - val_custom_mae: 0.3705\n",
            "Epoch 45/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1480 - mae: 0.2043 - mse: 0.1480 - val_loss: 0.0785 - val_mae: 0.1666 - val_mse: 0.0785 - learning_rate: 0.1000 - val_custom_mse: 0.2884 - val_custom_mae: 0.3717\n",
            "Epoch 46/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1479 - mae: 0.2039 - mse: 0.1479 - val_loss: 0.0781 - val_mae: 0.1661 - val_mse: 0.0781 - learning_rate: 0.1000 - val_custom_mse: 0.2872 - val_custom_mae: 0.3706\n",
            "Epoch 47/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1476 - mae: 0.2037 - mse: 0.1476 - val_loss: 0.0780 - val_mae: 0.1658 - val_mse: 0.0780 - learning_rate: 0.1000 - val_custom_mse: 0.2873 - val_custom_mae: 0.3707\n",
            "Epoch 48/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1475 - mae: 0.2035 - mse: 0.1475 - val_loss: 0.0774 - val_mae: 0.1650 - val_mse: 0.0774 - learning_rate: 0.1000 - val_custom_mse: 0.2851 - val_custom_mae: 0.3689\n",
            "Epoch 49/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1472 - mae: 0.2033 - mse: 0.1472 - val_loss: 0.0773 - val_mae: 0.1647 - val_mse: 0.0773 - learning_rate: 0.1000 - val_custom_mse: 0.2855 - val_custom_mae: 0.3690\n",
            "Epoch 50/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1472 - mae: 0.2031 - mse: 0.1472 - val_loss: 0.0771 - val_mae: 0.1645 - val_mse: 0.0771 - learning_rate: 0.1000 - val_custom_mse: 0.2846 - val_custom_mae: 0.3681\n",
            "Epoch 51/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1470 - mae: 0.2029 - mse: 0.1470 - val_loss: 0.0771 - val_mae: 0.1645 - val_mse: 0.0771 - learning_rate: 0.1000 - val_custom_mse: 0.2848 - val_custom_mae: 0.3683\n",
            "Epoch 52/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1468 - mae: 0.2027 - mse: 0.1468 - val_loss: 0.0769 - val_mae: 0.1641 - val_mse: 0.0769 - learning_rate: 0.1000 - val_custom_mse: 0.2846 - val_custom_mae: 0.3683\n",
            "Epoch 53/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1470 - mae: 0.2027 - mse: 0.1470 - val_loss: 0.0769 - val_mae: 0.1642 - val_mse: 0.0769 - learning_rate: 0.1000 - val_custom_mse: 0.2851 - val_custom_mae: 0.3687\n",
            "Epoch 54/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1466 - mae: 0.2024 - mse: 0.1466 - val_loss: 0.0768 - val_mae: 0.1638 - val_mse: 0.0768 - learning_rate: 0.1000 - val_custom_mse: 0.2850 - val_custom_mae: 0.3684\n",
            "Epoch 55/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1465 - mae: 0.2022 - mse: 0.1465 - val_loss: 0.0763 - val_mae: 0.1636 - val_mse: 0.0763 - learning_rate: 0.1000 - val_custom_mse: 0.2828 - val_custom_mae: 0.3668\n",
            "Epoch 56/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1464 - mae: 0.2022 - mse: 0.1464 - val_loss: 0.0765 - val_mae: 0.1634 - val_mse: 0.0765 - learning_rate: 0.1000 - val_custom_mse: 0.2842 - val_custom_mae: 0.3678\n",
            "Epoch 57/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1462 - mae: 0.2019 - mse: 0.1462 - val_loss: 0.0770 - val_mae: 0.1640 - val_mse: 0.0770 - learning_rate: 0.1000 - val_custom_mse: 0.2864 - val_custom_mae: 0.3698\n",
            "Epoch 58/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1461 - mae: 0.2018 - mse: 0.1461 - val_loss: 0.0769 - val_mae: 0.1637 - val_mse: 0.0769 - learning_rate: 0.1000 - val_custom_mse: 0.2867 - val_custom_mae: 0.3700\n",
            "Epoch 59/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1459 - mae: 0.2017 - mse: 0.1459 - val_loss: 0.0767 - val_mae: 0.1636 - val_mse: 0.0767 - learning_rate: 0.1000 - val_custom_mse: 0.2855 - val_custom_mae: 0.3692\n",
            "Epoch 60/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1457 - mae: 0.2015 - mse: 0.1457 - val_loss: 0.0764 - val_mae: 0.1630 - val_mse: 0.0764 - learning_rate: 0.1000 - val_custom_mse: 0.2849 - val_custom_mae: 0.3685\n",
            "Epoch 61/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1459 - mae: 0.2014 - mse: 0.1459 - val_loss: 0.0768 - val_mae: 0.1635 - val_mse: 0.0768 - learning_rate: 0.1000 - val_custom_mse: 0.2862 - val_custom_mae: 0.3697\n",
            "Epoch 62/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1456 - mae: 0.2013 - mse: 0.1456 - val_loss: 0.0763 - val_mae: 0.1627 - val_mse: 0.0763 - learning_rate: 0.1000 - val_custom_mse: 0.2848 - val_custom_mae: 0.3682\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1455 - mae: 0.2012 - mse: 0.1455 - val_loss: 0.0764 - val_mae: 0.1629 - val_mse: 0.0764 - learning_rate: 0.1000 - val_custom_mse: 0.2856 - val_custom_mae: 0.3689\n",
            "Epoch 64/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1453 - mae: 0.2010 - mse: 0.1453 - val_loss: 0.0765 - val_mae: 0.1631 - val_mse: 0.0765 - learning_rate: 0.0200 - val_custom_mse: 0.2854 - val_custom_mae: 0.3689\n",
            "Epoch 65/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1453 - mae: 0.2010 - mse: 0.1453 - val_loss: 0.0764 - val_mae: 0.1630 - val_mse: 0.0764 - learning_rate: 0.0200 - val_custom_mse: 0.2852 - val_custom_mae: 0.3688\n",
            "Epoch 66/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1453 - mae: 0.2010 - mse: 0.1453 - val_loss: 0.0764 - val_mae: 0.1630 - val_mse: 0.0764 - learning_rate: 0.0200 - val_custom_mse: 0.2853 - val_custom_mae: 0.3688\n",
            "Epoch 67/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1452 - mae: 0.2009 - mse: 0.1452 - val_loss: 0.0765 - val_mae: 0.1631 - val_mse: 0.0765 - learning_rate: 0.0200 - val_custom_mse: 0.2854 - val_custom_mae: 0.3690\n",
            "Epoch 68/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1451 - mae: 0.2009 - mse: 0.1451 - val_loss: 0.0763 - val_mae: 0.1628 - val_mse: 0.0763 - learning_rate: 0.0200 - val_custom_mse: 0.2846 - val_custom_mae: 0.3683\n",
            "Epoch 69/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1453 - mae: 0.2009 - mse: 0.1453 - val_loss: 0.0765 - val_mae: 0.1631 - val_mse: 0.0765 - learning_rate: 0.0200 - val_custom_mse: 0.2853 - val_custom_mae: 0.3690\n",
            "Epoch 70/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1455 - mae: 0.2010 - mse: 0.1455 - val_loss: 0.0763 - val_mae: 0.1629 - val_mse: 0.0763 - learning_rate: 0.0200 - val_custom_mse: 0.2849 - val_custom_mae: 0.3685\n",
            "Epoch 71/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1453 - mae: 0.2009 - mse: 0.1453 - val_loss: 0.0762 - val_mae: 0.1626 - val_mse: 0.0762 - learning_rate: 0.0200 - val_custom_mse: 0.2844 - val_custom_mae: 0.3680\n",
            "Epoch 72/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1451 - mae: 0.2008 - mse: 0.1451 - val_loss: 0.0763 - val_mae: 0.1629 - val_mse: 0.0763 - learning_rate: 0.0200 - val_custom_mse: 0.2848 - val_custom_mae: 0.3684\n",
            "Epoch 73/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1451 - mae: 0.2008 - mse: 0.1451 - val_loss: 0.0764 - val_mae: 0.1629 - val_mse: 0.0764 - learning_rate: 0.0200 - val_custom_mse: 0.2853 - val_custom_mae: 0.3689\n",
            "Epoch 74/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1450 - mae: 0.2008 - mse: 0.1450 - val_loss: 0.0764 - val_mae: 0.1630 - val_mse: 0.0764 - learning_rate: 0.0200 - val_custom_mse: 0.2851 - val_custom_mae: 0.3688\n",
            "Epoch 75/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1453 - mae: 0.2008 - mse: 0.1453 - val_loss: 0.0763 - val_mae: 0.1629 - val_mse: 0.0763 - learning_rate: 0.0200 - val_custom_mse: 0.2849 - val_custom_mae: 0.3685\n",
            "Epoch 76/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1452 - mae: 0.2007 - mse: 0.1452 - val_loss: 0.0763 - val_mae: 0.1628 - val_mse: 0.0763 - learning_rate: 0.0200 - val_custom_mse: 0.2850 - val_custom_mae: 0.3686\n",
            "Epoch 77/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1452 - mae: 0.2008 - mse: 0.1452 - val_loss: 0.0762 - val_mae: 0.1627 - val_mse: 0.0762 - learning_rate: 0.0200 - val_custom_mse: 0.2846 - val_custom_mae: 0.3683\n",
            "Epoch 78/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1452 - mae: 0.2007 - mse: 0.1452 - val_loss: 0.0762 - val_mae: 0.1627 - val_mse: 0.0762 - learning_rate: 0.0200 - val_custom_mse: 0.2846 - val_custom_mae: 0.3683\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1450 - mae: 0.2007 - mse: 0.1450 - val_loss: 0.0762 - val_mae: 0.1627 - val_mse: 0.0762 - learning_rate: 0.0200 - val_custom_mse: 0.2848 - val_custom_mae: 0.3684\n",
            "Epoch 80/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1451 - mae: 0.2006 - mse: 0.1451 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2842 - val_custom_mae: 0.3680\n",
            "Epoch 81/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1450 - mae: 0.2006 - mse: 0.1450 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2840 - val_custom_mae: 0.3678\n",
            "Epoch 82/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1452 - mae: 0.2007 - mse: 0.1452 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2839 - val_custom_mae: 0.3678\n",
            "Epoch 83/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1451 - mae: 0.2006 - mse: 0.1451 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2840 - val_custom_mae: 0.3679\n",
            "Epoch 84/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1451 - mae: 0.2007 - mse: 0.1451 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2840 - val_custom_mae: 0.3679\n",
            "Epoch 85/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1449 - mae: 0.2007 - mse: 0.1449 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2839 - val_custom_mae: 0.3678\n",
            "Epoch 86/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1454 - mae: 0.2008 - mse: 0.1454 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2840 - val_custom_mae: 0.3679\n",
            "Epoch 87/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1450 - mae: 0.2006 - mse: 0.1450 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2840 - val_custom_mae: 0.3679\n",
            "Epoch 88/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1449 - mae: 0.2006 - mse: 0.1449 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2840 - val_custom_mae: 0.3679\n",
            "Epoch 89/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1448 - mae: 0.2006 - mse: 0.1448 - val_loss: 0.0761 - val_mae: 0.1627 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2842 - val_custom_mae: 0.3681\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1451 - mae: 0.2006 - mse: 0.1451 - val_loss: 0.0761 - val_mae: 0.1627 - val_mse: 0.0761 - learning_rate: 0.0040 - val_custom_mse: 0.2843 - val_custom_mae: 0.3681\n",
            "Epoch 91/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1452 - mae: 0.2006 - mse: 0.1452 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2841 - val_custom_mae: 0.3680\n",
            "Epoch 92/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1450 - mae: 0.2006 - mse: 0.1450 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2841 - val_custom_mae: 0.3680\n",
            "Epoch 93/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1450 - mae: 0.2006 - mse: 0.1450 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2840 - val_custom_mae: 0.3679\n",
            "Epoch 94/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1449 - mae: 0.2006 - mse: 0.1449 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2840 - val_custom_mae: 0.3679\n",
            "Epoch 95/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1451 - mae: 0.2006 - mse: 0.1451 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2840 - val_custom_mae: 0.3679\n",
            "Epoch 96/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1447 - mae: 0.2005 - mse: 0.1447 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2840 - val_custom_mae: 0.3679\n",
            "Epoch 97/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1450 - mae: 0.2006 - mse: 0.1450 - val_loss: 0.0760 - val_mae: 0.1626 - val_mse: 0.0760 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2839 - val_custom_mae: 0.3679\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 98: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "233/233 - 2s - 8ms/step - loss: 0.1448 - mae: 0.2006 - mse: 0.1448 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2839 - val_custom_mae: 0.3679\n",
            "Epoch 99/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1449 - mae: 0.2006 - mse: 0.1449 - val_loss: 0.0760 - val_mae: 0.1626 - val_mse: 0.0760 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2839 - val_custom_mae: 0.3679\n",
            "Epoch 100/100\n",
            "233/233 - 2s - 8ms/step - loss: 0.1450 - mae: 0.2006 - mse: 0.1450 - val_loss: 0.0761 - val_mae: 0.1626 - val_mse: 0.0761 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2839 - val_custom_mae: 0.3679\n",
            "Running experiment: horizon=336, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_41', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228/228 - 6s - 26ms/step - loss: 0.6150 - mae: 0.4848 - mse: 0.6150 - val_loss: 0.3919 - val_mae: 0.4505 - val_mse: 0.3919 - learning_rate: 0.1000 - val_custom_mse: 0.5964 - val_custom_mae: 0.5631\n",
            "Epoch 2/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.5382 - mae: 0.4522 - mse: 0.5382 - val_loss: 0.3533 - val_mae: 0.4247 - val_mse: 0.3533 - learning_rate: 0.1000 - val_custom_mse: 0.5616 - val_custom_mae: 0.5420\n",
            "Epoch 3/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4788 - mae: 0.4262 - mse: 0.4788 - val_loss: 0.3108 - val_mae: 0.3959 - val_mse: 0.3108 - learning_rate: 0.1000 - val_custom_mse: 0.5223 - val_custom_mae: 0.5203\n",
            "Epoch 4/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4282 - mae: 0.3995 - mse: 0.4282 - val_loss: 0.2692 - val_mae: 0.3638 - val_mse: 0.2692 - learning_rate: 0.1000 - val_custom_mse: 0.4776 - val_custom_mae: 0.4932\n",
            "Epoch 5/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3813 - mae: 0.3716 - mse: 0.3813 - val_loss: 0.2345 - val_mae: 0.3334 - val_mse: 0.2345 - learning_rate: 0.1000 - val_custom_mse: 0.4462 - val_custom_mae: 0.4729\n",
            "Epoch 6/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3416 - mae: 0.3455 - mse: 0.3416 - val_loss: 0.2091 - val_mae: 0.3082 - val_mse: 0.2091 - learning_rate: 0.1000 - val_custom_mse: 0.4260 - val_custom_mae: 0.4590\n",
            "Epoch 7/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3116 - mae: 0.3234 - mse: 0.3116 - val_loss: 0.1937 - val_mae: 0.2912 - val_mse: 0.1937 - learning_rate: 0.1000 - val_custom_mse: 0.4171 - val_custom_mae: 0.4528\n",
            "Epoch 8/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2908 - mae: 0.3066 - mse: 0.2908 - val_loss: 0.1817 - val_mae: 0.2771 - val_mse: 0.1817 - learning_rate: 0.1000 - val_custom_mse: 0.4054 - val_custom_mae: 0.4440\n",
            "Epoch 9/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2772 - mae: 0.2944 - mse: 0.2772 - val_loss: 0.1750 - val_mae: 0.2686 - val_mse: 0.1750 - learning_rate: 0.1000 - val_custom_mse: 0.4003 - val_custom_mae: 0.4403\n",
            "Epoch 10/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2680 - mae: 0.2853 - mse: 0.2680 - val_loss: 0.1696 - val_mae: 0.2616 - val_mse: 0.1696 - learning_rate: 0.1000 - val_custom_mse: 0.3945 - val_custom_mae: 0.4360\n",
            "Epoch 11/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2613 - mae: 0.2783 - mse: 0.2613 - val_loss: 0.1661 - val_mae: 0.2564 - val_mse: 0.1661 - learning_rate: 0.1000 - val_custom_mse: 0.3925 - val_custom_mae: 0.4344\n",
            "Epoch 12/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2560 - mae: 0.2723 - mse: 0.2560 - val_loss: 0.1629 - val_mae: 0.2513 - val_mse: 0.1629 - learning_rate: 0.1000 - val_custom_mse: 0.3898 - val_custom_mae: 0.4322\n",
            "Epoch 13/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2515 - mae: 0.2670 - mse: 0.2515 - val_loss: 0.1607 - val_mae: 0.2475 - val_mse: 0.1607 - learning_rate: 0.1000 - val_custom_mse: 0.3896 - val_custom_mae: 0.4320\n",
            "Epoch 14/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2477 - mae: 0.2622 - mse: 0.2477 - val_loss: 0.1581 - val_mae: 0.2431 - val_mse: 0.1581 - learning_rate: 0.1000 - val_custom_mse: 0.3876 - val_custom_mae: 0.4302\n",
            "Epoch 15/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2443 - mae: 0.2579 - mse: 0.2443 - val_loss: 0.1562 - val_mae: 0.2398 - val_mse: 0.1562 - learning_rate: 0.1000 - val_custom_mse: 0.3867 - val_custom_mae: 0.4297\n",
            "Epoch 16/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2413 - mae: 0.2539 - mse: 0.2413 - val_loss: 0.1538 - val_mae: 0.2358 - val_mse: 0.1538 - learning_rate: 0.1000 - val_custom_mse: 0.3840 - val_custom_mae: 0.4275\n",
            "Epoch 17/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2387 - mae: 0.2503 - mse: 0.2387 - val_loss: 0.1524 - val_mae: 0.2328 - val_mse: 0.1524 - learning_rate: 0.1000 - val_custom_mse: 0.3841 - val_custom_mae: 0.4274\n",
            "Epoch 18/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2362 - mae: 0.2470 - mse: 0.2362 - val_loss: 0.1514 - val_mae: 0.2300 - val_mse: 0.1514 - learning_rate: 0.1000 - val_custom_mse: 0.3849 - val_custom_mae: 0.4275\n",
            "Epoch 19/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2340 - mae: 0.2439 - mse: 0.2340 - val_loss: 0.1499 - val_mae: 0.2276 - val_mse: 0.1499 - learning_rate: 0.1000 - val_custom_mse: 0.3836 - val_custom_mae: 0.4270\n",
            "Epoch 20/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2321 - mae: 0.2411 - mse: 0.2321 - val_loss: 0.1492 - val_mae: 0.2256 - val_mse: 0.1492 - learning_rate: 0.1000 - val_custom_mse: 0.3846 - val_custom_mae: 0.4277\n",
            "Epoch 21/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2303 - mae: 0.2384 - mse: 0.2303 - val_loss: 0.1477 - val_mae: 0.2233 - val_mse: 0.1477 - learning_rate: 0.1000 - val_custom_mse: 0.3827 - val_custom_mae: 0.4267\n",
            "Epoch 22/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2286 - mae: 0.2359 - mse: 0.2286 - val_loss: 0.1464 - val_mae: 0.2206 - val_mse: 0.1464 - learning_rate: 0.1000 - val_custom_mse: 0.3816 - val_custom_mae: 0.4252\n",
            "Epoch 23/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2272 - mae: 0.2336 - mse: 0.2272 - val_loss: 0.1459 - val_mae: 0.2186 - val_mse: 0.1459 - learning_rate: 0.1000 - val_custom_mse: 0.3830 - val_custom_mae: 0.4261\n",
            "Epoch 24/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2257 - mae: 0.2315 - mse: 0.2257 - val_loss: 0.1444 - val_mae: 0.2165 - val_mse: 0.1444 - learning_rate: 0.1000 - val_custom_mse: 0.3809 - val_custom_mae: 0.4249\n",
            "Epoch 25/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2244 - mae: 0.2295 - mse: 0.2244 - val_loss: 0.1445 - val_mae: 0.2151 - val_mse: 0.1445 - learning_rate: 0.1000 - val_custom_mse: 0.3834 - val_custom_mae: 0.4262\n",
            "Epoch 26/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2232 - mae: 0.2276 - mse: 0.2232 - val_loss: 0.1430 - val_mae: 0.2134 - val_mse: 0.1430 - learning_rate: 0.1000 - val_custom_mse: 0.3804 - val_custom_mae: 0.4247\n",
            "Epoch 27/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2221 - mae: 0.2258 - mse: 0.2221 - val_loss: 0.1425 - val_mae: 0.2116 - val_mse: 0.1425 - learning_rate: 0.1000 - val_custom_mse: 0.3813 - val_custom_mae: 0.4251\n",
            "Epoch 28/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2210 - mae: 0.2239 - mse: 0.2210 - val_loss: 0.1419 - val_mae: 0.2101 - val_mse: 0.1419 - learning_rate: 0.1000 - val_custom_mse: 0.3809 - val_custom_mae: 0.4246\n",
            "Epoch 29/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2200 - mae: 0.2224 - mse: 0.2200 - val_loss: 0.1416 - val_mae: 0.2088 - val_mse: 0.1416 - learning_rate: 0.1000 - val_custom_mse: 0.3821 - val_custom_mae: 0.4256\n",
            "Epoch 30/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2191 - mae: 0.2208 - mse: 0.2191 - val_loss: 0.1403 - val_mae: 0.2071 - val_mse: 0.1403 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4239\n",
            "Epoch 31/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2182 - mae: 0.2193 - mse: 0.2182 - val_loss: 0.1410 - val_mae: 0.2063 - val_mse: 0.1410 - learning_rate: 0.1000 - val_custom_mse: 0.3833 - val_custom_mae: 0.4260\n",
            "Epoch 32/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2174 - mae: 0.2179 - mse: 0.2174 - val_loss: 0.1396 - val_mae: 0.2050 - val_mse: 0.1396 - learning_rate: 0.1000 - val_custom_mse: 0.3806 - val_custom_mae: 0.4250\n",
            "Epoch 33/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2166 - mae: 0.2165 - mse: 0.2166 - val_loss: 0.1392 - val_mae: 0.2034 - val_mse: 0.1392 - learning_rate: 0.1000 - val_custom_mse: 0.3810 - val_custom_mae: 0.4250\n",
            "Epoch 34/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2158 - mae: 0.2151 - mse: 0.2158 - val_loss: 0.1384 - val_mae: 0.2024 - val_mse: 0.1384 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4242\n",
            "Epoch 35/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2151 - mae: 0.2139 - mse: 0.2151 - val_loss: 0.1383 - val_mae: 0.2009 - val_mse: 0.1383 - learning_rate: 0.1000 - val_custom_mse: 0.3811 - val_custom_mae: 0.4248\n",
            "Epoch 36/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2145 - mae: 0.2128 - mse: 0.2145 - val_loss: 0.1384 - val_mae: 0.2005 - val_mse: 0.1384 - learning_rate: 0.1000 - val_custom_mse: 0.3823 - val_custom_mae: 0.4258\n",
            "Epoch 37/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2138 - mae: 0.2115 - mse: 0.2138 - val_loss: 0.1376 - val_mae: 0.1991 - val_mse: 0.1376 - learning_rate: 0.1000 - val_custom_mse: 0.3814 - val_custom_mae: 0.4249\n",
            "Epoch 38/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2131 - mae: 0.2102 - mse: 0.2131 - val_loss: 0.1368 - val_mae: 0.1985 - val_mse: 0.1368 - learning_rate: 0.1000 - val_custom_mse: 0.3797 - val_custom_mae: 0.4246\n",
            "Epoch 39/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2125 - mae: 0.2092 - mse: 0.2125 - val_loss: 0.1367 - val_mae: 0.1970 - val_mse: 0.1367 - learning_rate: 0.1000 - val_custom_mse: 0.3809 - val_custom_mae: 0.4247\n",
            "Epoch 40/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2119 - mae: 0.2081 - mse: 0.2119 - val_loss: 0.1362 - val_mae: 0.1962 - val_mse: 0.1362 - learning_rate: 0.1000 - val_custom_mse: 0.3803 - val_custom_mae: 0.4244\n",
            "Epoch 41/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2113 - mae: 0.2071 - mse: 0.2113 - val_loss: 0.1362 - val_mae: 0.1953 - val_mse: 0.1362 - learning_rate: 0.1000 - val_custom_mse: 0.3815 - val_custom_mae: 0.4249\n",
            "Epoch 42/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2108 - mae: 0.2060 - mse: 0.2108 - val_loss: 0.1357 - val_mae: 0.1942 - val_mse: 0.1357 - learning_rate: 0.1000 - val_custom_mse: 0.3809 - val_custom_mae: 0.4245\n",
            "Epoch 43/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2103 - mae: 0.2050 - mse: 0.2103 - val_loss: 0.1350 - val_mae: 0.1930 - val_mse: 0.1350 - learning_rate: 0.1000 - val_custom_mse: 0.3800 - val_custom_mae: 0.4238\n",
            "Epoch 44/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2098 - mae: 0.2040 - mse: 0.2098 - val_loss: 0.1347 - val_mae: 0.1918 - val_mse: 0.1347 - learning_rate: 0.1000 - val_custom_mse: 0.3805 - val_custom_mae: 0.4242\n",
            "Epoch 45/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2093 - mae: 0.2031 - mse: 0.2093 - val_loss: 0.1341 - val_mae: 0.1911 - val_mse: 0.1341 - learning_rate: 0.1000 - val_custom_mse: 0.3791 - val_custom_mae: 0.4237\n",
            "Epoch 46/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2089 - mae: 0.2022 - mse: 0.2089 - val_loss: 0.1344 - val_mae: 0.1909 - val_mse: 0.1344 - learning_rate: 0.1000 - val_custom_mse: 0.3806 - val_custom_mae: 0.4243\n",
            "Epoch 47/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2085 - mae: 0.2013 - mse: 0.2085 - val_loss: 0.1340 - val_mae: 0.1896 - val_mse: 0.1340 - learning_rate: 0.1000 - val_custom_mse: 0.3807 - val_custom_mae: 0.4242\n",
            "Epoch 48/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2080 - mae: 0.2004 - mse: 0.2080 - val_loss: 0.1336 - val_mae: 0.1891 - val_mse: 0.1336 - learning_rate: 0.1000 - val_custom_mse: 0.3802 - val_custom_mae: 0.4245\n",
            "Epoch 49/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2076 - mae: 0.1996 - mse: 0.2076 - val_loss: 0.1329 - val_mae: 0.1881 - val_mse: 0.1329 - learning_rate: 0.1000 - val_custom_mse: 0.3788 - val_custom_mae: 0.4231\n",
            "Epoch 50/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2072 - mae: 0.1987 - mse: 0.2072 - val_loss: 0.1324 - val_mae: 0.1870 - val_mse: 0.1324 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4232\n",
            "Epoch 51/100\n",
            "228/228 - 2s - 9ms/step - loss: 0.2068 - mae: 0.1979 - mse: 0.2068 - val_loss: 0.1326 - val_mae: 0.1869 - val_mse: 0.1326 - learning_rate: 0.1000 - val_custom_mse: 0.3795 - val_custom_mae: 0.4241\n",
            "Epoch 52/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2065 - mae: 0.1971 - mse: 0.2065 - val_loss: 0.1329 - val_mae: 0.1870 - val_mse: 0.1329 - learning_rate: 0.1000 - val_custom_mse: 0.3805 - val_custom_mae: 0.4244\n",
            "Epoch 53/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2061 - mae: 0.1964 - mse: 0.2061 - val_loss: 0.1325 - val_mae: 0.1850 - val_mse: 0.1325 - learning_rate: 0.1000 - val_custom_mse: 0.3809 - val_custom_mae: 0.4247\n",
            "Epoch 54/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2059 - mae: 0.1958 - mse: 0.2059 - val_loss: 0.1324 - val_mae: 0.1847 - val_mse: 0.1324 - learning_rate: 0.1000 - val_custom_mse: 0.3808 - val_custom_mae: 0.4246\n",
            "Epoch 55/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2054 - mae: 0.1948 - mse: 0.2054 - val_loss: 0.1320 - val_mae: 0.1846 - val_mse: 0.1320 - learning_rate: 0.1000 - val_custom_mse: 0.3802 - val_custom_mae: 0.4246\n",
            "Epoch 56/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2052 - mae: 0.1943 - mse: 0.2052 - val_loss: 0.1317 - val_mae: 0.1833 - val_mse: 0.1317 - learning_rate: 0.1000 - val_custom_mse: 0.3803 - val_custom_mae: 0.4242\n",
            "Epoch 57/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2048 - mae: 0.1935 - mse: 0.2048 - val_loss: 0.1317 - val_mae: 0.1820 - val_mse: 0.1317 - learning_rate: 0.1000 - val_custom_mse: 0.3812 - val_custom_mae: 0.4244\n",
            "Epoch 58/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2045 - mae: 0.1927 - mse: 0.2045 - val_loss: 0.1309 - val_mae: 0.1819 - val_mse: 0.1309 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4234\n",
            "Epoch 59/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2041 - mae: 0.1921 - mse: 0.2041 - val_loss: 0.1317 - val_mae: 0.1813 - val_mse: 0.1317 - learning_rate: 0.1000 - val_custom_mse: 0.3821 - val_custom_mae: 0.4251\n",
            "Epoch 60/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2039 - mae: 0.1914 - mse: 0.2039 - val_loss: 0.1311 - val_mae: 0.1810 - val_mse: 0.1311 - learning_rate: 0.1000 - val_custom_mse: 0.3808 - val_custom_mae: 0.4248\n",
            "Epoch 61/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2036 - mae: 0.1909 - mse: 0.2036 - val_loss: 0.1306 - val_mae: 0.1805 - val_mse: 0.1306 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4237\n",
            "Epoch 62/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2034 - mae: 0.1902 - mse: 0.2034 - val_loss: 0.1300 - val_mae: 0.1795 - val_mse: 0.1300 - learning_rate: 0.1000 - val_custom_mse: 0.3785 - val_custom_mae: 0.4234\n",
            "Epoch 63/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2031 - mae: 0.1896 - mse: 0.2031 - val_loss: 0.1305 - val_mae: 0.1794 - val_mse: 0.1305 - learning_rate: 0.1000 - val_custom_mse: 0.3804 - val_custom_mae: 0.4241\n",
            "Epoch 64/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2029 - mae: 0.1890 - mse: 0.2029 - val_loss: 0.1302 - val_mae: 0.1779 - val_mse: 0.1302 - learning_rate: 0.1000 - val_custom_mse: 0.3804 - val_custom_mae: 0.4240\n",
            "Epoch 65/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2027 - mae: 0.1886 - mse: 0.2027 - val_loss: 0.1300 - val_mae: 0.1771 - val_mse: 0.1300 - learning_rate: 0.1000 - val_custom_mse: 0.3807 - val_custom_mae: 0.4243\n",
            "Epoch 66/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2024 - mae: 0.1879 - mse: 0.2024 - val_loss: 0.1300 - val_mae: 0.1771 - val_mse: 0.1300 - learning_rate: 0.1000 - val_custom_mse: 0.3805 - val_custom_mae: 0.4241\n",
            "Epoch 67/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2022 - mae: 0.1874 - mse: 0.2022 - val_loss: 0.1301 - val_mae: 0.1766 - val_mse: 0.1301 - learning_rate: 0.1000 - val_custom_mse: 0.3813 - val_custom_mae: 0.4245\n",
            "Epoch 68/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2020 - mae: 0.1869 - mse: 0.2020 - val_loss: 0.1305 - val_mae: 0.1766 - val_mse: 0.1305 - learning_rate: 0.1000 - val_custom_mse: 0.3830 - val_custom_mae: 0.4259\n",
            "Epoch 69/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2018 - mae: 0.1864 - mse: 0.2018 - val_loss: 0.1305 - val_mae: 0.1760 - val_mse: 0.1305 - learning_rate: 0.1000 - val_custom_mse: 0.3835 - val_custom_mae: 0.4260\n",
            "Epoch 70/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2016 - mae: 0.1858 - mse: 0.2016 - val_loss: 0.1295 - val_mae: 0.1752 - val_mse: 0.1295 - learning_rate: 0.1000 - val_custom_mse: 0.3809 - val_custom_mae: 0.4243\n",
            "Epoch 71/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2014 - mae: 0.1853 - mse: 0.2014 - val_loss: 0.1292 - val_mae: 0.1748 - val_mse: 0.1292 - learning_rate: 0.1000 - val_custom_mse: 0.3803 - val_custom_mae: 0.4243\n",
            "Epoch 72/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2011 - mae: 0.1848 - mse: 0.2011 - val_loss: 0.1292 - val_mae: 0.1745 - val_mse: 0.1292 - learning_rate: 0.1000 - val_custom_mse: 0.3805 - val_custom_mae: 0.4243\n",
            "Epoch 73/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2010 - mae: 0.1843 - mse: 0.2010 - val_loss: 0.1288 - val_mae: 0.1738 - val_mse: 0.1288 - learning_rate: 0.1000 - val_custom_mse: 0.3797 - val_custom_mae: 0.4241\n",
            "Epoch 74/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2008 - mae: 0.1839 - mse: 0.2008 - val_loss: 0.1288 - val_mae: 0.1737 - val_mse: 0.1288 - learning_rate: 0.1000 - val_custom_mse: 0.3799 - val_custom_mae: 0.4242\n",
            "Epoch 75/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2007 - mae: 0.1834 - mse: 0.2007 - val_loss: 0.1287 - val_mae: 0.1731 - val_mse: 0.1287 - learning_rate: 0.1000 - val_custom_mse: 0.3801 - val_custom_mae: 0.4242\n",
            "Epoch 76/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2005 - mae: 0.1830 - mse: 0.2005 - val_loss: 0.1286 - val_mae: 0.1735 - val_mse: 0.1286 - learning_rate: 0.1000 - val_custom_mse: 0.3795 - val_custom_mae: 0.4238\n",
            "Epoch 77/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2003 - mae: 0.1826 - mse: 0.2003 - val_loss: 0.1286 - val_mae: 0.1718 - val_mse: 0.1286 - learning_rate: 0.1000 - val_custom_mse: 0.3805 - val_custom_mae: 0.4240\n",
            "Epoch 78/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2002 - mae: 0.1822 - mse: 0.2002 - val_loss: 0.1287 - val_mae: 0.1720 - val_mse: 0.1287 - learning_rate: 0.1000 - val_custom_mse: 0.3808 - val_custom_mae: 0.4246\n",
            "Epoch 79/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2000 - mae: 0.1817 - mse: 0.2000 - val_loss: 0.1280 - val_mae: 0.1717 - val_mse: 0.1280 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4234\n",
            "Epoch 80/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1999 - mae: 0.1813 - mse: 0.1999 - val_loss: 0.1282 - val_mae: 0.1704 - val_mse: 0.1282 - learning_rate: 0.1000 - val_custom_mse: 0.3802 - val_custom_mae: 0.4239\n",
            "Epoch 81/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1997 - mae: 0.1809 - mse: 0.1997 - val_loss: 0.1294 - val_mae: 0.1710 - val_mse: 0.1294 - learning_rate: 0.1000 - val_custom_mse: 0.3839 - val_custom_mae: 0.4259\n",
            "Epoch 82/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1996 - mae: 0.1806 - mse: 0.1996 - val_loss: 0.1290 - val_mae: 0.1703 - val_mse: 0.1290 - learning_rate: 0.1000 - val_custom_mse: 0.3831 - val_custom_mae: 0.4254\n",
            "Epoch 83/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1995 - mae: 0.1803 - mse: 0.1995 - val_loss: 0.1280 - val_mae: 0.1708 - val_mse: 0.1280 - learning_rate: 0.1000 - val_custom_mse: 0.3798 - val_custom_mae: 0.4241\n",
            "Epoch 84/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1993 - mae: 0.1798 - mse: 0.1993 - val_loss: 0.1285 - val_mae: 0.1698 - val_mse: 0.1285 - learning_rate: 0.1000 - val_custom_mse: 0.3821 - val_custom_mae: 0.4254\n",
            "Epoch 85/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1992 - mae: 0.1793 - mse: 0.1992 - val_loss: 0.1278 - val_mae: 0.1695 - val_mse: 0.1278 - learning_rate: 0.1000 - val_custom_mse: 0.3800 - val_custom_mae: 0.4242\n",
            "Epoch 86/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1991 - mae: 0.1791 - mse: 0.1991 - val_loss: 0.1279 - val_mae: 0.1690 - val_mse: 0.1279 - learning_rate: 0.1000 - val_custom_mse: 0.3807 - val_custom_mae: 0.4245\n",
            "Epoch 87/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1990 - mae: 0.1788 - mse: 0.1990 - val_loss: 0.1284 - val_mae: 0.1688 - val_mse: 0.1284 - learning_rate: 0.1000 - val_custom_mse: 0.3824 - val_custom_mae: 0.4254\n",
            "Epoch 88/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1989 - mae: 0.1785 - mse: 0.1989 - val_loss: 0.1276 - val_mae: 0.1679 - val_mse: 0.1276 - learning_rate: 0.1000 - val_custom_mse: 0.3803 - val_custom_mae: 0.4244\n",
            "Epoch 89/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1988 - mae: 0.1780 - mse: 0.1988 - val_loss: 0.1280 - val_mae: 0.1680 - val_mse: 0.1280 - learning_rate: 0.1000 - val_custom_mse: 0.3815 - val_custom_mae: 0.4246\n",
            "Epoch 90/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1986 - mae: 0.1777 - mse: 0.1986 - val_loss: 0.1280 - val_mae: 0.1675 - val_mse: 0.1280 - learning_rate: 0.1000 - val_custom_mse: 0.3819 - val_custom_mae: 0.4252\n",
            "Epoch 91/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1986 - mae: 0.1775 - mse: 0.1986 - val_loss: 0.1277 - val_mae: 0.1677 - val_mse: 0.1277 - learning_rate: 0.1000 - val_custom_mse: 0.3810 - val_custom_mae: 0.4246\n",
            "Epoch 92/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1984 - mae: 0.1771 - mse: 0.1984 - val_loss: 0.1280 - val_mae: 0.1676 - val_mse: 0.1280 - learning_rate: 0.1000 - val_custom_mse: 0.3821 - val_custom_mae: 0.4251\n",
            "Epoch 93/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1984 - mae: 0.1769 - mse: 0.1984 - val_loss: 0.1280 - val_mae: 0.1671 - val_mse: 0.1280 - learning_rate: 0.1000 - val_custom_mse: 0.3823 - val_custom_mae: 0.4252\n",
            "Epoch 94/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1983 - mae: 0.1765 - mse: 0.1983 - val_loss: 0.1278 - val_mae: 0.1663 - val_mse: 0.1278 - learning_rate: 0.1000 - val_custom_mse: 0.3821 - val_custom_mae: 0.4251\n",
            "Epoch 95/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1982 - mae: 0.1763 - mse: 0.1982 - val_loss: 0.1280 - val_mae: 0.1675 - val_mse: 0.1280 - learning_rate: 0.1000 - val_custom_mse: 0.3820 - val_custom_mae: 0.4248\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "228/228 - 2s - 8ms/step - loss: 0.1981 - mae: 0.1760 - mse: 0.1981 - val_loss: 0.1281 - val_mae: 0.1661 - val_mse: 0.1281 - learning_rate: 0.1000 - val_custom_mse: 0.3831 - val_custom_mae: 0.4259\n",
            "Epoch 97/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1979 - mae: 0.1754 - mse: 0.1979 - val_loss: 0.1271 - val_mae: 0.1661 - val_mse: 0.1271 - learning_rate: 0.0200 - val_custom_mse: 0.3800 - val_custom_mae: 0.4243\n",
            "Epoch 98/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1978 - mae: 0.1756 - mse: 0.1978 - val_loss: 0.1272 - val_mae: 0.1661 - val_mse: 0.1272 - learning_rate: 0.0200 - val_custom_mse: 0.3803 - val_custom_mae: 0.4245\n",
            "Epoch 99/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1978 - mae: 0.1755 - mse: 0.1978 - val_loss: 0.1272 - val_mae: 0.1660 - val_mse: 0.1272 - learning_rate: 0.0200 - val_custom_mse: 0.3803 - val_custom_mae: 0.4245\n",
            "Epoch 100/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.1978 - mae: 0.1754 - mse: 0.1978 - val_loss: 0.1270 - val_mae: 0.1658 - val_mse: 0.1270 - learning_rate: 0.0200 - val_custom_mse: 0.3800 - val_custom_mae: 0.4242\n",
            "Running experiment: horizon=336, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_42', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228/228 - 6s - 27ms/step - loss: 0.6168 - mae: 0.4861 - mse: 0.6168 - val_loss: 0.3930 - val_mae: 0.4511 - val_mse: 0.3930 - learning_rate: 0.1000 - val_custom_mse: 0.6029 - val_custom_mae: 0.5663\n",
            "Epoch 2/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.5397 - mae: 0.4528 - mse: 0.5397 - val_loss: 0.3492 - val_mae: 0.4227 - val_mse: 0.3492 - learning_rate: 0.1000 - val_custom_mse: 0.5494 - val_custom_mae: 0.5363\n",
            "Epoch 3/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4788 - mae: 0.4260 - mse: 0.4788 - val_loss: 0.3087 - val_mae: 0.3944 - val_mse: 0.3087 - learning_rate: 0.1000 - val_custom_mse: 0.5113 - val_custom_mae: 0.5138\n",
            "Epoch 4/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4282 - mae: 0.3995 - mse: 0.4282 - val_loss: 0.2672 - val_mae: 0.3623 - val_mse: 0.2672 - learning_rate: 0.1000 - val_custom_mse: 0.4706 - val_custom_mae: 0.4888\n",
            "Epoch 5/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3820 - mae: 0.3721 - mse: 0.3820 - val_loss: 0.2332 - val_mae: 0.3324 - val_mse: 0.2332 - learning_rate: 0.1000 - val_custom_mse: 0.4415 - val_custom_mae: 0.4697\n",
            "Epoch 6/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3435 - mae: 0.3469 - mse: 0.3435 - val_loss: 0.2105 - val_mae: 0.3096 - val_mse: 0.2105 - learning_rate: 0.1000 - val_custom_mse: 0.4288 - val_custom_mae: 0.4611\n",
            "Epoch 7/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3146 - mae: 0.3258 - mse: 0.3146 - val_loss: 0.1935 - val_mae: 0.2915 - val_mse: 0.1935 - learning_rate: 0.1000 - val_custom_mse: 0.4143 - val_custom_mae: 0.4509\n",
            "Epoch 8/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2950 - mae: 0.3101 - mse: 0.2950 - val_loss: 0.1838 - val_mae: 0.2801 - val_mse: 0.1838 - learning_rate: 0.1000 - val_custom_mse: 0.4075 - val_custom_mae: 0.4461\n",
            "Epoch 9/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2823 - mae: 0.2990 - mse: 0.2823 - val_loss: 0.1766 - val_mae: 0.2710 - val_mse: 0.1766 - learning_rate: 0.1000 - val_custom_mse: 0.4010 - val_custom_mae: 0.4411\n",
            "Epoch 10/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2738 - mae: 0.2909 - mse: 0.2738 - val_loss: 0.1725 - val_mae: 0.2653 - val_mse: 0.1725 - learning_rate: 0.1000 - val_custom_mse: 0.3984 - val_custom_mae: 0.4391\n",
            "Epoch 11/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2675 - mae: 0.2847 - mse: 0.2675 - val_loss: 0.1678 - val_mae: 0.2590 - val_mse: 0.1678 - learning_rate: 0.1000 - val_custom_mse: 0.3931 - val_custom_mae: 0.4349\n",
            "Epoch 12/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2628 - mae: 0.2795 - mse: 0.2628 - val_loss: 0.1657 - val_mae: 0.2554 - val_mse: 0.1657 - learning_rate: 0.1000 - val_custom_mse: 0.3932 - val_custom_mae: 0.4349\n",
            "Epoch 13/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2588 - mae: 0.2750 - mse: 0.2588 - val_loss: 0.1631 - val_mae: 0.2510 - val_mse: 0.1631 - learning_rate: 0.1000 - val_custom_mse: 0.3919 - val_custom_mae: 0.4335\n",
            "Epoch 14/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2552 - mae: 0.2710 - mse: 0.2552 - val_loss: 0.1601 - val_mae: 0.2471 - val_mse: 0.1601 - learning_rate: 0.1000 - val_custom_mse: 0.3882 - val_custom_mae: 0.4311\n",
            "Epoch 15/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2524 - mae: 0.2675 - mse: 0.2524 - val_loss: 0.1580 - val_mae: 0.2434 - val_mse: 0.1580 - learning_rate: 0.1000 - val_custom_mse: 0.3868 - val_custom_mae: 0.4299\n",
            "Epoch 16/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2496 - mae: 0.2642 - mse: 0.2496 - val_loss: 0.1561 - val_mae: 0.2404 - val_mse: 0.1561 - learning_rate: 0.1000 - val_custom_mse: 0.3855 - val_custom_mae: 0.4290\n",
            "Epoch 17/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2474 - mae: 0.2613 - mse: 0.2474 - val_loss: 0.1547 - val_mae: 0.2376 - val_mse: 0.1547 - learning_rate: 0.1000 - val_custom_mse: 0.3853 - val_custom_mae: 0.4286\n",
            "Epoch 18/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2455 - mae: 0.2587 - mse: 0.2455 - val_loss: 0.1533 - val_mae: 0.2352 - val_mse: 0.1533 - learning_rate: 0.1000 - val_custom_mse: 0.3842 - val_custom_mae: 0.4279\n",
            "Epoch 19/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2435 - mae: 0.2562 - mse: 0.2435 - val_loss: 0.1521 - val_mae: 0.2327 - val_mse: 0.1521 - learning_rate: 0.1000 - val_custom_mse: 0.3842 - val_custom_mae: 0.4277\n",
            "Epoch 20/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2418 - mae: 0.2540 - mse: 0.2418 - val_loss: 0.1509 - val_mae: 0.2304 - val_mse: 0.1509 - learning_rate: 0.1000 - val_custom_mse: 0.3837 - val_custom_mae: 0.4273\n",
            "Epoch 21/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2401 - mae: 0.2520 - mse: 0.2401 - val_loss: 0.1500 - val_mae: 0.2283 - val_mse: 0.1500 - learning_rate: 0.1000 - val_custom_mse: 0.3837 - val_custom_mae: 0.4269\n",
            "Epoch 22/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2389 - mae: 0.2500 - mse: 0.2389 - val_loss: 0.1489 - val_mae: 0.2267 - val_mse: 0.1489 - learning_rate: 0.1000 - val_custom_mse: 0.3825 - val_custom_mae: 0.4266\n",
            "Epoch 23/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2376 - mae: 0.2484 - mse: 0.2376 - val_loss: 0.1479 - val_mae: 0.2245 - val_mse: 0.1479 - learning_rate: 0.1000 - val_custom_mse: 0.3819 - val_custom_mae: 0.4257\n",
            "Epoch 24/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2364 - mae: 0.2468 - mse: 0.2364 - val_loss: 0.1469 - val_mae: 0.2229 - val_mse: 0.1469 - learning_rate: 0.1000 - val_custom_mse: 0.3809 - val_custom_mae: 0.4252\n",
            "Epoch 25/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2353 - mae: 0.2452 - mse: 0.2353 - val_loss: 0.1458 - val_mae: 0.2214 - val_mse: 0.1458 - learning_rate: 0.1000 - val_custom_mse: 0.3795 - val_custom_mae: 0.4244\n",
            "Epoch 26/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2345 - mae: 0.2438 - mse: 0.2345 - val_loss: 0.1456 - val_mae: 0.2203 - val_mse: 0.1456 - learning_rate: 0.1000 - val_custom_mse: 0.3809 - val_custom_mae: 0.4253\n",
            "Epoch 27/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2336 - mae: 0.2426 - mse: 0.2336 - val_loss: 0.1445 - val_mae: 0.2187 - val_mse: 0.1445 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4246\n",
            "Epoch 28/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2328 - mae: 0.2414 - mse: 0.2328 - val_loss: 0.1442 - val_mae: 0.2175 - val_mse: 0.1442 - learning_rate: 0.1000 - val_custom_mse: 0.3803 - val_custom_mae: 0.4246\n",
            "Epoch 29/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2322 - mae: 0.2404 - mse: 0.2322 - val_loss: 0.1435 - val_mae: 0.2162 - val_mse: 0.1435 - learning_rate: 0.1000 - val_custom_mse: 0.3795 - val_custom_mae: 0.4242\n",
            "Epoch 30/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2312 - mae: 0.2391 - mse: 0.2312 - val_loss: 0.1429 - val_mae: 0.2157 - val_mse: 0.1429 - learning_rate: 0.1000 - val_custom_mse: 0.3786 - val_custom_mae: 0.4239\n",
            "Epoch 31/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2308 - mae: 0.2382 - mse: 0.2308 - val_loss: 0.1424 - val_mae: 0.2139 - val_mse: 0.1424 - learning_rate: 0.1000 - val_custom_mse: 0.3793 - val_custom_mae: 0.4240\n",
            "Epoch 32/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2300 - mae: 0.2373 - mse: 0.2300 - val_loss: 0.1425 - val_mae: 0.2134 - val_mse: 0.1425 - learning_rate: 0.1000 - val_custom_mse: 0.3807 - val_custom_mae: 0.4249\n",
            "Epoch 33/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2294 - mae: 0.2363 - mse: 0.2294 - val_loss: 0.1415 - val_mae: 0.2121 - val_mse: 0.1415 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4236\n",
            "Epoch 34/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2289 - mae: 0.2355 - mse: 0.2289 - val_loss: 0.1418 - val_mae: 0.2115 - val_mse: 0.1418 - learning_rate: 0.1000 - val_custom_mse: 0.3811 - val_custom_mae: 0.4248\n",
            "Epoch 35/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2285 - mae: 0.2347 - mse: 0.2285 - val_loss: 0.1404 - val_mae: 0.2105 - val_mse: 0.1404 - learning_rate: 0.1000 - val_custom_mse: 0.3776 - val_custom_mae: 0.4225\n",
            "Epoch 36/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2279 - mae: 0.2339 - mse: 0.2279 - val_loss: 0.1400 - val_mae: 0.2094 - val_mse: 0.1400 - learning_rate: 0.1000 - val_custom_mse: 0.3780 - val_custom_mae: 0.4230\n",
            "Epoch 37/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2276 - mae: 0.2332 - mse: 0.2276 - val_loss: 0.1398 - val_mae: 0.2088 - val_mse: 0.1398 - learning_rate: 0.1000 - val_custom_mse: 0.3779 - val_custom_mae: 0.4229\n",
            "Epoch 38/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2270 - mae: 0.2324 - mse: 0.2270 - val_loss: 0.1398 - val_mae: 0.2080 - val_mse: 0.1398 - learning_rate: 0.1000 - val_custom_mse: 0.3794 - val_custom_mae: 0.4236\n",
            "Epoch 39/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2265 - mae: 0.2317 - mse: 0.2265 - val_loss: 0.1396 - val_mae: 0.2072 - val_mse: 0.1396 - learning_rate: 0.1000 - val_custom_mse: 0.3798 - val_custom_mae: 0.4238\n",
            "Epoch 40/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2261 - mae: 0.2310 - mse: 0.2261 - val_loss: 0.1389 - val_mae: 0.2067 - val_mse: 0.1389 - learning_rate: 0.1000 - val_custom_mse: 0.3780 - val_custom_mae: 0.4229\n",
            "Epoch 41/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2257 - mae: 0.2304 - mse: 0.2257 - val_loss: 0.1385 - val_mae: 0.2061 - val_mse: 0.1385 - learning_rate: 0.1000 - val_custom_mse: 0.3776 - val_custom_mae: 0.4229\n",
            "Epoch 42/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2255 - mae: 0.2299 - mse: 0.2255 - val_loss: 0.1390 - val_mae: 0.2059 - val_mse: 0.1390 - learning_rate: 0.1000 - val_custom_mse: 0.3801 - val_custom_mae: 0.4244\n",
            "Epoch 43/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2253 - mae: 0.2294 - mse: 0.2253 - val_loss: 0.1387 - val_mae: 0.2050 - val_mse: 0.1387 - learning_rate: 0.1000 - val_custom_mse: 0.3801 - val_custom_mae: 0.4242\n",
            "Epoch 44/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2248 - mae: 0.2288 - mse: 0.2248 - val_loss: 0.1380 - val_mae: 0.2041 - val_mse: 0.1380 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4233\n",
            "Epoch 45/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2244 - mae: 0.2283 - mse: 0.2244 - val_loss: 0.1381 - val_mae: 0.2037 - val_mse: 0.1381 - learning_rate: 0.1000 - val_custom_mse: 0.3798 - val_custom_mae: 0.4235\n",
            "Epoch 46/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2243 - mae: 0.2277 - mse: 0.2243 - val_loss: 0.1371 - val_mae: 0.2032 - val_mse: 0.1371 - learning_rate: 0.1000 - val_custom_mse: 0.3773 - val_custom_mae: 0.4228\n",
            "Epoch 47/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2239 - mae: 0.2274 - mse: 0.2239 - val_loss: 0.1380 - val_mae: 0.2029 - val_mse: 0.1380 - learning_rate: 0.1000 - val_custom_mse: 0.3810 - val_custom_mae: 0.4245\n",
            "Epoch 48/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2237 - mae: 0.2268 - mse: 0.2237 - val_loss: 0.1362 - val_mae: 0.2019 - val_mse: 0.1362 - learning_rate: 0.1000 - val_custom_mse: 0.3760 - val_custom_mae: 0.4215\n",
            "Epoch 49/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2235 - mae: 0.2265 - mse: 0.2235 - val_loss: 0.1369 - val_mae: 0.2017 - val_mse: 0.1369 - learning_rate: 0.1000 - val_custom_mse: 0.3785 - val_custom_mae: 0.4232\n",
            "Epoch 50/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2232 - mae: 0.2261 - mse: 0.2232 - val_loss: 0.1367 - val_mae: 0.2010 - val_mse: 0.1367 - learning_rate: 0.1000 - val_custom_mse: 0.3788 - val_custom_mae: 0.4229\n",
            "Epoch 51/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2229 - mae: 0.2256 - mse: 0.2229 - val_loss: 0.1373 - val_mae: 0.2015 - val_mse: 0.1373 - learning_rate: 0.1000 - val_custom_mse: 0.3808 - val_custom_mae: 0.4248\n",
            "Epoch 52/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2228 - mae: 0.2252 - mse: 0.2228 - val_loss: 0.1364 - val_mae: 0.2002 - val_mse: 0.1364 - learning_rate: 0.1000 - val_custom_mse: 0.3791 - val_custom_mae: 0.4233\n",
            "Epoch 53/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2226 - mae: 0.2248 - mse: 0.2226 - val_loss: 0.1362 - val_mae: 0.1998 - val_mse: 0.1362 - learning_rate: 0.1000 - val_custom_mse: 0.3789 - val_custom_mae: 0.4232\n",
            "Epoch 54/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2223 - mae: 0.2244 - mse: 0.2223 - val_loss: 0.1356 - val_mae: 0.1993 - val_mse: 0.1356 - learning_rate: 0.1000 - val_custom_mse: 0.3775 - val_custom_mae: 0.4226\n",
            "Epoch 55/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2222 - mae: 0.2241 - mse: 0.2222 - val_loss: 0.1365 - val_mae: 0.1994 - val_mse: 0.1365 - learning_rate: 0.1000 - val_custom_mse: 0.3810 - val_custom_mae: 0.4245\n",
            "Epoch 56/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2219 - mae: 0.2237 - mse: 0.2219 - val_loss: 0.1351 - val_mae: 0.1987 - val_mse: 0.1351 - learning_rate: 0.1000 - val_custom_mse: 0.3767 - val_custom_mae: 0.4223\n",
            "Epoch 57/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2217 - mae: 0.2235 - mse: 0.2217 - val_loss: 0.1358 - val_mae: 0.1981 - val_mse: 0.1358 - learning_rate: 0.1000 - val_custom_mse: 0.3800 - val_custom_mae: 0.4235\n",
            "Epoch 58/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2217 - mae: 0.2232 - mse: 0.2217 - val_loss: 0.1353 - val_mae: 0.1978 - val_mse: 0.1353 - learning_rate: 0.1000 - val_custom_mse: 0.3784 - val_custom_mae: 0.4230\n",
            "Epoch 59/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2214 - mae: 0.2228 - mse: 0.2214 - val_loss: 0.1357 - val_mae: 0.1978 - val_mse: 0.1357 - learning_rate: 0.1000 - val_custom_mse: 0.3799 - val_custom_mae: 0.4235\n",
            "Epoch 60/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2214 - mae: 0.2227 - mse: 0.2214 - val_loss: 0.1345 - val_mae: 0.1971 - val_mse: 0.1345 - learning_rate: 0.1000 - val_custom_mse: 0.3768 - val_custom_mae: 0.4222\n",
            "Epoch 61/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2210 - mae: 0.2223 - mse: 0.2210 - val_loss: 0.1356 - val_mae: 0.1971 - val_mse: 0.1356 - learning_rate: 0.1000 - val_custom_mse: 0.3804 - val_custom_mae: 0.4236\n",
            "Epoch 62/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2210 - mae: 0.2221 - mse: 0.2210 - val_loss: 0.1350 - val_mae: 0.1967 - val_mse: 0.1350 - learning_rate: 0.1000 - val_custom_mse: 0.3791 - val_custom_mae: 0.4235\n",
            "Epoch 63/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2209 - mae: 0.2217 - mse: 0.2209 - val_loss: 0.1343 - val_mae: 0.1961 - val_mse: 0.1343 - learning_rate: 0.1000 - val_custom_mse: 0.3774 - val_custom_mae: 0.4223\n",
            "Epoch 64/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2207 - mae: 0.2216 - mse: 0.2207 - val_loss: 0.1349 - val_mae: 0.1962 - val_mse: 0.1349 - learning_rate: 0.1000 - val_custom_mse: 0.3793 - val_custom_mae: 0.4235\n",
            "Epoch 65/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2208 - mae: 0.2214 - mse: 0.2208 - val_loss: 0.1344 - val_mae: 0.1960 - val_mse: 0.1344 - learning_rate: 0.1000 - val_custom_mse: 0.3778 - val_custom_mae: 0.4224\n",
            "Epoch 66/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2206 - mae: 0.2211 - mse: 0.2206 - val_loss: 0.1347 - val_mae: 0.1956 - val_mse: 0.1347 - learning_rate: 0.1000 - val_custom_mse: 0.3793 - val_custom_mae: 0.4232\n",
            "Epoch 67/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2202 - mae: 0.2208 - mse: 0.2202 - val_loss: 0.1339 - val_mae: 0.1953 - val_mse: 0.1339 - learning_rate: 0.1000 - val_custom_mse: 0.3771 - val_custom_mae: 0.4222\n",
            "Epoch 68/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2202 - mae: 0.2206 - mse: 0.2202 - val_loss: 0.1343 - val_mae: 0.1954 - val_mse: 0.1343 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4229\n",
            "Epoch 69/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2202 - mae: 0.2204 - mse: 0.2202 - val_loss: 0.1343 - val_mae: 0.1950 - val_mse: 0.1343 - learning_rate: 0.1000 - val_custom_mse: 0.3789 - val_custom_mae: 0.4232\n",
            "Epoch 70/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2199 - mae: 0.2202 - mse: 0.2199 - val_loss: 0.1343 - val_mae: 0.1950 - val_mse: 0.1343 - learning_rate: 0.1000 - val_custom_mse: 0.3788 - val_custom_mae: 0.4231\n",
            "Epoch 71/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2198 - mae: 0.2199 - mse: 0.2198 - val_loss: 0.1343 - val_mae: 0.1944 - val_mse: 0.1343 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4234\n",
            "Epoch 72/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2198 - mae: 0.2198 - mse: 0.2198 - val_loss: 0.1344 - val_mae: 0.1945 - val_mse: 0.1344 - learning_rate: 0.1000 - val_custom_mse: 0.3797 - val_custom_mae: 0.4233\n",
            "Epoch 73/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2196 - mae: 0.2196 - mse: 0.2196 - val_loss: 0.1340 - val_mae: 0.1942 - val_mse: 0.1340 - learning_rate: 0.1000 - val_custom_mse: 0.3788 - val_custom_mae: 0.4230\n",
            "Epoch 74/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2195 - mae: 0.2194 - mse: 0.2195 - val_loss: 0.1341 - val_mae: 0.1944 - val_mse: 0.1341 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4233\n",
            "Epoch 75/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2196 - mae: 0.2193 - mse: 0.2196 - val_loss: 0.1331 - val_mae: 0.1938 - val_mse: 0.1331 - learning_rate: 0.1000 - val_custom_mse: 0.3764 - val_custom_mae: 0.4216\n",
            "Epoch 76/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2194 - mae: 0.2192 - mse: 0.2194 - val_loss: 0.1336 - val_mae: 0.1937 - val_mse: 0.1336 - learning_rate: 0.1000 - val_custom_mse: 0.3782 - val_custom_mae: 0.4226\n",
            "Epoch 77/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2194 - mae: 0.2190 - mse: 0.2194 - val_loss: 0.1335 - val_mae: 0.1937 - val_mse: 0.1335 - learning_rate: 0.1000 - val_custom_mse: 0.3779 - val_custom_mae: 0.4223\n",
            "Epoch 78/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2192 - mae: 0.2188 - mse: 0.2192 - val_loss: 0.1338 - val_mae: 0.1931 - val_mse: 0.1338 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4231\n",
            "Epoch 79/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2188 - mae: 0.2186 - mse: 0.2188 - val_loss: 0.1338 - val_mae: 0.1932 - val_mse: 0.1338 - learning_rate: 0.1000 - val_custom_mse: 0.3794 - val_custom_mae: 0.4234\n",
            "Epoch 80/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2191 - mae: 0.2186 - mse: 0.2191 - val_loss: 0.1332 - val_mae: 0.1927 - val_mse: 0.1332 - learning_rate: 0.1000 - val_custom_mse: 0.3779 - val_custom_mae: 0.4227\n",
            "Epoch 81/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2190 - mae: 0.2184 - mse: 0.2190 - val_loss: 0.1335 - val_mae: 0.1928 - val_mse: 0.1335 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4234\n",
            "Epoch 82/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2190 - mae: 0.2183 - mse: 0.2190 - val_loss: 0.1331 - val_mae: 0.1929 - val_mse: 0.1331 - learning_rate: 0.1000 - val_custom_mse: 0.3776 - val_custom_mae: 0.4222\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "228/228 - 2s - 8ms/step - loss: 0.2189 - mae: 0.2182 - mse: 0.2189 - val_loss: 0.1335 - val_mae: 0.1927 - val_mse: 0.1335 - learning_rate: 0.1000 - val_custom_mse: 0.3793 - val_custom_mae: 0.4236\n",
            "Epoch 84/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2189 - mae: 0.2181 - mse: 0.2189 - val_loss: 0.1335 - val_mae: 0.1926 - val_mse: 0.1335 - learning_rate: 0.0200 - val_custom_mse: 0.3790 - val_custom_mae: 0.4232\n",
            "Epoch 85/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2187 - mae: 0.2179 - mse: 0.2187 - val_loss: 0.1337 - val_mae: 0.1927 - val_mse: 0.1337 - learning_rate: 0.0200 - val_custom_mse: 0.3796 - val_custom_mae: 0.4236\n",
            "Epoch 86/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2185 - mae: 0.2179 - mse: 0.2185 - val_loss: 0.1335 - val_mae: 0.1927 - val_mse: 0.1335 - learning_rate: 0.0200 - val_custom_mse: 0.3791 - val_custom_mae: 0.4234\n",
            "Epoch 87/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2185 - mae: 0.2178 - mse: 0.2185 - val_loss: 0.1335 - val_mae: 0.1926 - val_mse: 0.1335 - learning_rate: 0.0200 - val_custom_mse: 0.3793 - val_custom_mae: 0.4234\n",
            "Epoch 88/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2186 - mae: 0.2178 - mse: 0.2186 - val_loss: 0.1333 - val_mae: 0.1926 - val_mse: 0.1333 - learning_rate: 0.0200 - val_custom_mse: 0.3785 - val_custom_mae: 0.4230\n",
            "Epoch 89/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2184 - mae: 0.2177 - mse: 0.2184 - val_loss: 0.1334 - val_mae: 0.1925 - val_mse: 0.1334 - learning_rate: 0.0200 - val_custom_mse: 0.3789 - val_custom_mae: 0.4232\n",
            "Epoch 90/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2185 - mae: 0.2177 - mse: 0.2185 - val_loss: 0.1335 - val_mae: 0.1926 - val_mse: 0.1335 - learning_rate: 0.0200 - val_custom_mse: 0.3794 - val_custom_mae: 0.4234\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "228/228 - 2s - 8ms/step - loss: 0.2187 - mae: 0.2177 - mse: 0.2187 - val_loss: 0.1335 - val_mae: 0.1926 - val_mse: 0.1335 - learning_rate: 0.0200 - val_custom_mse: 0.3792 - val_custom_mae: 0.4234\n",
            "Epoch 92/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2183 - mae: 0.2176 - mse: 0.2183 - val_loss: 0.1333 - val_mae: 0.1924 - val_mse: 0.1333 - learning_rate: 0.0040 - val_custom_mse: 0.3785 - val_custom_mae: 0.4230\n",
            "Epoch 93/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2184 - mae: 0.2176 - mse: 0.2184 - val_loss: 0.1332 - val_mae: 0.1924 - val_mse: 0.1332 - learning_rate: 0.0040 - val_custom_mse: 0.3785 - val_custom_mae: 0.4230\n",
            "Epoch 94/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2183 - mae: 0.2175 - mse: 0.2183 - val_loss: 0.1332 - val_mae: 0.1924 - val_mse: 0.1332 - learning_rate: 0.0040 - val_custom_mse: 0.3784 - val_custom_mae: 0.4230\n",
            "Epoch 95/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2186 - mae: 0.2177 - mse: 0.2186 - val_loss: 0.1332 - val_mae: 0.1924 - val_mse: 0.1332 - learning_rate: 0.0040 - val_custom_mse: 0.3785 - val_custom_mae: 0.4230\n",
            "Epoch 96/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2183 - mae: 0.2176 - mse: 0.2183 - val_loss: 0.1332 - val_mae: 0.1924 - val_mse: 0.1332 - learning_rate: 0.0040 - val_custom_mse: 0.3784 - val_custom_mae: 0.4230\n",
            "Epoch 97/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2186 - mae: 0.2177 - mse: 0.2186 - val_loss: 0.1332 - val_mae: 0.1924 - val_mse: 0.1332 - learning_rate: 0.0040 - val_custom_mse: 0.3784 - val_custom_mae: 0.4230\n",
            "Epoch 98/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2183 - mae: 0.2176 - mse: 0.2183 - val_loss: 0.1332 - val_mae: 0.1924 - val_mse: 0.1332 - learning_rate: 0.0040 - val_custom_mse: 0.3785 - val_custom_mae: 0.4230\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "228/228 - 2s - 8ms/step - loss: 0.2183 - mae: 0.2176 - mse: 0.2183 - val_loss: 0.1332 - val_mae: 0.1924 - val_mse: 0.1332 - learning_rate: 0.0040 - val_custom_mse: 0.3785 - val_custom_mae: 0.4230\n",
            "Epoch 100/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2184 - mae: 0.2175 - mse: 0.2184 - val_loss: 0.1332 - val_mae: 0.1924 - val_mse: 0.1332 - learning_rate: 8.0000e-04 - val_custom_mse: 0.3784 - val_custom_mae: 0.4230\n",
            "Running experiment: horizon=336, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_43', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228/228 - 6s - 27ms/step - loss: 0.6225 - mae: 0.4890 - mse: 0.6225 - val_loss: 0.3967 - val_mae: 0.4531 - val_mse: 0.3967 - learning_rate: 0.1000 - val_custom_mse: 0.6170 - val_custom_mae: 0.5735\n",
            "Epoch 2/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.5434 - mae: 0.4548 - mse: 0.5434 - val_loss: 0.3438 - val_mae: 0.4200 - val_mse: 0.3438 - learning_rate: 0.1000 - val_custom_mse: 0.5446 - val_custom_mae: 0.5349\n",
            "Epoch 3/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4797 - mae: 0.4261 - mse: 0.4797 - val_loss: 0.3061 - val_mae: 0.3926 - val_mse: 0.3061 - learning_rate: 0.1000 - val_custom_mse: 0.5013 - val_custom_mae: 0.5078\n",
            "Epoch 4/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4284 - mae: 0.3996 - mse: 0.4284 - val_loss: 0.2654 - val_mae: 0.3609 - val_mse: 0.2654 - learning_rate: 0.1000 - val_custom_mse: 0.4672 - val_custom_mae: 0.4866\n",
            "Epoch 5/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3830 - mae: 0.3728 - mse: 0.3830 - val_loss: 0.2334 - val_mae: 0.3326 - val_mse: 0.2334 - learning_rate: 0.1000 - val_custom_mse: 0.4425 - val_custom_mae: 0.4703\n",
            "Epoch 6/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3457 - mae: 0.3484 - mse: 0.3457 - val_loss: 0.2111 - val_mae: 0.3104 - val_mse: 0.2111 - learning_rate: 0.1000 - val_custom_mse: 0.4283 - val_custom_mae: 0.4609\n",
            "Epoch 7/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3182 - mae: 0.3285 - mse: 0.3182 - val_loss: 0.1943 - val_mae: 0.2926 - val_mse: 0.1943 - learning_rate: 0.1000 - val_custom_mse: 0.4138 - val_custom_mae: 0.4506\n",
            "Epoch 8/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2994 - mae: 0.3137 - mse: 0.2994 - val_loss: 0.1839 - val_mae: 0.2804 - val_mse: 0.1839 - learning_rate: 0.1000 - val_custom_mse: 0.4051 - val_custom_mae: 0.4440\n",
            "Epoch 9/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2875 - mae: 0.3035 - mse: 0.2875 - val_loss: 0.1779 - val_mae: 0.2729 - val_mse: 0.1779 - learning_rate: 0.1000 - val_custom_mse: 0.4014 - val_custom_mae: 0.4414\n",
            "Epoch 10/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2793 - mae: 0.2961 - mse: 0.2793 - val_loss: 0.1740 - val_mae: 0.2678 - val_mse: 0.1740 - learning_rate: 0.1000 - val_custom_mse: 0.3986 - val_custom_mae: 0.4394\n",
            "Epoch 11/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2737 - mae: 0.2907 - mse: 0.2737 - val_loss: 0.1693 - val_mae: 0.2614 - val_mse: 0.1693 - learning_rate: 0.1000 - val_custom_mse: 0.3936 - val_custom_mae: 0.4354\n",
            "Epoch 12/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2693 - mae: 0.2861 - mse: 0.2693 - val_loss: 0.1665 - val_mae: 0.2574 - val_mse: 0.1665 - learning_rate: 0.1000 - val_custom_mse: 0.3917 - val_custom_mae: 0.4340\n",
            "Epoch 13/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2657 - mae: 0.2823 - mse: 0.2657 - val_loss: 0.1647 - val_mae: 0.2541 - val_mse: 0.1647 - learning_rate: 0.1000 - val_custom_mse: 0.3916 - val_custom_mae: 0.4338\n",
            "Epoch 14/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2625 - mae: 0.2788 - mse: 0.2625 - val_loss: 0.1621 - val_mae: 0.2502 - val_mse: 0.1621 - learning_rate: 0.1000 - val_custom_mse: 0.3893 - val_custom_mae: 0.4319\n",
            "Epoch 15/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2597 - mae: 0.2757 - mse: 0.2597 - val_loss: 0.1597 - val_mae: 0.2467 - val_mse: 0.1597 - learning_rate: 0.1000 - val_custom_mse: 0.3870 - val_custom_mae: 0.4302\n",
            "Epoch 16/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2575 - mae: 0.2731 - mse: 0.2575 - val_loss: 0.1581 - val_mae: 0.2440 - val_mse: 0.1581 - learning_rate: 0.1000 - val_custom_mse: 0.3861 - val_custom_mae: 0.4296\n",
            "Epoch 17/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2554 - mae: 0.2708 - mse: 0.2554 - val_loss: 0.1570 - val_mae: 0.2421 - val_mse: 0.1570 - learning_rate: 0.1000 - val_custom_mse: 0.3858 - val_custom_mae: 0.4295\n",
            "Epoch 18/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2535 - mae: 0.2684 - mse: 0.2535 - val_loss: 0.1556 - val_mae: 0.2397 - val_mse: 0.1556 - learning_rate: 0.1000 - val_custom_mse: 0.3850 - val_custom_mae: 0.4289\n",
            "Epoch 19/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2519 - mae: 0.2665 - mse: 0.2519 - val_loss: 0.1542 - val_mae: 0.2373 - val_mse: 0.1542 - learning_rate: 0.1000 - val_custom_mse: 0.3839 - val_custom_mae: 0.4280\n",
            "Epoch 20/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2502 - mae: 0.2647 - mse: 0.2502 - val_loss: 0.1539 - val_mae: 0.2359 - val_mse: 0.1539 - learning_rate: 0.1000 - val_custom_mse: 0.3855 - val_custom_mae: 0.4287\n",
            "Epoch 21/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2488 - mae: 0.2630 - mse: 0.2488 - val_loss: 0.1522 - val_mae: 0.2335 - val_mse: 0.1522 - learning_rate: 0.1000 - val_custom_mse: 0.3833 - val_custom_mae: 0.4271\n",
            "Epoch 22/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2477 - mae: 0.2615 - mse: 0.2477 - val_loss: 0.1509 - val_mae: 0.2315 - val_mse: 0.1509 - learning_rate: 0.1000 - val_custom_mse: 0.3817 - val_custom_mae: 0.4259\n",
            "Epoch 23/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2467 - mae: 0.2601 - mse: 0.2467 - val_loss: 0.1502 - val_mae: 0.2301 - val_mse: 0.1502 - learning_rate: 0.1000 - val_custom_mse: 0.3818 - val_custom_mae: 0.4261\n",
            "Epoch 24/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2454 - mae: 0.2588 - mse: 0.2454 - val_loss: 0.1494 - val_mae: 0.2290 - val_mse: 0.1494 - learning_rate: 0.1000 - val_custom_mse: 0.3808 - val_custom_mae: 0.4257\n",
            "Epoch 25/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2447 - mae: 0.2576 - mse: 0.2447 - val_loss: 0.1495 - val_mae: 0.2281 - val_mse: 0.1495 - learning_rate: 0.1000 - val_custom_mse: 0.3828 - val_custom_mae: 0.4269\n",
            "Epoch 26/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2435 - mae: 0.2565 - mse: 0.2435 - val_loss: 0.1486 - val_mae: 0.2269 - val_mse: 0.1486 - learning_rate: 0.1000 - val_custom_mse: 0.3818 - val_custom_mae: 0.4262\n",
            "Epoch 27/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2429 - mae: 0.2556 - mse: 0.2429 - val_loss: 0.1477 - val_mae: 0.2252 - val_mse: 0.1477 - learning_rate: 0.1000 - val_custom_mse: 0.3812 - val_custom_mae: 0.4254\n",
            "Epoch 28/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2423 - mae: 0.2547 - mse: 0.2423 - val_loss: 0.1472 - val_mae: 0.2242 - val_mse: 0.1472 - learning_rate: 0.1000 - val_custom_mse: 0.3809 - val_custom_mae: 0.4253\n",
            "Epoch 29/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2414 - mae: 0.2537 - mse: 0.2414 - val_loss: 0.1462 - val_mae: 0.2229 - val_mse: 0.1462 - learning_rate: 0.1000 - val_custom_mse: 0.3792 - val_custom_mae: 0.4240\n",
            "Epoch 30/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2410 - mae: 0.2530 - mse: 0.2410 - val_loss: 0.1456 - val_mae: 0.2219 - val_mse: 0.1456 - learning_rate: 0.1000 - val_custom_mse: 0.3788 - val_custom_mae: 0.4238\n",
            "Epoch 31/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2401 - mae: 0.2522 - mse: 0.2401 - val_loss: 0.1454 - val_mae: 0.2214 - val_mse: 0.1454 - learning_rate: 0.1000 - val_custom_mse: 0.3792 - val_custom_mae: 0.4244\n",
            "Epoch 32/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2396 - mae: 0.2514 - mse: 0.2396 - val_loss: 0.1454 - val_mae: 0.2209 - val_mse: 0.1454 - learning_rate: 0.1000 - val_custom_mse: 0.3804 - val_custom_mae: 0.4251\n",
            "Epoch 33/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2391 - mae: 0.2508 - mse: 0.2391 - val_loss: 0.1446 - val_mae: 0.2197 - val_mse: 0.1446 - learning_rate: 0.1000 - val_custom_mse: 0.3791 - val_custom_mae: 0.4241\n",
            "Epoch 34/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2386 - mae: 0.2501 - mse: 0.2386 - val_loss: 0.1440 - val_mae: 0.2189 - val_mse: 0.1440 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4236\n",
            "Epoch 35/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2382 - mae: 0.2496 - mse: 0.2382 - val_loss: 0.1446 - val_mae: 0.2189 - val_mse: 0.1446 - learning_rate: 0.1000 - val_custom_mse: 0.3806 - val_custom_mae: 0.4251\n",
            "Epoch 36/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2377 - mae: 0.2490 - mse: 0.2377 - val_loss: 0.1439 - val_mae: 0.2180 - val_mse: 0.1439 - learning_rate: 0.1000 - val_custom_mse: 0.3799 - val_custom_mae: 0.4246\n",
            "Epoch 37/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2376 - mae: 0.2485 - mse: 0.2376 - val_loss: 0.1436 - val_mae: 0.2172 - val_mse: 0.1436 - learning_rate: 0.1000 - val_custom_mse: 0.3800 - val_custom_mae: 0.4245\n",
            "Epoch 38/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2370 - mae: 0.2480 - mse: 0.2370 - val_loss: 0.1433 - val_mae: 0.2170 - val_mse: 0.1433 - learning_rate: 0.1000 - val_custom_mse: 0.3792 - val_custom_mae: 0.4242\n",
            "Epoch 39/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2367 - mae: 0.2476 - mse: 0.2367 - val_loss: 0.1435 - val_mae: 0.2170 - val_mse: 0.1435 - learning_rate: 0.1000 - val_custom_mse: 0.3803 - val_custom_mae: 0.4250\n",
            "Epoch 40/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2363 - mae: 0.2472 - mse: 0.2363 - val_loss: 0.1430 - val_mae: 0.2160 - val_mse: 0.1430 - learning_rate: 0.1000 - val_custom_mse: 0.3800 - val_custom_mae: 0.4245\n",
            "Epoch 41/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2362 - mae: 0.2468 - mse: 0.2362 - val_loss: 0.1427 - val_mae: 0.2157 - val_mse: 0.1427 - learning_rate: 0.1000 - val_custom_mse: 0.3793 - val_custom_mae: 0.4243\n",
            "Epoch 42/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2359 - mae: 0.2465 - mse: 0.2359 - val_loss: 0.1427 - val_mae: 0.2151 - val_mse: 0.1427 - learning_rate: 0.1000 - val_custom_mse: 0.3803 - val_custom_mae: 0.4246\n",
            "Epoch 43/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2353 - mae: 0.2460 - mse: 0.2353 - val_loss: 0.1424 - val_mae: 0.2146 - val_mse: 0.1424 - learning_rate: 0.1000 - val_custom_mse: 0.3802 - val_custom_mae: 0.4245\n",
            "Epoch 44/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2355 - mae: 0.2458 - mse: 0.2355 - val_loss: 0.1418 - val_mae: 0.2139 - val_mse: 0.1418 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4238\n",
            "Epoch 45/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2351 - mae: 0.2455 - mse: 0.2351 - val_loss: 0.1417 - val_mae: 0.2139 - val_mse: 0.1417 - learning_rate: 0.1000 - val_custom_mse: 0.3787 - val_custom_mae: 0.4235\n",
            "Epoch 46/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2347 - mae: 0.2451 - mse: 0.2347 - val_loss: 0.1419 - val_mae: 0.2137 - val_mse: 0.1419 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4241\n",
            "Epoch 47/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2346 - mae: 0.2448 - mse: 0.2346 - val_loss: 0.1412 - val_mae: 0.2129 - val_mse: 0.1412 - learning_rate: 0.1000 - val_custom_mse: 0.3785 - val_custom_mae: 0.4234\n",
            "Epoch 48/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2340 - mae: 0.2444 - mse: 0.2340 - val_loss: 0.1415 - val_mae: 0.2128 - val_mse: 0.1415 - learning_rate: 0.1000 - val_custom_mse: 0.3797 - val_custom_mae: 0.4243\n",
            "Epoch 49/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2342 - mae: 0.2444 - mse: 0.2342 - val_loss: 0.1411 - val_mae: 0.2126 - val_mse: 0.1411 - learning_rate: 0.1000 - val_custom_mse: 0.3788 - val_custom_mae: 0.4237\n",
            "Epoch 50/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2336 - mae: 0.2439 - mse: 0.2336 - val_loss: 0.1412 - val_mae: 0.2121 - val_mse: 0.1412 - learning_rate: 0.1000 - val_custom_mse: 0.3798 - val_custom_mae: 0.4244\n",
            "Epoch 51/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2336 - mae: 0.2438 - mse: 0.2336 - val_loss: 0.1410 - val_mae: 0.2120 - val_mse: 0.1410 - learning_rate: 0.1000 - val_custom_mse: 0.3791 - val_custom_mae: 0.4240\n",
            "Epoch 52/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2337 - mae: 0.2437 - mse: 0.2337 - val_loss: 0.1416 - val_mae: 0.2120 - val_mse: 0.1416 - learning_rate: 0.1000 - val_custom_mse: 0.3814 - val_custom_mae: 0.4251\n",
            "Epoch 53/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2333 - mae: 0.2433 - mse: 0.2333 - val_loss: 0.1412 - val_mae: 0.2118 - val_mse: 0.1412 - learning_rate: 0.1000 - val_custom_mse: 0.3802 - val_custom_mae: 0.4242\n",
            "Epoch 54/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2332 - mae: 0.2432 - mse: 0.2332 - val_loss: 0.1407 - val_mae: 0.2115 - val_mse: 0.1407 - learning_rate: 0.1000 - val_custom_mse: 0.3793 - val_custom_mae: 0.4241\n",
            "Epoch 55/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2329 - mae: 0.2429 - mse: 0.2329 - val_loss: 0.1407 - val_mae: 0.2112 - val_mse: 0.1407 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4242\n",
            "Epoch 56/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2329 - mae: 0.2428 - mse: 0.2329 - val_loss: 0.1406 - val_mae: 0.2107 - val_mse: 0.1406 - learning_rate: 0.1000 - val_custom_mse: 0.3798 - val_custom_mae: 0.4240\n",
            "Epoch 57/100\n",
            "228/228 - 2s - 7ms/step - loss: 0.2327 - mae: 0.2426 - mse: 0.2327 - val_loss: 0.1406 - val_mae: 0.2108 - val_mse: 0.1406 - learning_rate: 0.1000 - val_custom_mse: 0.3801 - val_custom_mae: 0.4245\n",
            "Epoch 58/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2326 - mae: 0.2425 - mse: 0.2326 - val_loss: 0.1409 - val_mae: 0.2108 - val_mse: 0.1409 - learning_rate: 0.1000 - val_custom_mse: 0.3808 - val_custom_mae: 0.4244\n",
            "Epoch 59/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2325 - mae: 0.2423 - mse: 0.2325 - val_loss: 0.1406 - val_mae: 0.2105 - val_mse: 0.1406 - learning_rate: 0.1000 - val_custom_mse: 0.3802 - val_custom_mae: 0.4243\n",
            "Epoch 60/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2324 - mae: 0.2421 - mse: 0.2324 - val_loss: 0.1402 - val_mae: 0.2100 - val_mse: 0.1402 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4240\n",
            "Epoch 61/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2322 - mae: 0.2419 - mse: 0.2322 - val_loss: 0.1393 - val_mae: 0.2097 - val_mse: 0.1393 - learning_rate: 0.1000 - val_custom_mse: 0.3769 - val_custom_mae: 0.4225\n",
            "Epoch 62/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2321 - mae: 0.2419 - mse: 0.2321 - val_loss: 0.1403 - val_mae: 0.2098 - val_mse: 0.1403 - learning_rate: 0.1000 - val_custom_mse: 0.3801 - val_custom_mae: 0.4241\n",
            "Epoch 63/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2319 - mae: 0.2418 - mse: 0.2319 - val_loss: 0.1402 - val_mae: 0.2099 - val_mse: 0.1402 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4239\n",
            "Epoch 64/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2320 - mae: 0.2417 - mse: 0.2320 - val_loss: 0.1408 - val_mae: 0.2105 - val_mse: 0.1408 - learning_rate: 0.1000 - val_custom_mse: 0.3815 - val_custom_mae: 0.4254\n",
            "Epoch 65/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2320 - mae: 0.2416 - mse: 0.2320 - val_loss: 0.1401 - val_mae: 0.2097 - val_mse: 0.1401 - learning_rate: 0.1000 - val_custom_mse: 0.3798 - val_custom_mae: 0.4241\n",
            "Epoch 66/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2317 - mae: 0.2414 - mse: 0.2317 - val_loss: 0.1399 - val_mae: 0.2096 - val_mse: 0.1399 - learning_rate: 0.1000 - val_custom_mse: 0.3793 - val_custom_mae: 0.4238\n",
            "Epoch 67/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2315 - mae: 0.2412 - mse: 0.2315 - val_loss: 0.1399 - val_mae: 0.2092 - val_mse: 0.1399 - learning_rate: 0.1000 - val_custom_mse: 0.3797 - val_custom_mae: 0.4238\n",
            "Epoch 68/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2316 - mae: 0.2412 - mse: 0.2316 - val_loss: 0.1400 - val_mae: 0.2094 - val_mse: 0.1400 - learning_rate: 0.1000 - val_custom_mse: 0.3800 - val_custom_mae: 0.4243\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "228/228 - 2s - 8ms/step - loss: 0.2314 - mae: 0.2410 - mse: 0.2314 - val_loss: 0.1400 - val_mae: 0.2090 - val_mse: 0.1400 - learning_rate: 0.1000 - val_custom_mse: 0.3802 - val_custom_mae: 0.4241\n",
            "Epoch 70/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2313 - mae: 0.2409 - mse: 0.2313 - val_loss: 0.1395 - val_mae: 0.2087 - val_mse: 0.1395 - learning_rate: 0.0200 - val_custom_mse: 0.3787 - val_custom_mae: 0.4232\n",
            "Epoch 71/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2310 - mae: 0.2407 - mse: 0.2310 - val_loss: 0.1394 - val_mae: 0.2087 - val_mse: 0.1394 - learning_rate: 0.0200 - val_custom_mse: 0.3786 - val_custom_mae: 0.4232\n",
            "Epoch 72/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2312 - mae: 0.2407 - mse: 0.2312 - val_loss: 0.1395 - val_mae: 0.2086 - val_mse: 0.1395 - learning_rate: 0.0200 - val_custom_mse: 0.3787 - val_custom_mae: 0.4231\n",
            "Epoch 73/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2311 - mae: 0.2408 - mse: 0.2311 - val_loss: 0.1395 - val_mae: 0.2087 - val_mse: 0.1395 - learning_rate: 0.0200 - val_custom_mse: 0.3789 - val_custom_mae: 0.4233\n",
            "Epoch 74/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2310 - mae: 0.2407 - mse: 0.2310 - val_loss: 0.1394 - val_mae: 0.2087 - val_mse: 0.1394 - learning_rate: 0.0200 - val_custom_mse: 0.3785 - val_custom_mae: 0.4232\n",
            "Epoch 75/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2310 - mae: 0.2407 - mse: 0.2310 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 0.0200 - val_custom_mse: 0.3784 - val_custom_mae: 0.4230\n",
            "Epoch 76/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2311 - mae: 0.2406 - mse: 0.2311 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 0.0200 - val_custom_mse: 0.3783 - val_custom_mae: 0.4230\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "228/228 - 2s - 8ms/step - loss: 0.2308 - mae: 0.2405 - mse: 0.2308 - val_loss: 0.1395 - val_mae: 0.2086 - val_mse: 0.1395 - learning_rate: 0.0200 - val_custom_mse: 0.3791 - val_custom_mae: 0.4235\n",
            "Epoch 78/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2310 - mae: 0.2405 - mse: 0.2310 - val_loss: 0.1393 - val_mae: 0.2085 - val_mse: 0.1393 - learning_rate: 0.0040 - val_custom_mse: 0.3783 - val_custom_mae: 0.4230\n",
            "Epoch 79/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2307 - mae: 0.2404 - mse: 0.2307 - val_loss: 0.1393 - val_mae: 0.2085 - val_mse: 0.1393 - learning_rate: 0.0040 - val_custom_mse: 0.3782 - val_custom_mae: 0.4230\n",
            "Epoch 80/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2309 - mae: 0.2405 - mse: 0.2309 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 0.0040 - val_custom_mse: 0.3782 - val_custom_mae: 0.4230\n",
            "Epoch 81/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2308 - mae: 0.2406 - mse: 0.2308 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 0.0040 - val_custom_mse: 0.3783 - val_custom_mae: 0.4231\n",
            "Epoch 82/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2310 - mae: 0.2406 - mse: 0.2310 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 0.0040 - val_custom_mse: 0.3782 - val_custom_mae: 0.4230\n",
            "Epoch 83/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2311 - mae: 0.2406 - mse: 0.2311 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 0.0040 - val_custom_mse: 0.3784 - val_custom_mae: 0.4231\n",
            "Epoch 84/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2309 - mae: 0.2405 - mse: 0.2309 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 0.0040 - val_custom_mse: 0.3784 - val_custom_mae: 0.4231\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "228/228 - 2s - 8ms/step - loss: 0.2309 - mae: 0.2405 - mse: 0.2309 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 0.0040 - val_custom_mse: 0.3784 - val_custom_mae: 0.4231\n",
            "Epoch 86/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2308 - mae: 0.2405 - mse: 0.2308 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 8.0000e-04 - val_custom_mse: 0.3783 - val_custom_mae: 0.4231\n",
            "Epoch 87/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2306 - mae: 0.2404 - mse: 0.2306 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 8.0000e-04 - val_custom_mse: 0.3783 - val_custom_mae: 0.4230\n",
            "Epoch 88/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2306 - mae: 0.2404 - mse: 0.2306 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 8.0000e-04 - val_custom_mse: 0.3783 - val_custom_mae: 0.4231\n",
            "Epoch 89/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2306 - mae: 0.2404 - mse: 0.2306 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 8.0000e-04 - val_custom_mse: 0.3783 - val_custom_mae: 0.4231\n",
            "Epoch 90/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2307 - mae: 0.2405 - mse: 0.2307 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 8.0000e-04 - val_custom_mse: 0.3783 - val_custom_mae: 0.4231\n",
            "Epoch 91/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2310 - mae: 0.2406 - mse: 0.2310 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 8.0000e-04 - val_custom_mse: 0.3783 - val_custom_mae: 0.4231\n",
            "Epoch 92/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2308 - mae: 0.2406 - mse: 0.2308 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 8.0000e-04 - val_custom_mse: 0.3783 - val_custom_mae: 0.4231\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "228/228 - 2s - 8ms/step - loss: 0.2308 - mae: 0.2405 - mse: 0.2308 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 8.0000e-04 - val_custom_mse: 0.3783 - val_custom_mae: 0.4231\n",
            "Epoch 94/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2310 - mae: 0.2406 - mse: 0.2310 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 1.6000e-04 - val_custom_mse: 0.3783 - val_custom_mae: 0.4231\n",
            "Epoch 95/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2311 - mae: 0.2406 - mse: 0.2311 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 1.6000e-04 - val_custom_mse: 0.3784 - val_custom_mae: 0.4231\n",
            "Epoch 96/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2308 - mae: 0.2405 - mse: 0.2308 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 1.6000e-04 - val_custom_mse: 0.3784 - val_custom_mae: 0.4231\n",
            "Epoch 97/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2309 - mae: 0.2405 - mse: 0.2309 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 1.6000e-04 - val_custom_mse: 0.3784 - val_custom_mae: 0.4231\n",
            "Epoch 98/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2308 - mae: 0.2405 - mse: 0.2308 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 1.6000e-04 - val_custom_mse: 0.3784 - val_custom_mae: 0.4231\n",
            "Epoch 99/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2310 - mae: 0.2406 - mse: 0.2310 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 1.6000e-04 - val_custom_mse: 0.3784 - val_custom_mae: 0.4231\n",
            "Epoch 100/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2306 - mae: 0.2405 - mse: 0.2306 - val_loss: 0.1393 - val_mae: 0.2086 - val_mse: 0.1393 - learning_rate: 1.6000e-04 - val_custom_mse: 0.3784 - val_custom_mae: 0.4231\n",
            "Running experiment: horizon=336, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_44', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228/228 - 6s - 27ms/step - loss: 0.6206 - mae: 0.4886 - mse: 0.6206 - val_loss: 0.3896 - val_mae: 0.4491 - val_mse: 0.3896 - learning_rate: 0.1000 - val_custom_mse: 0.5900 - val_custom_mae: 0.5595\n",
            "Epoch 2/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.5377 - mae: 0.4532 - mse: 0.5377 - val_loss: 0.3518 - val_mae: 0.4235 - val_mse: 0.3518 - learning_rate: 0.1000 - val_custom_mse: 0.5524 - val_custom_mae: 0.5362\n",
            "Epoch 3/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4783 - mae: 0.4267 - mse: 0.4783 - val_loss: 0.3064 - val_mae: 0.3929 - val_mse: 0.3064 - learning_rate: 0.1000 - val_custom_mse: 0.5079 - val_custom_mae: 0.5117\n",
            "Epoch 4/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.4285 - mae: 0.4002 - mse: 0.4285 - val_loss: 0.2653 - val_mae: 0.3609 - val_mse: 0.2653 - learning_rate: 0.1000 - val_custom_mse: 0.4658 - val_custom_mae: 0.4856\n",
            "Epoch 5/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3840 - mae: 0.3737 - mse: 0.3840 - val_loss: 0.2334 - val_mae: 0.3327 - val_mse: 0.2334 - learning_rate: 0.1000 - val_custom_mse: 0.4428 - val_custom_mae: 0.4708\n",
            "Epoch 6/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3473 - mae: 0.3496 - mse: 0.3473 - val_loss: 0.2094 - val_mae: 0.3092 - val_mse: 0.2094 - learning_rate: 0.1000 - val_custom_mse: 0.4240 - val_custom_mae: 0.4581\n",
            "Epoch 7/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3202 - mae: 0.3301 - mse: 0.3202 - val_loss: 0.1942 - val_mae: 0.2926 - val_mse: 0.1942 - learning_rate: 0.1000 - val_custom_mse: 0.4139 - val_custom_mae: 0.4508\n",
            "Epoch 8/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.3027 - mae: 0.3162 - mse: 0.3027 - val_loss: 0.1842 - val_mae: 0.2810 - val_mse: 0.1842 - learning_rate: 0.1000 - val_custom_mse: 0.4059 - val_custom_mae: 0.4451\n",
            "Epoch 9/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2914 - mae: 0.3066 - mse: 0.2914 - val_loss: 0.1771 - val_mae: 0.2724 - val_mse: 0.1771 - learning_rate: 0.1000 - val_custom_mse: 0.3992 - val_custom_mae: 0.4401\n",
            "Epoch 10/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2839 - mae: 0.2998 - mse: 0.2839 - val_loss: 0.1733 - val_mae: 0.2673 - val_mse: 0.1733 - learning_rate: 0.1000 - val_custom_mse: 0.3972 - val_custom_mae: 0.4386\n",
            "Epoch 11/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2786 - mae: 0.2950 - mse: 0.2786 - val_loss: 0.1694 - val_mae: 0.2619 - val_mse: 0.1694 - learning_rate: 0.1000 - val_custom_mse: 0.3933 - val_custom_mae: 0.4356\n",
            "Epoch 12/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2748 - mae: 0.2910 - mse: 0.2748 - val_loss: 0.1665 - val_mae: 0.2580 - val_mse: 0.1665 - learning_rate: 0.1000 - val_custom_mse: 0.3908 - val_custom_mae: 0.4338\n",
            "Epoch 13/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2715 - mae: 0.2877 - mse: 0.2715 - val_loss: 0.1647 - val_mae: 0.2544 - val_mse: 0.1647 - learning_rate: 0.1000 - val_custom_mse: 0.3914 - val_custom_mae: 0.4336\n",
            "Epoch 14/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2686 - mae: 0.2847 - mse: 0.2686 - val_loss: 0.1627 - val_mae: 0.2520 - val_mse: 0.1627 - learning_rate: 0.1000 - val_custom_mse: 0.3890 - val_custom_mae: 0.4323\n",
            "Epoch 15/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2663 - mae: 0.2822 - mse: 0.2663 - val_loss: 0.1607 - val_mae: 0.2486 - val_mse: 0.1607 - learning_rate: 0.1000 - val_custom_mse: 0.3878 - val_custom_mae: 0.4311\n",
            "Epoch 16/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2641 - mae: 0.2799 - mse: 0.2641 - val_loss: 0.1594 - val_mae: 0.2466 - val_mse: 0.1594 - learning_rate: 0.1000 - val_custom_mse: 0.3868 - val_custom_mae: 0.4305\n",
            "Epoch 17/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2623 - mae: 0.2778 - mse: 0.2623 - val_loss: 0.1583 - val_mae: 0.2445 - val_mse: 0.1583 - learning_rate: 0.1000 - val_custom_mse: 0.3869 - val_custom_mae: 0.4304\n",
            "Epoch 18/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2605 - mae: 0.2760 - mse: 0.2605 - val_loss: 0.1560 - val_mae: 0.2415 - val_mse: 0.1560 - learning_rate: 0.1000 - val_custom_mse: 0.3834 - val_custom_mae: 0.4279\n",
            "Epoch 19/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2592 - mae: 0.2744 - mse: 0.2592 - val_loss: 0.1554 - val_mae: 0.2403 - val_mse: 0.1554 - learning_rate: 0.1000 - val_custom_mse: 0.3836 - val_custom_mae: 0.4281\n",
            "Epoch 20/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2579 - mae: 0.2730 - mse: 0.2579 - val_loss: 0.1542 - val_mae: 0.2382 - val_mse: 0.1542 - learning_rate: 0.1000 - val_custom_mse: 0.3826 - val_custom_mae: 0.4273\n",
            "Epoch 21/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2566 - mae: 0.2716 - mse: 0.2566 - val_loss: 0.1533 - val_mae: 0.2364 - val_mse: 0.1533 - learning_rate: 0.1000 - val_custom_mse: 0.3825 - val_custom_mae: 0.4270\n",
            "Epoch 22/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2557 - mae: 0.2704 - mse: 0.2557 - val_loss: 0.1526 - val_mae: 0.2355 - val_mse: 0.1526 - learning_rate: 0.1000 - val_custom_mse: 0.3815 - val_custom_mae: 0.4266\n",
            "Epoch 23/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2546 - mae: 0.2693 - mse: 0.2546 - val_loss: 0.1520 - val_mae: 0.2342 - val_mse: 0.1520 - learning_rate: 0.1000 - val_custom_mse: 0.3818 - val_custom_mae: 0.4265\n",
            "Epoch 24/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2537 - mae: 0.2683 - mse: 0.2537 - val_loss: 0.1513 - val_mae: 0.2339 - val_mse: 0.1513 - learning_rate: 0.1000 - val_custom_mse: 0.3801 - val_custom_mae: 0.4259\n",
            "Epoch 25/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2530 - mae: 0.2674 - mse: 0.2530 - val_loss: 0.1507 - val_mae: 0.2320 - val_mse: 0.1507 - learning_rate: 0.1000 - val_custom_mse: 0.3811 - val_custom_mae: 0.4259\n",
            "Epoch 26/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2519 - mae: 0.2665 - mse: 0.2519 - val_loss: 0.1502 - val_mae: 0.2311 - val_mse: 0.1502 - learning_rate: 0.1000 - val_custom_mse: 0.3806 - val_custom_mae: 0.4258\n",
            "Epoch 27/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2514 - mae: 0.2657 - mse: 0.2514 - val_loss: 0.1499 - val_mae: 0.2304 - val_mse: 0.1499 - learning_rate: 0.1000 - val_custom_mse: 0.3810 - val_custom_mae: 0.4259\n",
            "Epoch 28/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2504 - mae: 0.2648 - mse: 0.2504 - val_loss: 0.1494 - val_mae: 0.2293 - val_mse: 0.1494 - learning_rate: 0.1000 - val_custom_mse: 0.3810 - val_custom_mae: 0.4258\n",
            "Epoch 29/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2499 - mae: 0.2641 - mse: 0.2499 - val_loss: 0.1491 - val_mae: 0.2287 - val_mse: 0.1491 - learning_rate: 0.1000 - val_custom_mse: 0.3808 - val_custom_mae: 0.4256\n",
            "Epoch 30/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2493 - mae: 0.2636 - mse: 0.2493 - val_loss: 0.1484 - val_mae: 0.2276 - val_mse: 0.1484 - learning_rate: 0.1000 - val_custom_mse: 0.3799 - val_custom_mae: 0.4250\n",
            "Epoch 31/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2487 - mae: 0.2630 - mse: 0.2487 - val_loss: 0.1482 - val_mae: 0.2273 - val_mse: 0.1482 - learning_rate: 0.1000 - val_custom_mse: 0.3801 - val_custom_mae: 0.4251\n",
            "Epoch 32/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2484 - mae: 0.2625 - mse: 0.2484 - val_loss: 0.1472 - val_mae: 0.2264 - val_mse: 0.1472 - learning_rate: 0.1000 - val_custom_mse: 0.3779 - val_custom_mae: 0.4240\n",
            "Epoch 33/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2480 - mae: 0.2620 - mse: 0.2480 - val_loss: 0.1481 - val_mae: 0.2267 - val_mse: 0.1481 - learning_rate: 0.1000 - val_custom_mse: 0.3810 - val_custom_mae: 0.4256\n",
            "Epoch 34/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2475 - mae: 0.2615 - mse: 0.2475 - val_loss: 0.1468 - val_mae: 0.2255 - val_mse: 0.1468 - learning_rate: 0.1000 - val_custom_mse: 0.3778 - val_custom_mae: 0.4239\n",
            "Epoch 35/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2474 - mae: 0.2612 - mse: 0.2474 - val_loss: 0.1471 - val_mae: 0.2251 - val_mse: 0.1471 - learning_rate: 0.1000 - val_custom_mse: 0.3799 - val_custom_mae: 0.4248\n",
            "Epoch 36/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2466 - mae: 0.2606 - mse: 0.2466 - val_loss: 0.1467 - val_mae: 0.2247 - val_mse: 0.1467 - learning_rate: 0.1000 - val_custom_mse: 0.3791 - val_custom_mae: 0.4246\n",
            "Epoch 37/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2465 - mae: 0.2603 - mse: 0.2465 - val_loss: 0.1468 - val_mae: 0.2241 - val_mse: 0.1468 - learning_rate: 0.1000 - val_custom_mse: 0.3802 - val_custom_mae: 0.4249\n",
            "Epoch 38/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2459 - mae: 0.2598 - mse: 0.2459 - val_loss: 0.1459 - val_mae: 0.2237 - val_mse: 0.1459 - learning_rate: 0.1000 - val_custom_mse: 0.3776 - val_custom_mae: 0.4235\n",
            "Epoch 39/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2458 - mae: 0.2596 - mse: 0.2458 - val_loss: 0.1463 - val_mae: 0.2236 - val_mse: 0.1463 - learning_rate: 0.1000 - val_custom_mse: 0.3796 - val_custom_mae: 0.4247\n",
            "Epoch 40/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2454 - mae: 0.2593 - mse: 0.2454 - val_loss: 0.1461 - val_mae: 0.2235 - val_mse: 0.1461 - learning_rate: 0.1000 - val_custom_mse: 0.3791 - val_custom_mae: 0.4244\n",
            "Epoch 41/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2451 - mae: 0.2589 - mse: 0.2451 - val_loss: 0.1456 - val_mae: 0.2229 - val_mse: 0.1456 - learning_rate: 0.1000 - val_custom_mse: 0.3779 - val_custom_mae: 0.4237\n",
            "Epoch 42/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2452 - mae: 0.2589 - mse: 0.2452 - val_loss: 0.1458 - val_mae: 0.2229 - val_mse: 0.1458 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4244\n",
            "Epoch 43/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2446 - mae: 0.2584 - mse: 0.2446 - val_loss: 0.1454 - val_mae: 0.2222 - val_mse: 0.1454 - learning_rate: 0.1000 - val_custom_mse: 0.3787 - val_custom_mae: 0.4241\n",
            "Epoch 44/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2445 - mae: 0.2582 - mse: 0.2445 - val_loss: 0.1451 - val_mae: 0.2223 - val_mse: 0.1451 - learning_rate: 0.1000 - val_custom_mse: 0.3773 - val_custom_mae: 0.4235\n",
            "Epoch 45/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2442 - mae: 0.2580 - mse: 0.2442 - val_loss: 0.1453 - val_mae: 0.2221 - val_mse: 0.1453 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4237\n",
            "Epoch 46/100\n",
            "228/228 - 3s - 12ms/step - loss: 0.2438 - mae: 0.2577 - mse: 0.2438 - val_loss: 0.1447 - val_mae: 0.2218 - val_mse: 0.1447 - learning_rate: 0.1000 - val_custom_mse: 0.3770 - val_custom_mae: 0.4234\n",
            "Epoch 47/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2437 - mae: 0.2575 - mse: 0.2437 - val_loss: 0.1450 - val_mae: 0.2213 - val_mse: 0.1450 - learning_rate: 0.1000 - val_custom_mse: 0.3785 - val_custom_mae: 0.4238\n",
            "Epoch 48/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2432 - mae: 0.2571 - mse: 0.2432 - val_loss: 0.1448 - val_mae: 0.2212 - val_mse: 0.1448 - learning_rate: 0.1000 - val_custom_mse: 0.3782 - val_custom_mae: 0.4238\n",
            "Epoch 49/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2432 - mae: 0.2572 - mse: 0.2432 - val_loss: 0.1455 - val_mae: 0.2213 - val_mse: 0.1455 - learning_rate: 0.1000 - val_custom_mse: 0.3806 - val_custom_mae: 0.4249\n",
            "Epoch 50/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2433 - mae: 0.2570 - mse: 0.2433 - val_loss: 0.1450 - val_mae: 0.2211 - val_mse: 0.1450 - learning_rate: 0.1000 - val_custom_mse: 0.3791 - val_custom_mae: 0.4244\n",
            "Epoch 51/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2428 - mae: 0.2567 - mse: 0.2428 - val_loss: 0.1449 - val_mae: 0.2207 - val_mse: 0.1449 - learning_rate: 0.1000 - val_custom_mse: 0.3792 - val_custom_mae: 0.4243\n",
            "Epoch 52/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2428 - mae: 0.2567 - mse: 0.2428 - val_loss: 0.1451 - val_mae: 0.2208 - val_mse: 0.1451 - learning_rate: 0.1000 - val_custom_mse: 0.3797 - val_custom_mae: 0.4246\n",
            "Epoch 53/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2426 - mae: 0.2565 - mse: 0.2426 - val_loss: 0.1445 - val_mae: 0.2208 - val_mse: 0.1445 - learning_rate: 0.1000 - val_custom_mse: 0.3777 - val_custom_mae: 0.4237\n",
            "Epoch 54/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2427 - mae: 0.2565 - mse: 0.2427 - val_loss: 0.1446 - val_mae: 0.2205 - val_mse: 0.1446 - learning_rate: 0.1000 - val_custom_mse: 0.3787 - val_custom_mae: 0.4241\n",
            "Epoch 55/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2423 - mae: 0.2562 - mse: 0.2423 - val_loss: 0.1446 - val_mae: 0.2203 - val_mse: 0.1446 - learning_rate: 0.1000 - val_custom_mse: 0.3789 - val_custom_mae: 0.4243\n",
            "Epoch 56/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2422 - mae: 0.2560 - mse: 0.2422 - val_loss: 0.1444 - val_mae: 0.2205 - val_mse: 0.1444 - learning_rate: 0.1000 - val_custom_mse: 0.3778 - val_custom_mae: 0.4239\n",
            "Epoch 57/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2420 - mae: 0.2560 - mse: 0.2420 - val_loss: 0.1454 - val_mae: 0.2207 - val_mse: 0.1454 - learning_rate: 0.1000 - val_custom_mse: 0.3815 - val_custom_mae: 0.4257\n",
            "Epoch 58/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2421 - mae: 0.2559 - mse: 0.2421 - val_loss: 0.1443 - val_mae: 0.2202 - val_mse: 0.1443 - learning_rate: 0.1000 - val_custom_mse: 0.3781 - val_custom_mae: 0.4239\n",
            "Epoch 59/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2417 - mae: 0.2557 - mse: 0.2417 - val_loss: 0.1447 - val_mae: 0.2204 - val_mse: 0.1447 - learning_rate: 0.1000 - val_custom_mse: 0.3793 - val_custom_mae: 0.4246\n",
            "Epoch 60/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2411 - mae: 0.2553 - mse: 0.2411 - val_loss: 0.1440 - val_mae: 0.2201 - val_mse: 0.1440 - learning_rate: 0.1000 - val_custom_mse: 0.3771 - val_custom_mae: 0.4233\n",
            "Epoch 61/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2415 - mae: 0.2554 - mse: 0.2415 - val_loss: 0.1442 - val_mae: 0.2197 - val_mse: 0.1442 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4237\n",
            "Epoch 62/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2416 - mae: 0.2554 - mse: 0.2416 - val_loss: 0.1443 - val_mae: 0.2202 - val_mse: 0.1443 - learning_rate: 0.1000 - val_custom_mse: 0.3784 - val_custom_mae: 0.4243\n",
            "Epoch 63/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2414 - mae: 0.2553 - mse: 0.2414 - val_loss: 0.1440 - val_mae: 0.2194 - val_mse: 0.1440 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4239\n",
            "Epoch 64/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2412 - mae: 0.2551 - mse: 0.2412 - val_loss: 0.1439 - val_mae: 0.2198 - val_mse: 0.1439 - learning_rate: 0.1000 - val_custom_mse: 0.3775 - val_custom_mae: 0.4237\n",
            "Epoch 65/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2411 - mae: 0.2551 - mse: 0.2411 - val_loss: 0.1440 - val_mae: 0.2198 - val_mse: 0.1440 - learning_rate: 0.1000 - val_custom_mse: 0.3777 - val_custom_mae: 0.4238\n",
            "Epoch 66/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2409 - mae: 0.2551 - mse: 0.2409 - val_loss: 0.1441 - val_mae: 0.2193 - val_mse: 0.1441 - learning_rate: 0.1000 - val_custom_mse: 0.3788 - val_custom_mae: 0.4240\n",
            "Epoch 67/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2405 - mae: 0.2547 - mse: 0.2405 - val_loss: 0.1436 - val_mae: 0.2189 - val_mse: 0.1436 - learning_rate: 0.1000 - val_custom_mse: 0.3775 - val_custom_mae: 0.4234\n",
            "Epoch 68/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2408 - mae: 0.2549 - mse: 0.2408 - val_loss: 0.1439 - val_mae: 0.2190 - val_mse: 0.1439 - learning_rate: 0.1000 - val_custom_mse: 0.3787 - val_custom_mae: 0.4241\n",
            "Epoch 69/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2404 - mae: 0.2546 - mse: 0.2404 - val_loss: 0.1438 - val_mae: 0.2191 - val_mse: 0.1438 - learning_rate: 0.1000 - val_custom_mse: 0.3782 - val_custom_mae: 0.4238\n",
            "Epoch 70/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2402 - mae: 0.2544 - mse: 0.2402 - val_loss: 0.1438 - val_mae: 0.2188 - val_mse: 0.1438 - learning_rate: 0.1000 - val_custom_mse: 0.3786 - val_custom_mae: 0.4239\n",
            "Epoch 71/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2405 - mae: 0.2545 - mse: 0.2405 - val_loss: 0.1439 - val_mae: 0.2191 - val_mse: 0.1439 - learning_rate: 0.1000 - val_custom_mse: 0.3786 - val_custom_mae: 0.4241\n",
            "Epoch 72/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2403 - mae: 0.2544 - mse: 0.2403 - val_loss: 0.1439 - val_mae: 0.2188 - val_mse: 0.1439 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4242\n",
            "Epoch 73/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2402 - mae: 0.2544 - mse: 0.2402 - val_loss: 0.1441 - val_mae: 0.2187 - val_mse: 0.1441 - learning_rate: 0.1000 - val_custom_mse: 0.3797 - val_custom_mae: 0.4243\n",
            "Epoch 74/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2401 - mae: 0.2543 - mse: 0.2401 - val_loss: 0.1434 - val_mae: 0.2188 - val_mse: 0.1434 - learning_rate: 0.1000 - val_custom_mse: 0.3772 - val_custom_mae: 0.4233\n",
            "Epoch 75/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2401 - mae: 0.2542 - mse: 0.2401 - val_loss: 0.1439 - val_mae: 0.2188 - val_mse: 0.1439 - learning_rate: 0.1000 - val_custom_mse: 0.3790 - val_custom_mae: 0.4243\n",
            "Epoch 76/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2400 - mae: 0.2542 - mse: 0.2400 - val_loss: 0.1436 - val_mae: 0.2185 - val_mse: 0.1436 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4239\n",
            "Epoch 77/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2398 - mae: 0.2541 - mse: 0.2398 - val_loss: 0.1435 - val_mae: 0.2182 - val_mse: 0.1435 - learning_rate: 0.1000 - val_custom_mse: 0.3781 - val_custom_mae: 0.4233\n",
            "Epoch 78/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2398 - mae: 0.2540 - mse: 0.2398 - val_loss: 0.1443 - val_mae: 0.2190 - val_mse: 0.1443 - learning_rate: 0.1000 - val_custom_mse: 0.3804 - val_custom_mae: 0.4251\n",
            "Epoch 79/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2400 - mae: 0.2540 - mse: 0.2400 - val_loss: 0.1434 - val_mae: 0.2183 - val_mse: 0.1434 - learning_rate: 0.1000 - val_custom_mse: 0.3779 - val_custom_mae: 0.4236\n",
            "Epoch 80/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2397 - mae: 0.2539 - mse: 0.2397 - val_loss: 0.1431 - val_mae: 0.2180 - val_mse: 0.1431 - learning_rate: 0.1000 - val_custom_mse: 0.3771 - val_custom_mae: 0.4229\n",
            "Epoch 81/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2395 - mae: 0.2539 - mse: 0.2395 - val_loss: 0.1441 - val_mae: 0.2184 - val_mse: 0.1441 - learning_rate: 0.1000 - val_custom_mse: 0.3801 - val_custom_mae: 0.4244\n",
            "Epoch 82/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2392 - mae: 0.2537 - mse: 0.2392 - val_loss: 0.1435 - val_mae: 0.2181 - val_mse: 0.1435 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4238\n",
            "Epoch 83/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2394 - mae: 0.2537 - mse: 0.2394 - val_loss: 0.1435 - val_mae: 0.2182 - val_mse: 0.1435 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4237\n",
            "Epoch 84/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2393 - mae: 0.2536 - mse: 0.2393 - val_loss: 0.1440 - val_mae: 0.2182 - val_mse: 0.1440 - learning_rate: 0.1000 - val_custom_mse: 0.3800 - val_custom_mae: 0.4242\n",
            "Epoch 85/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2392 - mae: 0.2535 - mse: 0.2392 - val_loss: 0.1434 - val_mae: 0.2180 - val_mse: 0.1434 - learning_rate: 0.1000 - val_custom_mse: 0.3780 - val_custom_mae: 0.4234\n",
            "Epoch 86/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2393 - mae: 0.2536 - mse: 0.2393 - val_loss: 0.1441 - val_mae: 0.2185 - val_mse: 0.1441 - learning_rate: 0.1000 - val_custom_mse: 0.3800 - val_custom_mae: 0.4246\n",
            "Epoch 87/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2390 - mae: 0.2534 - mse: 0.2390 - val_loss: 0.1434 - val_mae: 0.2177 - val_mse: 0.1434 - learning_rate: 0.1000 - val_custom_mse: 0.3784 - val_custom_mae: 0.4234\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "228/228 - 2s - 8ms/step - loss: 0.2391 - mae: 0.2534 - mse: 0.2391 - val_loss: 0.1435 - val_mae: 0.2182 - val_mse: 0.1435 - learning_rate: 0.1000 - val_custom_mse: 0.3783 - val_custom_mae: 0.4239\n",
            "Epoch 89/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2388 - mae: 0.2534 - mse: 0.2388 - val_loss: 0.1438 - val_mae: 0.2180 - val_mse: 0.1438 - learning_rate: 0.0200 - val_custom_mse: 0.3797 - val_custom_mae: 0.4240\n",
            "Epoch 90/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2387 - mae: 0.2532 - mse: 0.2387 - val_loss: 0.1439 - val_mae: 0.2180 - val_mse: 0.1439 - learning_rate: 0.0200 - val_custom_mse: 0.3799 - val_custom_mae: 0.4241\n",
            "Epoch 91/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2391 - mae: 0.2533 - mse: 0.2391 - val_loss: 0.1439 - val_mae: 0.2180 - val_mse: 0.1439 - learning_rate: 0.0200 - val_custom_mse: 0.3799 - val_custom_mae: 0.4241\n",
            "Epoch 92/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2387 - mae: 0.2532 - mse: 0.2387 - val_loss: 0.1440 - val_mae: 0.2180 - val_mse: 0.1440 - learning_rate: 0.0200 - val_custom_mse: 0.3802 - val_custom_mae: 0.4242\n",
            "Epoch 93/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2388 - mae: 0.2532 - mse: 0.2388 - val_loss: 0.1440 - val_mae: 0.2180 - val_mse: 0.1440 - learning_rate: 0.0200 - val_custom_mse: 0.3804 - val_custom_mae: 0.4244\n",
            "Epoch 94/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2389 - mae: 0.2533 - mse: 0.2389 - val_loss: 0.1436 - val_mae: 0.2178 - val_mse: 0.1436 - learning_rate: 0.0200 - val_custom_mse: 0.3792 - val_custom_mae: 0.4236\n",
            "Epoch 95/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2389 - mae: 0.2532 - mse: 0.2389 - val_loss: 0.1440 - val_mae: 0.2180 - val_mse: 0.1440 - learning_rate: 0.0200 - val_custom_mse: 0.3804 - val_custom_mae: 0.4244\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "228/228 - 2s - 8ms/step - loss: 0.2388 - mae: 0.2531 - mse: 0.2388 - val_loss: 0.1440 - val_mae: 0.2180 - val_mse: 0.1440 - learning_rate: 0.0200 - val_custom_mse: 0.3804 - val_custom_mae: 0.4243\n",
            "Epoch 97/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2388 - mae: 0.2531 - mse: 0.2388 - val_loss: 0.1438 - val_mae: 0.2179 - val_mse: 0.1438 - learning_rate: 0.0040 - val_custom_mse: 0.3797 - val_custom_mae: 0.4240\n",
            "Epoch 98/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2390 - mae: 0.2532 - mse: 0.2390 - val_loss: 0.1438 - val_mae: 0.2179 - val_mse: 0.1438 - learning_rate: 0.0040 - val_custom_mse: 0.3797 - val_custom_mae: 0.4240\n",
            "Epoch 99/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2384 - mae: 0.2530 - mse: 0.2384 - val_loss: 0.1438 - val_mae: 0.2179 - val_mse: 0.1438 - learning_rate: 0.0040 - val_custom_mse: 0.3797 - val_custom_mae: 0.4240\n",
            "Epoch 100/100\n",
            "228/228 - 2s - 8ms/step - loss: 0.2386 - mae: 0.2530 - mse: 0.2386 - val_loss: 0.1439 - val_mae: 0.2179 - val_mse: 0.1439 - learning_rate: 0.0040 - val_custom_mse: 0.3799 - val_custom_mae: 0.4241\n",
            "Running experiment: horizon=720, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_45', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216/216 - 5s - 23ms/step - loss: 0.8565 - mae: 0.5967 - mse: 0.8565 - val_loss: 0.6798 - val_mae: 0.5804 - val_mse: 0.6798 - learning_rate: 0.1000 - val_custom_mse: 0.8445 - val_custom_mae: 0.6569\n",
            "Epoch 2/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.7712 - mae: 0.5678 - mse: 0.7712 - val_loss: 0.6507 - val_mae: 0.5641 - val_mse: 0.6507 - learning_rate: 0.1000 - val_custom_mse: 0.8209 - val_custom_mae: 0.6462\n",
            "Epoch 3/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.7311 - mae: 0.5500 - mse: 0.7311 - val_loss: 0.6213 - val_mae: 0.5465 - val_mse: 0.6213 - learning_rate: 0.1000 - val_custom_mse: 0.7958 - val_custom_mae: 0.6335\n",
            "Epoch 4/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.7042 - mae: 0.5363 - mse: 0.7042 - val_loss: 0.6344 - val_mae: 0.5465 - val_mse: 0.6344 - learning_rate: 0.1000 - val_custom_mse: 0.8301 - val_custom_mae: 0.6464\n",
            "Epoch 5/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6800 - mae: 0.5230 - mse: 0.6800 - val_loss: 0.5970 - val_mae: 0.5254 - val_mse: 0.5970 - learning_rate: 0.1000 - val_custom_mse: 0.7885 - val_custom_mae: 0.6273\n",
            "Epoch 6/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6567 - mae: 0.5099 - mse: 0.6567 - val_loss: 0.5738 - val_mae: 0.5112 - val_mse: 0.5738 - learning_rate: 0.1000 - val_custom_mse: 0.7597 - val_custom_mae: 0.6111\n",
            "Epoch 7/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6403 - mae: 0.4998 - mse: 0.6403 - val_loss: 0.5587 - val_mae: 0.5012 - val_mse: 0.5587 - learning_rate: 0.1000 - val_custom_mse: 0.7439 - val_custom_mae: 0.6032\n",
            "Epoch 8/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6284 - mae: 0.4919 - mse: 0.6284 - val_loss: 0.5464 - val_mae: 0.4916 - val_mse: 0.5464 - learning_rate: 0.1000 - val_custom_mse: 0.7346 - val_custom_mae: 0.5989\n",
            "Epoch 9/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6168 - mae: 0.4839 - mse: 0.6168 - val_loss: 0.5437 - val_mae: 0.4873 - val_mse: 0.5437 - learning_rate: 0.1000 - val_custom_mse: 0.7340 - val_custom_mae: 0.5968\n",
            "Epoch 10/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6088 - mae: 0.4778 - mse: 0.6088 - val_loss: 0.5310 - val_mae: 0.4802 - val_mse: 0.5310 - learning_rate: 0.1000 - val_custom_mse: 0.7172 - val_custom_mae: 0.5888\n",
            "Epoch 11/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6014 - mae: 0.4721 - mse: 0.6014 - val_loss: 0.5181 - val_mae: 0.4764 - val_mse: 0.5181 - learning_rate: 0.1000 - val_custom_mse: 0.6973 - val_custom_mae: 0.5820\n",
            "Epoch 12/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5948 - mae: 0.4670 - mse: 0.5948 - val_loss: 0.5176 - val_mae: 0.4694 - val_mse: 0.5176 - learning_rate: 0.1000 - val_custom_mse: 0.7047 - val_custom_mae: 0.5825\n",
            "Epoch 13/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5894 - mae: 0.4622 - mse: 0.5894 - val_loss: 0.5083 - val_mae: 0.4631 - val_mse: 0.5083 - learning_rate: 0.1000 - val_custom_mse: 0.6947 - val_custom_mae: 0.5783\n",
            "Epoch 14/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5855 - mae: 0.4584 - mse: 0.5855 - val_loss: 0.5087 - val_mae: 0.4627 - val_mse: 0.5087 - learning_rate: 0.1000 - val_custom_mse: 0.6944 - val_custom_mae: 0.5769\n",
            "Epoch 15/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5818 - mae: 0.4549 - mse: 0.5818 - val_loss: 0.5082 - val_mae: 0.4593 - val_mse: 0.5082 - learning_rate: 0.1000 - val_custom_mse: 0.6973 - val_custom_mae: 0.5776\n",
            "Epoch 16/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5775 - mae: 0.4511 - mse: 0.5775 - val_loss: 0.5052 - val_mae: 0.4561 - val_mse: 0.5052 - learning_rate: 0.1000 - val_custom_mse: 0.6952 - val_custom_mae: 0.5765\n",
            "Epoch 17/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5765 - mae: 0.4493 - mse: 0.5765 - val_loss: 0.5073 - val_mae: 0.4548 - val_mse: 0.5073 - learning_rate: 0.1000 - val_custom_mse: 0.7003 - val_custom_mae: 0.5788\n",
            "Epoch 18/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5733 - mae: 0.4462 - mse: 0.5733 - val_loss: 0.4903 - val_mae: 0.4516 - val_mse: 0.4903 - learning_rate: 0.1000 - val_custom_mse: 0.6726 - val_custom_mae: 0.5681\n",
            "Epoch 19/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5715 - mae: 0.4439 - mse: 0.5715 - val_loss: 0.5016 - val_mae: 0.4497 - val_mse: 0.5016 - learning_rate: 0.1000 - val_custom_mse: 0.6943 - val_custom_mae: 0.5754\n",
            "Epoch 20/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5693 - mae: 0.4417 - mse: 0.5693 - val_loss: 0.4999 - val_mae: 0.4483 - val_mse: 0.4999 - learning_rate: 0.1000 - val_custom_mse: 0.6928 - val_custom_mae: 0.5754\n",
            "Epoch 21/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5682 - mae: 0.4400 - mse: 0.5682 - val_loss: 0.4930 - val_mae: 0.4441 - val_mse: 0.4930 - learning_rate: 0.1000 - val_custom_mse: 0.6832 - val_custom_mae: 0.5701\n",
            "Epoch 22/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5665 - mae: 0.4381 - mse: 0.5665 - val_loss: 0.4908 - val_mae: 0.4421 - val_mse: 0.4908 - learning_rate: 0.1000 - val_custom_mse: 0.6811 - val_custom_mae: 0.5691\n",
            "Epoch 23/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5649 - mae: 0.4362 - mse: 0.5649 - val_loss: 0.4935 - val_mae: 0.4417 - val_mse: 0.4935 - learning_rate: 0.1000 - val_custom_mse: 0.6861 - val_custom_mae: 0.5711\n",
            "Epoch 24/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5635 - mae: 0.4345 - mse: 0.5635 - val_loss: 0.4988 - val_mae: 0.4431 - val_mse: 0.4988 - learning_rate: 0.1000 - val_custom_mse: 0.6944 - val_custom_mae: 0.5750\n",
            "Epoch 25/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5630 - mae: 0.4337 - mse: 0.5630 - val_loss: 0.4775 - val_mae: 0.4403 - val_mse: 0.4775 - learning_rate: 0.1000 - val_custom_mse: 0.6605 - val_custom_mae: 0.5626\n",
            "Epoch 26/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5627 - mae: 0.4326 - mse: 0.5627 - val_loss: 0.4833 - val_mae: 0.4368 - val_mse: 0.4833 - learning_rate: 0.1000 - val_custom_mse: 0.6718 - val_custom_mae: 0.5645\n",
            "Epoch 27/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5597 - mae: 0.4300 - mse: 0.5597 - val_loss: 0.4769 - val_mae: 0.4380 - val_mse: 0.4769 - learning_rate: 0.1000 - val_custom_mse: 0.6608 - val_custom_mae: 0.5618\n",
            "Epoch 28/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5620 - mae: 0.4311 - mse: 0.5620 - val_loss: 0.4879 - val_mae: 0.4348 - val_mse: 0.4879 - learning_rate: 0.1000 - val_custom_mse: 0.6811 - val_custom_mae: 0.5680\n",
            "Epoch 29/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5585 - mae: 0.4280 - mse: 0.5585 - val_loss: 0.4884 - val_mae: 0.4343 - val_mse: 0.4884 - learning_rate: 0.1000 - val_custom_mse: 0.6823 - val_custom_mae: 0.5686\n",
            "Epoch 30/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5595 - mae: 0.4281 - mse: 0.5595 - val_loss: 0.4745 - val_mae: 0.4344 - val_mse: 0.4745 - learning_rate: 0.1000 - val_custom_mse: 0.6594 - val_custom_mae: 0.5608\n",
            "Epoch 31/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5574 - mae: 0.4261 - mse: 0.5574 - val_loss: 0.5066 - val_mae: 0.4456 - val_mse: 0.5066 - learning_rate: 0.1000 - val_custom_mse: 0.7073 - val_custom_mae: 0.5817\n",
            "Epoch 32/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5564 - mae: 0.4250 - mse: 0.5564 - val_loss: 0.4863 - val_mae: 0.4311 - val_mse: 0.4863 - learning_rate: 0.1000 - val_custom_mse: 0.6808 - val_custom_mae: 0.5676\n",
            "Epoch 33/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5556 - mae: 0.4240 - mse: 0.5556 - val_loss: 0.4858 - val_mae: 0.4301 - val_mse: 0.4858 - learning_rate: 0.1000 - val_custom_mse: 0.6805 - val_custom_mae: 0.5677\n",
            "Epoch 34/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5569 - mae: 0.4246 - mse: 0.5569 - val_loss: 0.5068 - val_mae: 0.4417 - val_mse: 0.5068 - learning_rate: 0.1000 - val_custom_mse: 0.7098 - val_custom_mae: 0.5821\n",
            "Epoch 35/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5545 - mae: 0.4220 - mse: 0.5545 - val_loss: 0.4796 - val_mae: 0.4276 - val_mse: 0.4796 - learning_rate: 0.1000 - val_custom_mse: 0.6716 - val_custom_mae: 0.5633\n",
            "Epoch 36/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5551 - mae: 0.4224 - mse: 0.5551 - val_loss: 0.4895 - val_mae: 0.4306 - val_mse: 0.4895 - learning_rate: 0.1000 - val_custom_mse: 0.6867 - val_custom_mae: 0.5704\n",
            "Epoch 37/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5538 - mae: 0.4210 - mse: 0.5538 - val_loss: 0.4788 - val_mae: 0.4282 - val_mse: 0.4788 - learning_rate: 0.1000 - val_custom_mse: 0.6699 - val_custom_mae: 0.5624\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5534 - mae: 0.4202 - mse: 0.5534 - val_loss: 0.4806 - val_mae: 0.4249 - val_mse: 0.4806 - learning_rate: 0.1000 - val_custom_mse: 0.6748 - val_custom_mae: 0.5649\n",
            "Epoch 39/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5505 - mae: 0.4171 - mse: 0.5505 - val_loss: 0.4849 - val_mae: 0.4268 - val_mse: 0.4849 - learning_rate: 0.0200 - val_custom_mse: 0.6809 - val_custom_mae: 0.5672\n",
            "Epoch 40/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5502 - mae: 0.4170 - mse: 0.5502 - val_loss: 0.4820 - val_mae: 0.4261 - val_mse: 0.4820 - learning_rate: 0.0200 - val_custom_mse: 0.6765 - val_custom_mae: 0.5651\n",
            "Epoch 41/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5500 - mae: 0.4168 - mse: 0.5500 - val_loss: 0.4851 - val_mae: 0.4266 - val_mse: 0.4851 - learning_rate: 0.0200 - val_custom_mse: 0.6812 - val_custom_mae: 0.5673\n",
            "Epoch 42/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5499 - mae: 0.4167 - mse: 0.5499 - val_loss: 0.4783 - val_mae: 0.4253 - val_mse: 0.4783 - learning_rate: 0.0200 - val_custom_mse: 0.6710 - val_custom_mae: 0.5628\n",
            "Epoch 43/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5499 - mae: 0.4167 - mse: 0.5499 - val_loss: 0.4801 - val_mae: 0.4249 - val_mse: 0.4801 - learning_rate: 0.0200 - val_custom_mse: 0.6740 - val_custom_mae: 0.5641\n",
            "Epoch 44/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5496 - mae: 0.4164 - mse: 0.5496 - val_loss: 0.4821 - val_mae: 0.4253 - val_mse: 0.4821 - learning_rate: 0.0200 - val_custom_mse: 0.6772 - val_custom_mae: 0.5655\n",
            "Epoch 45/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5496 - mae: 0.4162 - mse: 0.5496 - val_loss: 0.4834 - val_mae: 0.4254 - val_mse: 0.4834 - learning_rate: 0.0200 - val_custom_mse: 0.6792 - val_custom_mae: 0.5665\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5495 - mae: 0.4162 - mse: 0.5495 - val_loss: 0.4787 - val_mae: 0.4241 - val_mse: 0.4787 - learning_rate: 0.0200 - val_custom_mse: 0.6722 - val_custom_mae: 0.5633\n",
            "Epoch 47/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5490 - mae: 0.4156 - mse: 0.5490 - val_loss: 0.4807 - val_mae: 0.4245 - val_mse: 0.4807 - learning_rate: 0.0040 - val_custom_mse: 0.6752 - val_custom_mae: 0.5646\n",
            "Epoch 48/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5490 - mae: 0.4156 - mse: 0.5490 - val_loss: 0.4803 - val_mae: 0.4245 - val_mse: 0.4803 - learning_rate: 0.0040 - val_custom_mse: 0.6747 - val_custom_mae: 0.5644\n",
            "Epoch 49/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5489 - mae: 0.4155 - mse: 0.5489 - val_loss: 0.4805 - val_mae: 0.4245 - val_mse: 0.4805 - learning_rate: 0.0040 - val_custom_mse: 0.6749 - val_custom_mae: 0.5645\n",
            "Epoch 50/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5489 - mae: 0.4155 - mse: 0.5489 - val_loss: 0.4792 - val_mae: 0.4243 - val_mse: 0.4792 - learning_rate: 0.0040 - val_custom_mse: 0.6730 - val_custom_mae: 0.5637\n",
            "Epoch 51/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5490 - mae: 0.4156 - mse: 0.5490 - val_loss: 0.4788 - val_mae: 0.4243 - val_mse: 0.4788 - learning_rate: 0.0040 - val_custom_mse: 0.6723 - val_custom_mae: 0.5633\n",
            "Epoch 52/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5489 - mae: 0.4154 - mse: 0.5489 - val_loss: 0.4814 - val_mae: 0.4247 - val_mse: 0.4814 - learning_rate: 0.0040 - val_custom_mse: 0.6764 - val_custom_mae: 0.5652\n",
            "Epoch 53/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5488 - mae: 0.4154 - mse: 0.5488 - val_loss: 0.4795 - val_mae: 0.4243 - val_mse: 0.4795 - learning_rate: 0.0040 - val_custom_mse: 0.6734 - val_custom_mae: 0.5638\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5488 - mae: 0.4154 - mse: 0.5488 - val_loss: 0.4800 - val_mae: 0.4243 - val_mse: 0.4800 - learning_rate: 0.0040 - val_custom_mse: 0.6742 - val_custom_mae: 0.5642\n",
            "Epoch 55/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5488 - mae: 0.4154 - mse: 0.5488 - val_loss: 0.4820 - val_mae: 0.4248 - val_mse: 0.4820 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6773 - val_custom_mae: 0.5656\n",
            "Epoch 56/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4153 - mse: 0.5487 - val_loss: 0.4817 - val_mae: 0.4247 - val_mse: 0.4817 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6768 - val_custom_mae: 0.5654\n",
            "Epoch 57/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4153 - mse: 0.5487 - val_loss: 0.4821 - val_mae: 0.4248 - val_mse: 0.4821 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6774 - val_custom_mae: 0.5657\n",
            "Epoch 58/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5487 - mae: 0.4153 - mse: 0.5487 - val_loss: 0.4818 - val_mae: 0.4247 - val_mse: 0.4818 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6770 - val_custom_mae: 0.5655\n",
            "Epoch 59/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5487 - mae: 0.4153 - mse: 0.5487 - val_loss: 0.4817 - val_mae: 0.4247 - val_mse: 0.4817 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6768 - val_custom_mae: 0.5654\n",
            "Epoch 60/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4153 - mse: 0.5487 - val_loss: 0.4821 - val_mae: 0.4248 - val_mse: 0.4821 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6774 - val_custom_mae: 0.5656\n",
            "Epoch 61/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4153 - mse: 0.5487 - val_loss: 0.4818 - val_mae: 0.4247 - val_mse: 0.4818 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6769 - val_custom_mae: 0.5654\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4818 - val_mae: 0.4247 - val_mse: 0.4818 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6769 - val_custom_mae: 0.5655\n",
            "Epoch 63/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5487 - mae: 0.4153 - mse: 0.5487 - val_loss: 0.4822 - val_mae: 0.4248 - val_mse: 0.4822 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6776 - val_custom_mae: 0.5658\n",
            "Epoch 64/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4823 - val_mae: 0.4248 - val_mse: 0.4823 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6778 - val_custom_mae: 0.5658\n",
            "Epoch 65/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4824 - val_mae: 0.4249 - val_mse: 0.4824 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6778 - val_custom_mae: 0.5659\n",
            "Epoch 66/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4823 - val_mae: 0.4248 - val_mse: 0.4823 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6777 - val_custom_mae: 0.5658\n",
            "Epoch 67/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4824 - val_mae: 0.4249 - val_mse: 0.4824 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6778 - val_custom_mae: 0.5659\n",
            "Epoch 68/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4824 - val_mae: 0.4249 - val_mse: 0.4824 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6779 - val_custom_mae: 0.5659\n",
            "Epoch 69/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4822 - val_mae: 0.4248 - val_mse: 0.4822 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6776 - val_custom_mae: 0.5658\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4823 - val_mae: 0.4248 - val_mse: 0.4823 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6777 - val_custom_mae: 0.5658\n",
            "Epoch 71/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4823 - val_mae: 0.4248 - val_mse: 0.4823 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6777 - val_custom_mae: 0.5658\n",
            "Epoch 72/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4824 - val_mae: 0.4249 - val_mse: 0.4824 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6779 - val_custom_mae: 0.5659\n",
            "Epoch 73/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4825 - val_mae: 0.4249 - val_mse: 0.4825 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6780 - val_custom_mae: 0.5659\n",
            "Epoch 74/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4825 - val_mae: 0.4249 - val_mse: 0.4825 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6780 - val_custom_mae: 0.5660\n",
            "Epoch 75/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4825 - val_mae: 0.4249 - val_mse: 0.4825 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6781 - val_custom_mae: 0.5660\n",
            "Epoch 76/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6781 - val_custom_mae: 0.5660\n",
            "Epoch 77/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6781 - val_custom_mae: 0.5660\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6781 - val_custom_mae: 0.5660\n",
            "Epoch 79/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6781 - val_custom_mae: 0.5660\n",
            "Epoch 80/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6781 - val_custom_mae: 0.5660\n",
            "Epoch 81/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6781 - val_custom_mae: 0.5660\n",
            "Epoch 82/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6781 - val_custom_mae: 0.5660\n",
            "Epoch 83/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6781 - val_custom_mae: 0.5660\n",
            "Epoch 84/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 85/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.2799998330592645e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 87/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 88/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 89/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 90/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 91/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 92/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 93/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 2.559999757067999e-07.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 95/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 2.5600e-07 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 96/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 2.5600e-07 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 97/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 2.5600e-07 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 98/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 2.5600e-07 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 99/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 2.5600e-07 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Epoch 100/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5487 - mae: 0.4152 - mse: 0.5487 - val_loss: 0.4826 - val_mae: 0.4249 - val_mse: 0.4826 - learning_rate: 2.5600e-07 - val_custom_mse: 0.6782 - val_custom_mae: 0.5660\n",
            "Running experiment: horizon=720, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_46', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216/216 - 6s - 28ms/step - loss: 0.8575 - mae: 0.5972 - mse: 0.8575 - val_loss: 0.6813 - val_mae: 0.5816 - val_mse: 0.6813 - learning_rate: 0.1000 - val_custom_mse: 0.8454 - val_custom_mae: 0.6576\n",
            "Epoch 2/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.7746 - mae: 0.5692 - mse: 0.7746 - val_loss: 0.6530 - val_mae: 0.5668 - val_mse: 0.6530 - learning_rate: 0.1000 - val_custom_mse: 0.8217 - val_custom_mae: 0.6483\n",
            "Epoch 3/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.7376 - mae: 0.5525 - mse: 0.7376 - val_loss: 0.6244 - val_mae: 0.5474 - val_mse: 0.6244 - learning_rate: 0.1000 - val_custom_mse: 0.7999 - val_custom_mae: 0.6346\n",
            "Epoch 4/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.7022 - mae: 0.5350 - mse: 0.7022 - val_loss: 0.6441 - val_mae: 0.5525 - val_mse: 0.6441 - learning_rate: 0.1000 - val_custom_mse: 0.8427 - val_custom_mae: 0.6544\n",
            "Epoch 5/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6785 - mae: 0.5221 - mse: 0.6785 - val_loss: 0.6198 - val_mae: 0.5363 - val_mse: 0.6198 - learning_rate: 0.1000 - val_custom_mse: 0.8202 - val_custom_mae: 0.6423\n",
            "Epoch 6/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6567 - mae: 0.5099 - mse: 0.6567 - val_loss: 0.5696 - val_mae: 0.5090 - val_mse: 0.5696 - learning_rate: 0.1000 - val_custom_mse: 0.7562 - val_custom_mae: 0.6105\n",
            "Epoch 7/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6419 - mae: 0.5007 - mse: 0.6419 - val_loss: 0.5539 - val_mae: 0.4990 - val_mse: 0.5539 - learning_rate: 0.1000 - val_custom_mse: 0.7388 - val_custom_mae: 0.6017\n",
            "Epoch 8/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6295 - mae: 0.4928 - mse: 0.6295 - val_loss: 0.5529 - val_mae: 0.4940 - val_mse: 0.5529 - learning_rate: 0.1000 - val_custom_mse: 0.7445 - val_custom_mae: 0.6029\n",
            "Epoch 9/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6184 - mae: 0.4855 - mse: 0.6184 - val_loss: 0.5505 - val_mae: 0.4904 - val_mse: 0.5505 - learning_rate: 0.1000 - val_custom_mse: 0.7451 - val_custom_mae: 0.6030\n",
            "Epoch 10/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6112 - mae: 0.4801 - mse: 0.6112 - val_loss: 0.5380 - val_mae: 0.4822 - val_mse: 0.5380 - learning_rate: 0.1000 - val_custom_mse: 0.7302 - val_custom_mae: 0.5951\n",
            "Epoch 11/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6041 - mae: 0.4749 - mse: 0.6041 - val_loss: 0.5362 - val_mae: 0.4792 - val_mse: 0.5362 - learning_rate: 0.1000 - val_custom_mse: 0.7301 - val_custom_mae: 0.5945\n",
            "Epoch 12/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5993 - mae: 0.4709 - mse: 0.5993 - val_loss: 0.5486 - val_mae: 0.4844 - val_mse: 0.5486 - learning_rate: 0.1000 - val_custom_mse: 0.7494 - val_custom_mae: 0.6043\n",
            "Epoch 13/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5957 - mae: 0.4674 - mse: 0.5957 - val_loss: 0.5290 - val_mae: 0.4723 - val_mse: 0.5290 - learning_rate: 0.1000 - val_custom_mse: 0.7239 - val_custom_mae: 0.5908\n",
            "Epoch 14/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5907 - mae: 0.4637 - mse: 0.5907 - val_loss: 0.5183 - val_mae: 0.4659 - val_mse: 0.5183 - learning_rate: 0.1000 - val_custom_mse: 0.7104 - val_custom_mae: 0.5844\n",
            "Epoch 15/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5879 - mae: 0.4609 - mse: 0.5879 - val_loss: 0.5048 - val_mae: 0.4599 - val_mse: 0.5048 - learning_rate: 0.1000 - val_custom_mse: 0.6912 - val_custom_mae: 0.5758\n",
            "Epoch 16/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5856 - mae: 0.4586 - mse: 0.5856 - val_loss: 0.5109 - val_mae: 0.4599 - val_mse: 0.5109 - learning_rate: 0.1000 - val_custom_mse: 0.7024 - val_custom_mae: 0.5802\n",
            "Epoch 17/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5832 - mae: 0.4563 - mse: 0.5832 - val_loss: 0.5066 - val_mae: 0.4568 - val_mse: 0.5066 - learning_rate: 0.1000 - val_custom_mse: 0.6976 - val_custom_mae: 0.5779\n",
            "Epoch 18/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5810 - mae: 0.4543 - mse: 0.5810 - val_loss: 0.5003 - val_mae: 0.4534 - val_mse: 0.5003 - learning_rate: 0.1000 - val_custom_mse: 0.6891 - val_custom_mae: 0.5740\n",
            "Epoch 19/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5801 - mae: 0.4528 - mse: 0.5801 - val_loss: 0.4922 - val_mae: 0.4519 - val_mse: 0.4922 - learning_rate: 0.1000 - val_custom_mse: 0.6760 - val_custom_mae: 0.5693\n",
            "Epoch 20/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5778 - mae: 0.4508 - mse: 0.5778 - val_loss: 0.5015 - val_mae: 0.4514 - val_mse: 0.5015 - learning_rate: 0.1000 - val_custom_mse: 0.6929 - val_custom_mae: 0.5750\n",
            "Epoch 21/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5763 - mae: 0.4491 - mse: 0.5763 - val_loss: 0.4971 - val_mae: 0.4489 - val_mse: 0.4971 - learning_rate: 0.1000 - val_custom_mse: 0.6872 - val_custom_mae: 0.5724\n",
            "Epoch 22/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5750 - mae: 0.4477 - mse: 0.5750 - val_loss: 0.4940 - val_mae: 0.4468 - val_mse: 0.4940 - learning_rate: 0.1000 - val_custom_mse: 0.6835 - val_custom_mae: 0.5708\n",
            "Epoch 23/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5735 - mae: 0.4464 - mse: 0.5735 - val_loss: 0.4919 - val_mae: 0.4452 - val_mse: 0.4919 - learning_rate: 0.1000 - val_custom_mse: 0.6813 - val_custom_mae: 0.5697\n",
            "Epoch 24/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5730 - mae: 0.4454 - mse: 0.5730 - val_loss: 0.4936 - val_mae: 0.4450 - val_mse: 0.4936 - learning_rate: 0.1000 - val_custom_mse: 0.6842 - val_custom_mae: 0.5705\n",
            "Epoch 25/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5723 - mae: 0.4445 - mse: 0.5723 - val_loss: 0.4977 - val_mae: 0.4458 - val_mse: 0.4977 - learning_rate: 0.1000 - val_custom_mse: 0.6909 - val_custom_mae: 0.5733\n",
            "Epoch 26/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5710 - mae: 0.4432 - mse: 0.5710 - val_loss: 0.4906 - val_mae: 0.4423 - val_mse: 0.4906 - learning_rate: 0.1000 - val_custom_mse: 0.6812 - val_custom_mae: 0.5691\n",
            "Epoch 27/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5702 - mae: 0.4422 - mse: 0.5702 - val_loss: 0.4951 - val_mae: 0.4434 - val_mse: 0.4951 - learning_rate: 0.1000 - val_custom_mse: 0.6882 - val_custom_mae: 0.5720\n",
            "Epoch 28/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5696 - mae: 0.4416 - mse: 0.5696 - val_loss: 0.4838 - val_mae: 0.4400 - val_mse: 0.4838 - learning_rate: 0.1000 - val_custom_mse: 0.6711 - val_custom_mae: 0.5650\n",
            "Epoch 29/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5693 - mae: 0.4409 - mse: 0.5693 - val_loss: 0.4940 - val_mae: 0.4419 - val_mse: 0.4940 - learning_rate: 0.1000 - val_custom_mse: 0.6875 - val_custom_mae: 0.5716\n",
            "Epoch 30/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5679 - mae: 0.4398 - mse: 0.5679 - val_loss: 0.4931 - val_mae: 0.4410 - val_mse: 0.4931 - learning_rate: 0.1000 - val_custom_mse: 0.6866 - val_custom_mae: 0.5711\n",
            "Epoch 31/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5673 - mae: 0.4390 - mse: 0.5673 - val_loss: 0.4849 - val_mae: 0.4376 - val_mse: 0.4849 - learning_rate: 0.1000 - val_custom_mse: 0.6748 - val_custom_mae: 0.5659\n",
            "Epoch 32/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5667 - mae: 0.4383 - mse: 0.5667 - val_loss: 0.4880 - val_mae: 0.4378 - val_mse: 0.4880 - learning_rate: 0.1000 - val_custom_mse: 0.6799 - val_custom_mae: 0.5678\n",
            "Epoch 33/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5667 - mae: 0.4380 - mse: 0.5667 - val_loss: 0.4888 - val_mae: 0.4377 - val_mse: 0.4888 - learning_rate: 0.1000 - val_custom_mse: 0.6814 - val_custom_mae: 0.5684\n",
            "Epoch 34/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5658 - mae: 0.4372 - mse: 0.5658 - val_loss: 0.4912 - val_mae: 0.4384 - val_mse: 0.4912 - learning_rate: 0.1000 - val_custom_mse: 0.6851 - val_custom_mae: 0.5701\n",
            "Epoch 35/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5661 - mae: 0.4370 - mse: 0.5661 - val_loss: 0.4832 - val_mae: 0.4348 - val_mse: 0.4832 - learning_rate: 0.1000 - val_custom_mse: 0.6738 - val_custom_mae: 0.5652\n",
            "Epoch 36/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5650 - mae: 0.4361 - mse: 0.5650 - val_loss: 0.4908 - val_mae: 0.4374 - val_mse: 0.4908 - learning_rate: 0.1000 - val_custom_mse: 0.6851 - val_custom_mae: 0.5698\n",
            "Epoch 37/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5654 - mae: 0.4361 - mse: 0.5654 - val_loss: 0.4827 - val_mae: 0.4342 - val_mse: 0.4827 - learning_rate: 0.1000 - val_custom_mse: 0.6734 - val_custom_mae: 0.5647\n",
            "Epoch 38/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5644 - mae: 0.4352 - mse: 0.5644 - val_loss: 0.4899 - val_mae: 0.4363 - val_mse: 0.4899 - learning_rate: 0.1000 - val_custom_mse: 0.6843 - val_custom_mae: 0.5694\n",
            "Epoch 39/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5641 - mae: 0.4350 - mse: 0.5641 - val_loss: 0.4838 - val_mae: 0.4336 - val_mse: 0.4838 - learning_rate: 0.1000 - val_custom_mse: 0.6757 - val_custom_mae: 0.5656\n",
            "Epoch 40/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5638 - mae: 0.4344 - mse: 0.5638 - val_loss: 0.4827 - val_mae: 0.4329 - val_mse: 0.4827 - learning_rate: 0.1000 - val_custom_mse: 0.6743 - val_custom_mae: 0.5649\n",
            "Epoch 41/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5632 - mae: 0.4339 - mse: 0.5632 - val_loss: 0.4872 - val_mae: 0.4342 - val_mse: 0.4872 - learning_rate: 0.1000 - val_custom_mse: 0.6812 - val_custom_mae: 0.5678\n",
            "Epoch 42/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5630 - mae: 0.4336 - mse: 0.5630 - val_loss: 0.4851 - val_mae: 0.4331 - val_mse: 0.4851 - learning_rate: 0.1000 - val_custom_mse: 0.6783 - val_custom_mae: 0.5667\n",
            "Epoch 43/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5627 - mae: 0.4332 - mse: 0.5627 - val_loss: 0.4871 - val_mae: 0.4336 - val_mse: 0.4871 - learning_rate: 0.1000 - val_custom_mse: 0.6813 - val_custom_mae: 0.5677\n",
            "Epoch 44/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5629 - mae: 0.4333 - mse: 0.5629 - val_loss: 0.4804 - val_mae: 0.4311 - val_mse: 0.4804 - learning_rate: 0.1000 - val_custom_mse: 0.6715 - val_custom_mae: 0.5635\n",
            "Epoch 45/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5622 - mae: 0.4326 - mse: 0.5622 - val_loss: 0.4880 - val_mae: 0.4336 - val_mse: 0.4880 - learning_rate: 0.1000 - val_custom_mse: 0.6829 - val_custom_mae: 0.5685\n",
            "Epoch 46/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5621 - mae: 0.4323 - mse: 0.5621 - val_loss: 0.4868 - val_mae: 0.4327 - val_mse: 0.4868 - learning_rate: 0.1000 - val_custom_mse: 0.6814 - val_custom_mae: 0.5676\n",
            "Epoch 47/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5621 - mae: 0.4322 - mse: 0.5621 - val_loss: 0.4791 - val_mae: 0.4300 - val_mse: 0.4791 - learning_rate: 0.1000 - val_custom_mse: 0.6701 - val_custom_mae: 0.5628\n",
            "Epoch 48/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5616 - mae: 0.4317 - mse: 0.5616 - val_loss: 0.4851 - val_mae: 0.4315 - val_mse: 0.4851 - learning_rate: 0.1000 - val_custom_mse: 0.6793 - val_custom_mae: 0.5666\n",
            "Epoch 49/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5615 - mae: 0.4316 - mse: 0.5615 - val_loss: 0.4846 - val_mae: 0.4311 - val_mse: 0.4846 - learning_rate: 0.1000 - val_custom_mse: 0.6787 - val_custom_mae: 0.5663\n",
            "Epoch 50/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5610 - mae: 0.4312 - mse: 0.5610 - val_loss: 0.4891 - val_mae: 0.4331 - val_mse: 0.4891 - learning_rate: 0.1000 - val_custom_mse: 0.6852 - val_custom_mae: 0.5692\n",
            "Epoch 51/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5609 - mae: 0.4311 - mse: 0.5609 - val_loss: 0.4870 - val_mae: 0.4320 - val_mse: 0.4870 - learning_rate: 0.1000 - val_custom_mse: 0.6823 - val_custom_mae: 0.5680\n",
            "Epoch 52/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5620 - mae: 0.4316 - mse: 0.5620 - val_loss: 0.4941 - val_mae: 0.4360 - val_mse: 0.4941 - learning_rate: 0.1000 - val_custom_mse: 0.6921 - val_custom_mae: 0.5725\n",
            "Epoch 53/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5608 - mae: 0.4308 - mse: 0.5608 - val_loss: 0.4805 - val_mae: 0.4289 - val_mse: 0.4805 - learning_rate: 0.1000 - val_custom_mse: 0.6731 - val_custom_mae: 0.5637\n",
            "Epoch 54/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5603 - mae: 0.4303 - mse: 0.5603 - val_loss: 0.4888 - val_mae: 0.4324 - val_mse: 0.4888 - learning_rate: 0.1000 - val_custom_mse: 0.6850 - val_custom_mae: 0.5691\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5603 - mae: 0.4302 - mse: 0.5603 - val_loss: 0.4900 - val_mae: 0.4332 - val_mse: 0.4900 - learning_rate: 0.1000 - val_custom_mse: 0.6868 - val_custom_mae: 0.5700\n",
            "Epoch 56/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5593 - mae: 0.4290 - mse: 0.5593 - val_loss: 0.4829 - val_mae: 0.4293 - val_mse: 0.4829 - learning_rate: 0.0200 - val_custom_mse: 0.6768 - val_custom_mae: 0.5651\n",
            "Epoch 57/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5591 - mae: 0.4291 - mse: 0.5591 - val_loss: 0.4830 - val_mae: 0.4293 - val_mse: 0.4830 - learning_rate: 0.0200 - val_custom_mse: 0.6770 - val_custom_mae: 0.5652\n",
            "Epoch 58/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5591 - mae: 0.4291 - mse: 0.5591 - val_loss: 0.4831 - val_mae: 0.4294 - val_mse: 0.4831 - learning_rate: 0.0200 - val_custom_mse: 0.6772 - val_custom_mae: 0.5653\n",
            "Epoch 59/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5591 - mae: 0.4292 - mse: 0.5591 - val_loss: 0.4839 - val_mae: 0.4297 - val_mse: 0.4839 - learning_rate: 0.0200 - val_custom_mse: 0.6784 - val_custom_mae: 0.5659\n",
            "Epoch 60/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5590 - mae: 0.4292 - mse: 0.5590 - val_loss: 0.4811 - val_mae: 0.4286 - val_mse: 0.4811 - learning_rate: 0.0200 - val_custom_mse: 0.6743 - val_custom_mae: 0.5641\n",
            "Epoch 61/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5592 - mae: 0.4291 - mse: 0.5592 - val_loss: 0.4827 - val_mae: 0.4291 - val_mse: 0.4827 - learning_rate: 0.0200 - val_custom_mse: 0.6767 - val_custom_mae: 0.5651\n",
            "Epoch 62/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5590 - mae: 0.4291 - mse: 0.5590 - val_loss: 0.4866 - val_mae: 0.4309 - val_mse: 0.4866 - learning_rate: 0.0200 - val_custom_mse: 0.6822 - val_custom_mae: 0.5677\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5591 - mae: 0.4291 - mse: 0.5591 - val_loss: 0.4843 - val_mae: 0.4298 - val_mse: 0.4843 - learning_rate: 0.0200 - val_custom_mse: 0.6791 - val_custom_mae: 0.5662\n",
            "Epoch 64/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5586 - mae: 0.4289 - mse: 0.5586 - val_loss: 0.4831 - val_mae: 0.4292 - val_mse: 0.4831 - learning_rate: 0.0040 - val_custom_mse: 0.6773 - val_custom_mae: 0.5654\n",
            "Epoch 65/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5587 - mae: 0.4288 - mse: 0.5587 - val_loss: 0.4839 - val_mae: 0.4296 - val_mse: 0.4839 - learning_rate: 0.0040 - val_custom_mse: 0.6785 - val_custom_mae: 0.5659\n",
            "Epoch 66/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5589 - mae: 0.4288 - mse: 0.5589 - val_loss: 0.4838 - val_mae: 0.4295 - val_mse: 0.4838 - learning_rate: 0.0040 - val_custom_mse: 0.6784 - val_custom_mae: 0.5659\n",
            "Epoch 67/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5589 - mae: 0.4289 - mse: 0.5589 - val_loss: 0.4831 - val_mae: 0.4292 - val_mse: 0.4831 - learning_rate: 0.0040 - val_custom_mse: 0.6773 - val_custom_mae: 0.5654\n",
            "Epoch 68/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5591 - mae: 0.4290 - mse: 0.5591 - val_loss: 0.4835 - val_mae: 0.4294 - val_mse: 0.4835 - learning_rate: 0.0040 - val_custom_mse: 0.6778 - val_custom_mae: 0.5657\n",
            "Epoch 69/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5590 - mae: 0.4289 - mse: 0.5590 - val_loss: 0.4832 - val_mae: 0.4293 - val_mse: 0.4832 - learning_rate: 0.0040 - val_custom_mse: 0.6775 - val_custom_mae: 0.5655\n",
            "Epoch 70/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5588 - mae: 0.4289 - mse: 0.5588 - val_loss: 0.4839 - val_mae: 0.4295 - val_mse: 0.4839 - learning_rate: 0.0040 - val_custom_mse: 0.6784 - val_custom_mae: 0.5659\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "216/216 - 2s - 8ms/step - loss: 0.5585 - mae: 0.4288 - mse: 0.5585 - val_loss: 0.4838 - val_mae: 0.4295 - val_mse: 0.4838 - learning_rate: 0.0040 - val_custom_mse: 0.6784 - val_custom_mae: 0.5659\n",
            "Epoch 72/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5585 - mae: 0.4288 - mse: 0.5585 - val_loss: 0.4841 - val_mae: 0.4296 - val_mse: 0.4841 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6787 - val_custom_mae: 0.5661\n",
            "Epoch 73/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5589 - mae: 0.4289 - mse: 0.5589 - val_loss: 0.4841 - val_mae: 0.4296 - val_mse: 0.4841 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6788 - val_custom_mae: 0.5661\n",
            "Epoch 74/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5587 - mae: 0.4288 - mse: 0.5587 - val_loss: 0.4841 - val_mae: 0.4297 - val_mse: 0.4841 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6788 - val_custom_mae: 0.5661\n",
            "Epoch 75/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5588 - mae: 0.4288 - mse: 0.5588 - val_loss: 0.4842 - val_mae: 0.4297 - val_mse: 0.4842 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6789 - val_custom_mae: 0.5661\n",
            "Epoch 76/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5586 - mae: 0.4288 - mse: 0.5586 - val_loss: 0.4842 - val_mae: 0.4297 - val_mse: 0.4842 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6789 - val_custom_mae: 0.5662\n",
            "Epoch 77/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5587 - mae: 0.4289 - mse: 0.5587 - val_loss: 0.4841 - val_mae: 0.4296 - val_mse: 0.4841 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6788 - val_custom_mae: 0.5661\n",
            "Epoch 78/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5587 - mae: 0.4288 - mse: 0.5587 - val_loss: 0.4843 - val_mae: 0.4297 - val_mse: 0.4843 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6791 - val_custom_mae: 0.5662\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "216/216 - 2s - 8ms/step - loss: 0.5586 - mae: 0.4288 - mse: 0.5586 - val_loss: 0.4842 - val_mae: 0.4297 - val_mse: 0.4842 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6789 - val_custom_mae: 0.5662\n",
            "Epoch 80/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5587 - mae: 0.4288 - mse: 0.5587 - val_loss: 0.4844 - val_mae: 0.4297 - val_mse: 0.4844 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6791 - val_custom_mae: 0.5663\n",
            "Epoch 81/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5587 - mae: 0.4288 - mse: 0.5587 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 82/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5585 - mae: 0.4287 - mse: 0.5585 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 83/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5588 - mae: 0.4289 - mse: 0.5588 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 84/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5586 - mae: 0.4288 - mse: 0.5586 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 85/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5586 - mae: 0.4288 - mse: 0.5586 - val_loss: 0.4845 - val_mae: 0.4298 - val_mse: 0.4845 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6793 - val_custom_mae: 0.5663\n",
            "Epoch 86/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5587 - mae: 0.4288 - mse: 0.5587 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5589 - mae: 0.4288 - mse: 0.5589 - val_loss: 0.4845 - val_mae: 0.4298 - val_mse: 0.4845 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6793 - val_custom_mae: 0.5663\n",
            "Epoch 88/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5586 - mae: 0.4288 - mse: 0.5586 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 89/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5585 - mae: 0.4287 - mse: 0.5585 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 90/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5586 - mae: 0.4288 - mse: 0.5586 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 91/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5585 - mae: 0.4287 - mse: 0.5585 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 92/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5584 - mae: 0.4287 - mse: 0.5584 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 93/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5585 - mae: 0.4287 - mse: 0.5585 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 94/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5587 - mae: 0.4288 - mse: 0.5587 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "216/216 - 2s - 8ms/step - loss: 0.5585 - mae: 0.4287 - mse: 0.5585 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 96/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5586 - mae: 0.4288 - mse: 0.5586 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 97/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5590 - mae: 0.4289 - mse: 0.5590 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 98/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5587 - mae: 0.4288 - mse: 0.5587 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 99/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5586 - mae: 0.4287 - mse: 0.5586 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Epoch 100/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5584 - mae: 0.4287 - mse: 0.5584 - val_loss: 0.4844 - val_mae: 0.4298 - val_mse: 0.4844 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6792 - val_custom_mae: 0.5663\n",
            "Running experiment: horizon=720, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_47', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216/216 - 6s - 28ms/step - loss: 0.8590 - mae: 0.5977 - mse: 0.8590 - val_loss: 0.6819 - val_mae: 0.5811 - val_mse: 0.6819 - learning_rate: 0.1000 - val_custom_mse: 0.8483 - val_custom_mae: 0.6584\n",
            "Epoch 2/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.7793 - mae: 0.5696 - mse: 0.7793 - val_loss: 0.7215 - val_mae: 0.6039 - val_mse: 0.7215 - learning_rate: 0.1000 - val_custom_mse: 0.9040 - val_custom_mae: 0.6903\n",
            "Epoch 3/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.7374 - mae: 0.5521 - mse: 0.7374 - val_loss: 0.6613 - val_mae: 0.5646 - val_mse: 0.6613 - learning_rate: 0.1000 - val_custom_mse: 0.8505 - val_custom_mae: 0.6581\n",
            "Epoch 4/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.7038 - mae: 0.5357 - mse: 0.7038 - val_loss: 0.6250 - val_mae: 0.5429 - val_mse: 0.6250 - learning_rate: 0.1000 - val_custom_mse: 0.8163 - val_custom_mae: 0.6414\n",
            "Epoch 5/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6796 - mae: 0.5224 - mse: 0.6796 - val_loss: 0.6343 - val_mae: 0.5439 - val_mse: 0.6343 - learning_rate: 0.1000 - val_custom_mse: 0.8398 - val_custom_mae: 0.6523\n",
            "Epoch 6/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6601 - mae: 0.5114 - mse: 0.6601 - val_loss: 0.5911 - val_mae: 0.5186 - val_mse: 0.5911 - learning_rate: 0.1000 - val_custom_mse: 0.7882 - val_custom_mae: 0.6257\n",
            "Epoch 7/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6431 - mae: 0.5016 - mse: 0.6431 - val_loss: 0.5708 - val_mae: 0.5054 - val_mse: 0.5708 - learning_rate: 0.1000 - val_custom_mse: 0.7645 - val_custom_mae: 0.6126\n",
            "Epoch 8/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6311 - mae: 0.4939 - mse: 0.6311 - val_loss: 0.5527 - val_mae: 0.4943 - val_mse: 0.5527 - learning_rate: 0.1000 - val_custom_mse: 0.7433 - val_custom_mae: 0.6023\n",
            "Epoch 9/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6216 - mae: 0.4875 - mse: 0.6216 - val_loss: 0.5531 - val_mae: 0.4918 - val_mse: 0.5531 - learning_rate: 0.1000 - val_custom_mse: 0.7482 - val_custom_mae: 0.6041\n",
            "Epoch 10/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6137 - mae: 0.4819 - mse: 0.6137 - val_loss: 0.5489 - val_mae: 0.4877 - val_mse: 0.5489 - learning_rate: 0.1000 - val_custom_mse: 0.7452 - val_custom_mae: 0.6023\n",
            "Epoch 11/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6080 - mae: 0.4775 - mse: 0.6080 - val_loss: 0.5219 - val_mae: 0.4747 - val_mse: 0.5219 - learning_rate: 0.1000 - val_custom_mse: 0.7076 - val_custom_mae: 0.5852\n",
            "Epoch 12/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6040 - mae: 0.4743 - mse: 0.6040 - val_loss: 0.5142 - val_mae: 0.4704 - val_mse: 0.5142 - learning_rate: 0.1000 - val_custom_mse: 0.6981 - val_custom_mae: 0.5811\n",
            "Epoch 13/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5987 - mae: 0.4702 - mse: 0.5987 - val_loss: 0.5271 - val_mae: 0.4726 - val_mse: 0.5271 - learning_rate: 0.1000 - val_custom_mse: 0.7204 - val_custom_mae: 0.5898\n",
            "Epoch 14/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5953 - mae: 0.4672 - mse: 0.5953 - val_loss: 0.5156 - val_mae: 0.4661 - val_mse: 0.5156 - learning_rate: 0.1000 - val_custom_mse: 0.7050 - val_custom_mae: 0.5823\n",
            "Epoch 15/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5923 - mae: 0.4646 - mse: 0.5923 - val_loss: 0.5112 - val_mae: 0.4628 - val_mse: 0.5112 - learning_rate: 0.1000 - val_custom_mse: 0.7003 - val_custom_mae: 0.5800\n",
            "Epoch 16/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5907 - mae: 0.4627 - mse: 0.5907 - val_loss: 0.5137 - val_mae: 0.4625 - val_mse: 0.5137 - learning_rate: 0.1000 - val_custom_mse: 0.7052 - val_custom_mae: 0.5818\n",
            "Epoch 17/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5880 - mae: 0.4605 - mse: 0.5880 - val_loss: 0.5191 - val_mae: 0.4645 - val_mse: 0.5191 - learning_rate: 0.1000 - val_custom_mse: 0.7138 - val_custom_mae: 0.5860\n",
            "Epoch 18/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5862 - mae: 0.4589 - mse: 0.5862 - val_loss: 0.5057 - val_mae: 0.4571 - val_mse: 0.5057 - learning_rate: 0.1000 - val_custom_mse: 0.6960 - val_custom_mae: 0.5776\n",
            "Epoch 19/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5848 - mae: 0.4572 - mse: 0.5848 - val_loss: 0.5061 - val_mae: 0.4562 - val_mse: 0.5061 - learning_rate: 0.1000 - val_custom_mse: 0.6973 - val_custom_mae: 0.5776\n",
            "Epoch 20/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5836 - mae: 0.4561 - mse: 0.5836 - val_loss: 0.5001 - val_mae: 0.4529 - val_mse: 0.5001 - learning_rate: 0.1000 - val_custom_mse: 0.6894 - val_custom_mae: 0.5740\n",
            "Epoch 21/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5818 - mae: 0.4544 - mse: 0.5818 - val_loss: 0.5082 - val_mae: 0.4560 - val_mse: 0.5082 - learning_rate: 0.1000 - val_custom_mse: 0.7015 - val_custom_mae: 0.5796\n",
            "Epoch 22/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5808 - mae: 0.4535 - mse: 0.5808 - val_loss: 0.4955 - val_mae: 0.4495 - val_mse: 0.4955 - learning_rate: 0.1000 - val_custom_mse: 0.6840 - val_custom_mae: 0.5714\n",
            "Epoch 23/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5793 - mae: 0.4521 - mse: 0.5793 - val_loss: 0.4906 - val_mae: 0.4478 - val_mse: 0.4906 - learning_rate: 0.1000 - val_custom_mse: 0.6768 - val_custom_mae: 0.5686\n",
            "Epoch 24/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5789 - mae: 0.4513 - mse: 0.5789 - val_loss: 0.4955 - val_mae: 0.4481 - val_mse: 0.4955 - learning_rate: 0.1000 - val_custom_mse: 0.6852 - val_custom_mae: 0.5716\n",
            "Epoch 25/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5781 - mae: 0.4506 - mse: 0.5781 - val_loss: 0.5000 - val_mae: 0.4496 - val_mse: 0.5000 - learning_rate: 0.1000 - val_custom_mse: 0.6923 - val_custom_mae: 0.5748\n",
            "Epoch 26/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5781 - mae: 0.4502 - mse: 0.5781 - val_loss: 0.4912 - val_mae: 0.4453 - val_mse: 0.4912 - learning_rate: 0.1000 - val_custom_mse: 0.6800 - val_custom_mae: 0.5692\n",
            "Epoch 27/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5768 - mae: 0.4490 - mse: 0.5768 - val_loss: 0.4974 - val_mae: 0.4474 - val_mse: 0.4974 - learning_rate: 0.1000 - val_custom_mse: 0.6895 - val_custom_mae: 0.5733\n",
            "Epoch 28/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5757 - mae: 0.4481 - mse: 0.5757 - val_loss: 0.4858 - val_mae: 0.4433 - val_mse: 0.4858 - learning_rate: 0.1000 - val_custom_mse: 0.6723 - val_custom_mae: 0.5661\n",
            "Epoch 29/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5762 - mae: 0.4479 - mse: 0.5762 - val_loss: 0.4934 - val_mae: 0.4446 - val_mse: 0.4934 - learning_rate: 0.1000 - val_custom_mse: 0.6845 - val_custom_mae: 0.5708\n",
            "Epoch 30/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5745 - mae: 0.4468 - mse: 0.5745 - val_loss: 0.4978 - val_mae: 0.4464 - val_mse: 0.4978 - learning_rate: 0.1000 - val_custom_mse: 0.6910 - val_custom_mae: 0.5737\n",
            "Epoch 31/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5745 - mae: 0.4466 - mse: 0.5745 - val_loss: 0.4897 - val_mae: 0.4422 - val_mse: 0.4897 - learning_rate: 0.1000 - val_custom_mse: 0.6799 - val_custom_mae: 0.5687\n",
            "Epoch 32/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5739 - mae: 0.4460 - mse: 0.5739 - val_loss: 0.4839 - val_mae: 0.4408 - val_mse: 0.4839 - learning_rate: 0.1000 - val_custom_mse: 0.6709 - val_custom_mae: 0.5651\n",
            "Epoch 33/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5738 - mae: 0.4455 - mse: 0.5738 - val_loss: 0.4898 - val_mae: 0.4416 - val_mse: 0.4898 - learning_rate: 0.1000 - val_custom_mse: 0.6805 - val_custom_mae: 0.5688\n",
            "Epoch 34/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5725 - mae: 0.4446 - mse: 0.5725 - val_loss: 0.4874 - val_mae: 0.4404 - val_mse: 0.4874 - learning_rate: 0.1000 - val_custom_mse: 0.6770 - val_custom_mae: 0.5671\n",
            "Epoch 35/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5716 - mae: 0.4441 - mse: 0.5716 - val_loss: 0.4886 - val_mae: 0.4405 - val_mse: 0.4886 - learning_rate: 0.1000 - val_custom_mse: 0.6793 - val_custom_mae: 0.5681\n",
            "Epoch 36/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5720 - mae: 0.4441 - mse: 0.5720 - val_loss: 0.4915 - val_mae: 0.4415 - val_mse: 0.4915 - learning_rate: 0.1000 - val_custom_mse: 0.6836 - val_custom_mae: 0.5699\n",
            "Epoch 37/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5711 - mae: 0.4434 - mse: 0.5711 - val_loss: 0.4957 - val_mae: 0.4434 - val_mse: 0.4957 - learning_rate: 0.1000 - val_custom_mse: 0.6896 - val_custom_mae: 0.5727\n",
            "Epoch 38/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5709 - mae: 0.4431 - mse: 0.5709 - val_loss: 0.4844 - val_mae: 0.4382 - val_mse: 0.4844 - learning_rate: 0.1000 - val_custom_mse: 0.6736 - val_custom_mae: 0.5655\n",
            "Epoch 39/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5705 - mae: 0.4427 - mse: 0.5705 - val_loss: 0.4938 - val_mae: 0.4419 - val_mse: 0.4938 - learning_rate: 0.1000 - val_custom_mse: 0.6872 - val_custom_mae: 0.5714\n",
            "Epoch 40/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5707 - mae: 0.4427 - mse: 0.5707 - val_loss: 0.4809 - val_mae: 0.4370 - val_mse: 0.4809 - learning_rate: 0.1000 - val_custom_mse: 0.6685 - val_custom_mae: 0.5634\n",
            "Epoch 41/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5703 - mae: 0.4423 - mse: 0.5703 - val_loss: 0.4960 - val_mae: 0.4428 - val_mse: 0.4960 - learning_rate: 0.1000 - val_custom_mse: 0.6906 - val_custom_mae: 0.5730\n",
            "Epoch 42/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5699 - mae: 0.4422 - mse: 0.5699 - val_loss: 0.4889 - val_mae: 0.4388 - val_mse: 0.4889 - learning_rate: 0.1000 - val_custom_mse: 0.6808 - val_custom_mae: 0.5683\n",
            "Epoch 43/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5700 - mae: 0.4419 - mse: 0.5700 - val_loss: 0.4883 - val_mae: 0.4385 - val_mse: 0.4883 - learning_rate: 0.1000 - val_custom_mse: 0.6801 - val_custom_mae: 0.5681\n",
            "Epoch 44/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5689 - mae: 0.4413 - mse: 0.5689 - val_loss: 0.4882 - val_mae: 0.4381 - val_mse: 0.4882 - learning_rate: 0.1000 - val_custom_mse: 0.6800 - val_custom_mae: 0.5678\n",
            "Epoch 45/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5692 - mae: 0.4412 - mse: 0.5692 - val_loss: 0.4882 - val_mae: 0.4381 - val_mse: 0.4882 - learning_rate: 0.1000 - val_custom_mse: 0.6802 - val_custom_mae: 0.5679\n",
            "Epoch 46/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5690 - mae: 0.4410 - mse: 0.5690 - val_loss: 0.4932 - val_mae: 0.4404 - val_mse: 0.4932 - learning_rate: 0.1000 - val_custom_mse: 0.6873 - val_custom_mae: 0.5711\n",
            "Epoch 47/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5686 - mae: 0.4407 - mse: 0.5686 - val_loss: 0.4839 - val_mae: 0.4360 - val_mse: 0.4839 - learning_rate: 0.1000 - val_custom_mse: 0.6741 - val_custom_mae: 0.5651\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5687 - mae: 0.4407 - mse: 0.5687 - val_loss: 0.4883 - val_mae: 0.4376 - val_mse: 0.4883 - learning_rate: 0.1000 - val_custom_mse: 0.6806 - val_custom_mae: 0.5678\n",
            "Epoch 49/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5675 - mae: 0.4401 - mse: 0.5675 - val_loss: 0.4895 - val_mae: 0.4381 - val_mse: 0.4895 - learning_rate: 0.0200 - val_custom_mse: 0.6823 - val_custom_mae: 0.5686\n",
            "Epoch 50/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5676 - mae: 0.4401 - mse: 0.5676 - val_loss: 0.4914 - val_mae: 0.4389 - val_mse: 0.4914 - learning_rate: 0.0200 - val_custom_mse: 0.6850 - val_custom_mae: 0.5697\n",
            "Epoch 51/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5672 - mae: 0.4399 - mse: 0.5672 - val_loss: 0.4913 - val_mae: 0.4388 - val_mse: 0.4913 - learning_rate: 0.0200 - val_custom_mse: 0.6849 - val_custom_mae: 0.5697\n",
            "Epoch 52/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5673 - mae: 0.4399 - mse: 0.5673 - val_loss: 0.4949 - val_mae: 0.4407 - val_mse: 0.4949 - learning_rate: 0.0200 - val_custom_mse: 0.6898 - val_custom_mae: 0.5719\n",
            "Epoch 53/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5672 - mae: 0.4398 - mse: 0.5672 - val_loss: 0.4895 - val_mae: 0.4379 - val_mse: 0.4895 - learning_rate: 0.0200 - val_custom_mse: 0.6824 - val_custom_mae: 0.5684\n",
            "Epoch 54/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5674 - mae: 0.4399 - mse: 0.5674 - val_loss: 0.4914 - val_mae: 0.4388 - val_mse: 0.4914 - learning_rate: 0.0200 - val_custom_mse: 0.6851 - val_custom_mae: 0.5696\n",
            "Epoch 55/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5673 - mae: 0.4398 - mse: 0.5673 - val_loss: 0.4877 - val_mae: 0.4370 - val_mse: 0.4877 - learning_rate: 0.0200 - val_custom_mse: 0.6799 - val_custom_mae: 0.5672\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5673 - mae: 0.4397 - mse: 0.5673 - val_loss: 0.4905 - val_mae: 0.4382 - val_mse: 0.4905 - learning_rate: 0.0200 - val_custom_mse: 0.6838 - val_custom_mae: 0.5690\n",
            "Epoch 57/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5671 - mae: 0.4396 - mse: 0.5671 - val_loss: 0.4896 - val_mae: 0.4378 - val_mse: 0.4896 - learning_rate: 0.0040 - val_custom_mse: 0.6826 - val_custom_mae: 0.5684\n",
            "Epoch 58/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5670 - mae: 0.4396 - mse: 0.5670 - val_loss: 0.4892 - val_mae: 0.4376 - val_mse: 0.4892 - learning_rate: 0.0040 - val_custom_mse: 0.6820 - val_custom_mae: 0.5682\n",
            "Epoch 59/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5667 - mae: 0.4395 - mse: 0.5667 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 0.0040 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 60/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5669 - mae: 0.4396 - mse: 0.5669 - val_loss: 0.4893 - val_mae: 0.4377 - val_mse: 0.4893 - learning_rate: 0.0040 - val_custom_mse: 0.6821 - val_custom_mae: 0.5682\n",
            "Epoch 61/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5670 - mae: 0.4396 - mse: 0.5670 - val_loss: 0.4896 - val_mae: 0.4378 - val_mse: 0.4896 - learning_rate: 0.0040 - val_custom_mse: 0.6826 - val_custom_mae: 0.5685\n",
            "Epoch 62/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5670 - mae: 0.4395 - mse: 0.5670 - val_loss: 0.4895 - val_mae: 0.4378 - val_mse: 0.4895 - learning_rate: 0.0040 - val_custom_mse: 0.6824 - val_custom_mae: 0.5684\n",
            "Epoch 63/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5669 - mae: 0.4395 - mse: 0.5669 - val_loss: 0.4902 - val_mae: 0.4381 - val_mse: 0.4902 - learning_rate: 0.0040 - val_custom_mse: 0.6834 - val_custom_mae: 0.5688\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5670 - mae: 0.4396 - mse: 0.5670 - val_loss: 0.4893 - val_mae: 0.4376 - val_mse: 0.4893 - learning_rate: 0.0040 - val_custom_mse: 0.6821 - val_custom_mae: 0.5682\n",
            "Epoch 65/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5674 - mae: 0.4396 - mse: 0.5674 - val_loss: 0.4898 - val_mae: 0.4379 - val_mse: 0.4898 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6829 - val_custom_mae: 0.5686\n",
            "Epoch 66/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5669 - mae: 0.4396 - mse: 0.5669 - val_loss: 0.4898 - val_mae: 0.4379 - val_mse: 0.4898 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6829 - val_custom_mae: 0.5686\n",
            "Epoch 67/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5673 - mae: 0.4397 - mse: 0.5673 - val_loss: 0.4896 - val_mae: 0.4378 - val_mse: 0.4896 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6826 - val_custom_mae: 0.5685\n",
            "Epoch 68/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5671 - mae: 0.4395 - mse: 0.5671 - val_loss: 0.4898 - val_mae: 0.4379 - val_mse: 0.4898 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6829 - val_custom_mae: 0.5686\n",
            "Epoch 69/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5669 - mae: 0.4395 - mse: 0.5669 - val_loss: 0.4899 - val_mae: 0.4380 - val_mse: 0.4899 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6830 - val_custom_mae: 0.5686\n",
            "Epoch 70/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5672 - mae: 0.4396 - mse: 0.5672 - val_loss: 0.4899 - val_mae: 0.4379 - val_mse: 0.4899 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6830 - val_custom_mae: 0.5686\n",
            "Epoch 71/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5672 - mae: 0.4396 - mse: 0.5672 - val_loss: 0.4898 - val_mae: 0.4379 - val_mse: 0.4898 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6828 - val_custom_mae: 0.5685\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5672 - mae: 0.4396 - mse: 0.5672 - val_loss: 0.4898 - val_mae: 0.4379 - val_mse: 0.4898 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6829 - val_custom_mae: 0.5686\n",
            "Epoch 73/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5667 - mae: 0.4395 - mse: 0.5667 - val_loss: 0.4896 - val_mae: 0.4378 - val_mse: 0.4896 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6826 - val_custom_mae: 0.5684\n",
            "Epoch 74/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5669 - mae: 0.4395 - mse: 0.5669 - val_loss: 0.4895 - val_mae: 0.4378 - val_mse: 0.4895 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6825 - val_custom_mae: 0.5684\n",
            "Epoch 75/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5674 - mae: 0.4397 - mse: 0.5674 - val_loss: 0.4895 - val_mae: 0.4378 - val_mse: 0.4895 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6825 - val_custom_mae: 0.5684\n",
            "Epoch 76/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5669 - mae: 0.4395 - mse: 0.5669 - val_loss: 0.4895 - val_mae: 0.4378 - val_mse: 0.4895 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6825 - val_custom_mae: 0.5684\n",
            "Epoch 77/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5670 - mae: 0.4396 - mse: 0.5670 - val_loss: 0.4895 - val_mae: 0.4377 - val_mse: 0.4895 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6824 - val_custom_mae: 0.5684\n",
            "Epoch 78/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5666 - mae: 0.4395 - mse: 0.5666 - val_loss: 0.4895 - val_mae: 0.4377 - val_mse: 0.4895 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6825 - val_custom_mae: 0.5684\n",
            "Epoch 79/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5674 - mae: 0.4396 - mse: 0.5674 - val_loss: 0.4895 - val_mae: 0.4377 - val_mse: 0.4895 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6824 - val_custom_mae: 0.5684\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 80: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5671 - mae: 0.4395 - mse: 0.5671 - val_loss: 0.4895 - val_mae: 0.4377 - val_mse: 0.4895 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6824 - val_custom_mae: 0.5683\n",
            "Epoch 81/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5666 - mae: 0.4394 - mse: 0.5666 - val_loss: 0.4895 - val_mae: 0.4377 - val_mse: 0.4895 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6824 - val_custom_mae: 0.5683\n",
            "Epoch 82/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5669 - mae: 0.4395 - mse: 0.5669 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6824 - val_custom_mae: 0.5683\n",
            "Epoch 83/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5670 - mae: 0.4396 - mse: 0.5670 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6824 - val_custom_mae: 0.5683\n",
            "Epoch 84/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5670 - mae: 0.4396 - mse: 0.5670 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 85/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5668 - mae: 0.4395 - mse: 0.5668 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 86/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5668 - mae: 0.4395 - mse: 0.5668 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 87/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5666 - mae: 0.4395 - mse: 0.5666 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5666 - mae: 0.4394 - mse: 0.5666 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 89/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5666 - mae: 0.4394 - mse: 0.5666 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 90/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5667 - mae: 0.4394 - mse: 0.5667 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 91/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5668 - mae: 0.4394 - mse: 0.5668 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 92/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5675 - mae: 0.4397 - mse: 0.5675 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 93/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5667 - mae: 0.4395 - mse: 0.5667 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 94/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5670 - mae: 0.4395 - mse: 0.5670 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 95/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5668 - mae: 0.4395 - mse: 0.5668 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 1.2799998330592645e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5663 - mae: 0.4394 - mse: 0.5663 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 97/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5668 - mae: 0.4395 - mse: 0.5668 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 98/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5667 - mae: 0.4395 - mse: 0.5667 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 99/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5667 - mae: 0.4394 - mse: 0.5667 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Epoch 100/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5668 - mae: 0.4395 - mse: 0.5668 - val_loss: 0.4894 - val_mae: 0.4377 - val_mse: 0.4894 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6823 - val_custom_mae: 0.5683\n",
            "Running experiment: horizon=720, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_48', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216/216 - 6s - 27ms/step - loss: 0.8633 - mae: 0.5993 - mse: 0.8633 - val_loss: 0.7172 - val_mae: 0.5975 - val_mse: 0.7172 - learning_rate: 0.1000 - val_custom_mse: 0.8863 - val_custom_mae: 0.6738\n",
            "Epoch 2/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.7684 - mae: 0.5666 - mse: 0.7684 - val_loss: 0.6442 - val_mae: 0.5629 - val_mse: 0.6442 - learning_rate: 0.1000 - val_custom_mse: 0.8090 - val_custom_mae: 0.6426\n",
            "Epoch 3/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.7407 - mae: 0.5536 - mse: 0.7407 - val_loss: 0.6472 - val_mae: 0.5584 - val_mse: 0.6472 - learning_rate: 0.1000 - val_custom_mse: 0.8307 - val_custom_mae: 0.6493\n",
            "Epoch 4/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.7061 - mae: 0.5370 - mse: 0.7061 - val_loss: 0.6345 - val_mae: 0.5477 - val_mse: 0.6345 - learning_rate: 0.1000 - val_custom_mse: 0.8290 - val_custom_mae: 0.6475\n",
            "Epoch 5/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6813 - mae: 0.5235 - mse: 0.6813 - val_loss: 0.6397 - val_mae: 0.5469 - val_mse: 0.6397 - learning_rate: 0.1000 - val_custom_mse: 0.8471 - val_custom_mae: 0.6562\n",
            "Epoch 6/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6622 - mae: 0.5128 - mse: 0.6622 - val_loss: 0.6051 - val_mae: 0.5261 - val_mse: 0.6051 - learning_rate: 0.1000 - val_custom_mse: 0.8072 - val_custom_mae: 0.6356\n",
            "Epoch 7/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6475 - mae: 0.5040 - mse: 0.6475 - val_loss: 0.6015 - val_mae: 0.5203 - val_mse: 0.6015 - learning_rate: 0.1000 - val_custom_mse: 0.8085 - val_custom_mae: 0.6341\n",
            "Epoch 8/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6346 - mae: 0.4961 - mse: 0.6346 - val_loss: 0.5876 - val_mae: 0.5118 - val_mse: 0.5876 - learning_rate: 0.1000 - val_custom_mse: 0.7929 - val_custom_mae: 0.6269\n",
            "Epoch 9/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6254 - mae: 0.4900 - mse: 0.6254 - val_loss: 0.5624 - val_mae: 0.4964 - val_mse: 0.5624 - learning_rate: 0.1000 - val_custom_mse: 0.7606 - val_custom_mae: 0.6096\n",
            "Epoch 10/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.6172 - mae: 0.4844 - mse: 0.6172 - val_loss: 0.5593 - val_mae: 0.4933 - val_mse: 0.5593 - learning_rate: 0.1000 - val_custom_mse: 0.7591 - val_custom_mae: 0.6090\n",
            "Epoch 11/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6111 - mae: 0.4801 - mse: 0.6111 - val_loss: 0.5453 - val_mae: 0.4848 - val_mse: 0.5453 - learning_rate: 0.1000 - val_custom_mse: 0.7416 - val_custom_mae: 0.6002\n",
            "Epoch 12/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6072 - mae: 0.4766 - mse: 0.6072 - val_loss: 0.5554 - val_mae: 0.4891 - val_mse: 0.5554 - learning_rate: 0.1000 - val_custom_mse: 0.7572 - val_custom_mae: 0.6079\n",
            "Epoch 13/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6032 - mae: 0.4736 - mse: 0.6032 - val_loss: 0.5293 - val_mae: 0.4741 - val_mse: 0.5293 - learning_rate: 0.1000 - val_custom_mse: 0.7224 - val_custom_mae: 0.5904\n",
            "Epoch 14/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.6005 - mae: 0.4709 - mse: 0.6005 - val_loss: 0.5298 - val_mae: 0.4731 - val_mse: 0.5298 - learning_rate: 0.1000 - val_custom_mse: 0.7246 - val_custom_mae: 0.5909\n",
            "Epoch 15/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5971 - mae: 0.4683 - mse: 0.5971 - val_loss: 0.5079 - val_mae: 0.4650 - val_mse: 0.5079 - learning_rate: 0.1000 - val_custom_mse: 0.6914 - val_custom_mae: 0.5768\n",
            "Epoch 16/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5957 - mae: 0.4666 - mse: 0.5957 - val_loss: 0.5253 - val_mae: 0.4689 - val_mse: 0.5253 - learning_rate: 0.1000 - val_custom_mse: 0.7205 - val_custom_mae: 0.5887\n",
            "Epoch 17/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5924 - mae: 0.4644 - mse: 0.5924 - val_loss: 0.5109 - val_mae: 0.4615 - val_mse: 0.5109 - learning_rate: 0.1000 - val_custom_mse: 0.7007 - val_custom_mae: 0.5794\n",
            "Epoch 18/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5920 - mae: 0.4634 - mse: 0.5920 - val_loss: 0.5106 - val_mae: 0.4602 - val_mse: 0.5106 - learning_rate: 0.1000 - val_custom_mse: 0.7015 - val_custom_mae: 0.5795\n",
            "Epoch 19/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5889 - mae: 0.4611 - mse: 0.5889 - val_loss: 0.5090 - val_mae: 0.4588 - val_mse: 0.5090 - learning_rate: 0.1000 - val_custom_mse: 0.6998 - val_custom_mae: 0.5785\n",
            "Epoch 20/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5881 - mae: 0.4601 - mse: 0.5881 - val_loss: 0.5137 - val_mae: 0.4604 - val_mse: 0.5137 - learning_rate: 0.1000 - val_custom_mse: 0.7074 - val_custom_mae: 0.5822\n",
            "Epoch 21/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5873 - mae: 0.4591 - mse: 0.5873 - val_loss: 0.5076 - val_mae: 0.4567 - val_mse: 0.5076 - learning_rate: 0.1000 - val_custom_mse: 0.6992 - val_custom_mae: 0.5780\n",
            "Epoch 22/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5866 - mae: 0.4582 - mse: 0.5866 - val_loss: 0.5034 - val_mae: 0.4544 - val_mse: 0.5034 - learning_rate: 0.1000 - val_custom_mse: 0.6936 - val_custom_mae: 0.5753\n",
            "Epoch 23/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5846 - mae: 0.4569 - mse: 0.5846 - val_loss: 0.5057 - val_mae: 0.4547 - val_mse: 0.5057 - learning_rate: 0.1000 - val_custom_mse: 0.6978 - val_custom_mae: 0.5772\n",
            "Epoch 24/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5840 - mae: 0.4562 - mse: 0.5840 - val_loss: 0.5133 - val_mae: 0.4580 - val_mse: 0.5133 - learning_rate: 0.1000 - val_custom_mse: 0.7089 - val_custom_mae: 0.5823\n",
            "Epoch 25/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5837 - mae: 0.4555 - mse: 0.5837 - val_loss: 0.5105 - val_mae: 0.4566 - val_mse: 0.5105 - learning_rate: 0.1000 - val_custom_mse: 0.7053 - val_custom_mae: 0.5809\n",
            "Epoch 26/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5831 - mae: 0.4548 - mse: 0.5831 - val_loss: 0.5019 - val_mae: 0.4515 - val_mse: 0.5019 - learning_rate: 0.1000 - val_custom_mse: 0.6935 - val_custom_mae: 0.5748\n",
            "Epoch 27/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5819 - mae: 0.4539 - mse: 0.5819 - val_loss: 0.4978 - val_mae: 0.4494 - val_mse: 0.4978 - learning_rate: 0.1000 - val_custom_mse: 0.6878 - val_custom_mae: 0.5722\n",
            "Epoch 28/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5819 - mae: 0.4538 - mse: 0.5819 - val_loss: 0.4938 - val_mae: 0.4477 - val_mse: 0.4938 - learning_rate: 0.1000 - val_custom_mse: 0.6822 - val_custom_mae: 0.5698\n",
            "Epoch 29/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5808 - mae: 0.4528 - mse: 0.5808 - val_loss: 0.4984 - val_mae: 0.4488 - val_mse: 0.4984 - learning_rate: 0.1000 - val_custom_mse: 0.6897 - val_custom_mae: 0.5730\n",
            "Epoch 30/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5801 - mae: 0.4522 - mse: 0.5801 - val_loss: 0.5044 - val_mae: 0.4515 - val_mse: 0.5044 - learning_rate: 0.1000 - val_custom_mse: 0.6983 - val_custom_mae: 0.5770\n",
            "Epoch 31/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5790 - mae: 0.4515 - mse: 0.5790 - val_loss: 0.5078 - val_mae: 0.4531 - val_mse: 0.5078 - learning_rate: 0.1000 - val_custom_mse: 0.7032 - val_custom_mae: 0.5792\n",
            "Epoch 32/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5794 - mae: 0.4514 - mse: 0.5794 - val_loss: 0.5045 - val_mae: 0.4509 - val_mse: 0.5045 - learning_rate: 0.1000 - val_custom_mse: 0.6989 - val_custom_mae: 0.5770\n",
            "Epoch 33/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5787 - mae: 0.4509 - mse: 0.5787 - val_loss: 0.5010 - val_mae: 0.4489 - val_mse: 0.5010 - learning_rate: 0.1000 - val_custom_mse: 0.6943 - val_custom_mae: 0.5748\n",
            "Epoch 34/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5782 - mae: 0.4504 - mse: 0.5782 - val_loss: 0.4951 - val_mae: 0.4459 - val_mse: 0.4951 - learning_rate: 0.1000 - val_custom_mse: 0.6859 - val_custom_mae: 0.5708\n",
            "Epoch 35/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5778 - mae: 0.4501 - mse: 0.5778 - val_loss: 0.4923 - val_mae: 0.4447 - val_mse: 0.4923 - learning_rate: 0.1000 - val_custom_mse: 0.6820 - val_custom_mae: 0.5691\n",
            "Epoch 36/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5771 - mae: 0.4495 - mse: 0.5771 - val_loss: 0.5001 - val_mae: 0.4479 - val_mse: 0.5001 - learning_rate: 0.1000 - val_custom_mse: 0.6934 - val_custom_mae: 0.5744\n",
            "Epoch 37/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5763 - mae: 0.4492 - mse: 0.5763 - val_loss: 0.4962 - val_mae: 0.4457 - val_mse: 0.4962 - learning_rate: 0.1000 - val_custom_mse: 0.6881 - val_custom_mae: 0.5717\n",
            "Epoch 38/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5769 - mae: 0.4492 - mse: 0.5769 - val_loss: 0.4976 - val_mae: 0.4462 - val_mse: 0.4976 - learning_rate: 0.1000 - val_custom_mse: 0.6903 - val_custom_mae: 0.5726\n",
            "Epoch 39/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5773 - mae: 0.4492 - mse: 0.5773 - val_loss: 0.4905 - val_mae: 0.4431 - val_mse: 0.4905 - learning_rate: 0.1000 - val_custom_mse: 0.6801 - val_custom_mae: 0.5681\n",
            "Epoch 40/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5764 - mae: 0.4486 - mse: 0.5764 - val_loss: 0.4967 - val_mae: 0.4454 - val_mse: 0.4967 - learning_rate: 0.1000 - val_custom_mse: 0.6892 - val_custom_mae: 0.5721\n",
            "Epoch 41/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5756 - mae: 0.4481 - mse: 0.5756 - val_loss: 0.5028 - val_mae: 0.4486 - val_mse: 0.5028 - learning_rate: 0.1000 - val_custom_mse: 0.6977 - val_custom_mae: 0.5762\n",
            "Epoch 42/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5757 - mae: 0.4481 - mse: 0.5757 - val_loss: 0.4981 - val_mae: 0.4458 - val_mse: 0.4981 - learning_rate: 0.1000 - val_custom_mse: 0.6914 - val_custom_mae: 0.5729\n",
            "Epoch 43/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5758 - mae: 0.4480 - mse: 0.5758 - val_loss: 0.5028 - val_mae: 0.4485 - val_mse: 0.5028 - learning_rate: 0.1000 - val_custom_mse: 0.6979 - val_custom_mae: 0.5764\n",
            "Epoch 44/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5757 - mae: 0.4480 - mse: 0.5757 - val_loss: 0.4929 - val_mae: 0.4430 - val_mse: 0.4929 - learning_rate: 0.1000 - val_custom_mse: 0.6842 - val_custom_mae: 0.5695\n",
            "Epoch 45/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5755 - mae: 0.4477 - mse: 0.5755 - val_loss: 0.5057 - val_mae: 0.4499 - val_mse: 0.5057 - learning_rate: 0.1000 - val_custom_mse: 0.7018 - val_custom_mae: 0.5780\n",
            "Epoch 46/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5751 - mae: 0.4474 - mse: 0.5751 - val_loss: 0.4913 - val_mae: 0.4422 - val_mse: 0.4913 - learning_rate: 0.1000 - val_custom_mse: 0.6820 - val_custom_mae: 0.5686\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5745 - mae: 0.4470 - mse: 0.5745 - val_loss: 0.4981 - val_mae: 0.4453 - val_mse: 0.4981 - learning_rate: 0.1000 - val_custom_mse: 0.6917 - val_custom_mae: 0.5731\n",
            "Epoch 48/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5742 - mae: 0.4467 - mse: 0.5742 - val_loss: 0.4967 - val_mae: 0.4447 - val_mse: 0.4967 - learning_rate: 0.0200 - val_custom_mse: 0.6897 - val_custom_mae: 0.5722\n",
            "Epoch 49/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5739 - mae: 0.4465 - mse: 0.5739 - val_loss: 0.4995 - val_mae: 0.4463 - val_mse: 0.4995 - learning_rate: 0.0200 - val_custom_mse: 0.6936 - val_custom_mae: 0.5741\n",
            "Epoch 50/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5740 - mae: 0.4465 - mse: 0.5740 - val_loss: 0.4950 - val_mae: 0.4438 - val_mse: 0.4950 - learning_rate: 0.0200 - val_custom_mse: 0.6874 - val_custom_mae: 0.5711\n",
            "Epoch 51/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5741 - mae: 0.4466 - mse: 0.5741 - val_loss: 0.4950 - val_mae: 0.4438 - val_mse: 0.4950 - learning_rate: 0.0200 - val_custom_mse: 0.6875 - val_custom_mae: 0.5711\n",
            "Epoch 52/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5733 - mae: 0.4463 - mse: 0.5733 - val_loss: 0.4937 - val_mae: 0.4431 - val_mse: 0.4937 - learning_rate: 0.0200 - val_custom_mse: 0.6857 - val_custom_mae: 0.5703\n",
            "Epoch 53/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5734 - mae: 0.4463 - mse: 0.5734 - val_loss: 0.4943 - val_mae: 0.4433 - val_mse: 0.4943 - learning_rate: 0.0200 - val_custom_mse: 0.6865 - val_custom_mae: 0.5706\n",
            "Epoch 54/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5735 - mae: 0.4463 - mse: 0.5735 - val_loss: 0.4947 - val_mae: 0.4436 - val_mse: 0.4947 - learning_rate: 0.0200 - val_custom_mse: 0.6871 - val_custom_mae: 0.5709\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5736 - mae: 0.4463 - mse: 0.5736 - val_loss: 0.4937 - val_mae: 0.4430 - val_mse: 0.4937 - learning_rate: 0.0200 - val_custom_mse: 0.6857 - val_custom_mae: 0.5702\n",
            "Epoch 56/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5732 - mae: 0.4461 - mse: 0.5732 - val_loss: 0.4942 - val_mae: 0.4433 - val_mse: 0.4942 - learning_rate: 0.0040 - val_custom_mse: 0.6864 - val_custom_mae: 0.5706\n",
            "Epoch 57/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5731 - mae: 0.4462 - mse: 0.5731 - val_loss: 0.4941 - val_mae: 0.4432 - val_mse: 0.4941 - learning_rate: 0.0040 - val_custom_mse: 0.6862 - val_custom_mae: 0.5705\n",
            "Epoch 58/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5732 - mae: 0.4463 - mse: 0.5732 - val_loss: 0.4931 - val_mae: 0.4428 - val_mse: 0.4931 - learning_rate: 0.0040 - val_custom_mse: 0.6849 - val_custom_mae: 0.5699\n",
            "Epoch 59/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5732 - mae: 0.4461 - mse: 0.5732 - val_loss: 0.4941 - val_mae: 0.4432 - val_mse: 0.4941 - learning_rate: 0.0040 - val_custom_mse: 0.6863 - val_custom_mae: 0.5705\n",
            "Epoch 60/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5730 - mae: 0.4461 - mse: 0.5730 - val_loss: 0.4942 - val_mae: 0.4433 - val_mse: 0.4942 - learning_rate: 0.0040 - val_custom_mse: 0.6863 - val_custom_mae: 0.5706\n",
            "Epoch 61/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5731 - mae: 0.4461 - mse: 0.5731 - val_loss: 0.4944 - val_mae: 0.4434 - val_mse: 0.4944 - learning_rate: 0.0040 - val_custom_mse: 0.6867 - val_custom_mae: 0.5707\n",
            "Epoch 62/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5734 - mae: 0.4461 - mse: 0.5734 - val_loss: 0.4944 - val_mae: 0.4434 - val_mse: 0.4944 - learning_rate: 0.0040 - val_custom_mse: 0.6866 - val_custom_mae: 0.5707\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5730 - mae: 0.4461 - mse: 0.5730 - val_loss: 0.4944 - val_mae: 0.4434 - val_mse: 0.4944 - learning_rate: 0.0040 - val_custom_mse: 0.6866 - val_custom_mae: 0.5707\n",
            "Epoch 64/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5733 - mae: 0.4462 - mse: 0.5733 - val_loss: 0.4934 - val_mae: 0.4429 - val_mse: 0.4934 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6853 - val_custom_mae: 0.5701\n",
            "Epoch 65/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5729 - mae: 0.4461 - mse: 0.5729 - val_loss: 0.4933 - val_mae: 0.4428 - val_mse: 0.4933 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6851 - val_custom_mae: 0.5700\n",
            "Epoch 66/100\n",
            "216/216 - 2s - 8ms/step - loss: 0.5727 - mae: 0.4459 - mse: 0.5727 - val_loss: 0.4935 - val_mae: 0.4429 - val_mse: 0.4935 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6853 - val_custom_mae: 0.5701\n",
            "Epoch 67/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5733 - mae: 0.4462 - mse: 0.5733 - val_loss: 0.4933 - val_mae: 0.4428 - val_mse: 0.4933 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6851 - val_custom_mae: 0.5700\n",
            "Epoch 68/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5732 - mae: 0.4461 - mse: 0.5732 - val_loss: 0.4935 - val_mae: 0.4429 - val_mse: 0.4935 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6854 - val_custom_mae: 0.5701\n",
            "Epoch 69/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5732 - mae: 0.4461 - mse: 0.5732 - val_loss: 0.4934 - val_mae: 0.4429 - val_mse: 0.4934 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6852 - val_custom_mae: 0.5701\n",
            "Epoch 70/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5732 - mae: 0.4461 - mse: 0.5732 - val_loss: 0.4933 - val_mae: 0.4428 - val_mse: 0.4933 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6851 - val_custom_mae: 0.5700\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5731 - mae: 0.4460 - mse: 0.5731 - val_loss: 0.4936 - val_mae: 0.4430 - val_mse: 0.4936 - learning_rate: 8.0000e-04 - val_custom_mse: 0.6855 - val_custom_mae: 0.5702\n",
            "Epoch 72/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5730 - mae: 0.4460 - mse: 0.5730 - val_loss: 0.4932 - val_mae: 0.4428 - val_mse: 0.4932 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6850 - val_custom_mae: 0.5700\n",
            "Epoch 73/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5728 - mae: 0.4461 - mse: 0.5728 - val_loss: 0.4932 - val_mae: 0.4428 - val_mse: 0.4932 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6850 - val_custom_mae: 0.5699\n",
            "Epoch 74/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5733 - mae: 0.4462 - mse: 0.5733 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6849 - val_custom_mae: 0.5699\n",
            "Epoch 75/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5728 - mae: 0.4460 - mse: 0.5728 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6848 - val_custom_mae: 0.5699\n",
            "Epoch 76/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5727 - mae: 0.4460 - mse: 0.5727 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6849 - val_custom_mae: 0.5699\n",
            "Epoch 77/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5728 - mae: 0.4460 - mse: 0.5728 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 78/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5732 - mae: 0.4461 - mse: 0.5732 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5728 - mae: 0.4460 - mse: 0.5728 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 1.6000e-04 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 80/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5736 - mae: 0.4462 - mse: 0.5736 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 81/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5728 - mae: 0.4460 - mse: 0.5728 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 82/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5730 - mae: 0.4461 - mse: 0.5730 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 83/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5728 - mae: 0.4460 - mse: 0.5728 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 84/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5727 - mae: 0.4460 - mse: 0.5727 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 85/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5729 - mae: 0.4461 - mse: 0.5729 - val_loss: 0.4931 - val_mae: 0.4427 - val_mse: 0.4931 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 86/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5731 - mae: 0.4460 - mse: 0.5731 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5732 - mae: 0.4461 - mse: 0.5732 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 3.2000e-05 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 88/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5731 - mae: 0.4461 - mse: 0.5731 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 89/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5730 - mae: 0.4461 - mse: 0.5730 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 90/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5734 - mae: 0.4462 - mse: 0.5734 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 91/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5735 - mae: 0.4462 - mse: 0.5735 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 92/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5733 - mae: 0.4462 - mse: 0.5733 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 93/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5730 - mae: 0.4460 - mse: 0.5730 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 94/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5732 - mae: 0.4461 - mse: 0.5732 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.2799998330592645e-06.\n",
            "216/216 - 2s - 7ms/step - loss: 0.5733 - mae: 0.4461 - mse: 0.5733 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 6.4000e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 96/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5729 - mae: 0.4460 - mse: 0.5729 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 97/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5730 - mae: 0.4460 - mse: 0.5730 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 98/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5735 - mae: 0.4462 - mse: 0.5735 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 99/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5730 - mae: 0.4461 - mse: 0.5730 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "Epoch 100/100\n",
            "216/216 - 2s - 7ms/step - loss: 0.5731 - mae: 0.4461 - mse: 0.5731 - val_loss: 0.4930 - val_mae: 0.4427 - val_mse: 0.4930 - learning_rate: 1.2800e-06 - val_custom_mse: 0.6848 - val_custom_mae: 0.5698\n",
            "\n",
            " ETTh2 Final Results:\n",
            "\n",
            "MSE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%  DR=20%  DR=30%\n",
            "Horizon                                \n",
            "96       0.2655  0.2642  0.2648  0.2655\n",
            "192      0.3219  0.3205  0.3205  0.3214\n",
            "336      0.3468  0.3449  0.3445  0.3444\n",
            "720      0.4237  0.4204  0.4222  0.4229\n",
            "\n",
            "MAE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%  DR=20%  DR=30%\n",
            "Horizon                                \n",
            "96       0.3304  0.3298  0.3301  0.3308\n",
            "192      0.3679  0.3685  0.3683  0.3691\n",
            "336      0.3984  0.3965  0.3958  0.3962\n",
            "720      0.4551  0.4525  0.4536  0.4540\n",
            "\n",
            "Results saved to: ./flowmixer_results/ETTh2_experiment_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Run the experiments\n",
        "data_name='ETTh2'\n",
        "results = run_experiments(data_name,horizons=[96,192,336,720], dropout_rates=[0.0, 0.1, 0.2, 0.3], revin=1, seq_len_=1024, learning_rate=1e-1, mopt='sgd')\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\n {data_name} Final Results:\")\n",
        "df = pd.DataFrame(results)\n",
        "# Assuming results is a list of dictionaries with horizon, dropout, MSE, and MAE values\n",
        "display_results_tables(results[0])\n",
        "\n",
        "\n",
        "# The results are already saved in CSV format after each experiment\n",
        "print(f\"\\nResults saved to: ./flowmixer_results/{data_name}_experiment_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETTm1"
      ],
      "metadata": {
        "id": "v4DHt_Bg0avt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pATeqaHW_NJV",
        "outputId": "c5844a38-0f7e-4804-b8f7-b97e493603f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: horizon=96, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1046/1046 - 14s - 13ms/step - loss: 0.1296 - mae: 0.2149 - mse: 0.1296 - val_loss: 0.0547 - val_mae: 0.1264 - val_mse: 0.0547 - learning_rate: 0.0010 - val_custom_mse: 0.3998 - val_custom_mae: 0.4218\n",
            "Epoch 2/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0292 - mae: 0.0769 - mse: 0.0292 - val_loss: 0.0398 - val_mae: 0.0721 - val_mse: 0.0398 - learning_rate: 0.0010 - val_custom_mse: 0.3998 - val_custom_mae: 0.4222\n",
            "Epoch 3/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0249 - mae: 0.0525 - mse: 0.0249 - val_loss: 0.0381 - val_mae: 0.0583 - val_mse: 0.0381 - learning_rate: 0.0010 - val_custom_mse: 0.3980 - val_custom_mae: 0.4204\n",
            "Epoch 4/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0242 - mae: 0.0447 - mse: 0.0242 - val_loss: 0.0375 - val_mae: 0.0529 - val_mse: 0.0375 - learning_rate: 0.0010 - val_custom_mse: 0.3952 - val_custom_mae: 0.4179\n",
            "Epoch 5/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0239 - mae: 0.0414 - mse: 0.0239 - val_loss: 0.0371 - val_mae: 0.0505 - val_mse: 0.0371 - learning_rate: 0.0010 - val_custom_mse: 0.3930 - val_custom_mae: 0.4161\n",
            "Epoch 6/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0237 - mae: 0.0398 - mse: 0.0237 - val_loss: 0.0366 - val_mae: 0.0510 - val_mse: 0.0366 - learning_rate: 0.0010 - val_custom_mse: 0.3862 - val_custom_mae: 0.4114\n",
            "Epoch 7/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0235 - mae: 0.0390 - mse: 0.0235 - val_loss: 0.0364 - val_mae: 0.0485 - val_mse: 0.0364 - learning_rate: 0.0010 - val_custom_mse: 0.3864 - val_custom_mae: 0.4118\n",
            "Epoch 8/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0233 - mae: 0.0385 - mse: 0.0233 - val_loss: 0.0361 - val_mae: 0.0488 - val_mse: 0.0361 - learning_rate: 0.0010 - val_custom_mse: 0.3829 - val_custom_mae: 0.4085\n",
            "Epoch 9/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0232 - mae: 0.0382 - mse: 0.0232 - val_loss: 0.0362 - val_mae: 0.0465 - val_mse: 0.0362 - learning_rate: 0.0010 - val_custom_mse: 0.3839 - val_custom_mae: 0.4094\n",
            "Epoch 10/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0231 - mae: 0.0377 - mse: 0.0231 - val_loss: 0.0361 - val_mae: 0.0460 - val_mse: 0.0361 - learning_rate: 0.0010 - val_custom_mse: 0.3836 - val_custom_mae: 0.4088\n",
            "Epoch 11/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0231 - mae: 0.0372 - mse: 0.0231 - val_loss: 0.0361 - val_mae: 0.0453 - val_mse: 0.0361 - learning_rate: 0.0010 - val_custom_mse: 0.3840 - val_custom_mae: 0.4090\n",
            "Epoch 12/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0230 - mae: 0.0368 - mse: 0.0230 - val_loss: 0.0360 - val_mae: 0.0447 - val_mse: 0.0360 - learning_rate: 0.0010 - val_custom_mse: 0.3825 - val_custom_mae: 0.4077\n",
            "Epoch 13/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0230 - mae: 0.0365 - mse: 0.0230 - val_loss: 0.0360 - val_mae: 0.0447 - val_mse: 0.0360 - learning_rate: 0.0010 - val_custom_mse: 0.3833 - val_custom_mae: 0.4081\n",
            "Epoch 14/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0229 - mae: 0.0362 - mse: 0.0229 - val_loss: 0.0362 - val_mae: 0.0444 - val_mse: 0.0362 - learning_rate: 0.0010 - val_custom_mse: 0.3848 - val_custom_mae: 0.4100\n",
            "Epoch 15/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0229 - mae: 0.0360 - mse: 0.0229 - val_loss: 0.0360 - val_mae: 0.0441 - val_mse: 0.0360 - learning_rate: 0.0010 - val_custom_mse: 0.3827 - val_custom_mae: 0.4083\n",
            "Epoch 16/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0229 - mae: 0.0359 - mse: 0.0229 - val_loss: 0.0362 - val_mae: 0.0440 - val_mse: 0.0362 - learning_rate: 0.0010 - val_custom_mse: 0.3850 - val_custom_mae: 0.4092\n",
            "Epoch 17/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0229 - mae: 0.0358 - mse: 0.0229 - val_loss: 0.0360 - val_mae: 0.0439 - val_mse: 0.0360 - learning_rate: 0.0010 - val_custom_mse: 0.3828 - val_custom_mae: 0.4079\n",
            "Epoch 18/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0229 - mae: 0.0357 - mse: 0.0229 - val_loss: 0.0361 - val_mae: 0.0437 - val_mse: 0.0361 - learning_rate: 0.0010 - val_custom_mse: 0.3842 - val_custom_mae: 0.4086\n",
            "Epoch 19/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0229 - mae: 0.0357 - mse: 0.0229 - val_loss: 0.0359 - val_mae: 0.0438 - val_mse: 0.0359 - learning_rate: 0.0010 - val_custom_mse: 0.3823 - val_custom_mae: 0.4076\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0228 - mae: 0.0356 - mse: 0.0228 - val_loss: 0.0361 - val_mae: 0.0439 - val_mse: 0.0361 - learning_rate: 0.0010 - val_custom_mse: 0.3837 - val_custom_mae: 0.4089\n",
            "Epoch 21/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0227 - mae: 0.0350 - mse: 0.0227 - val_loss: 0.0357 - val_mae: 0.0427 - val_mse: 0.0357 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3798 - val_custom_mae: 0.4058\n",
            "Epoch 22/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0227 - mae: 0.0350 - mse: 0.0227 - val_loss: 0.0357 - val_mae: 0.0427 - val_mse: 0.0357 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3798 - val_custom_mae: 0.4059\n",
            "Epoch 23/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0227 - mae: 0.0350 - mse: 0.0227 - val_loss: 0.0357 - val_mae: 0.0427 - val_mse: 0.0357 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3800 - val_custom_mae: 0.4060\n",
            "Epoch 24/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0227 - mae: 0.0350 - mse: 0.0227 - val_loss: 0.0357 - val_mae: 0.0427 - val_mse: 0.0357 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3797 - val_custom_mae: 0.4057\n",
            "Epoch 25/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0227 - mae: 0.0349 - mse: 0.0227 - val_loss: 0.0357 - val_mae: 0.0427 - val_mse: 0.0357 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3797 - val_custom_mae: 0.4058\n",
            "Epoch 26/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0227 - mae: 0.0349 - mse: 0.0227 - val_loss: 0.0357 - val_mae: 0.0427 - val_mse: 0.0357 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3800 - val_custom_mae: 0.4060\n",
            "Epoch 27/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0227 - mae: 0.0349 - mse: 0.0227 - val_loss: 0.0357 - val_mae: 0.0427 - val_mse: 0.0357 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3797 - val_custom_mae: 0.4058\n",
            "Epoch 28/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0227 - mae: 0.0349 - mse: 0.0227 - val_loss: 0.0356 - val_mae: 0.0426 - val_mse: 0.0356 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3795 - val_custom_mae: 0.4057\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0227 - mae: 0.0349 - mse: 0.0227 - val_loss: 0.0356 - val_mae: 0.0427 - val_mse: 0.0356 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3795 - val_custom_mae: 0.4058\n",
            "Epoch 30/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3782 - val_custom_mae: 0.4048\n",
            "Epoch 31/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3782 - val_custom_mae: 0.4048\n",
            "Epoch 32/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3782 - val_custom_mae: 0.4047\n",
            "Epoch 33/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3781 - val_custom_mae: 0.4047\n",
            "Epoch 34/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3782 - val_custom_mae: 0.4047\n",
            "Epoch 35/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3782 - val_custom_mae: 0.4047\n",
            "Epoch 36/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3783 - val_custom_mae: 0.4048\n",
            "Epoch 37/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3784 - val_custom_mae: 0.4048\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3784 - val_custom_mae: 0.4049\n",
            "Epoch 39/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0424 - val_mse: 0.0355 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3781 - val_custom_mae: 0.4047\n",
            "Epoch 40/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3780 - val_custom_mae: 0.4046\n",
            "Epoch 41/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3780 - val_custom_mae: 0.4045\n",
            "Epoch 42/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3779 - val_custom_mae: 0.4045\n",
            "Epoch 43/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3778 - val_custom_mae: 0.4045\n",
            "Epoch 44/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3778 - val_custom_mae: 0.4044\n",
            "Epoch 45/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3778 - val_custom_mae: 0.4045\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3778 - val_custom_mae: 0.4044\n",
            "Epoch 47/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3778 - val_custom_mae: 0.4044\n",
            "Epoch 48/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3778 - val_custom_mae: 0.4044\n",
            "Epoch 49/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3778 - val_custom_mae: 0.4044\n",
            "Epoch 50/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3778 - val_custom_mae: 0.4044\n",
            "Epoch 51/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3778 - val_custom_mae: 0.4044\n",
            "Epoch 52/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 53/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 55/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 56/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 57/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 58/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 59/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 60/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 61/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 63/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 64/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 65/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 66/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 67/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 68/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 69/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 71/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 72/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 73/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 74/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 75/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 76/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 77/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 79/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 80/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 81/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 82/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 83/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 84/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 85/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 87/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 88/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 89/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 90/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 91/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 92/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 93/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 95/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 96/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 97/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 98/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 99/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Epoch 100/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0226 - mae: 0.0348 - mse: 0.0226 - val_loss: 0.0355 - val_mae: 0.0423 - val_mse: 0.0355 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3777 - val_custom_mae: 0.4044\n",
            "Running experiment: horizon=96, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1046/1046 - 13s - 13ms/step - loss: 0.1590 - mae: 0.2489 - mse: 0.1590 - val_loss: 0.0600 - val_mae: 0.1361 - val_mse: 0.0600 - learning_rate: 0.0010 - val_custom_mse: 0.3998 - val_custom_mae: 0.4209\n",
            "Epoch 2/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0490 - mae: 0.1280 - mse: 0.0490 - val_loss: 0.0502 - val_mae: 0.1104 - val_mse: 0.0502 - learning_rate: 0.0010 - val_custom_mse: 0.3946 - val_custom_mae: 0.4170\n",
            "Epoch 3/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0442 - mae: 0.1144 - mse: 0.0442 - val_loss: 0.0491 - val_mae: 0.1072 - val_mse: 0.0491 - learning_rate: 0.0010 - val_custom_mse: 0.3919 - val_custom_mae: 0.4165\n",
            "Epoch 4/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0433 - mae: 0.1118 - mse: 0.0433 - val_loss: 0.0483 - val_mae: 0.1058 - val_mse: 0.0483 - learning_rate: 0.0010 - val_custom_mse: 0.3866 - val_custom_mae: 0.4113\n",
            "Epoch 5/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0429 - mae: 0.1109 - mse: 0.0429 - val_loss: 0.0482 - val_mae: 0.1052 - val_mse: 0.0482 - learning_rate: 0.0010 - val_custom_mse: 0.3877 - val_custom_mae: 0.4113\n",
            "Epoch 6/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0428 - mae: 0.1106 - mse: 0.0428 - val_loss: 0.0481 - val_mae: 0.1048 - val_mse: 0.0481 - learning_rate: 0.0010 - val_custom_mse: 0.3886 - val_custom_mae: 0.4118\n",
            "Epoch 7/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0428 - mae: 0.1104 - mse: 0.0428 - val_loss: 0.0479 - val_mae: 0.1044 - val_mse: 0.0479 - learning_rate: 0.0010 - val_custom_mse: 0.3865 - val_custom_mae: 0.4103\n",
            "Epoch 8/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0427 - mae: 0.1104 - mse: 0.0427 - val_loss: 0.0482 - val_mae: 0.1046 - val_mse: 0.0482 - learning_rate: 0.0010 - val_custom_mse: 0.3899 - val_custom_mae: 0.4131\n",
            "Epoch 9/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0427 - mae: 0.1104 - mse: 0.0427 - val_loss: 0.0480 - val_mae: 0.1044 - val_mse: 0.0480 - learning_rate: 0.0010 - val_custom_mse: 0.3877 - val_custom_mae: 0.4114\n",
            "Epoch 10/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0426 - mae: 0.1103 - mse: 0.0426 - val_loss: 0.0477 - val_mae: 0.1041 - val_mse: 0.0477 - learning_rate: 0.0010 - val_custom_mse: 0.3853 - val_custom_mae: 0.4099\n",
            "Epoch 11/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1103 - mse: 0.0426 - val_loss: 0.0479 - val_mae: 0.1042 - val_mse: 0.0479 - learning_rate: 0.0010 - val_custom_mse: 0.3872 - val_custom_mae: 0.4114\n",
            "Epoch 12/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1103 - mse: 0.0426 - val_loss: 0.0481 - val_mae: 0.1045 - val_mse: 0.0481 - learning_rate: 0.0010 - val_custom_mse: 0.3893 - val_custom_mae: 0.4127\n",
            "Epoch 13/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0477 - val_mae: 0.1042 - val_mse: 0.0477 - learning_rate: 0.0010 - val_custom_mse: 0.3850 - val_custom_mae: 0.4104\n",
            "Epoch 14/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0475 - val_mae: 0.1040 - val_mse: 0.0475 - learning_rate: 0.0010 - val_custom_mse: 0.3837 - val_custom_mae: 0.4086\n",
            "Epoch 15/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0476 - val_mae: 0.1041 - val_mse: 0.0476 - learning_rate: 0.0010 - val_custom_mse: 0.3839 - val_custom_mae: 0.4094\n",
            "Epoch 16/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0477 - val_mae: 0.1041 - val_mse: 0.0477 - learning_rate: 0.0010 - val_custom_mse: 0.3848 - val_custom_mae: 0.4099\n",
            "Epoch 17/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0477 - val_mae: 0.1040 - val_mse: 0.0477 - learning_rate: 0.0010 - val_custom_mse: 0.3857 - val_custom_mae: 0.4107\n",
            "Epoch 18/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0476 - val_mae: 0.1040 - val_mse: 0.0476 - learning_rate: 0.0010 - val_custom_mse: 0.3847 - val_custom_mae: 0.4097\n",
            "Epoch 19/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0477 - val_mae: 0.1041 - val_mse: 0.0477 - learning_rate: 0.0010 - val_custom_mse: 0.3850 - val_custom_mae: 0.4102\n",
            "Epoch 20/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0476 - val_mae: 0.1041 - val_mse: 0.0476 - learning_rate: 0.0010 - val_custom_mse: 0.3842 - val_custom_mae: 0.4100\n",
            "Epoch 21/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0477 - val_mae: 0.1041 - val_mse: 0.0477 - learning_rate: 0.0010 - val_custom_mse: 0.3855 - val_custom_mae: 0.4108\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1102 - mse: 0.0426 - val_loss: 0.0475 - val_mae: 0.1039 - val_mse: 0.0475 - learning_rate: 0.0010 - val_custom_mse: 0.3835 - val_custom_mae: 0.4092\n",
            "Epoch 23/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0424 - mae: 0.1098 - mse: 0.0424 - val_loss: 0.0473 - val_mae: 0.1032 - val_mse: 0.0473 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3834 - val_custom_mae: 0.4092\n",
            "Epoch 24/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0424 - mae: 0.1098 - mse: 0.0424 - val_loss: 0.0473 - val_mae: 0.1032 - val_mse: 0.0473 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3828 - val_custom_mae: 0.4087\n",
            "Epoch 25/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0424 - mae: 0.1098 - mse: 0.0424 - val_loss: 0.0473 - val_mae: 0.1032 - val_mse: 0.0473 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3826 - val_custom_mae: 0.4086\n",
            "Epoch 26/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1098 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1032 - val_mse: 0.0473 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3828 - val_custom_mae: 0.4087\n",
            "Epoch 27/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0424 - mae: 0.1098 - mse: 0.0424 - val_loss: 0.0473 - val_mae: 0.1032 - val_mse: 0.0473 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 28/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0424 - mae: 0.1098 - mse: 0.0424 - val_loss: 0.0473 - val_mae: 0.1032 - val_mse: 0.0473 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3828 - val_custom_mae: 0.4088\n",
            "Epoch 29/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1098 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1032 - val_mse: 0.0473 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3826 - val_custom_mae: 0.4087\n",
            "Epoch 30/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1098 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1032 - val_mse: 0.0473 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3828 - val_custom_mae: 0.4088\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1098 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1032 - val_mse: 0.0473 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3827 - val_custom_mae: 0.4088\n",
            "Epoch 32/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3829 - val_custom_mae: 0.4088\n",
            "Epoch 33/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1098 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 34/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3829 - val_custom_mae: 0.4087\n",
            "Epoch 35/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3829 - val_custom_mae: 0.4088\n",
            "Epoch 36/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3829 - val_custom_mae: 0.4087\n",
            "Epoch 37/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3829 - val_custom_mae: 0.4088\n",
            "Epoch 38/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3827 - val_custom_mae: 0.4086\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3826 - val_custom_mae: 0.4086\n",
            "Epoch 40/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1030 - val_mse: 0.0473 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3828 - val_custom_mae: 0.4087\n",
            "Epoch 41/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3829 - val_custom_mae: 0.4088\n",
            "Epoch 42/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3829 - val_custom_mae: 0.4088\n",
            "Epoch 43/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 44/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 45/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 46/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 48/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 49/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 50/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 51/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 52/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0422 - mae: 0.1097 - mse: 0.0422 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 53/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 54/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 56/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 57/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 58/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 59/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 60/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 61/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3830 - val_custom_mae: 0.4088\n",
            "Epoch 62/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 64/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 65/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 66/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 67/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 68/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 69/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 70/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 72/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 73/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 74/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 75/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 76/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 77/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 78/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 80/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 81/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 82/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 83/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 84/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 85/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 86/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 88/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 89/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 90/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 91/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 92/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 93/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 94/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 96/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 97/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 98/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 99/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Epoch 100/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1097 - mse: 0.0423 - val_loss: 0.0473 - val_mae: 0.1031 - val_mse: 0.0473 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3830 - val_custom_mae: 0.4089\n",
            "Running experiment: horizon=96, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1046/1046 - 13s - 13ms/step - loss: 0.1910 - mae: 0.2791 - mse: 0.1910 - val_loss: 0.0651 - val_mae: 0.1443 - val_mse: 0.0651 - learning_rate: 0.0010 - val_custom_mse: 0.4049 - val_custom_mae: 0.4255\n",
            "Epoch 2/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0587 - mae: 0.1486 - mse: 0.0587 - val_loss: 0.0581 - val_mae: 0.1276 - val_mse: 0.0581 - learning_rate: 0.0010 - val_custom_mse: 0.4028 - val_custom_mae: 0.4242\n",
            "Epoch 3/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0536 - mae: 0.1376 - mse: 0.0536 - val_loss: 0.0571 - val_mae: 0.1261 - val_mse: 0.0571 - learning_rate: 0.0010 - val_custom_mse: 0.3961 - val_custom_mae: 0.4188\n",
            "Epoch 4/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0530 - mae: 0.1362 - mse: 0.0530 - val_loss: 0.0570 - val_mae: 0.1257 - val_mse: 0.0570 - learning_rate: 0.0010 - val_custom_mse: 0.3967 - val_custom_mae: 0.4190\n",
            "Epoch 5/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0527 - mae: 0.1359 - mse: 0.0527 - val_loss: 0.0569 - val_mae: 0.1256 - val_mse: 0.0569 - learning_rate: 0.0010 - val_custom_mse: 0.3970 - val_custom_mae: 0.4189\n",
            "Epoch 6/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0527 - mae: 0.1358 - mse: 0.0527 - val_loss: 0.0567 - val_mae: 0.1255 - val_mse: 0.0567 - learning_rate: 0.0010 - val_custom_mse: 0.3955 - val_custom_mae: 0.4174\n",
            "Epoch 7/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0526 - mae: 0.1357 - mse: 0.0526 - val_loss: 0.0567 - val_mae: 0.1254 - val_mse: 0.0567 - learning_rate: 0.0010 - val_custom_mse: 0.3945 - val_custom_mae: 0.4165\n",
            "Epoch 8/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0526 - mae: 0.1357 - mse: 0.0526 - val_loss: 0.0567 - val_mae: 0.1253 - val_mse: 0.0567 - learning_rate: 0.0010 - val_custom_mse: 0.3956 - val_custom_mae: 0.4180\n",
            "Epoch 9/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0526 - mae: 0.1357 - mse: 0.0526 - val_loss: 0.0565 - val_mae: 0.1252 - val_mse: 0.0565 - learning_rate: 0.0010 - val_custom_mse: 0.3940 - val_custom_mae: 0.4163\n",
            "Epoch 10/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1356 - mse: 0.0525 - val_loss: 0.0564 - val_mae: 0.1250 - val_mse: 0.0564 - learning_rate: 0.0010 - val_custom_mse: 0.3927 - val_custom_mae: 0.4155\n",
            "Epoch 11/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1356 - mse: 0.0525 - val_loss: 0.0563 - val_mae: 0.1250 - val_mse: 0.0563 - learning_rate: 0.0010 - val_custom_mse: 0.3916 - val_custom_mae: 0.4152\n",
            "Epoch 12/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0526 - mae: 0.1357 - mse: 0.0526 - val_loss: 0.0565 - val_mae: 0.1252 - val_mse: 0.0565 - learning_rate: 0.0010 - val_custom_mse: 0.3940 - val_custom_mae: 0.4168\n",
            "Epoch 13/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1356 - mse: 0.0525 - val_loss: 0.0565 - val_mae: 0.1251 - val_mse: 0.0565 - learning_rate: 0.0010 - val_custom_mse: 0.3946 - val_custom_mae: 0.4173\n",
            "Epoch 14/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1356 - mse: 0.0525 - val_loss: 0.0564 - val_mae: 0.1250 - val_mse: 0.0564 - learning_rate: 0.0010 - val_custom_mse: 0.3931 - val_custom_mae: 0.4162\n",
            "Epoch 15/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1356 - mse: 0.0525 - val_loss: 0.0564 - val_mae: 0.1250 - val_mse: 0.0564 - learning_rate: 0.0010 - val_custom_mse: 0.3931 - val_custom_mae: 0.4157\n",
            "Epoch 16/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1356 - mse: 0.0525 - val_loss: 0.0565 - val_mae: 0.1250 - val_mse: 0.0565 - learning_rate: 0.0010 - val_custom_mse: 0.3942 - val_custom_mae: 0.4166\n",
            "Epoch 17/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1356 - mse: 0.0525 - val_loss: 0.0566 - val_mae: 0.1253 - val_mse: 0.0566 - learning_rate: 0.0010 - val_custom_mse: 0.3943 - val_custom_mae: 0.4169\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1356 - mse: 0.0525 - val_loss: 0.0564 - val_mae: 0.1250 - val_mse: 0.0564 - learning_rate: 0.0010 - val_custom_mse: 0.3929 - val_custom_mae: 0.4161\n",
            "Epoch 19/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0523 - mae: 0.1352 - mse: 0.0523 - val_loss: 0.0560 - val_mae: 0.1240 - val_mse: 0.0560 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3909 - val_custom_mae: 0.4146\n",
            "Epoch 20/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0523 - mae: 0.1352 - mse: 0.0523 - val_loss: 0.0559 - val_mae: 0.1240 - val_mse: 0.0559 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3903 - val_custom_mae: 0.4141\n",
            "Epoch 21/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0523 - mae: 0.1352 - mse: 0.0523 - val_loss: 0.0559 - val_mae: 0.1240 - val_mse: 0.0559 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3906 - val_custom_mae: 0.4143\n",
            "Epoch 22/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0523 - mae: 0.1352 - mse: 0.0523 - val_loss: 0.0560 - val_mae: 0.1241 - val_mse: 0.0560 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3908 - val_custom_mae: 0.4146\n",
            "Epoch 23/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1352 - mse: 0.0522 - val_loss: 0.0560 - val_mae: 0.1241 - val_mse: 0.0560 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3907 - val_custom_mae: 0.4144\n",
            "Epoch 24/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0523 - mae: 0.1352 - mse: 0.0523 - val_loss: 0.0560 - val_mae: 0.1241 - val_mse: 0.0560 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3912 - val_custom_mae: 0.4146\n",
            "Epoch 25/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1352 - mse: 0.0522 - val_loss: 0.0560 - val_mae: 0.1241 - val_mse: 0.0560 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3909 - val_custom_mae: 0.4144\n",
            "Epoch 26/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1352 - mse: 0.0522 - val_loss: 0.0560 - val_mae: 0.1241 - val_mse: 0.0560 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3909 - val_custom_mae: 0.4145\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1352 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1241 - val_mse: 0.0559 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3905 - val_custom_mae: 0.4143\n",
            "Epoch 28/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3907 - val_custom_mae: 0.4141\n",
            "Epoch 29/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3909 - val_custom_mae: 0.4142\n",
            "Epoch 30/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0560 - val_mae: 0.1239 - val_mse: 0.0560 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3910 - val_custom_mae: 0.4142\n",
            "Epoch 31/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0560 - val_mae: 0.1239 - val_mse: 0.0560 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3910 - val_custom_mae: 0.4142\n",
            "Epoch 32/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3907 - val_custom_mae: 0.4141\n",
            "Epoch 33/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3906 - val_custom_mae: 0.4140\n",
            "Epoch 34/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3908 - val_custom_mae: 0.4141\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0560 - val_mae: 0.1239 - val_mse: 0.0560 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3909 - val_custom_mae: 0.4142\n",
            "Epoch 36/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4141\n",
            "Epoch 37/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3908 - val_custom_mae: 0.4141\n",
            "Epoch 38/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3908 - val_custom_mae: 0.4140\n",
            "Epoch 39/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4141\n",
            "Epoch 40/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4141\n",
            "Epoch 41/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4141\n",
            "Epoch 42/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4141\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4141\n",
            "Epoch 44/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 45/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 46/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 47/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 48/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 49/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 50/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 52/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 53/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 54/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 55/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 56/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 57/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 58/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 60/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 61/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 62/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 63/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 64/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 65/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 66/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 68/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 69/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 70/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 71/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 72/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 73/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 74/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 75: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 76/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 77/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 78/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 79/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 80/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 81/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 82/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 84/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 85/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 86/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 87/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 88/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 89/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 90/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 92/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 93/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 94/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 95/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 96/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 97/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 98/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0521 - mae: 0.1351 - mse: 0.0521 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Epoch 100/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0522 - mae: 0.1351 - mse: 0.0522 - val_loss: 0.0559 - val_mae: 0.1239 - val_mse: 0.0559 - learning_rate: 2.0480e-11 - val_custom_mse: 0.3909 - val_custom_mae: 0.4140\n",
            "Running experiment: horizon=96, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1046/1046 - 13s - 13ms/step - loss: 0.2293 - mae: 0.3089 - mse: 0.2293 - val_loss: 0.0693 - val_mae: 0.1511 - val_mse: 0.0693 - learning_rate: 0.0010 - val_custom_mse: 0.4060 - val_custom_mae: 0.4267\n",
            "Epoch 2/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0668 - mae: 0.1625 - mse: 0.0668 - val_loss: 0.0634 - val_mae: 0.1393 - val_mse: 0.0634 - learning_rate: 0.0010 - val_custom_mse: 0.3994 - val_custom_mae: 0.4229\n",
            "Epoch 3/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0617 - mae: 0.1529 - mse: 0.0617 - val_loss: 0.0627 - val_mae: 0.1386 - val_mse: 0.0627 - learning_rate: 0.0010 - val_custom_mse: 0.3947 - val_custom_mae: 0.4187\n",
            "Epoch 4/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0612 - mae: 0.1521 - mse: 0.0612 - val_loss: 0.0624 - val_mae: 0.1383 - val_mse: 0.0624 - learning_rate: 0.0010 - val_custom_mse: 0.3929 - val_custom_mae: 0.4170\n",
            "Epoch 5/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0611 - mae: 0.1519 - mse: 0.0611 - val_loss: 0.0627 - val_mae: 0.1385 - val_mse: 0.0627 - learning_rate: 0.0010 - val_custom_mse: 0.3951 - val_custom_mae: 0.4182\n",
            "Epoch 6/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0610 - mae: 0.1518 - mse: 0.0610 - val_loss: 0.0624 - val_mae: 0.1384 - val_mse: 0.0624 - learning_rate: 0.0010 - val_custom_mse: 0.3926 - val_custom_mae: 0.4165\n",
            "Epoch 7/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0609 - mae: 0.1518 - mse: 0.0609 - val_loss: 0.0625 - val_mae: 0.1384 - val_mse: 0.0625 - learning_rate: 0.0010 - val_custom_mse: 0.3939 - val_custom_mae: 0.4176\n",
            "Epoch 8/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0609 - mae: 0.1518 - mse: 0.0609 - val_loss: 0.0626 - val_mae: 0.1387 - val_mse: 0.0626 - learning_rate: 0.0010 - val_custom_mse: 0.3940 - val_custom_mae: 0.4178\n",
            "Epoch 9/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0609 - mae: 0.1518 - mse: 0.0609 - val_loss: 0.0627 - val_mae: 0.1385 - val_mse: 0.0627 - learning_rate: 0.0010 - val_custom_mse: 0.3955 - val_custom_mae: 0.4185\n",
            "Epoch 10/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0609 - mae: 0.1518 - mse: 0.0609 - val_loss: 0.0625 - val_mae: 0.1384 - val_mse: 0.0625 - learning_rate: 0.0010 - val_custom_mse: 0.3930 - val_custom_mae: 0.4171\n",
            "Epoch 11/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0609 - mae: 0.1518 - mse: 0.0609 - val_loss: 0.0626 - val_mae: 0.1384 - val_mse: 0.0626 - learning_rate: 0.0010 - val_custom_mse: 0.3949 - val_custom_mae: 0.4180\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0609 - mae: 0.1518 - mse: 0.0609 - val_loss: 0.0625 - val_mae: 0.1384 - val_mse: 0.0625 - learning_rate: 0.0010 - val_custom_mse: 0.3929 - val_custom_mae: 0.4166\n",
            "Epoch 13/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0606 - mae: 0.1514 - mse: 0.0606 - val_loss: 0.0623 - val_mae: 0.1374 - val_mse: 0.0623 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3945 - val_custom_mae: 0.4169\n",
            "Epoch 14/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0606 - mae: 0.1513 - mse: 0.0606 - val_loss: 0.0623 - val_mae: 0.1375 - val_mse: 0.0623 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3944 - val_custom_mae: 0.4170\n",
            "Epoch 15/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0606 - mae: 0.1513 - mse: 0.0606 - val_loss: 0.0624 - val_mae: 0.1375 - val_mse: 0.0624 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3951 - val_custom_mae: 0.4175\n",
            "Epoch 16/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1513 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1375 - val_mse: 0.0623 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3949 - val_custom_mae: 0.4173\n",
            "Epoch 17/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0606 - mae: 0.1513 - mse: 0.0606 - val_loss: 0.0624 - val_mae: 0.1376 - val_mse: 0.0624 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3953 - val_custom_mae: 0.4176\n",
            "Epoch 18/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0606 - mae: 0.1513 - mse: 0.0606 - val_loss: 0.0623 - val_mae: 0.1375 - val_mse: 0.0623 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3946 - val_custom_mae: 0.4170\n",
            "Epoch 19/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0606 - mae: 0.1513 - mse: 0.0606 - val_loss: 0.0623 - val_mae: 0.1375 - val_mse: 0.0623 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3941 - val_custom_mae: 0.4168\n",
            "Epoch 20/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0606 - mae: 0.1513 - mse: 0.0606 - val_loss: 0.0623 - val_mae: 0.1375 - val_mse: 0.0623 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3945 - val_custom_mae: 0.4171\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1513 - mse: 0.0605 - val_loss: 0.0624 - val_mae: 0.1375 - val_mse: 0.0624 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3948 - val_custom_mae: 0.4173\n",
            "Epoch 22/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3950 - val_custom_mae: 0.4170\n",
            "Epoch 23/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0624 - val_mae: 0.1374 - val_mse: 0.0624 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 24/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3954 - val_custom_mae: 0.4172\n",
            "Epoch 25/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3951 - val_custom_mae: 0.4170\n",
            "Epoch 26/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1374 - val_mse: 0.0623 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3953 - val_custom_mae: 0.4171\n",
            "Epoch 27/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3952 - val_custom_mae: 0.4171\n",
            "Epoch 28/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1374 - val_mse: 0.0623 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3953 - val_custom_mae: 0.4172\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0604 - mae: 0.1512 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3949 - val_custom_mae: 0.4169\n",
            "Epoch 30/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3950 - val_custom_mae: 0.4171\n",
            "Epoch 31/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3951 - val_custom_mae: 0.4171\n",
            "Epoch 32/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3953 - val_custom_mae: 0.4173\n",
            "Epoch 33/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 34/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 35/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 36/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0624 - val_mae: 0.1373 - val_mse: 0.0624 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 38/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 39/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 40/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 41/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 42/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 43/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 44/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 46/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0624 - val_mae: 0.1373 - val_mse: 0.0624 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 47/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0624 - val_mae: 0.1373 - val_mse: 0.0624 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 48/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 49/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 50/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0624 - val_mae: 0.1373 - val_mse: 0.0624 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 51/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0624 - val_mae: 0.1373 - val_mse: 0.0624 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 52/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 54/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 55/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 56/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 57/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 58/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 59/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 60/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 62/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 63/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 64/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 65/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 66/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 67/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 68/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 70/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 71/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 72/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 73/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 74/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 75/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 76/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 78/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 79/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 80/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 81/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 82/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 83/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 84/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 86/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 87/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 88/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 89/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 90/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 91/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 92/100\n",
            "1046/1046 - 8s - 8ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 94/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1512 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.0480e-11 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 95/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.0480e-11 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 96/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.0480e-11 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 97/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.0480e-11 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 98/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0604 - mae: 0.1511 - mse: 0.0604 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.0480e-11 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 99/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1512 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.0480e-11 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Epoch 100/100\n",
            "1046/1046 - 8s - 7ms/step - loss: 0.0605 - mae: 0.1511 - mse: 0.0605 - val_loss: 0.0623 - val_mae: 0.1373 - val_mse: 0.0623 - learning_rate: 2.0480e-11 - val_custom_mse: 0.3954 - val_custom_mae: 0.4173\n",
            "Running experiment: horizon=192, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1043/1043 - 11s - 11ms/step - loss: 0.1496 - mae: 0.2328 - mse: 0.1496 - val_loss: 0.1114 - val_mae: 0.1678 - val_mse: 0.1114 - learning_rate: 0.0010 - val_custom_mse: 0.5111 - val_custom_mae: 0.4764\n",
            "Epoch 2/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0594 - mae: 0.1107 - mse: 0.0594 - val_loss: 0.0964 - val_mae: 0.1203 - val_mse: 0.0964 - learning_rate: 0.0010 - val_custom_mse: 0.5011 - val_custom_mae: 0.4714\n",
            "Epoch 3/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0554 - mae: 0.0893 - mse: 0.0554 - val_loss: 0.0945 - val_mae: 0.1073 - val_mse: 0.0945 - learning_rate: 0.0010 - val_custom_mse: 0.4992 - val_custom_mae: 0.4683\n",
            "Epoch 4/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0546 - mae: 0.0823 - mse: 0.0546 - val_loss: 0.0942 - val_mae: 0.1035 - val_mse: 0.0942 - learning_rate: 0.0010 - val_custom_mse: 0.4993 - val_custom_mae: 0.4672\n",
            "Epoch 5/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0542 - mae: 0.0795 - mse: 0.0542 - val_loss: 0.0935 - val_mae: 0.1006 - val_mse: 0.0935 - learning_rate: 0.0010 - val_custom_mse: 0.4962 - val_custom_mae: 0.4661\n",
            "Epoch 6/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0538 - mae: 0.0783 - mse: 0.0538 - val_loss: 0.0929 - val_mae: 0.0995 - val_mse: 0.0929 - learning_rate: 0.0010 - val_custom_mse: 0.4936 - val_custom_mae: 0.4637\n",
            "Epoch 7/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0535 - mae: 0.0773 - mse: 0.0535 - val_loss: 0.0936 - val_mae: 0.1002 - val_mse: 0.0936 - learning_rate: 0.0010 - val_custom_mse: 0.4971 - val_custom_mae: 0.4659\n",
            "Epoch 8/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0533 - mae: 0.0765 - mse: 0.0533 - val_loss: 0.0930 - val_mae: 0.0995 - val_mse: 0.0930 - learning_rate: 0.0010 - val_custom_mse: 0.4940 - val_custom_mae: 0.4641\n",
            "Epoch 9/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0531 - mae: 0.0757 - mse: 0.0531 - val_loss: 0.0923 - val_mae: 0.0966 - val_mse: 0.0923 - learning_rate: 0.0010 - val_custom_mse: 0.4911 - val_custom_mae: 0.4624\n",
            "Epoch 10/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0530 - mae: 0.0751 - mse: 0.0530 - val_loss: 0.0926 - val_mae: 0.0959 - val_mse: 0.0926 - learning_rate: 0.0010 - val_custom_mse: 0.4925 - val_custom_mae: 0.4625\n",
            "Epoch 11/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0529 - mae: 0.0745 - mse: 0.0529 - val_loss: 0.0925 - val_mae: 0.0947 - val_mse: 0.0925 - learning_rate: 0.0010 - val_custom_mse: 0.4923 - val_custom_mae: 0.4621\n",
            "Epoch 12/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0528 - mae: 0.0741 - mse: 0.0528 - val_loss: 0.0928 - val_mae: 0.0951 - val_mse: 0.0928 - learning_rate: 0.0010 - val_custom_mse: 0.4936 - val_custom_mae: 0.4630\n",
            "Epoch 13/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0528 - mae: 0.0739 - mse: 0.0528 - val_loss: 0.0922 - val_mae: 0.0939 - val_mse: 0.0922 - learning_rate: 0.0010 - val_custom_mse: 0.4908 - val_custom_mae: 0.4614\n",
            "Epoch 14/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0527 - mae: 0.0736 - mse: 0.0527 - val_loss: 0.0927 - val_mae: 0.0943 - val_mse: 0.0927 - learning_rate: 0.0010 - val_custom_mse: 0.4936 - val_custom_mae: 0.4633\n",
            "Epoch 15/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0527 - mae: 0.0735 - mse: 0.0527 - val_loss: 0.0927 - val_mae: 0.0939 - val_mse: 0.0927 - learning_rate: 0.0010 - val_custom_mse: 0.4935 - val_custom_mae: 0.4631\n",
            "Epoch 16/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0526 - mae: 0.0733 - mse: 0.0526 - val_loss: 0.0921 - val_mae: 0.0940 - val_mse: 0.0921 - learning_rate: 0.0010 - val_custom_mse: 0.4905 - val_custom_mae: 0.4622\n",
            "Epoch 17/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0526 - mae: 0.0733 - mse: 0.0526 - val_loss: 0.0923 - val_mae: 0.0937 - val_mse: 0.0923 - learning_rate: 0.0010 - val_custom_mse: 0.4916 - val_custom_mae: 0.4622\n",
            "Epoch 18/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0526 - mae: 0.0732 - mse: 0.0526 - val_loss: 0.0924 - val_mae: 0.0935 - val_mse: 0.0924 - learning_rate: 0.0010 - val_custom_mse: 0.4922 - val_custom_mae: 0.4623\n",
            "Epoch 19/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0526 - mae: 0.0731 - mse: 0.0526 - val_loss: 0.0926 - val_mae: 0.0937 - val_mse: 0.0926 - learning_rate: 0.0010 - val_custom_mse: 0.4930 - val_custom_mae: 0.4632\n",
            "Epoch 20/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0526 - mae: 0.0731 - mse: 0.0526 - val_loss: 0.0923 - val_mae: 0.0933 - val_mse: 0.0923 - learning_rate: 0.0010 - val_custom_mse: 0.4914 - val_custom_mae: 0.4619\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0525 - mae: 0.0731 - mse: 0.0525 - val_loss: 0.0929 - val_mae: 0.0938 - val_mse: 0.0929 - learning_rate: 0.0010 - val_custom_mse: 0.4947 - val_custom_mae: 0.4643\n",
            "Epoch 22/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0523 - mae: 0.0724 - mse: 0.0523 - val_loss: 0.0917 - val_mae: 0.0922 - val_mse: 0.0917 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4885 - val_custom_mae: 0.4603\n",
            "Epoch 23/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0523 - mae: 0.0723 - mse: 0.0523 - val_loss: 0.0917 - val_mae: 0.0922 - val_mse: 0.0917 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4886 - val_custom_mae: 0.4605\n",
            "Epoch 24/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0523 - mae: 0.0723 - mse: 0.0523 - val_loss: 0.0917 - val_mae: 0.0921 - val_mse: 0.0917 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4886 - val_custom_mae: 0.4603\n",
            "Epoch 25/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0523 - mae: 0.0723 - mse: 0.0523 - val_loss: 0.0917 - val_mae: 0.0921 - val_mse: 0.0917 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4885 - val_custom_mae: 0.4603\n",
            "Epoch 26/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0523 - mae: 0.0723 - mse: 0.0523 - val_loss: 0.0917 - val_mae: 0.0921 - val_mse: 0.0917 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4882 - val_custom_mae: 0.4602\n",
            "Epoch 27/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0523 - mae: 0.0723 - mse: 0.0523 - val_loss: 0.0917 - val_mae: 0.0921 - val_mse: 0.0917 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4886 - val_custom_mae: 0.4603\n",
            "Epoch 28/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0523 - mae: 0.0722 - mse: 0.0523 - val_loss: 0.0917 - val_mae: 0.0921 - val_mse: 0.0917 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4885 - val_custom_mae: 0.4604\n",
            "Epoch 29/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0523 - mae: 0.0722 - mse: 0.0523 - val_loss: 0.0917 - val_mae: 0.0922 - val_mse: 0.0917 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4885 - val_custom_mae: 0.4604\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0523 - mae: 0.0722 - mse: 0.0523 - val_loss: 0.0917 - val_mae: 0.0922 - val_mse: 0.0917 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4886 - val_custom_mae: 0.4605\n",
            "Epoch 31/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0918 - val_mae: 0.0920 - val_mse: 0.0918 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4891 - val_custom_mae: 0.4607\n",
            "Epoch 32/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0918 - val_mae: 0.0921 - val_mse: 0.0918 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4892 - val_custom_mae: 0.4608\n",
            "Epoch 33/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0921 - val_mse: 0.0919 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4893 - val_custom_mae: 0.4608\n",
            "Epoch 34/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4893 - val_custom_mae: 0.4608\n",
            "Epoch 35/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0918 - val_mae: 0.0920 - val_mse: 0.0918 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4893 - val_custom_mae: 0.4608\n",
            "Epoch 36/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0918 - val_mae: 0.0920 - val_mse: 0.0918 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4892 - val_custom_mae: 0.4608\n",
            "Epoch 37/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0921 - val_mse: 0.0919 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4893 - val_custom_mae: 0.4608\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0921 - val_mse: 0.0919 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4893 - val_custom_mae: 0.4609\n",
            "Epoch 39/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.4609\n",
            "Epoch 40/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4895 - val_custom_mae: 0.4610\n",
            "Epoch 41/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4895 - val_custom_mae: 0.4610\n",
            "Epoch 42/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0721 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0921 - val_mse: 0.0919 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 43/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0921 - val_mse: 0.0919 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 44/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0921 - val_mse: 0.0919 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 45/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0921 - val_mse: 0.0919 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0921 - val_mse: 0.0919 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 47/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 48/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 49/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 50/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 51/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 52/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 53/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 55/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 56/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 57/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 58/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 59/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 60/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 61/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 63/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 64/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 65/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 66/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 67/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 68/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 69/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 71/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 72/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 73/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 74/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 75/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 76/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 77/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 79/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 80/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 81/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 82/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 83/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 84/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 85/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 87/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 88/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 89/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 90/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 91/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 92/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 93/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 95/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 96/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 97/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 98/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 99/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Epoch 100/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0522 - mae: 0.0720 - mse: 0.0522 - val_loss: 0.0919 - val_mae: 0.0920 - val_mse: 0.0919 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4896 - val_custom_mae: 0.4610\n",
            "Running experiment: horizon=192, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1043/1043 - 12s - 12ms/step - loss: 0.1788 - mae: 0.2650 - mse: 0.1788 - val_loss: 0.1177 - val_mae: 0.1788 - val_mse: 0.1177 - learning_rate: 0.0010 - val_custom_mse: 0.5172 - val_custom_mae: 0.4812\n",
            "Epoch 2/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0773 - mae: 0.1558 - mse: 0.0773 - val_loss: 0.1067 - val_mae: 0.1536 - val_mse: 0.1067 - learning_rate: 0.0010 - val_custom_mse: 0.5048 - val_custom_mae: 0.4732\n",
            "Epoch 3/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0728 - mae: 0.1432 - mse: 0.0728 - val_loss: 0.1056 - val_mae: 0.1506 - val_mse: 0.1056 - learning_rate: 0.0010 - val_custom_mse: 0.5028 - val_custom_mae: 0.4715\n",
            "Epoch 4/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0718 - mae: 0.1406 - mse: 0.0718 - val_loss: 0.1047 - val_mae: 0.1491 - val_mse: 0.1047 - learning_rate: 0.0010 - val_custom_mse: 0.4996 - val_custom_mae: 0.4693\n",
            "Epoch 5/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0714 - mae: 0.1397 - mse: 0.0714 - val_loss: 0.1037 - val_mae: 0.1479 - val_mse: 0.1037 - learning_rate: 0.0010 - val_custom_mse: 0.4955 - val_custom_mae: 0.4662\n",
            "Epoch 6/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0712 - mae: 0.1393 - mse: 0.0712 - val_loss: 0.1039 - val_mae: 0.1477 - val_mse: 0.1039 - learning_rate: 0.0010 - val_custom_mse: 0.4971 - val_custom_mae: 0.4678\n",
            "Epoch 7/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0712 - mae: 0.1392 - mse: 0.0712 - val_loss: 0.1034 - val_mae: 0.1474 - val_mse: 0.1034 - learning_rate: 0.0010 - val_custom_mse: 0.4947 - val_custom_mae: 0.4662\n",
            "Epoch 8/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0711 - mae: 0.1390 - mse: 0.0711 - val_loss: 0.1036 - val_mae: 0.1475 - val_mse: 0.1036 - learning_rate: 0.0010 - val_custom_mse: 0.4960 - val_custom_mae: 0.4670\n",
            "Epoch 9/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0711 - mae: 0.1390 - mse: 0.0711 - val_loss: 0.1035 - val_mae: 0.1471 - val_mse: 0.1035 - learning_rate: 0.0010 - val_custom_mse: 0.4959 - val_custom_mae: 0.4663\n",
            "Epoch 10/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0710 - mae: 0.1390 - mse: 0.0710 - val_loss: 0.1028 - val_mae: 0.1466 - val_mse: 0.1028 - learning_rate: 0.0010 - val_custom_mse: 0.4919 - val_custom_mae: 0.4638\n",
            "Epoch 11/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0710 - mae: 0.1389 - mse: 0.0710 - val_loss: 0.1033 - val_mae: 0.1470 - val_mse: 0.1033 - learning_rate: 0.0010 - val_custom_mse: 0.4946 - val_custom_mae: 0.4660\n",
            "Epoch 12/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0710 - mae: 0.1389 - mse: 0.0710 - val_loss: 0.1033 - val_mae: 0.1471 - val_mse: 0.1033 - learning_rate: 0.0010 - val_custom_mse: 0.4944 - val_custom_mae: 0.4661\n",
            "Epoch 13/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0709 - mae: 0.1389 - mse: 0.0709 - val_loss: 0.1028 - val_mae: 0.1466 - val_mse: 0.1028 - learning_rate: 0.0010 - val_custom_mse: 0.4921 - val_custom_mae: 0.4642\n",
            "Epoch 14/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0709 - mae: 0.1389 - mse: 0.0709 - val_loss: 0.1032 - val_mae: 0.1469 - val_mse: 0.1032 - learning_rate: 0.0010 - val_custom_mse: 0.4943 - val_custom_mae: 0.4658\n",
            "Epoch 15/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0709 - mae: 0.1389 - mse: 0.0709 - val_loss: 0.1031 - val_mae: 0.1469 - val_mse: 0.1031 - learning_rate: 0.0010 - val_custom_mse: 0.4938 - val_custom_mae: 0.4659\n",
            "Epoch 16/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0709 - mae: 0.1389 - mse: 0.0709 - val_loss: 0.1029 - val_mae: 0.1467 - val_mse: 0.1029 - learning_rate: 0.0010 - val_custom_mse: 0.4924 - val_custom_mae: 0.4643\n",
            "Epoch 17/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0709 - mae: 0.1388 - mse: 0.0709 - val_loss: 0.1029 - val_mae: 0.1466 - val_mse: 0.1029 - learning_rate: 0.0010 - val_custom_mse: 0.4926 - val_custom_mae: 0.4642\n",
            "Epoch 18/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0709 - mae: 0.1388 - mse: 0.0709 - val_loss: 0.1027 - val_mae: 0.1464 - val_mse: 0.1027 - learning_rate: 0.0010 - val_custom_mse: 0.4915 - val_custom_mae: 0.4635\n",
            "Epoch 19/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0709 - mae: 0.1388 - mse: 0.0709 - val_loss: 0.1024 - val_mae: 0.1463 - val_mse: 0.1024 - learning_rate: 0.0010 - val_custom_mse: 0.4904 - val_custom_mae: 0.4631\n",
            "Epoch 20/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0709 - mae: 0.1388 - mse: 0.0709 - val_loss: 0.1029 - val_mae: 0.1465 - val_mse: 0.1029 - learning_rate: 0.0010 - val_custom_mse: 0.4926 - val_custom_mae: 0.4644\n",
            "Epoch 21/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0709 - mae: 0.1388 - mse: 0.0709 - val_loss: 0.1031 - val_mae: 0.1467 - val_mse: 0.1031 - learning_rate: 0.0010 - val_custom_mse: 0.4941 - val_custom_mae: 0.4655\n",
            "Epoch 22/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0708 - mae: 0.1388 - mse: 0.0708 - val_loss: 0.1032 - val_mae: 0.1468 - val_mse: 0.1032 - learning_rate: 0.0010 - val_custom_mse: 0.4946 - val_custom_mae: 0.4658\n",
            "Epoch 23/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0709 - mae: 0.1388 - mse: 0.0709 - val_loss: 0.1029 - val_mae: 0.1466 - val_mse: 0.1029 - learning_rate: 0.0010 - val_custom_mse: 0.4928 - val_custom_mae: 0.4642\n",
            "Epoch 24/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0709 - mae: 0.1389 - mse: 0.0709 - val_loss: 0.1033 - val_mae: 0.1468 - val_mse: 0.1033 - learning_rate: 0.0010 - val_custom_mse: 0.4951 - val_custom_mae: 0.4663\n",
            "Epoch 25/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0709 - mae: 0.1388 - mse: 0.0709 - val_loss: 0.1029 - val_mae: 0.1466 - val_mse: 0.1029 - learning_rate: 0.0010 - val_custom_mse: 0.4932 - val_custom_mae: 0.4653\n",
            "Epoch 26/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0708 - mae: 0.1388 - mse: 0.0708 - val_loss: 0.1028 - val_mae: 0.1465 - val_mse: 0.1028 - learning_rate: 0.0010 - val_custom_mse: 0.4925 - val_custom_mae: 0.4643\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0708 - mae: 0.1388 - mse: 0.0708 - val_loss: 0.1028 - val_mae: 0.1465 - val_mse: 0.1028 - learning_rate: 0.0010 - val_custom_mse: 0.4919 - val_custom_mae: 0.4640\n",
            "Epoch 28/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1385 - mse: 0.0706 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4894 - val_custom_mae: 0.4614\n",
            "Epoch 29/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1384 - mse: 0.0706 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4900 - val_custom_mae: 0.4620\n",
            "Epoch 30/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1385 - mse: 0.0706 - val_loss: 0.1021 - val_mae: 0.1455 - val_mse: 0.1021 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4891 - val_custom_mae: 0.4613\n",
            "Epoch 31/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1385 - mse: 0.0706 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4893 - val_custom_mae: 0.4614\n",
            "Epoch 32/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1385 - mse: 0.0706 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4895 - val_custom_mae: 0.4615\n",
            "Epoch 33/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1385 - mse: 0.0706 - val_loss: 0.1021 - val_mae: 0.1455 - val_mse: 0.1021 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4889 - val_custom_mae: 0.4612\n",
            "Epoch 34/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1021 - val_mae: 0.1455 - val_mse: 0.1021 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4888 - val_custom_mae: 0.4610\n",
            "Epoch 35/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1385 - mse: 0.0706 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4893 - val_custom_mae: 0.4613\n",
            "Epoch 36/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1022 - val_mae: 0.1456 - val_mse: 0.1022 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4895 - val_custom_mae: 0.4615\n",
            "Epoch 37/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1385 - mse: 0.0706 - val_loss: 0.1021 - val_mae: 0.1455 - val_mse: 0.1021 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4888 - val_custom_mae: 0.4611\n",
            "Epoch 38/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1385 - mse: 0.0706 - val_loss: 0.1021 - val_mae: 0.1455 - val_mse: 0.1021 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4889 - val_custom_mae: 0.4612\n",
            "Epoch 39/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1385 - mse: 0.0705 - val_loss: 0.1021 - val_mae: 0.1455 - val_mse: 0.1021 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4889 - val_custom_mae: 0.4611\n",
            "Epoch 40/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1022 - val_mae: 0.1456 - val_mse: 0.1022 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4895 - val_custom_mae: 0.4616\n",
            "Epoch 41/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0706 - mae: 0.1385 - mse: 0.0706 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4894 - val_custom_mae: 0.4616\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1385 - mse: 0.0705 - val_loss: 0.1021 - val_mae: 0.1455 - val_mse: 0.1021 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4889 - val_custom_mae: 0.4611\n",
            "Epoch 43/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4896 - val_custom_mae: 0.4617\n",
            "Epoch 44/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4898 - val_custom_mae: 0.4617\n",
            "Epoch 45/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4898 - val_custom_mae: 0.4617\n",
            "Epoch 46/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4897 - val_custom_mae: 0.4617\n",
            "Epoch 47/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4899 - val_custom_mae: 0.4618\n",
            "Epoch 48/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1455 - val_mse: 0.1023 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4898 - val_custom_mae: 0.4618\n",
            "Epoch 49/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1455 - val_mse: 0.1023 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4898 - val_custom_mae: 0.4618\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 50: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1022 - val_mae: 0.1455 - val_mse: 0.1022 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4897 - val_custom_mae: 0.4616\n",
            "Epoch 51/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1455 - val_mse: 0.1023 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4899 - val_custom_mae: 0.4618\n",
            "Epoch 52/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1455 - val_mse: 0.1023 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4900 - val_custom_mae: 0.4619\n",
            "Epoch 53/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4900 - val_custom_mae: 0.4620\n",
            "Epoch 54/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 55/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4900 - val_custom_mae: 0.4620\n",
            "Epoch 56/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 57/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 59/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 60/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 61/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 62/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 63/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 64/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 65/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 66: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 67/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 68/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 69/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 70/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 71/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 72/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 73/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 74: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 75/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 76/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0704 - mae: 0.1383 - mse: 0.0704 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 77/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 78/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 79/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 80/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 81/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 83/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 84/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 85/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 86/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 87/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 88/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 89/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 90: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 91/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 92/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0704 - mae: 0.1383 - mse: 0.0704 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 93/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 94/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 95/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 96/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 97/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1384 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 98: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 99/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0704 - mae: 0.1384 - mse: 0.0704 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Epoch 100/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0705 - mae: 0.1383 - mse: 0.0705 - val_loss: 0.1023 - val_mae: 0.1456 - val_mse: 0.1023 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4901 - val_custom_mae: 0.4620\n",
            "Running experiment: horizon=192, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1043/1043 - 12s - 12ms/step - loss: 0.2104 - mae: 0.2941 - mse: 0.2104 - val_loss: 0.1210 - val_mae: 0.1872 - val_mse: 0.1210 - learning_rate: 0.0010 - val_custom_mse: 0.5083 - val_custom_mae: 0.4767\n",
            "Epoch 2/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0862 - mae: 0.1742 - mse: 0.0862 - val_loss: 0.1133 - val_mae: 0.1688 - val_mse: 0.1133 - learning_rate: 0.0010 - val_custom_mse: 0.5053 - val_custom_mae: 0.4737\n",
            "Epoch 3/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0815 - mae: 0.1640 - mse: 0.0815 - val_loss: 0.1119 - val_mae: 0.1665 - val_mse: 0.1119 - learning_rate: 0.0010 - val_custom_mse: 0.5007 - val_custom_mae: 0.4698\n",
            "Epoch 4/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0808 - mae: 0.1627 - mse: 0.0808 - val_loss: 0.1114 - val_mae: 0.1658 - val_mse: 0.1114 - learning_rate: 0.0010 - val_custom_mse: 0.4993 - val_custom_mae: 0.4686\n",
            "Epoch 5/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0804 - mae: 0.1622 - mse: 0.0804 - val_loss: 0.1113 - val_mae: 0.1659 - val_mse: 0.1113 - learning_rate: 0.0010 - val_custom_mse: 0.4992 - val_custom_mae: 0.4687\n",
            "Epoch 6/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0804 - mae: 0.1621 - mse: 0.0804 - val_loss: 0.1117 - val_mae: 0.1661 - val_mse: 0.1117 - learning_rate: 0.0010 - val_custom_mse: 0.5012 - val_custom_mae: 0.4697\n",
            "Epoch 7/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0803 - mae: 0.1621 - mse: 0.0803 - val_loss: 0.1114 - val_mae: 0.1658 - val_mse: 0.1114 - learning_rate: 0.0010 - val_custom_mse: 0.4999 - val_custom_mae: 0.4689\n",
            "Epoch 8/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0803 - mae: 0.1620 - mse: 0.0803 - val_loss: 0.1113 - val_mae: 0.1658 - val_mse: 0.1113 - learning_rate: 0.0010 - val_custom_mse: 0.4990 - val_custom_mae: 0.4683\n",
            "Epoch 9/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0802 - mae: 0.1620 - mse: 0.0802 - val_loss: 0.1110 - val_mae: 0.1657 - val_mse: 0.1110 - learning_rate: 0.0010 - val_custom_mse: 0.4975 - val_custom_mae: 0.4677\n",
            "Epoch 10/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0802 - mae: 0.1619 - mse: 0.0802 - val_loss: 0.1107 - val_mae: 0.1654 - val_mse: 0.1107 - learning_rate: 0.0010 - val_custom_mse: 0.4957 - val_custom_mae: 0.4662\n",
            "Epoch 11/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0802 - mae: 0.1619 - mse: 0.0802 - val_loss: 0.1112 - val_mae: 0.1657 - val_mse: 0.1112 - learning_rate: 0.0010 - val_custom_mse: 0.4982 - val_custom_mae: 0.4676\n",
            "Epoch 12/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0801 - mae: 0.1619 - mse: 0.0801 - val_loss: 0.1110 - val_mae: 0.1656 - val_mse: 0.1110 - learning_rate: 0.0010 - val_custom_mse: 0.4976 - val_custom_mae: 0.4677\n",
            "Epoch 13/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0801 - mae: 0.1619 - mse: 0.0801 - val_loss: 0.1108 - val_mae: 0.1655 - val_mse: 0.1108 - learning_rate: 0.0010 - val_custom_mse: 0.4960 - val_custom_mae: 0.4666\n",
            "Epoch 14/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0801 - mae: 0.1619 - mse: 0.0801 - val_loss: 0.1107 - val_mae: 0.1653 - val_mse: 0.1107 - learning_rate: 0.0010 - val_custom_mse: 0.4961 - val_custom_mae: 0.4666\n",
            "Epoch 15/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0802 - mae: 0.1619 - mse: 0.0802 - val_loss: 0.1107 - val_mae: 0.1654 - val_mse: 0.1107 - learning_rate: 0.0010 - val_custom_mse: 0.4962 - val_custom_mae: 0.4666\n",
            "Epoch 16/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0801 - mae: 0.1619 - mse: 0.0801 - val_loss: 0.1108 - val_mae: 0.1654 - val_mse: 0.1108 - learning_rate: 0.0010 - val_custom_mse: 0.4964 - val_custom_mae: 0.4669\n",
            "Epoch 17/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0801 - mae: 0.1619 - mse: 0.0801 - val_loss: 0.1107 - val_mae: 0.1653 - val_mse: 0.1107 - learning_rate: 0.0010 - val_custom_mse: 0.4959 - val_custom_mae: 0.4664\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0801 - mae: 0.1619 - mse: 0.0801 - val_loss: 0.1107 - val_mae: 0.1654 - val_mse: 0.1107 - learning_rate: 0.0010 - val_custom_mse: 0.4960 - val_custom_mae: 0.4666\n",
            "Epoch 19/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0798 - mae: 0.1615 - mse: 0.0798 - val_loss: 0.1102 - val_mae: 0.1642 - val_mse: 0.1102 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4949 - val_custom_mae: 0.4652\n",
            "Epoch 20/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0798 - mae: 0.1614 - mse: 0.0798 - val_loss: 0.1102 - val_mae: 0.1642 - val_mse: 0.1102 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4949 - val_custom_mae: 0.4653\n",
            "Epoch 21/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0798 - mae: 0.1614 - mse: 0.0798 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4941 - val_custom_mae: 0.4649\n",
            "Epoch 22/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0798 - mae: 0.1614 - mse: 0.0798 - val_loss: 0.1103 - val_mae: 0.1643 - val_mse: 0.1103 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4955 - val_custom_mae: 0.4658\n",
            "Epoch 23/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0798 - mae: 0.1614 - mse: 0.0798 - val_loss: 0.1102 - val_mae: 0.1643 - val_mse: 0.1102 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4949 - val_custom_mae: 0.4654\n",
            "Epoch 24/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0798 - mae: 0.1614 - mse: 0.0798 - val_loss: 0.1102 - val_mae: 0.1642 - val_mse: 0.1102 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4949 - val_custom_mae: 0.4653\n",
            "Epoch 25/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0798 - mae: 0.1615 - mse: 0.0798 - val_loss: 0.1103 - val_mae: 0.1642 - val_mse: 0.1103 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4950 - val_custom_mae: 0.4654\n",
            "Epoch 26/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0798 - mae: 0.1614 - mse: 0.0798 - val_loss: 0.1103 - val_mae: 0.1643 - val_mse: 0.1103 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4954 - val_custom_mae: 0.4657\n",
            "Epoch 27/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0798 - mae: 0.1615 - mse: 0.0798 - val_loss: 0.1102 - val_mae: 0.1643 - val_mse: 0.1102 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4949 - val_custom_mae: 0.4654\n",
            "Epoch 28/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0798 - mae: 0.1614 - mse: 0.0798 - val_loss: 0.1102 - val_mae: 0.1643 - val_mse: 0.1102 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4948 - val_custom_mae: 0.4653\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0798 - mae: 0.1614 - mse: 0.0798 - val_loss: 0.1102 - val_mae: 0.1642 - val_mse: 0.1102 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 30/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4944 - val_custom_mae: 0.4649\n",
            "Epoch 31/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 32/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 33/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0798 - mae: 0.1614 - mse: 0.0798 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4944 - val_custom_mae: 0.4649\n",
            "Epoch 34/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1640 - val_mse: 0.1101 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4942 - val_custom_mae: 0.4648\n",
            "Epoch 35/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4943 - val_custom_mae: 0.4649\n",
            "Epoch 36/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4944 - val_custom_mae: 0.4650\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4943 - val_custom_mae: 0.4649\n",
            "Epoch 38/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1640 - val_mse: 0.1101 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4944 - val_custom_mae: 0.4650\n",
            "Epoch 39/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4945 - val_custom_mae: 0.4650\n",
            "Epoch 40/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4945 - val_custom_mae: 0.4650\n",
            "Epoch 41/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1101 - val_mae: 0.1641 - val_mse: 0.1101 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 42/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0796 - mae: 0.1613 - mse: 0.0796 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 43/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 44/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4947 - val_custom_mae: 0.4652\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 46/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 47/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0796 - mae: 0.1613 - mse: 0.0796 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 48/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 49/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 50/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4651\n",
            "Epoch 51/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 52/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0796 - mae: 0.1613 - mse: 0.0796 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 53: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 54/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 55/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 56/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 57/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 58/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 59/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 60/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 62/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 63/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 64/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 65/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 66/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 67/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0796 - mae: 0.1613 - mse: 0.0796 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 68/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 70/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 71/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 72/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 73/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 74/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 75/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 76/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 78/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 79/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 80/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 81/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 82/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 83/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 84/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 86/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 87/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 88/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 89/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 90/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 91/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 92/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 94/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 95/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 96/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 97/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 98/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 99/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Epoch 100/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0797 - mae: 0.1613 - mse: 0.0797 - val_loss: 0.1102 - val_mae: 0.1641 - val_mse: 0.1102 - learning_rate: 1.0240e-10 - val_custom_mse: 0.4946 - val_custom_mae: 0.4652\n",
            "Running experiment: horizon=192, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_7', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1043/1043 - 12s - 12ms/step - loss: 0.2472 - mae: 0.3228 - mse: 0.2472 - val_loss: 0.1250 - val_mae: 0.1930 - val_mse: 0.1250 - learning_rate: 0.0010 - val_custom_mse: 0.5096 - val_custom_mae: 0.4754\n",
            "Epoch 2/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0939 - mae: 0.1870 - mse: 0.0939 - val_loss: 0.1185 - val_mae: 0.1787 - val_mse: 0.1185 - learning_rate: 0.0010 - val_custom_mse: 0.5070 - val_custom_mae: 0.4728\n",
            "Epoch 3/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0889 - mae: 0.1779 - mse: 0.0889 - val_loss: 0.1180 - val_mae: 0.1781 - val_mse: 0.1180 - learning_rate: 0.0010 - val_custom_mse: 0.5054 - val_custom_mae: 0.4717\n",
            "Epoch 4/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0884 - mae: 0.1771 - mse: 0.0884 - val_loss: 0.1176 - val_mae: 0.1776 - val_mse: 0.1176 - learning_rate: 0.0010 - val_custom_mse: 0.5041 - val_custom_mae: 0.4703\n",
            "Epoch 5/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0882 - mae: 0.1768 - mse: 0.0882 - val_loss: 0.1170 - val_mae: 0.1772 - val_mse: 0.1170 - learning_rate: 0.0010 - val_custom_mse: 0.5016 - val_custom_mae: 0.4686\n",
            "Epoch 6/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0881 - mae: 0.1767 - mse: 0.0881 - val_loss: 0.1171 - val_mae: 0.1774 - val_mse: 0.1171 - learning_rate: 0.0010 - val_custom_mse: 0.5018 - val_custom_mae: 0.4692\n",
            "Epoch 7/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0880 - mae: 0.1767 - mse: 0.0880 - val_loss: 0.1172 - val_mae: 0.1775 - val_mse: 0.1172 - learning_rate: 0.0010 - val_custom_mse: 0.5023 - val_custom_mae: 0.4693\n",
            "Epoch 8/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0880 - mae: 0.1767 - mse: 0.0880 - val_loss: 0.1170 - val_mae: 0.1773 - val_mse: 0.1170 - learning_rate: 0.0010 - val_custom_mse: 0.5013 - val_custom_mae: 0.4687\n",
            "Epoch 9/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1168 - val_mae: 0.1772 - val_mse: 0.1168 - learning_rate: 0.0010 - val_custom_mse: 0.5004 - val_custom_mae: 0.4678\n",
            "Epoch 10/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0880 - mae: 0.1766 - mse: 0.0880 - val_loss: 0.1170 - val_mae: 0.1773 - val_mse: 0.1170 - learning_rate: 0.0010 - val_custom_mse: 0.5013 - val_custom_mae: 0.4685\n",
            "Epoch 11/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1171 - val_mae: 0.1773 - val_mse: 0.1171 - learning_rate: 0.0010 - val_custom_mse: 0.5024 - val_custom_mae: 0.4692\n",
            "Epoch 12/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1168 - val_mae: 0.1770 - val_mse: 0.1168 - learning_rate: 0.0010 - val_custom_mse: 0.5009 - val_custom_mae: 0.4681\n",
            "Epoch 13/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0880 - mae: 0.1766 - mse: 0.0880 - val_loss: 0.1168 - val_mae: 0.1770 - val_mse: 0.1168 - learning_rate: 0.0010 - val_custom_mse: 0.5004 - val_custom_mae: 0.4676\n",
            "Epoch 14/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1168 - val_mae: 0.1772 - val_mse: 0.1168 - learning_rate: 0.0010 - val_custom_mse: 0.5000 - val_custom_mae: 0.4678\n",
            "Epoch 15/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1167 - val_mae: 0.1769 - val_mse: 0.1167 - learning_rate: 0.0010 - val_custom_mse: 0.5002 - val_custom_mae: 0.4678\n",
            "Epoch 16/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1169 - val_mae: 0.1771 - val_mse: 0.1169 - learning_rate: 0.0010 - val_custom_mse: 0.5014 - val_custom_mae: 0.4684\n",
            "Epoch 17/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1171 - val_mae: 0.1773 - val_mse: 0.1171 - learning_rate: 0.0010 - val_custom_mse: 0.5020 - val_custom_mae: 0.4687\n",
            "Epoch 18/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1165 - val_mae: 0.1769 - val_mse: 0.1165 - learning_rate: 0.0010 - val_custom_mse: 0.4988 - val_custom_mae: 0.4667\n",
            "Epoch 19/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1165 - val_mae: 0.1770 - val_mse: 0.1165 - learning_rate: 0.0010 - val_custom_mse: 0.4985 - val_custom_mae: 0.4667\n",
            "Epoch 20/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1168 - val_mae: 0.1771 - val_mse: 0.1168 - learning_rate: 0.0010 - val_custom_mse: 0.5001 - val_custom_mae: 0.4675\n",
            "Epoch 21/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1166 - val_mae: 0.1770 - val_mse: 0.1166 - learning_rate: 0.0010 - val_custom_mse: 0.4994 - val_custom_mae: 0.4673\n",
            "Epoch 22/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1170 - val_mae: 0.1772 - val_mse: 0.1170 - learning_rate: 0.0010 - val_custom_mse: 0.5018 - val_custom_mae: 0.4686\n",
            "Epoch 23/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1168 - val_mae: 0.1771 - val_mse: 0.1168 - learning_rate: 0.0010 - val_custom_mse: 0.5005 - val_custom_mae: 0.4676\n",
            "Epoch 24/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0880 - mae: 0.1766 - mse: 0.0880 - val_loss: 0.1170 - val_mae: 0.1771 - val_mse: 0.1170 - learning_rate: 0.0010 - val_custom_mse: 0.5017 - val_custom_mae: 0.4684\n",
            "Epoch 25/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1167 - val_mae: 0.1770 - val_mse: 0.1167 - learning_rate: 0.0010 - val_custom_mse: 0.5005 - val_custom_mae: 0.4676\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0879 - mae: 0.1766 - mse: 0.0879 - val_loss: 0.1168 - val_mae: 0.1770 - val_mse: 0.1168 - learning_rate: 0.0010 - val_custom_mse: 0.5005 - val_custom_mae: 0.4677\n",
            "Epoch 27/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0876 - mae: 0.1762 - mse: 0.0876 - val_loss: 0.1165 - val_mae: 0.1764 - val_mse: 0.1165 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4999 - val_custom_mae: 0.4682\n",
            "Epoch 28/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0876 - mae: 0.1761 - mse: 0.0876 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4990 - val_custom_mae: 0.4676\n",
            "Epoch 29/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0876 - mae: 0.1761 - mse: 0.0876 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4991 - val_custom_mae: 0.4677\n",
            "Epoch 30/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0875 - mae: 0.1761 - mse: 0.0875 - val_loss: 0.1164 - val_mae: 0.1764 - val_mse: 0.1164 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4993 - val_custom_mae: 0.4676\n",
            "Epoch 31/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0875 - mae: 0.1761 - mse: 0.0875 - val_loss: 0.1164 - val_mae: 0.1764 - val_mse: 0.1164 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4997 - val_custom_mae: 0.4681\n",
            "Epoch 32/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0875 - mae: 0.1761 - mse: 0.0875 - val_loss: 0.1164 - val_mae: 0.1765 - val_mse: 0.1164 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4994 - val_custom_mae: 0.4679\n",
            "Epoch 33/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0875 - mae: 0.1761 - mse: 0.0875 - val_loss: 0.1164 - val_mae: 0.1765 - val_mse: 0.1164 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4994 - val_custom_mae: 0.4679\n",
            "Epoch 34/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0875 - mae: 0.1761 - mse: 0.0875 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4992 - val_custom_mae: 0.4678\n",
            "Epoch 35/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0875 - mae: 0.1761 - mse: 0.0875 - val_loss: 0.1165 - val_mae: 0.1765 - val_mse: 0.1165 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4997 - val_custom_mae: 0.4682\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0875 - mae: 0.1761 - mse: 0.0875 - val_loss: 0.1164 - val_mae: 0.1765 - val_mse: 0.1164 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4996 - val_custom_mae: 0.4681\n",
            "Epoch 37/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0875 - mae: 0.1760 - mse: 0.0875 - val_loss: 0.1164 - val_mae: 0.1763 - val_mse: 0.1164 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4994 - val_custom_mae: 0.4682\n",
            "Epoch 38/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4993 - val_custom_mae: 0.4681\n",
            "Epoch 39/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0875 - mae: 0.1760 - mse: 0.0875 - val_loss: 0.1164 - val_mae: 0.1764 - val_mse: 0.1164 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4994 - val_custom_mae: 0.4682\n",
            "Epoch 40/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0875 - mae: 0.1760 - mse: 0.0875 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4992 - val_custom_mae: 0.4682\n",
            "Epoch 41/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4992 - val_custom_mae: 0.4682\n",
            "Epoch 42/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4991 - val_custom_mae: 0.4681\n",
            "Epoch 43/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4990 - val_custom_mae: 0.4681\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0875 - mae: 0.1760 - mse: 0.0875 - val_loss: 0.1164 - val_mae: 0.1764 - val_mse: 0.1164 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 45/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1164 - val_mae: 0.1764 - val_mse: 0.1164 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 46/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1164 - val_mae: 0.1764 - val_mse: 0.1164 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 47/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1759 - mse: 0.0874 - val_loss: 0.1164 - val_mae: 0.1764 - val_mse: 0.1164 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 48/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 49/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 50/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1164 - val_mae: 0.1764 - val_mse: 0.1164 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 51/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1164 - val_mae: 0.1764 - val_mse: 0.1164 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 53/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 54/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 55/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1759 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 56/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 57/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1763 - val_mse: 0.1163 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4683\n",
            "Epoch 58/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 59/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 60: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1759 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 61/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1759 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 62/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 63/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 64/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 65/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1759 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 66/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 67/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 69/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 70/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 71/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 72/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 73/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 74/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 75/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 76: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 77/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 78/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 79/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 80/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 81/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 82/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 83/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 84: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 85/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1759 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 86/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 87/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 88/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 89/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 90/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 91/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 93/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 94/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 95/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 96/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1759 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 97/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 98/100\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 99/100\n",
            "1043/1043 - 8s - 8ms/step - loss: 0.0875 - mae: 0.1760 - mse: 0.0875 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 100: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1043/1043 - 8s - 7ms/step - loss: 0.0874 - mae: 0.1760 - mse: 0.0874 - val_loss: 0.1163 - val_mae: 0.1764 - val_mse: 0.1163 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4994 - val_custom_mae: 0.4684\n",
            "Running experiment: horizon=336, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_8', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1038/1038 - 13s - 12ms/step - loss: 0.2673 - mae: 0.3376 - mse: 0.2673 - val_loss: 0.2393 - val_mae: 0.2663 - val_mse: 0.2393 - learning_rate: 0.0010 - val_custom_mse: 0.6632 - val_custom_mae: 0.5463\n",
            "Epoch 2/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1169 - mae: 0.1783 - mse: 0.1169 - val_loss: 0.2157 - val_mae: 0.2131 - val_mse: 0.2157 - learning_rate: 0.0010 - val_custom_mse: 0.6445 - val_custom_mae: 0.5327\n",
            "Epoch 3/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1094 - mae: 0.1527 - mse: 0.1094 - val_loss: 0.2106 - val_mae: 0.1968 - val_mse: 0.2106 - learning_rate: 0.0010 - val_custom_mse: 0.6369 - val_custom_mae: 0.5275\n",
            "Epoch 4/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1079 - mae: 0.1446 - mse: 0.1079 - val_loss: 0.2087 - val_mae: 0.1907 - val_mse: 0.2087 - learning_rate: 0.0010 - val_custom_mse: 0.6329 - val_custom_mae: 0.5263\n",
            "Epoch 5/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1071 - mae: 0.1407 - mse: 0.1071 - val_loss: 0.2090 - val_mae: 0.1869 - val_mse: 0.2090 - learning_rate: 0.0010 - val_custom_mse: 0.6349 - val_custom_mae: 0.5255\n",
            "Epoch 6/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1065 - mae: 0.1385 - mse: 0.1065 - val_loss: 0.2081 - val_mae: 0.1847 - val_mse: 0.2081 - learning_rate: 0.0010 - val_custom_mse: 0.6328 - val_custom_mae: 0.5255\n",
            "Epoch 7/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1059 - mae: 0.1369 - mse: 0.1059 - val_loss: 0.2083 - val_mae: 0.1830 - val_mse: 0.2083 - learning_rate: 0.0010 - val_custom_mse: 0.6336 - val_custom_mae: 0.5255\n",
            "Epoch 8/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1056 - mae: 0.1358 - mse: 0.1056 - val_loss: 0.2077 - val_mae: 0.1817 - val_mse: 0.2077 - learning_rate: 0.0010 - val_custom_mse: 0.6319 - val_custom_mae: 0.5241\n",
            "Epoch 9/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1053 - mae: 0.1351 - mse: 0.1053 - val_loss: 0.2080 - val_mae: 0.1809 - val_mse: 0.2080 - learning_rate: 0.0010 - val_custom_mse: 0.6332 - val_custom_mae: 0.5241\n",
            "Epoch 10/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1051 - mae: 0.1346 - mse: 0.1051 - val_loss: 0.2088 - val_mae: 0.1812 - val_mse: 0.2088 - learning_rate: 0.0010 - val_custom_mse: 0.6357 - val_custom_mae: 0.5261\n",
            "Epoch 11/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1050 - mae: 0.1342 - mse: 0.1050 - val_loss: 0.2080 - val_mae: 0.1805 - val_mse: 0.2080 - learning_rate: 0.0010 - val_custom_mse: 0.6330 - val_custom_mae: 0.5248\n",
            "Epoch 12/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1049 - mae: 0.1339 - mse: 0.1049 - val_loss: 0.2082 - val_mae: 0.1805 - val_mse: 0.2082 - learning_rate: 0.0010 - val_custom_mse: 0.6337 - val_custom_mae: 0.5255\n",
            "Epoch 13/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1048 - mae: 0.1337 - mse: 0.1048 - val_loss: 0.2090 - val_mae: 0.1804 - val_mse: 0.2090 - learning_rate: 0.0010 - val_custom_mse: 0.6361 - val_custom_mae: 0.5258\n",
            "Epoch 14/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1047 - mae: 0.1335 - mse: 0.1047 - val_loss: 0.2073 - val_mae: 0.1794 - val_mse: 0.2073 - learning_rate: 0.0010 - val_custom_mse: 0.6310 - val_custom_mae: 0.5236\n",
            "Epoch 15/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1046 - mae: 0.1334 - mse: 0.1046 - val_loss: 0.2080 - val_mae: 0.1795 - val_mse: 0.2080 - learning_rate: 0.0010 - val_custom_mse: 0.6333 - val_custom_mae: 0.5244\n",
            "Epoch 16/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1046 - mae: 0.1332 - mse: 0.1046 - val_loss: 0.2076 - val_mae: 0.1793 - val_mse: 0.2076 - learning_rate: 0.0010 - val_custom_mse: 0.6322 - val_custom_mae: 0.5240\n",
            "Epoch 17/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1045 - mae: 0.1331 - mse: 0.1045 - val_loss: 0.2083 - val_mae: 0.1794 - val_mse: 0.2083 - learning_rate: 0.0010 - val_custom_mse: 0.6342 - val_custom_mae: 0.5245\n",
            "Epoch 18/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1045 - mae: 0.1331 - mse: 0.1045 - val_loss: 0.2076 - val_mae: 0.1791 - val_mse: 0.2076 - learning_rate: 0.0010 - val_custom_mse: 0.6321 - val_custom_mae: 0.5240\n",
            "Epoch 19/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1044 - mae: 0.1330 - mse: 0.1044 - val_loss: 0.2082 - val_mae: 0.1796 - val_mse: 0.2082 - learning_rate: 0.0010 - val_custom_mse: 0.6340 - val_custom_mae: 0.5255\n",
            "Epoch 20/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1044 - mae: 0.1329 - mse: 0.1044 - val_loss: 0.2076 - val_mae: 0.1792 - val_mse: 0.2076 - learning_rate: 0.0010 - val_custom_mse: 0.6321 - val_custom_mae: 0.5243\n",
            "Epoch 21/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1044 - mae: 0.1329 - mse: 0.1044 - val_loss: 0.2084 - val_mae: 0.1794 - val_mse: 0.2084 - learning_rate: 0.0010 - val_custom_mse: 0.6346 - val_custom_mae: 0.5254\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1043 - mae: 0.1328 - mse: 0.1043 - val_loss: 0.2077 - val_mae: 0.1790 - val_mse: 0.2077 - learning_rate: 0.0010 - val_custom_mse: 0.6324 - val_custom_mae: 0.5244\n",
            "Epoch 23/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1039 - mae: 0.1319 - mse: 0.1039 - val_loss: 0.2064 - val_mae: 0.1773 - val_mse: 0.2064 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6286 - val_custom_mae: 0.5220\n",
            "Epoch 24/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1038 - mae: 0.1317 - mse: 0.1038 - val_loss: 0.2065 - val_mae: 0.1773 - val_mse: 0.2065 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6289 - val_custom_mae: 0.5222\n",
            "Epoch 25/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1038 - mae: 0.1317 - mse: 0.1038 - val_loss: 0.2065 - val_mae: 0.1773 - val_mse: 0.2065 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6289 - val_custom_mae: 0.5220\n",
            "Epoch 26/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1038 - mae: 0.1317 - mse: 0.1038 - val_loss: 0.2065 - val_mae: 0.1773 - val_mse: 0.2065 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6289 - val_custom_mae: 0.5221\n",
            "Epoch 27/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1038 - mae: 0.1317 - mse: 0.1038 - val_loss: 0.2064 - val_mae: 0.1772 - val_mse: 0.2064 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6287 - val_custom_mae: 0.5219\n",
            "Epoch 28/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1037 - mae: 0.1317 - mse: 0.1037 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6289 - val_custom_mae: 0.5220\n",
            "Epoch 29/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1037 - mae: 0.1317 - mse: 0.1037 - val_loss: 0.2066 - val_mae: 0.1773 - val_mse: 0.2066 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6294 - val_custom_mae: 0.5223\n",
            "Epoch 30/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1037 - mae: 0.1317 - mse: 0.1037 - val_loss: 0.2066 - val_mae: 0.1773 - val_mse: 0.2066 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6294 - val_custom_mae: 0.5224\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1037 - mae: 0.1317 - mse: 0.1037 - val_loss: 0.2067 - val_mae: 0.1773 - val_mse: 0.2067 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6295 - val_custom_mae: 0.5224\n",
            "Epoch 32/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1315 - mse: 0.1036 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6290 - val_custom_mae: 0.5224\n",
            "Epoch 33/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1315 - mse: 0.1036 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6290 - val_custom_mae: 0.5223\n",
            "Epoch 34/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1314 - mse: 0.1036 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6289 - val_custom_mae: 0.5223\n",
            "Epoch 35/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1314 - mse: 0.1036 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6289 - val_custom_mae: 0.5223\n",
            "Epoch 36/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1314 - mse: 0.1036 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6289 - val_custom_mae: 0.5223\n",
            "Epoch 37/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1314 - mse: 0.1036 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6289 - val_custom_mae: 0.5223\n",
            "Epoch 38/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1314 - mse: 0.1036 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6289 - val_custom_mae: 0.5223\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1314 - mse: 0.1036 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6288 - val_custom_mae: 0.5223\n",
            "Epoch 40/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1314 - mse: 0.1036 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6290 - val_custom_mae: 0.5224\n",
            "Epoch 41/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6290 - val_custom_mae: 0.5225\n",
            "Epoch 42/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6290 - val_custom_mae: 0.5225\n",
            "Epoch 43/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6290 - val_custom_mae: 0.5225\n",
            "Epoch 44/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 45/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 46/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 48/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 49/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 50/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1771 - val_mse: 0.2065 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 51/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 52/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 53/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 54/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 56/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 57/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 58/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 59/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 60/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 61/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 62/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 64/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 65/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 66/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 67/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 68/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 69/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 70/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 72/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 73/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 74/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 75/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 76/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 77/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 78/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 80/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 81/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 82/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 83/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 84/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 85/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 86/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 88/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 89/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 90/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 91/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 92/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 93/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 94/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 96/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.0240e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 97/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.0240e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 98/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.0240e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 99/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.0240e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Epoch 100/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1035 - mae: 0.1314 - mse: 0.1035 - val_loss: 0.2065 - val_mae: 0.1772 - val_mse: 0.2065 - learning_rate: 1.0240e-10 - val_custom_mse: 0.6291 - val_custom_mae: 0.5226\n",
            "Running experiment: horizon=336, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_9', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1038/1038 - 13s - 13ms/step - loss: 0.2971 - mae: 0.3633 - mse: 0.2971 - val_loss: 0.2413 - val_mae: 0.2696 - val_mse: 0.2413 - learning_rate: 0.0010 - val_custom_mse: 0.6622 - val_custom_mae: 0.5448\n",
            "Epoch 2/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1326 - mae: 0.2119 - mse: 0.1326 - val_loss: 0.2231 - val_mae: 0.2329 - val_mse: 0.2231 - learning_rate: 0.0010 - val_custom_mse: 0.6466 - val_custom_mae: 0.5327\n",
            "Epoch 3/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1239 - mae: 0.1931 - mse: 0.1239 - val_loss: 0.2202 - val_mae: 0.2272 - val_mse: 0.2202 - learning_rate: 0.0010 - val_custom_mse: 0.6413 - val_custom_mae: 0.5293\n",
            "Epoch 4/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1224 - mae: 0.1894 - mse: 0.1224 - val_loss: 0.2199 - val_mae: 0.2261 - val_mse: 0.2199 - learning_rate: 0.0010 - val_custom_mse: 0.6416 - val_custom_mae: 0.5302\n",
            "Epoch 5/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1219 - mae: 0.1883 - mse: 0.1219 - val_loss: 0.2187 - val_mae: 0.2249 - val_mse: 0.2187 - learning_rate: 0.0010 - val_custom_mse: 0.6383 - val_custom_mae: 0.5283\n",
            "Epoch 6/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1216 - mae: 0.1879 - mse: 0.1216 - val_loss: 0.2191 - val_mae: 0.2247 - val_mse: 0.2191 - learning_rate: 0.0010 - val_custom_mse: 0.6399 - val_custom_mae: 0.5291\n",
            "Epoch 7/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1213 - mae: 0.1877 - mse: 0.1213 - val_loss: 0.2176 - val_mae: 0.2237 - val_mse: 0.2176 - learning_rate: 0.0010 - val_custom_mse: 0.6358 - val_custom_mae: 0.5272\n",
            "Epoch 8/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1212 - mae: 0.1876 - mse: 0.1212 - val_loss: 0.2173 - val_mae: 0.2235 - val_mse: 0.2173 - learning_rate: 0.0010 - val_custom_mse: 0.6350 - val_custom_mae: 0.5272\n",
            "Epoch 9/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1211 - mae: 0.1875 - mse: 0.1211 - val_loss: 0.2177 - val_mae: 0.2236 - val_mse: 0.2177 - learning_rate: 0.0010 - val_custom_mse: 0.6364 - val_custom_mae: 0.5279\n",
            "Epoch 10/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1209 - mae: 0.1874 - mse: 0.1209 - val_loss: 0.2172 - val_mae: 0.2232 - val_mse: 0.2172 - learning_rate: 0.0010 - val_custom_mse: 0.6349 - val_custom_mae: 0.5270\n",
            "Epoch 11/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1209 - mae: 0.1874 - mse: 0.1209 - val_loss: 0.2173 - val_mae: 0.2233 - val_mse: 0.2173 - learning_rate: 0.0010 - val_custom_mse: 0.6351 - val_custom_mae: 0.5273\n",
            "Epoch 12/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1209 - mae: 0.1873 - mse: 0.1209 - val_loss: 0.2163 - val_mae: 0.2226 - val_mse: 0.2163 - learning_rate: 0.0010 - val_custom_mse: 0.6321 - val_custom_mae: 0.5253\n",
            "Epoch 13/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1208 - mae: 0.1873 - mse: 0.1208 - val_loss: 0.2166 - val_mae: 0.2227 - val_mse: 0.2166 - learning_rate: 0.0010 - val_custom_mse: 0.6332 - val_custom_mae: 0.5261\n",
            "Epoch 14/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1208 - mae: 0.1873 - mse: 0.1208 - val_loss: 0.2161 - val_mae: 0.2223 - val_mse: 0.2161 - learning_rate: 0.0010 - val_custom_mse: 0.6317 - val_custom_mae: 0.5251\n",
            "Epoch 15/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1208 - mae: 0.1873 - mse: 0.1208 - val_loss: 0.2162 - val_mae: 0.2224 - val_mse: 0.2162 - learning_rate: 0.0010 - val_custom_mse: 0.6319 - val_custom_mae: 0.5250\n",
            "Epoch 16/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1208 - mae: 0.1872 - mse: 0.1208 - val_loss: 0.2171 - val_mae: 0.2228 - val_mse: 0.2171 - learning_rate: 0.0010 - val_custom_mse: 0.6345 - val_custom_mae: 0.5266\n",
            "Epoch 17/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1207 - mae: 0.1872 - mse: 0.1207 - val_loss: 0.2165 - val_mae: 0.2224 - val_mse: 0.2165 - learning_rate: 0.0010 - val_custom_mse: 0.6328 - val_custom_mae: 0.5256\n",
            "Epoch 18/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1207 - mae: 0.1872 - mse: 0.1207 - val_loss: 0.2170 - val_mae: 0.2227 - val_mse: 0.2170 - learning_rate: 0.0010 - val_custom_mse: 0.6344 - val_custom_mae: 0.5263\n",
            "Epoch 19/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1207 - mae: 0.1872 - mse: 0.1207 - val_loss: 0.2162 - val_mae: 0.2223 - val_mse: 0.2162 - learning_rate: 0.0010 - val_custom_mse: 0.6321 - val_custom_mae: 0.5250\n",
            "Epoch 20/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1207 - mae: 0.1872 - mse: 0.1207 - val_loss: 0.2169 - val_mae: 0.2224 - val_mse: 0.2169 - learning_rate: 0.0010 - val_custom_mse: 0.6342 - val_custom_mae: 0.5259\n",
            "Epoch 21/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1206 - mae: 0.1872 - mse: 0.1206 - val_loss: 0.2168 - val_mae: 0.2227 - val_mse: 0.2168 - learning_rate: 0.0010 - val_custom_mse: 0.6339 - val_custom_mae: 0.5265\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1206 - mae: 0.1872 - mse: 0.1206 - val_loss: 0.2163 - val_mae: 0.2222 - val_mse: 0.2163 - learning_rate: 0.0010 - val_custom_mse: 0.6322 - val_custom_mae: 0.5251\n",
            "Epoch 23/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1202 - mae: 0.1867 - mse: 0.1202 - val_loss: 0.2142 - val_mae: 0.2206 - val_mse: 0.2142 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6263 - val_custom_mae: 0.5222\n",
            "Epoch 24/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1202 - mae: 0.1867 - mse: 0.1202 - val_loss: 0.2142 - val_mae: 0.2206 - val_mse: 0.2142 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6262 - val_custom_mae: 0.5222\n",
            "Epoch 25/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1202 - mae: 0.1867 - mse: 0.1202 - val_loss: 0.2142 - val_mae: 0.2206 - val_mse: 0.2142 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6262 - val_custom_mae: 0.5222\n",
            "Epoch 26/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1202 - mae: 0.1867 - mse: 0.1202 - val_loss: 0.2141 - val_mae: 0.2205 - val_mse: 0.2141 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6259 - val_custom_mae: 0.5219\n",
            "Epoch 27/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1201 - mae: 0.1866 - mse: 0.1201 - val_loss: 0.2143 - val_mae: 0.2207 - val_mse: 0.2143 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6265 - val_custom_mae: 0.5222\n",
            "Epoch 28/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1202 - mae: 0.1867 - mse: 0.1202 - val_loss: 0.2143 - val_mae: 0.2207 - val_mse: 0.2143 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6266 - val_custom_mae: 0.5223\n",
            "Epoch 29/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1202 - mae: 0.1867 - mse: 0.1202 - val_loss: 0.2141 - val_mae: 0.2205 - val_mse: 0.2141 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6259 - val_custom_mae: 0.5220\n",
            "Epoch 30/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1202 - mae: 0.1867 - mse: 0.1202 - val_loss: 0.2141 - val_mae: 0.2206 - val_mse: 0.2141 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6260 - val_custom_mae: 0.5220\n",
            "Epoch 31/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1202 - mae: 0.1867 - mse: 0.1202 - val_loss: 0.2141 - val_mae: 0.2206 - val_mse: 0.2141 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6259 - val_custom_mae: 0.5219\n",
            "Epoch 32/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1201 - mae: 0.1867 - mse: 0.1201 - val_loss: 0.2141 - val_mae: 0.2206 - val_mse: 0.2141 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6260 - val_custom_mae: 0.5220\n",
            "Epoch 33/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1201 - mae: 0.1867 - mse: 0.1201 - val_loss: 0.2143 - val_mae: 0.2206 - val_mse: 0.2143 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6266 - val_custom_mae: 0.5222\n",
            "Epoch 34/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1201 - mae: 0.1867 - mse: 0.1201 - val_loss: 0.2143 - val_mae: 0.2207 - val_mse: 0.2143 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6267 - val_custom_mae: 0.5224\n",
            "Epoch 35/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1201 - mae: 0.1867 - mse: 0.1201 - val_loss: 0.2142 - val_mae: 0.2206 - val_mse: 0.2142 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6264 - val_custom_mae: 0.5222\n",
            "Epoch 36/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2141 - val_mae: 0.2205 - val_mse: 0.2141 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6260 - val_custom_mae: 0.5219\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1201 - mae: 0.1867 - mse: 0.1201 - val_loss: 0.2141 - val_mae: 0.2205 - val_mse: 0.2141 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6261 - val_custom_mae: 0.5219\n",
            "Epoch 38/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2137 - val_mae: 0.2202 - val_mse: 0.2137 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6249 - val_custom_mae: 0.5215\n",
            "Epoch 39/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6247 - val_custom_mae: 0.5214\n",
            "Epoch 40/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 41/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1201 - mae: 0.1866 - mse: 0.1201 - val_loss: 0.2135 - val_mae: 0.2201 - val_mse: 0.2135 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6244 - val_custom_mae: 0.5212\n",
            "Epoch 42/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6247 - val_custom_mae: 0.5214\n",
            "Epoch 43/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 44/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 45/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6247 - val_custom_mae: 0.5214\n",
            "Epoch 46/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 47/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2137 - val_mae: 0.2202 - val_mse: 0.2137 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6248 - val_custom_mae: 0.5215\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 49/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 50/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6247 - val_custom_mae: 0.5214\n",
            "Epoch 51/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6247 - val_custom_mae: 0.5214\n",
            "Epoch 52/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6247 - val_custom_mae: 0.5214\n",
            "Epoch 53/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 54/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 55/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6245 - val_custom_mae: 0.5214\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6245 - val_custom_mae: 0.5214\n",
            "Epoch 57/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 58/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 59/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 60/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 61/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 62/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 63/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 65/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 66/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 67/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 68/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 69/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 70/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 71/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 73/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 74/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 75/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 76/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 77/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 78/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 79/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 81/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 82/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 83/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 84/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 85/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 86/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 87/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 89/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 90/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 91/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 92/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 93/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 94/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 95/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 97/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 98/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1200 - mae: 0.1865 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 99/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1199 - mae: 0.1865 - mse: 0.1199 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Epoch 100/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1200 - mae: 0.1866 - mse: 0.1200 - val_loss: 0.2136 - val_mae: 0.2202 - val_mse: 0.2136 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6246 - val_custom_mae: 0.5214\n",
            "Running experiment: horizon=336, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_10', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1038/1038 - 13s - 13ms/step - loss: 0.3280 - mae: 0.3855 - mse: 0.3280 - val_loss: 0.2421 - val_mae: 0.2720 - val_mse: 0.2421 - learning_rate: 0.0010 - val_custom_mse: 0.6589 - val_custom_mae: 0.5442\n",
            "Epoch 2/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1404 - mae: 0.2265 - mse: 0.1404 - val_loss: 0.2261 - val_mae: 0.2425 - val_mse: 0.2261 - learning_rate: 0.0010 - val_custom_mse: 0.6407 - val_custom_mae: 0.5302\n",
            "Epoch 3/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1311 - mae: 0.2094 - mse: 0.1311 - val_loss: 0.2247 - val_mae: 0.2399 - val_mse: 0.2247 - learning_rate: 0.0010 - val_custom_mse: 0.6386 - val_custom_mae: 0.5292\n",
            "Epoch 4/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1301 - mae: 0.2073 - mse: 0.1301 - val_loss: 0.2246 - val_mae: 0.2394 - val_mse: 0.2246 - learning_rate: 0.0010 - val_custom_mse: 0.6388 - val_custom_mae: 0.5296\n",
            "Epoch 5/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1297 - mae: 0.2069 - mse: 0.1297 - val_loss: 0.2235 - val_mae: 0.2386 - val_mse: 0.2235 - learning_rate: 0.0010 - val_custom_mse: 0.6360 - val_custom_mae: 0.5280\n",
            "Epoch 6/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1295 - mae: 0.2067 - mse: 0.1295 - val_loss: 0.2241 - val_mae: 0.2389 - val_mse: 0.2241 - learning_rate: 0.0010 - val_custom_mse: 0.6381 - val_custom_mae: 0.5293\n",
            "Epoch 7/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1294 - mae: 0.2067 - mse: 0.1294 - val_loss: 0.2226 - val_mae: 0.2379 - val_mse: 0.2226 - learning_rate: 0.0010 - val_custom_mse: 0.6336 - val_custom_mae: 0.5269\n",
            "Epoch 8/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1293 - mae: 0.2066 - mse: 0.1293 - val_loss: 0.2229 - val_mae: 0.2380 - val_mse: 0.2229 - learning_rate: 0.0010 - val_custom_mse: 0.6345 - val_custom_mae: 0.5275\n",
            "Epoch 9/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1292 - mae: 0.2065 - mse: 0.1292 - val_loss: 0.2224 - val_mae: 0.2378 - val_mse: 0.2224 - learning_rate: 0.0010 - val_custom_mse: 0.6331 - val_custom_mae: 0.5270\n",
            "Epoch 10/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1292 - mae: 0.2066 - mse: 0.1292 - val_loss: 0.2225 - val_mae: 0.2380 - val_mse: 0.2225 - learning_rate: 0.0010 - val_custom_mse: 0.6334 - val_custom_mae: 0.5275\n",
            "Epoch 11/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1291 - mae: 0.2065 - mse: 0.1291 - val_loss: 0.2218 - val_mae: 0.2373 - val_mse: 0.2218 - learning_rate: 0.0010 - val_custom_mse: 0.6314 - val_custom_mae: 0.5259\n",
            "Epoch 12/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1290 - mae: 0.2065 - mse: 0.1290 - val_loss: 0.2221 - val_mae: 0.2376 - val_mse: 0.2221 - learning_rate: 0.0010 - val_custom_mse: 0.6322 - val_custom_mae: 0.5264\n",
            "Epoch 13/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1290 - mae: 0.2065 - mse: 0.1290 - val_loss: 0.2217 - val_mae: 0.2373 - val_mse: 0.2217 - learning_rate: 0.0010 - val_custom_mse: 0.6311 - val_custom_mae: 0.5260\n",
            "Epoch 14/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1291 - mae: 0.2065 - mse: 0.1291 - val_loss: 0.2218 - val_mae: 0.2375 - val_mse: 0.2218 - learning_rate: 0.0010 - val_custom_mse: 0.6313 - val_custom_mae: 0.5263\n",
            "Epoch 15/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1290 - mae: 0.2066 - mse: 0.1290 - val_loss: 0.2221 - val_mae: 0.2376 - val_mse: 0.2221 - learning_rate: 0.0010 - val_custom_mse: 0.6323 - val_custom_mae: 0.5267\n",
            "Epoch 16/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1290 - mae: 0.2065 - mse: 0.1290 - val_loss: 0.2210 - val_mae: 0.2368 - val_mse: 0.2210 - learning_rate: 0.0010 - val_custom_mse: 0.6289 - val_custom_mae: 0.5244\n",
            "Epoch 17/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1290 - mae: 0.2066 - mse: 0.1290 - val_loss: 0.2218 - val_mae: 0.2374 - val_mse: 0.2218 - learning_rate: 0.0010 - val_custom_mse: 0.6313 - val_custom_mae: 0.5260\n",
            "Epoch 18/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1290 - mae: 0.2066 - mse: 0.1290 - val_loss: 0.2223 - val_mae: 0.2377 - val_mse: 0.2223 - learning_rate: 0.0010 - val_custom_mse: 0.6326 - val_custom_mae: 0.5265\n",
            "Epoch 19/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1290 - mae: 0.2066 - mse: 0.1290 - val_loss: 0.2225 - val_mae: 0.2378 - val_mse: 0.2225 - learning_rate: 0.0010 - val_custom_mse: 0.6331 - val_custom_mae: 0.5269\n",
            "Epoch 20/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1289 - mae: 0.2065 - mse: 0.1289 - val_loss: 0.2219 - val_mae: 0.2375 - val_mse: 0.2219 - learning_rate: 0.0010 - val_custom_mse: 0.6312 - val_custom_mae: 0.5261\n",
            "Epoch 21/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1290 - mae: 0.2066 - mse: 0.1290 - val_loss: 0.2222 - val_mae: 0.2376 - val_mse: 0.2222 - learning_rate: 0.0010 - val_custom_mse: 0.6322 - val_custom_mae: 0.5263\n",
            "Epoch 22/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1290 - mae: 0.2066 - mse: 0.1290 - val_loss: 0.2216 - val_mae: 0.2373 - val_mse: 0.2216 - learning_rate: 0.0010 - val_custom_mse: 0.6305 - val_custom_mae: 0.5256\n",
            "Epoch 23/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1289 - mae: 0.2065 - mse: 0.1289 - val_loss: 0.2224 - val_mae: 0.2377 - val_mse: 0.2224 - learning_rate: 0.0010 - val_custom_mse: 0.6330 - val_custom_mae: 0.5271\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1290 - mae: 0.2066 - mse: 0.1290 - val_loss: 0.2218 - val_mae: 0.2373 - val_mse: 0.2218 - learning_rate: 0.0010 - val_custom_mse: 0.6313 - val_custom_mae: 0.5257\n",
            "Epoch 25/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1285 - mae: 0.2061 - mse: 0.1285 - val_loss: 0.2200 - val_mae: 0.2358 - val_mse: 0.2200 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6262 - val_custom_mae: 0.5232\n",
            "Epoch 26/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1285 - mae: 0.2060 - mse: 0.1285 - val_loss: 0.2201 - val_mae: 0.2358 - val_mse: 0.2201 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6265 - val_custom_mae: 0.5233\n",
            "Epoch 27/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1285 - mae: 0.2060 - mse: 0.1285 - val_loss: 0.2201 - val_mae: 0.2359 - val_mse: 0.2201 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6264 - val_custom_mae: 0.5233\n",
            "Epoch 28/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1285 - mae: 0.2061 - mse: 0.1285 - val_loss: 0.2202 - val_mae: 0.2359 - val_mse: 0.2202 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6267 - val_custom_mae: 0.5233\n",
            "Epoch 29/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1285 - mae: 0.2060 - mse: 0.1285 - val_loss: 0.2202 - val_mae: 0.2359 - val_mse: 0.2202 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6266 - val_custom_mae: 0.5233\n",
            "Epoch 30/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1285 - mae: 0.2061 - mse: 0.1285 - val_loss: 0.2200 - val_mae: 0.2358 - val_mse: 0.2200 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6261 - val_custom_mae: 0.5230\n",
            "Epoch 31/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1285 - mae: 0.2060 - mse: 0.1285 - val_loss: 0.2200 - val_mae: 0.2359 - val_mse: 0.2200 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6263 - val_custom_mae: 0.5232\n",
            "Epoch 32/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1285 - mae: 0.2061 - mse: 0.1285 - val_loss: 0.2198 - val_mae: 0.2357 - val_mse: 0.2198 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6257 - val_custom_mae: 0.5227\n",
            "Epoch 33/100\n",
            "1038/1038 - 10s - 10ms/step - loss: 0.1285 - mae: 0.2060 - mse: 0.1285 - val_loss: 0.2203 - val_mae: 0.2360 - val_mse: 0.2203 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6269 - val_custom_mae: 0.5235\n",
            "Epoch 34/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1285 - mae: 0.2061 - mse: 0.1285 - val_loss: 0.2203 - val_mae: 0.2360 - val_mse: 0.2203 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6271 - val_custom_mae: 0.5236\n",
            "Epoch 35/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1284 - mae: 0.2060 - mse: 0.1284 - val_loss: 0.2200 - val_mae: 0.2358 - val_mse: 0.2200 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6261 - val_custom_mae: 0.5230\n",
            "Epoch 36/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1284 - mae: 0.2060 - mse: 0.1284 - val_loss: 0.2199 - val_mae: 0.2358 - val_mse: 0.2199 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6260 - val_custom_mae: 0.5230\n",
            "Epoch 37/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1284 - mae: 0.2060 - mse: 0.1284 - val_loss: 0.2200 - val_mae: 0.2358 - val_mse: 0.2200 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6262 - val_custom_mae: 0.5230\n",
            "Epoch 38/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1284 - mae: 0.2060 - mse: 0.1284 - val_loss: 0.2203 - val_mae: 0.2360 - val_mse: 0.2203 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6269 - val_custom_mae: 0.5235\n",
            "Epoch 39/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1284 - mae: 0.2060 - mse: 0.1284 - val_loss: 0.2203 - val_mae: 0.2361 - val_mse: 0.2203 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6271 - val_custom_mae: 0.5236\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1284 - mae: 0.2060 - mse: 0.1284 - val_loss: 0.2200 - val_mae: 0.2358 - val_mse: 0.2200 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6261 - val_custom_mae: 0.5231\n",
            "Epoch 41/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2060 - mse: 0.1283 - val_loss: 0.2199 - val_mae: 0.2357 - val_mse: 0.2199 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6260 - val_custom_mae: 0.5230\n",
            "Epoch 42/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2199 - val_mae: 0.2357 - val_mse: 0.2199 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6260 - val_custom_mae: 0.5230\n",
            "Epoch 43/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2358 - val_mse: 0.2200 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6262 - val_custom_mae: 0.5232\n",
            "Epoch 44/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2199 - val_mae: 0.2357 - val_mse: 0.2199 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6261 - val_custom_mae: 0.5230\n",
            "Epoch 45/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6261 - val_custom_mae: 0.5230\n",
            "Epoch 46/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6263 - val_custom_mae: 0.5231\n",
            "Epoch 47/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2358 - val_mse: 0.2201 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2199 - val_mae: 0.2357 - val_mse: 0.2199 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6261 - val_custom_mae: 0.5230\n",
            "Epoch 49/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6262 - val_custom_mae: 0.5230\n",
            "Epoch 50/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1282 - mae: 0.2059 - mse: 0.1282 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6262 - val_custom_mae: 0.5231\n",
            "Epoch 51/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6262 - val_custom_mae: 0.5231\n",
            "Epoch 52/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6263 - val_custom_mae: 0.5231\n",
            "Epoch 53/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6263 - val_custom_mae: 0.5231\n",
            "Epoch 54/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6263 - val_custom_mae: 0.5232\n",
            "Epoch 55/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6263 - val_custom_mae: 0.5231\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2200 - val_mae: 0.2357 - val_mse: 0.2200 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 57/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 58/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 59/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 60/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 61/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 62/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 63/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 65/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1282 - mae: 0.2059 - mse: 0.1282 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 66/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1282 - mae: 0.2059 - mse: 0.1282 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 67/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 68/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 69/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1282 - mae: 0.2059 - mse: 0.1282 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 70/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 71/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 73/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 74/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 75/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 76/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 77/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 78/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 79/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1282 - mae: 0.2058 - mse: 0.1282 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 81/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 82/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 83/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 84/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 85/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1282 - mae: 0.2059 - mse: 0.1282 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 86/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 87/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 89/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 90/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1282 - mae: 0.2059 - mse: 0.1282 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 91/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1282 - mae: 0.2058 - mse: 0.1282 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 92/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 93/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 94/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1282 - mae: 0.2059 - mse: 0.1282 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 95/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 97/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 98/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 99/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Epoch 100/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1283 - mae: 0.2059 - mse: 0.1283 - val_loss: 0.2201 - val_mae: 0.2357 - val_mse: 0.2201 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6264 - val_custom_mae: 0.5232\n",
            "Running experiment: horizon=336, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_11', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1038/1038 - 13s - 12ms/step - loss: 0.3708 - mae: 0.4115 - mse: 0.3708 - val_loss: 0.2445 - val_mae: 0.2761 - val_mse: 0.2445 - learning_rate: 0.0010 - val_custom_mse: 0.6581 - val_custom_mae: 0.5451\n",
            "Epoch 2/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1469 - mae: 0.2366 - mse: 0.1469 - val_loss: 0.2314 - val_mae: 0.2521 - val_mse: 0.2314 - learning_rate: 0.0010 - val_custom_mse: 0.6445 - val_custom_mae: 0.5345\n",
            "Epoch 3/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1374 - mae: 0.2206 - mse: 0.1374 - val_loss: 0.2295 - val_mae: 0.2497 - val_mse: 0.2295 - learning_rate: 0.0010 - val_custom_mse: 0.6402 - val_custom_mae: 0.5315\n",
            "Epoch 4/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1368 - mae: 0.2194 - mse: 0.1368 - val_loss: 0.2298 - val_mae: 0.2494 - val_mse: 0.2298 - learning_rate: 0.0010 - val_custom_mse: 0.6415 - val_custom_mae: 0.5319\n",
            "Epoch 5/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1366 - mae: 0.2192 - mse: 0.1366 - val_loss: 0.2296 - val_mae: 0.2493 - val_mse: 0.2296 - learning_rate: 0.0010 - val_custom_mse: 0.6410 - val_custom_mae: 0.5317\n",
            "Epoch 6/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1363 - mae: 0.2190 - mse: 0.1363 - val_loss: 0.2296 - val_mae: 0.2493 - val_mse: 0.2296 - learning_rate: 0.0010 - val_custom_mse: 0.6412 - val_custom_mae: 0.5323\n",
            "Epoch 7/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1362 - mae: 0.2190 - mse: 0.1362 - val_loss: 0.2292 - val_mae: 0.2488 - val_mse: 0.2292 - learning_rate: 0.0010 - val_custom_mse: 0.6401 - val_custom_mae: 0.5310\n",
            "Epoch 8/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1361 - mae: 0.2189 - mse: 0.1361 - val_loss: 0.2288 - val_mae: 0.2487 - val_mse: 0.2288 - learning_rate: 0.0010 - val_custom_mse: 0.6386 - val_custom_mae: 0.5306\n",
            "Epoch 9/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1361 - mae: 0.2189 - mse: 0.1361 - val_loss: 0.2282 - val_mae: 0.2483 - val_mse: 0.2282 - learning_rate: 0.0010 - val_custom_mse: 0.6371 - val_custom_mae: 0.5298\n",
            "Epoch 10/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1361 - mae: 0.2189 - mse: 0.1361 - val_loss: 0.2280 - val_mae: 0.2483 - val_mse: 0.2280 - learning_rate: 0.0010 - val_custom_mse: 0.6365 - val_custom_mae: 0.5298\n",
            "Epoch 11/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1360 - mae: 0.2189 - mse: 0.1360 - val_loss: 0.2282 - val_mae: 0.2483 - val_mse: 0.2282 - learning_rate: 0.0010 - val_custom_mse: 0.6369 - val_custom_mae: 0.5297\n",
            "Epoch 12/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1361 - mae: 0.2189 - mse: 0.1361 - val_loss: 0.2291 - val_mae: 0.2490 - val_mse: 0.2291 - learning_rate: 0.0010 - val_custom_mse: 0.6398 - val_custom_mae: 0.5313\n",
            "Epoch 13/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1360 - mae: 0.2189 - mse: 0.1360 - val_loss: 0.2270 - val_mae: 0.2478 - val_mse: 0.2270 - learning_rate: 0.0010 - val_custom_mse: 0.6334 - val_custom_mae: 0.5280\n",
            "Epoch 14/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1359 - mae: 0.2189 - mse: 0.1359 - val_loss: 0.2274 - val_mae: 0.2479 - val_mse: 0.2274 - learning_rate: 0.0010 - val_custom_mse: 0.6346 - val_custom_mae: 0.5286\n",
            "Epoch 15/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1360 - mae: 0.2189 - mse: 0.1360 - val_loss: 0.2275 - val_mae: 0.2480 - val_mse: 0.2275 - learning_rate: 0.0010 - val_custom_mse: 0.6352 - val_custom_mae: 0.5288\n",
            "Epoch 16/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1360 - mae: 0.2189 - mse: 0.1360 - val_loss: 0.2277 - val_mae: 0.2482 - val_mse: 0.2277 - learning_rate: 0.0010 - val_custom_mse: 0.6356 - val_custom_mae: 0.5290\n",
            "Epoch 17/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1360 - mae: 0.2189 - mse: 0.1360 - val_loss: 0.2282 - val_mae: 0.2486 - val_mse: 0.2282 - learning_rate: 0.0010 - val_custom_mse: 0.6367 - val_custom_mae: 0.5299\n",
            "Epoch 18/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1360 - mae: 0.2189 - mse: 0.1360 - val_loss: 0.2277 - val_mae: 0.2483 - val_mse: 0.2277 - learning_rate: 0.0010 - val_custom_mse: 0.6354 - val_custom_mae: 0.5294\n",
            "Epoch 19/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1358 - mae: 0.2189 - mse: 0.1358 - val_loss: 0.2278 - val_mae: 0.2483 - val_mse: 0.2278 - learning_rate: 0.0010 - val_custom_mse: 0.6354 - val_custom_mae: 0.5290\n",
            "Epoch 20/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1359 - mae: 0.2189 - mse: 0.1359 - val_loss: 0.2268 - val_mae: 0.2477 - val_mse: 0.2268 - learning_rate: 0.0010 - val_custom_mse: 0.6328 - val_custom_mae: 0.5276\n",
            "Epoch 21/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1359 - mae: 0.2189 - mse: 0.1359 - val_loss: 0.2276 - val_mae: 0.2481 - val_mse: 0.2276 - learning_rate: 0.0010 - val_custom_mse: 0.6351 - val_custom_mae: 0.5289\n",
            "Epoch 22/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1359 - mae: 0.2189 - mse: 0.1359 - val_loss: 0.2279 - val_mae: 0.2482 - val_mse: 0.2279 - learning_rate: 0.0010 - val_custom_mse: 0.6360 - val_custom_mae: 0.5292\n",
            "Epoch 23/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1359 - mae: 0.2189 - mse: 0.1359 - val_loss: 0.2276 - val_mae: 0.2484 - val_mse: 0.2276 - learning_rate: 0.0010 - val_custom_mse: 0.6349 - val_custom_mae: 0.5291\n",
            "Epoch 24/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1359 - mae: 0.2189 - mse: 0.1359 - val_loss: 0.2278 - val_mae: 0.2483 - val_mse: 0.2278 - learning_rate: 0.0010 - val_custom_mse: 0.6356 - val_custom_mae: 0.5292\n",
            "Epoch 25/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1359 - mae: 0.2189 - mse: 0.1359 - val_loss: 0.2271 - val_mae: 0.2479 - val_mse: 0.2271 - learning_rate: 0.0010 - val_custom_mse: 0.6333 - val_custom_mae: 0.5279\n",
            "Epoch 26/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1359 - mae: 0.2189 - mse: 0.1359 - val_loss: 0.2276 - val_mae: 0.2481 - val_mse: 0.2276 - learning_rate: 0.0010 - val_custom_mse: 0.6350 - val_custom_mae: 0.5287\n",
            "Epoch 27/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1359 - mae: 0.2189 - mse: 0.1359 - val_loss: 0.2288 - val_mae: 0.2488 - val_mse: 0.2288 - learning_rate: 0.0010 - val_custom_mse: 0.6387 - val_custom_mae: 0.5309\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1360 - mae: 0.2190 - mse: 0.1360 - val_loss: 0.2291 - val_mae: 0.2490 - val_mse: 0.2291 - learning_rate: 0.0010 - val_custom_mse: 0.6394 - val_custom_mae: 0.5311\n",
            "Epoch 29/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1355 - mae: 0.2185 - mse: 0.1355 - val_loss: 0.2260 - val_mae: 0.2465 - val_mse: 0.2260 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6311 - val_custom_mae: 0.5262\n",
            "Epoch 30/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1354 - mae: 0.2184 - mse: 0.1354 - val_loss: 0.2263 - val_mae: 0.2467 - val_mse: 0.2263 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6317 - val_custom_mae: 0.5267\n",
            "Epoch 31/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1354 - mae: 0.2184 - mse: 0.1354 - val_loss: 0.2261 - val_mae: 0.2465 - val_mse: 0.2261 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6311 - val_custom_mae: 0.5263\n",
            "Epoch 32/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1354 - mae: 0.2184 - mse: 0.1354 - val_loss: 0.2261 - val_mae: 0.2465 - val_mse: 0.2261 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6312 - val_custom_mae: 0.5264\n",
            "Epoch 33/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1354 - mae: 0.2184 - mse: 0.1354 - val_loss: 0.2261 - val_mae: 0.2465 - val_mse: 0.2261 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6312 - val_custom_mae: 0.5264\n",
            "Epoch 34/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1354 - mae: 0.2185 - mse: 0.1354 - val_loss: 0.2263 - val_mae: 0.2467 - val_mse: 0.2263 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6319 - val_custom_mae: 0.5267\n",
            "Epoch 35/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1354 - mae: 0.2184 - mse: 0.1354 - val_loss: 0.2262 - val_mae: 0.2467 - val_mse: 0.2262 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6315 - val_custom_mae: 0.5265\n",
            "Epoch 36/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2184 - mse: 0.1353 - val_loss: 0.2260 - val_mae: 0.2465 - val_mse: 0.2260 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6308 - val_custom_mae: 0.5261\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2184 - mse: 0.1353 - val_loss: 0.2260 - val_mae: 0.2465 - val_mse: 0.2260 - learning_rate: 2.0000e-04 - val_custom_mse: 0.6308 - val_custom_mae: 0.5261\n",
            "Epoch 38/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2462 - val_mse: 0.2256 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6298 - val_custom_mae: 0.5255\n",
            "Epoch 39/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2462 - val_mse: 0.2256 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6298 - val_custom_mae: 0.5254\n",
            "Epoch 40/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2257 - val_mae: 0.2462 - val_mse: 0.2257 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6299 - val_custom_mae: 0.5255\n",
            "Epoch 41/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6298 - val_custom_mae: 0.5254\n",
            "Epoch 42/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2462 - val_mse: 0.2256 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6298 - val_custom_mae: 0.5254\n",
            "Epoch 43/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2462 - val_mse: 0.2256 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6297 - val_custom_mae: 0.5254\n",
            "Epoch 44/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2462 - val_mse: 0.2256 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6298 - val_custom_mae: 0.5254\n",
            "Epoch 45/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6297 - val_custom_mae: 0.5254\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 4.0000e-05 - val_custom_mse: 0.6297 - val_custom_mae: 0.5253\n",
            "Epoch 47/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6297 - val_custom_mae: 0.5253\n",
            "Epoch 48/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2182 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6298 - val_custom_mae: 0.5254\n",
            "Epoch 49/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6297 - val_custom_mae: 0.5253\n",
            "Epoch 50/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6297 - val_custom_mae: 0.5253\n",
            "Epoch 51/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6297 - val_custom_mae: 0.5253\n",
            "Epoch 52/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6298 - val_custom_mae: 0.5254\n",
            "Epoch 53/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 8.0000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 55/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 56/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 57/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2182 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 58/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 59/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 60/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 61/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.6000e-06 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 63/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2182 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 64/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 65/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 66/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1351 - mae: 0.2182 - mse: 0.1351 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 67/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 68/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 69/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 3.2000e-07 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 71/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 72/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 73/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 74/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1351 - mae: 0.2182 - mse: 0.1351 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 75/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2182 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 76/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 77/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 6.4000e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 79/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 80/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 81/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 82/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 83/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 84/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2182 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 85/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 1.2800e-08 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 87/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 88/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 89/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 90/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 91/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1353 - mae: 0.2183 - mse: 0.1353 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 92/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 93/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 2.5600e-09 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 95/100\n",
            "1038/1038 - 8s - 8ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 96/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2182 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 97/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2182 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 98/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2183 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 99/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2182 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Epoch 100/100\n",
            "1038/1038 - 8s - 7ms/step - loss: 0.1352 - mae: 0.2182 - mse: 0.1352 - val_loss: 0.2256 - val_mae: 0.2461 - val_mse: 0.2256 - learning_rate: 5.1200e-10 - val_custom_mse: 0.6299 - val_custom_mae: 0.5254\n",
            "Running experiment: horizon=720, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1026/1026 - 11s - 11ms/step - loss: 0.4179 - mae: 0.4429 - mse: 0.4179 - val_loss: 0.6760 - val_mae: 0.5018 - val_mse: 0.6760 - learning_rate: 0.0010 - val_custom_mse: 0.9432 - val_custom_mae: 0.6507\n",
            "Epoch 2/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2797 - mae: 0.3304 - mse: 0.2797 - val_loss: 0.6543 - val_mae: 0.4699 - val_mse: 0.6543 - learning_rate: 0.0010 - val_custom_mse: 0.9254 - val_custom_mae: 0.6356\n",
            "Epoch 3/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2724 - mae: 0.3162 - mse: 0.2724 - val_loss: 0.6509 - val_mae: 0.4621 - val_mse: 0.6509 - learning_rate: 0.0010 - val_custom_mse: 0.9231 - val_custom_mae: 0.6344\n",
            "Epoch 4/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2704 - mae: 0.3114 - mse: 0.2704 - val_loss: 0.6509 - val_mae: 0.4577 - val_mse: 0.6509 - learning_rate: 0.0010 - val_custom_mse: 0.9242 - val_custom_mae: 0.6339\n",
            "Epoch 5/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2692 - mae: 0.3088 - mse: 0.2692 - val_loss: 0.6538 - val_mae: 0.4561 - val_mse: 0.6538 - learning_rate: 0.0010 - val_custom_mse: 0.9288 - val_custom_mae: 0.6348\n",
            "Epoch 6/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2682 - mae: 0.3072 - mse: 0.2682 - val_loss: 0.6564 - val_mae: 0.4551 - val_mse: 0.6564 - learning_rate: 0.0010 - val_custom_mse: 0.9329 - val_custom_mae: 0.6356\n",
            "Epoch 7/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2674 - mae: 0.3060 - mse: 0.2674 - val_loss: 0.6558 - val_mae: 0.4533 - val_mse: 0.6558 - learning_rate: 0.0010 - val_custom_mse: 0.9321 - val_custom_mae: 0.6343\n",
            "Epoch 8/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2668 - mae: 0.3052 - mse: 0.2668 - val_loss: 0.6570 - val_mae: 0.4529 - val_mse: 0.6570 - learning_rate: 0.0010 - val_custom_mse: 0.9339 - val_custom_mae: 0.6347\n",
            "Epoch 9/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2664 - mae: 0.3046 - mse: 0.2664 - val_loss: 0.6566 - val_mae: 0.4527 - val_mse: 0.6566 - learning_rate: 0.0010 - val_custom_mse: 0.9335 - val_custom_mae: 0.6351\n",
            "Epoch 10/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2660 - mae: 0.3043 - mse: 0.2660 - val_loss: 0.6600 - val_mae: 0.4537 - val_mse: 0.6600 - learning_rate: 0.0010 - val_custom_mse: 0.9383 - val_custom_mae: 0.6365\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2658 - mae: 0.3041 - mse: 0.2658 - val_loss: 0.6560 - val_mae: 0.4521 - val_mse: 0.6560 - learning_rate: 0.0010 - val_custom_mse: 0.9326 - val_custom_mae: 0.6346\n",
            "Epoch 12/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2642 - mae: 0.3027 - mse: 0.2642 - val_loss: 0.6519 - val_mae: 0.4500 - val_mse: 0.6519 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9268 - val_custom_mae: 0.6325\n",
            "Epoch 13/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2641 - mae: 0.3025 - mse: 0.2641 - val_loss: 0.6524 - val_mae: 0.4501 - val_mse: 0.6524 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9275 - val_custom_mae: 0.6328\n",
            "Epoch 14/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2640 - mae: 0.3024 - mse: 0.2640 - val_loss: 0.6525 - val_mae: 0.4501 - val_mse: 0.6525 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9278 - val_custom_mae: 0.6329\n",
            "Epoch 15/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2640 - mae: 0.3023 - mse: 0.2640 - val_loss: 0.6528 - val_mae: 0.4501 - val_mse: 0.6528 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9281 - val_custom_mae: 0.6328\n",
            "Epoch 16/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2639 - mae: 0.3023 - mse: 0.2639 - val_loss: 0.6527 - val_mae: 0.4499 - val_mse: 0.6527 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9281 - val_custom_mae: 0.6327\n",
            "Epoch 17/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2639 - mae: 0.3023 - mse: 0.2639 - val_loss: 0.6534 - val_mae: 0.4502 - val_mse: 0.6534 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9291 - val_custom_mae: 0.6331\n",
            "Epoch 18/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2638 - mae: 0.3022 - mse: 0.2638 - val_loss: 0.6536 - val_mae: 0.4504 - val_mse: 0.6536 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9293 - val_custom_mae: 0.6334\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2638 - mae: 0.3022 - mse: 0.2638 - val_loss: 0.6535 - val_mae: 0.4502 - val_mse: 0.6535 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9292 - val_custom_mae: 0.6331\n",
            "Epoch 20/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2635 - mae: 0.3020 - mse: 0.2635 - val_loss: 0.6515 - val_mae: 0.4494 - val_mse: 0.6515 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9263 - val_custom_mae: 0.6322\n",
            "Epoch 21/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2634 - mae: 0.3019 - mse: 0.2634 - val_loss: 0.6513 - val_mae: 0.4493 - val_mse: 0.6513 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9260 - val_custom_mae: 0.6322\n",
            "Epoch 22/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2634 - mae: 0.3019 - mse: 0.2634 - val_loss: 0.6514 - val_mae: 0.4494 - val_mse: 0.6514 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9261 - val_custom_mae: 0.6322\n",
            "Epoch 23/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2634 - mae: 0.3019 - mse: 0.2634 - val_loss: 0.6513 - val_mae: 0.4493 - val_mse: 0.6513 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9261 - val_custom_mae: 0.6322\n",
            "Epoch 24/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2634 - mae: 0.3019 - mse: 0.2634 - val_loss: 0.6512 - val_mae: 0.4493 - val_mse: 0.6512 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9260 - val_custom_mae: 0.6322\n",
            "Epoch 25/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2634 - mae: 0.3019 - mse: 0.2634 - val_loss: 0.6514 - val_mae: 0.4493 - val_mse: 0.6514 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9262 - val_custom_mae: 0.6322\n",
            "Epoch 26/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2634 - mae: 0.3019 - mse: 0.2634 - val_loss: 0.6514 - val_mae: 0.4493 - val_mse: 0.6514 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9261 - val_custom_mae: 0.6322\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2633 - mae: 0.3018 - mse: 0.2633 - val_loss: 0.6514 - val_mae: 0.4493 - val_mse: 0.6514 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9262 - val_custom_mae: 0.6322\n",
            "Epoch 28/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2633 - mae: 0.3018 - mse: 0.2633 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6322\n",
            "Epoch 29/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2633 - mae: 0.3018 - mse: 0.2633 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 30/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2633 - mae: 0.3018 - mse: 0.2633 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9260 - val_custom_mae: 0.6321\n",
            "Epoch 31/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 32/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 33/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9260 - val_custom_mae: 0.6321\n",
            "Epoch 34/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 36/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 37/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 38/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 39/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 40/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 41/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 42/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 44/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 45/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 46/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 47/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 48/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 49/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 50/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 52/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 53/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 54/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 55/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 56/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 57/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 58/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 60/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 61/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 62/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 63/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 64/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 65/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 66/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 68/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 69/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 70/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 71/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 72/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 73/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 74/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 76/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 77/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 78/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 79/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 80/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 81/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 82/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 84/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 85/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 86/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 87/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 88/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 89/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 90/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 92/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 93/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 94/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 95/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 96/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 97/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 98/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Epoch 100/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2632 - mae: 0.3018 - mse: 0.2632 - val_loss: 0.6513 - val_mae: 0.4492 - val_mse: 0.6513 - learning_rate: 4.0960e-12 - val_custom_mse: 0.9261 - val_custom_mae: 0.6321\n",
            "Running experiment: horizon=720, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_13', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1026/1026 - 12s - 12ms/step - loss: 0.4461 - mae: 0.4622 - mse: 0.4461 - val_loss: 0.6722 - val_mae: 0.4997 - val_mse: 0.6722 - learning_rate: 0.0010 - val_custom_mse: 0.9367 - val_custom_mae: 0.6461\n",
            "Epoch 2/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2891 - mae: 0.3457 - mse: 0.2891 - val_loss: 0.6512 - val_mae: 0.4758 - val_mse: 0.6512 - learning_rate: 0.0010 - val_custom_mse: 0.9171 - val_custom_mae: 0.6341\n",
            "Epoch 3/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2800 - mae: 0.3329 - mse: 0.2800 - val_loss: 0.6450 - val_mae: 0.4698 - val_mse: 0.6450 - learning_rate: 0.0010 - val_custom_mse: 0.9099 - val_custom_mae: 0.6308\n",
            "Epoch 4/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2783 - mae: 0.3302 - mse: 0.2783 - val_loss: 0.6455 - val_mae: 0.4686 - val_mse: 0.6455 - learning_rate: 0.0010 - val_custom_mse: 0.9113 - val_custom_mae: 0.6308\n",
            "Epoch 5/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2776 - mae: 0.3293 - mse: 0.2776 - val_loss: 0.6465 - val_mae: 0.4679 - val_mse: 0.6465 - learning_rate: 0.0010 - val_custom_mse: 0.9129 - val_custom_mae: 0.6308\n",
            "Epoch 6/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2769 - mae: 0.3288 - mse: 0.2769 - val_loss: 0.6454 - val_mae: 0.4666 - val_mse: 0.6454 - learning_rate: 0.0010 - val_custom_mse: 0.9116 - val_custom_mae: 0.6295\n",
            "Epoch 7/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2765 - mae: 0.3286 - mse: 0.2765 - val_loss: 0.6475 - val_mae: 0.4669 - val_mse: 0.6475 - learning_rate: 0.0010 - val_custom_mse: 0.9146 - val_custom_mae: 0.6302\n",
            "Epoch 8/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2761 - mae: 0.3283 - mse: 0.2761 - val_loss: 0.6489 - val_mae: 0.4670 - val_mse: 0.6489 - learning_rate: 0.0010 - val_custom_mse: 0.9168 - val_custom_mae: 0.6306\n",
            "Epoch 9/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2758 - mae: 0.3281 - mse: 0.2758 - val_loss: 0.6506 - val_mae: 0.4671 - val_mse: 0.6506 - learning_rate: 0.0010 - val_custom_mse: 0.9192 - val_custom_mae: 0.6311\n",
            "Epoch 10/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2755 - mae: 0.3279 - mse: 0.2755 - val_loss: 0.6497 - val_mae: 0.4662 - val_mse: 0.6497 - learning_rate: 0.0010 - val_custom_mse: 0.9180 - val_custom_mae: 0.6301\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2753 - mae: 0.3278 - mse: 0.2753 - val_loss: 0.6525 - val_mae: 0.4672 - val_mse: 0.6525 - learning_rate: 0.0010 - val_custom_mse: 0.9221 - val_custom_mae: 0.6316\n",
            "Epoch 12/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2743 - mae: 0.3270 - mse: 0.2743 - val_loss: 0.6491 - val_mae: 0.4660 - val_mse: 0.6491 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9174 - val_custom_mae: 0.6306\n",
            "Epoch 13/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2741 - mae: 0.3269 - mse: 0.2741 - val_loss: 0.6495 - val_mae: 0.4661 - val_mse: 0.6495 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9180 - val_custom_mae: 0.6307\n",
            "Epoch 14/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2740 - mae: 0.3269 - mse: 0.2740 - val_loss: 0.6498 - val_mae: 0.4662 - val_mse: 0.6498 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9183 - val_custom_mae: 0.6308\n",
            "Epoch 15/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2740 - mae: 0.3268 - mse: 0.2740 - val_loss: 0.6501 - val_mae: 0.4663 - val_mse: 0.6501 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9188 - val_custom_mae: 0.6309\n",
            "Epoch 16/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2739 - mae: 0.3269 - mse: 0.2739 - val_loss: 0.6500 - val_mae: 0.4661 - val_mse: 0.6500 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9187 - val_custom_mae: 0.6308\n",
            "Epoch 17/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2738 - mae: 0.3268 - mse: 0.2738 - val_loss: 0.6502 - val_mae: 0.4662 - val_mse: 0.6502 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9189 - val_custom_mae: 0.6309\n",
            "Epoch 18/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2738 - mae: 0.3268 - mse: 0.2738 - val_loss: 0.6504 - val_mae: 0.4662 - val_mse: 0.6504 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9193 - val_custom_mae: 0.6309\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2739 - mae: 0.3269 - mse: 0.2739 - val_loss: 0.6504 - val_mae: 0.4661 - val_mse: 0.6504 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9193 - val_custom_mae: 0.6308\n",
            "Epoch 20/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2737 - mae: 0.3267 - mse: 0.2737 - val_loss: 0.6505 - val_mae: 0.4661 - val_mse: 0.6505 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9195 - val_custom_mae: 0.6309\n",
            "Epoch 21/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2736 - mae: 0.3267 - mse: 0.2736 - val_loss: 0.6505 - val_mae: 0.4661 - val_mse: 0.6505 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9194 - val_custom_mae: 0.6308\n",
            "Epoch 22/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2736 - mae: 0.3267 - mse: 0.2736 - val_loss: 0.6506 - val_mae: 0.4661 - val_mse: 0.6506 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9195 - val_custom_mae: 0.6309\n",
            "Epoch 23/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2736 - mae: 0.3267 - mse: 0.2736 - val_loss: 0.6506 - val_mae: 0.4661 - val_mse: 0.6506 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9195 - val_custom_mae: 0.6309\n",
            "Epoch 24/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6507 - val_mae: 0.4661 - val_mse: 0.6507 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9197 - val_custom_mae: 0.6310\n",
            "Epoch 25/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4661 - val_mse: 0.6506 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9195 - val_custom_mae: 0.6309\n",
            "Epoch 26/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6505 - val_mae: 0.4660 - val_mse: 0.6505 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9194 - val_custom_mae: 0.6308\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2736 - mae: 0.3267 - mse: 0.2736 - val_loss: 0.6506 - val_mae: 0.4661 - val_mse: 0.6506 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9196 - val_custom_mae: 0.6309\n",
            "Epoch 28/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 29/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6309\n",
            "Epoch 30/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 31/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9195 - val_custom_mae: 0.6308\n",
            "Epoch 32/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9195 - val_custom_mae: 0.6308\n",
            "Epoch 33/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9195 - val_custom_mae: 0.6308\n",
            "Epoch 34/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9195 - val_custom_mae: 0.6308\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 36/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2733 - mae: 0.3265 - mse: 0.2733 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 37/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 38/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 39/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 40/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 41/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 42/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 44/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 45/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 46/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 47/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 48/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 49/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 50/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 52/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 53/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 54/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 55/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 56/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 57/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 58/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 60/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 61/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 62/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 63/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 64/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 65/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 66/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 68/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2733 - mae: 0.3266 - mse: 0.2733 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 69/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 70/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 71/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 72/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 73/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 74/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 76/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 77/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 78/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 79/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 80/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 81/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 82/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 84/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 85/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 86/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 87/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 88/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 89/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 90/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 92/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 93/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 94/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 95/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 96/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 97/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2734 - mae: 0.3266 - mse: 0.2734 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 98/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2733 - mae: 0.3265 - mse: 0.2733 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Epoch 100/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2735 - mae: 0.3266 - mse: 0.2735 - val_loss: 0.6506 - val_mae: 0.4660 - val_mse: 0.6506 - learning_rate: 4.0960e-12 - val_custom_mse: 0.9196 - val_custom_mae: 0.6308\n",
            "Running experiment: horizon=720, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1026/1026 - 12s - 12ms/step - loss: 0.4786 - mae: 0.4813 - mse: 0.4786 - val_loss: 0.6594 - val_mae: 0.4963 - val_mse: 0.6594 - learning_rate: 0.0010 - val_custom_mse: 0.9188 - val_custom_mae: 0.6416\n",
            "Epoch 2/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2940 - mae: 0.3528 - mse: 0.2940 - val_loss: 0.6455 - val_mae: 0.4783 - val_mse: 0.6455 - learning_rate: 0.0010 - val_custom_mse: 0.9066 - val_custom_mae: 0.6332\n",
            "Epoch 3/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2842 - mae: 0.3403 - mse: 0.2842 - val_loss: 0.6432 - val_mae: 0.4755 - val_mse: 0.6432 - learning_rate: 0.0010 - val_custom_mse: 0.9044 - val_custom_mae: 0.6320\n",
            "Epoch 4/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2830 - mae: 0.3386 - mse: 0.2830 - val_loss: 0.6421 - val_mae: 0.4739 - val_mse: 0.6421 - learning_rate: 0.0010 - val_custom_mse: 0.9030 - val_custom_mae: 0.6306\n",
            "Epoch 5/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2825 - mae: 0.3382 - mse: 0.2825 - val_loss: 0.6435 - val_mae: 0.4739 - val_mse: 0.6435 - learning_rate: 0.0010 - val_custom_mse: 0.9051 - val_custom_mae: 0.6309\n",
            "Epoch 6/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2821 - mae: 0.3379 - mse: 0.2821 - val_loss: 0.6445 - val_mae: 0.4741 - val_mse: 0.6445 - learning_rate: 0.0010 - val_custom_mse: 0.9065 - val_custom_mae: 0.6312\n",
            "Epoch 7/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2816 - mae: 0.3378 - mse: 0.2816 - val_loss: 0.6459 - val_mae: 0.4737 - val_mse: 0.6459 - learning_rate: 0.0010 - val_custom_mse: 0.9086 - val_custom_mae: 0.6308\n",
            "Epoch 8/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2814 - mae: 0.3377 - mse: 0.2814 - val_loss: 0.6457 - val_mae: 0.4733 - val_mse: 0.6457 - learning_rate: 0.0010 - val_custom_mse: 0.9084 - val_custom_mae: 0.6304\n",
            "Epoch 9/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2812 - mae: 0.3376 - mse: 0.2812 - val_loss: 0.6464 - val_mae: 0.4731 - val_mse: 0.6464 - learning_rate: 0.0010 - val_custom_mse: 0.9093 - val_custom_mae: 0.6302\n",
            "Epoch 10/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2810 - mae: 0.3376 - mse: 0.2810 - val_loss: 0.6479 - val_mae: 0.4732 - val_mse: 0.6479 - learning_rate: 0.0010 - val_custom_mse: 0.9115 - val_custom_mae: 0.6303\n",
            "Epoch 11/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2808 - mae: 0.3375 - mse: 0.2808 - val_loss: 0.6475 - val_mae: 0.4729 - val_mse: 0.6475 - learning_rate: 0.0010 - val_custom_mse: 0.9110 - val_custom_mae: 0.6301\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2808 - mae: 0.3375 - mse: 0.2808 - val_loss: 0.6482 - val_mae: 0.4728 - val_mse: 0.6482 - learning_rate: 0.0010 - val_custom_mse: 0.9120 - val_custom_mae: 0.6300\n",
            "Epoch 13/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2798 - mae: 0.3368 - mse: 0.2798 - val_loss: 0.6492 - val_mae: 0.4728 - val_mse: 0.6492 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9137 - val_custom_mae: 0.6305\n",
            "Epoch 14/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2797 - mae: 0.3367 - mse: 0.2797 - val_loss: 0.6497 - val_mae: 0.4730 - val_mse: 0.6497 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9143 - val_custom_mae: 0.6308\n",
            "Epoch 15/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2798 - mae: 0.3367 - mse: 0.2798 - val_loss: 0.6498 - val_mae: 0.4728 - val_mse: 0.6498 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9144 - val_custom_mae: 0.6306\n",
            "Epoch 16/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2796 - mae: 0.3367 - mse: 0.2796 - val_loss: 0.6494 - val_mae: 0.4727 - val_mse: 0.6494 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9139 - val_custom_mae: 0.6305\n",
            "Epoch 17/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2796 - mae: 0.3367 - mse: 0.2796 - val_loss: 0.6498 - val_mae: 0.4728 - val_mse: 0.6498 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9145 - val_custom_mae: 0.6305\n",
            "Epoch 18/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2795 - mae: 0.3366 - mse: 0.2795 - val_loss: 0.6503 - val_mae: 0.4730 - val_mse: 0.6503 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9151 - val_custom_mae: 0.6309\n",
            "Epoch 19/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2796 - mae: 0.3367 - mse: 0.2796 - val_loss: 0.6504 - val_mae: 0.4731 - val_mse: 0.6504 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9153 - val_custom_mae: 0.6309\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2795 - mae: 0.3367 - mse: 0.2795 - val_loss: 0.6504 - val_mae: 0.4731 - val_mse: 0.6504 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9153 - val_custom_mae: 0.6309\n",
            "Epoch 21/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2793 - mae: 0.3366 - mse: 0.2793 - val_loss: 0.6500 - val_mae: 0.4727 - val_mse: 0.6500 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9147 - val_custom_mae: 0.6305\n",
            "Epoch 22/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2794 - mae: 0.3366 - mse: 0.2794 - val_loss: 0.6500 - val_mae: 0.4727 - val_mse: 0.6500 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9148 - val_custom_mae: 0.6305\n",
            "Epoch 23/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6499 - val_mae: 0.4726 - val_mse: 0.6499 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9146 - val_custom_mae: 0.6304\n",
            "Epoch 24/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6500 - val_mae: 0.4726 - val_mse: 0.6500 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9147 - val_custom_mae: 0.6304\n",
            "Epoch 25/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6500 - val_mae: 0.4727 - val_mse: 0.6500 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9148 - val_custom_mae: 0.6305\n",
            "Epoch 26/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6501 - val_mae: 0.4727 - val_mse: 0.6501 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9149 - val_custom_mae: 0.6305\n",
            "Epoch 27/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2794 - mae: 0.3366 - mse: 0.2794 - val_loss: 0.6502 - val_mae: 0.4728 - val_mse: 0.6502 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9150 - val_custom_mae: 0.6306\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6501 - val_mae: 0.4727 - val_mse: 0.6501 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9149 - val_custom_mae: 0.6305\n",
            "Epoch 29/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6500 - val_mae: 0.4726 - val_mse: 0.6500 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9148 - val_custom_mae: 0.6304\n",
            "Epoch 30/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6500 - val_mae: 0.4726 - val_mse: 0.6500 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9148 - val_custom_mae: 0.6304\n",
            "Epoch 31/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6499 - val_mae: 0.4726 - val_mse: 0.6499 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9147 - val_custom_mae: 0.6304\n",
            "Epoch 32/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6499 - val_mae: 0.4725 - val_mse: 0.6499 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 33/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 34/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 35/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3364 - mse: 0.2791 - val_loss: 0.6499 - val_mae: 0.4725 - val_mse: 0.6499 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 37/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6499 - val_mae: 0.4725 - val_mse: 0.6499 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 38/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6499 - val_mae: 0.4725 - val_mse: 0.6499 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 39/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6499 - val_mae: 0.4725 - val_mse: 0.6499 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 40/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 41/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 42/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 43/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9146 - val_custom_mae: 0.6303\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 45/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 46/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 47/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 48/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3364 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 49/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 50/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 51/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9145 - val_custom_mae: 0.6303\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 53/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 54/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3364 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 55/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 56/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 57/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 58/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 59/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 60: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 61/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 62/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 63/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 64/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 65/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 66/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 67/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2791 - mae: 0.3364 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 69/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 70/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 71/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 72/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 73/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 74/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 75/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 77/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 78/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 79/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 80/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 81/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 82/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 83/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 84: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 85/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 86/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 87/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 88/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3364 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 89/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3364 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 90/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2791 - mae: 0.3365 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 91/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 93/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 94/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 95/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2793 - mae: 0.3365 - mse: 0.2793 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 96/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 97/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2791 - mae: 0.3364 - mse: 0.2791 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 98/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 99/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2792 - mae: 0.3365 - mse: 0.2792 - val_loss: 0.6498 - val_mae: 0.4725 - val_mse: 0.6498 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9145 - val_custom_mae: 0.6302\n",
            "Running experiment: horizon=720, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_15', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1026/1026 - 13s - 13ms/step - loss: 0.5193 - mae: 0.5017 - mse: 0.5193 - val_loss: 0.6596 - val_mae: 0.4968 - val_mse: 0.6596 - learning_rate: 0.0010 - val_custom_mse: 0.9179 - val_custom_mae: 0.6409\n",
            "Epoch 2/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2983 - mae: 0.3578 - mse: 0.2983 - val_loss: 0.6461 - val_mae: 0.4816 - val_mse: 0.6461 - learning_rate: 0.0010 - val_custom_mse: 0.9052 - val_custom_mae: 0.6341\n",
            "Epoch 3/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2882 - mae: 0.3457 - mse: 0.2882 - val_loss: 0.6422 - val_mae: 0.4787 - val_mse: 0.6422 - learning_rate: 0.0010 - val_custom_mse: 0.9003 - val_custom_mae: 0.6315\n",
            "Epoch 4/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2872 - mae: 0.3445 - mse: 0.2872 - val_loss: 0.6432 - val_mae: 0.4793 - val_mse: 0.6432 - learning_rate: 0.0010 - val_custom_mse: 0.9019 - val_custom_mae: 0.6327\n",
            "Epoch 5/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2867 - mae: 0.3442 - mse: 0.2867 - val_loss: 0.6432 - val_mae: 0.4786 - val_mse: 0.6432 - learning_rate: 0.0010 - val_custom_mse: 0.9020 - val_custom_mae: 0.6320\n",
            "Epoch 6/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2866 - mae: 0.3441 - mse: 0.2866 - val_loss: 0.6434 - val_mae: 0.4783 - val_mse: 0.6434 - learning_rate: 0.0010 - val_custom_mse: 0.9022 - val_custom_mae: 0.6315\n",
            "Epoch 7/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2863 - mae: 0.3440 - mse: 0.2863 - val_loss: 0.6438 - val_mae: 0.4778 - val_mse: 0.6438 - learning_rate: 0.0010 - val_custom_mse: 0.9027 - val_custom_mae: 0.6307\n",
            "Epoch 8/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2861 - mae: 0.3440 - mse: 0.2861 - val_loss: 0.6442 - val_mae: 0.4776 - val_mse: 0.6442 - learning_rate: 0.0010 - val_custom_mse: 0.9034 - val_custom_mae: 0.6305\n",
            "Epoch 9/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2860 - mae: 0.3440 - mse: 0.2860 - val_loss: 0.6447 - val_mae: 0.4775 - val_mse: 0.6447 - learning_rate: 0.0010 - val_custom_mse: 0.9040 - val_custom_mae: 0.6304\n",
            "Epoch 10/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2859 - mae: 0.3439 - mse: 0.2859 - val_loss: 0.6448 - val_mae: 0.4770 - val_mse: 0.6448 - learning_rate: 0.0010 - val_custom_mse: 0.9042 - val_custom_mae: 0.6299\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2856 - mae: 0.3438 - mse: 0.2856 - val_loss: 0.6463 - val_mae: 0.4778 - val_mse: 0.6463 - learning_rate: 0.0010 - val_custom_mse: 0.9064 - val_custom_mae: 0.6310\n",
            "Epoch 12/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2850 - mae: 0.3434 - mse: 0.2850 - val_loss: 0.6477 - val_mae: 0.4774 - val_mse: 0.6477 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9085 - val_custom_mae: 0.6308\n",
            "Epoch 13/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2848 - mae: 0.3432 - mse: 0.2848 - val_loss: 0.6479 - val_mae: 0.4773 - val_mse: 0.6479 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9087 - val_custom_mae: 0.6306\n",
            "Epoch 14/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2847 - mae: 0.3432 - mse: 0.2847 - val_loss: 0.6482 - val_mae: 0.4774 - val_mse: 0.6482 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9092 - val_custom_mae: 0.6309\n",
            "Epoch 15/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2846 - mae: 0.3432 - mse: 0.2846 - val_loss: 0.6480 - val_mae: 0.4774 - val_mse: 0.6480 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9090 - val_custom_mae: 0.6307\n",
            "Epoch 16/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2848 - mae: 0.3432 - mse: 0.2848 - val_loss: 0.6485 - val_mae: 0.4774 - val_mse: 0.6485 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9096 - val_custom_mae: 0.6308\n",
            "Epoch 17/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2848 - mae: 0.3433 - mse: 0.2848 - val_loss: 0.6486 - val_mae: 0.4774 - val_mse: 0.6486 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9097 - val_custom_mae: 0.6308\n",
            "Epoch 18/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2846 - mae: 0.3432 - mse: 0.2846 - val_loss: 0.6483 - val_mae: 0.4773 - val_mse: 0.6483 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9094 - val_custom_mae: 0.6306\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2845 - mae: 0.3432 - mse: 0.2845 - val_loss: 0.6488 - val_mae: 0.4776 - val_mse: 0.6488 - learning_rate: 2.0000e-04 - val_custom_mse: 0.9101 - val_custom_mae: 0.6310\n",
            "Epoch 20/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2845 - mae: 0.3431 - mse: 0.2845 - val_loss: 0.6482 - val_mae: 0.4770 - val_mse: 0.6482 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9092 - val_custom_mae: 0.6303\n",
            "Epoch 21/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2845 - mae: 0.3431 - mse: 0.2845 - val_loss: 0.6481 - val_mae: 0.4770 - val_mse: 0.6481 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9091 - val_custom_mae: 0.6303\n",
            "Epoch 22/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6480 - val_mae: 0.4769 - val_mse: 0.6480 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9089 - val_custom_mae: 0.6302\n",
            "Epoch 23/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2844 - mae: 0.3431 - mse: 0.2844 - val_loss: 0.6480 - val_mae: 0.4769 - val_mse: 0.6480 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9089 - val_custom_mae: 0.6302\n",
            "Epoch 24/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2844 - mae: 0.3431 - mse: 0.2844 - val_loss: 0.6480 - val_mae: 0.4769 - val_mse: 0.6480 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9090 - val_custom_mae: 0.6302\n",
            "Epoch 25/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6481 - val_mae: 0.4769 - val_mse: 0.6481 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9091 - val_custom_mae: 0.6302\n",
            "Epoch 26/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6480 - val_mae: 0.4769 - val_mse: 0.6480 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9089 - val_custom_mae: 0.6301\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2844 - mae: 0.3431 - mse: 0.2844 - val_loss: 0.6480 - val_mae: 0.4769 - val_mse: 0.6480 - learning_rate: 4.0000e-05 - val_custom_mse: 0.9091 - val_custom_mae: 0.6302\n",
            "Epoch 28/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6480 - val_mae: 0.4769 - val_mse: 0.6480 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9089 - val_custom_mae: 0.6301\n",
            "Epoch 29/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6479 - val_mae: 0.4768 - val_mse: 0.6479 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9088 - val_custom_mae: 0.6300\n",
            "Epoch 30/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6479 - val_mae: 0.4768 - val_mse: 0.6479 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9088 - val_custom_mae: 0.6300\n",
            "Epoch 31/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6479 - val_mae: 0.4768 - val_mse: 0.6479 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9088 - val_custom_mae: 0.6300\n",
            "Epoch 32/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6479 - val_mae: 0.4768 - val_mse: 0.6479 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9088 - val_custom_mae: 0.6300\n",
            "Epoch 33/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2844 - mae: 0.3431 - mse: 0.2844 - val_loss: 0.6479 - val_mae: 0.4768 - val_mse: 0.6479 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9088 - val_custom_mae: 0.6300\n",
            "Epoch 34/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4768 - val_mse: 0.6478 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9088 - val_custom_mae: 0.6300\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4768 - val_mse: 0.6478 - learning_rate: 8.0000e-06 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 36/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2844 - mae: 0.3430 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 37/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4768 - val_mse: 0.6478 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 38/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2844 - mae: 0.3430 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 39/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 40/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 41/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2844 - mae: 0.3431 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4768 - val_mse: 0.6478 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 42/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2841 - mae: 0.3430 - mse: 0.2841 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.6000e-06 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 44/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 45/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 46/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2844 - mae: 0.3430 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 47/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 48/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2844 - mae: 0.3430 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 49/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 50/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3431 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 3.2000e-07 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 52/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 53/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 54/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 55/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 56/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2844 - mae: 0.3430 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 57/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 58/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2844 - mae: 0.3431 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 6.4000e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 60/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 61/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 62/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 63/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 64/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2844 - mae: 0.3431 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 65/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2842 - mae: 0.3429 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 66/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2841 - mae: 0.3429 - mse: 0.2841 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.2800e-08 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 68/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 69/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 70/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 71/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 72/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2844 - mae: 0.3430 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 73/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2844 - mae: 0.3431 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 74/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2844 - mae: 0.3430 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.5600e-09 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 76/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 77/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2844 - mae: 0.3430 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 78/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 79/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 80/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 81/100\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 82/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3431 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 5.1200e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 84/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 85/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 86/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 87/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 88/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 89/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 90/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 1.0240e-10 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 92/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 93/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 94/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 95/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 96/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 97/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 98/100\n",
            "1026/1026 - 7s - 7ms/step - loss: 0.2843 - mae: 0.3430 - mse: 0.2843 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
            "1026/1026 - 8s - 8ms/step - loss: 0.2844 - mae: 0.3431 - mse: 0.2844 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 2.0480e-11 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "Epoch 100/100\n",
            "1026/1026 - 8s - 7ms/step - loss: 0.2842 - mae: 0.3430 - mse: 0.2842 - val_loss: 0.6478 - val_mae: 0.4767 - val_mse: 0.6478 - learning_rate: 4.0960e-12 - val_custom_mse: 0.9087 - val_custom_mae: 0.6300\n",
            "\n",
            " ETTm1 Final Results:\n",
            "\n",
            "MSE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%  DR=20%  DR=30%\n",
            "Horizon                                \n",
            "96       0.2985  0.3016  0.3062  0.3088\n",
            "192      0.3348  0.3331  0.3347  0.3360\n",
            "336      0.3656  0.3625  0.3629  0.3637\n",
            "720      0.4075  0.4056  0.4051  0.4047\n",
            "\n",
            "MAE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%  DR=20%  DR=30%\n",
            "Horizon                                \n",
            "96       0.3454  0.3473  0.3499  0.3515\n",
            "192      0.3659  0.3653  0.3664  0.3676\n",
            "336      0.3848  0.3833  0.3836  0.3840\n",
            "720      0.4113  0.4099  0.4095  0.4093\n",
            "\n",
            "Results saved to: ./flowmixer_results/ETTm1_experiment_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Run the experiments\n",
        "data_name='ETTm1'\n",
        "results = run_experiments(data_name,horizons=[96,192,336,720], dropout_rates=[0.0, 0.1, 0.2, 0.3], seq_len_=1024, revin=2,learning_rate=1e-3, mopt='adamw')\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\n {data_name} Final Results:\")\n",
        "df = pd.DataFrame(results)\n",
        "# Assuming results is a list of dictionaries with horizon, dropout, MSE, and MAE values\n",
        "display_results_tables(results[0])\n",
        "\n",
        "\n",
        "# The results are already saved in CSV format after each experiment\n",
        "print(f\"\\nResults saved to: ./flowmixer_results/{data_name}_experiment_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETTm2"
      ],
      "metadata": {
        "id": "hFsq4RP91fx_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgaAHkLn9hT-",
        "outputId": "16064193-9c99-461e-f32e-72ee65531d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: horizon=96, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_16', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1040/1040 - 13s - 13ms/step - loss: 0.2435 - mae: 0.2858 - mse: 0.2435 - val_loss: 0.1077 - val_mae: 0.2348 - val_mse: 0.1077 - learning_rate: 0.1000 - val_custom_mse: 0.1599 - val_custom_mae: 0.2840\n",
            "Epoch 2/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.1691 - mae: 0.2459 - mse: 0.1691 - val_loss: 0.0774 - val_mae: 0.1971 - val_mse: 0.0774 - learning_rate: 0.1000 - val_custom_mse: 0.1484 - val_custom_mae: 0.2739\n",
            "Epoch 3/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.1070 - mae: 0.2015 - mse: 0.1070 - val_loss: 0.0582 - val_mae: 0.1675 - val_mse: 0.0582 - learning_rate: 0.1000 - val_custom_mse: 0.1394 - val_custom_mae: 0.2652\n",
            "Epoch 4/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0753 - mae: 0.1699 - mse: 0.0753 - val_loss: 0.0489 - val_mae: 0.1502 - val_mse: 0.0489 - learning_rate: 0.1000 - val_custom_mse: 0.1356 - val_custom_mae: 0.2612\n",
            "Epoch 5/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0616 - mae: 0.1506 - mse: 0.0616 - val_loss: 0.0431 - val_mae: 0.1384 - val_mse: 0.0431 - learning_rate: 0.1000 - val_custom_mse: 0.1316 - val_custom_mae: 0.2564\n",
            "Epoch 6/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0540 - mae: 0.1376 - mse: 0.0540 - val_loss: 0.0388 - val_mae: 0.1290 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.1278 - val_custom_mae: 0.2514\n",
            "Epoch 7/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0488 - mae: 0.1280 - mse: 0.0488 - val_loss: 0.0355 - val_mae: 0.1216 - val_mse: 0.0355 - learning_rate: 0.1000 - val_custom_mse: 0.1252 - val_custom_mae: 0.2480\n",
            "Epoch 8/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0448 - mae: 0.1202 - mse: 0.0448 - val_loss: 0.0329 - val_mae: 0.1155 - val_mse: 0.0329 - learning_rate: 0.1000 - val_custom_mse: 0.1239 - val_custom_mae: 0.2463\n",
            "Epoch 9/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0416 - mae: 0.1139 - mse: 0.0416 - val_loss: 0.0305 - val_mae: 0.1101 - val_mse: 0.0305 - learning_rate: 0.1000 - val_custom_mse: 0.1209 - val_custom_mae: 0.2421\n",
            "Epoch 10/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1084 - mse: 0.0390 - val_loss: 0.0286 - val_mae: 0.1053 - val_mse: 0.0286 - learning_rate: 0.1000 - val_custom_mse: 0.1201 - val_custom_mae: 0.2410\n",
            "Epoch 11/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0368 - mae: 0.1036 - mse: 0.0368 - val_loss: 0.0270 - val_mae: 0.1010 - val_mse: 0.0270 - learning_rate: 0.1000 - val_custom_mse: 0.1194 - val_custom_mae: 0.2401\n",
            "Epoch 12/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0350 - mae: 0.0993 - mse: 0.0350 - val_loss: 0.0256 - val_mae: 0.0972 - val_mse: 0.0256 - learning_rate: 0.1000 - val_custom_mse: 0.1189 - val_custom_mae: 0.2394\n",
            "Epoch 13/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0334 - mae: 0.0954 - mse: 0.0334 - val_loss: 0.0242 - val_mae: 0.0937 - val_mse: 0.0242 - learning_rate: 0.1000 - val_custom_mse: 0.1171 - val_custom_mae: 0.2368\n",
            "Epoch 14/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0319 - mae: 0.0919 - mse: 0.0319 - val_loss: 0.0231 - val_mae: 0.0904 - val_mse: 0.0231 - learning_rate: 0.1000 - val_custom_mse: 0.1167 - val_custom_mae: 0.2361\n",
            "Epoch 15/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0307 - mae: 0.0886 - mse: 0.0307 - val_loss: 0.0221 - val_mae: 0.0871 - val_mse: 0.0221 - learning_rate: 0.1000 - val_custom_mse: 0.1168 - val_custom_mae: 0.2362\n",
            "Epoch 16/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0296 - mae: 0.0856 - mse: 0.0296 - val_loss: 0.0211 - val_mae: 0.0842 - val_mse: 0.0211 - learning_rate: 0.1000 - val_custom_mse: 0.1164 - val_custom_mae: 0.2356\n",
            "Epoch 17/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0286 - mae: 0.0827 - mse: 0.0286 - val_loss: 0.0203 - val_mae: 0.0815 - val_mse: 0.0203 - learning_rate: 0.1000 - val_custom_mse: 0.1161 - val_custom_mae: 0.2353\n",
            "Epoch 18/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0277 - mae: 0.0801 - mse: 0.0277 - val_loss: 0.0195 - val_mae: 0.0791 - val_mse: 0.0195 - learning_rate: 0.1000 - val_custom_mse: 0.1157 - val_custom_mae: 0.2346\n",
            "Epoch 19/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0269 - mae: 0.0776 - mse: 0.0269 - val_loss: 0.0188 - val_mae: 0.0766 - val_mse: 0.0188 - learning_rate: 0.1000 - val_custom_mse: 0.1156 - val_custom_mae: 0.2345\n",
            "Epoch 20/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0262 - mae: 0.0752 - mse: 0.0262 - val_loss: 0.0182 - val_mae: 0.0742 - val_mse: 0.0182 - learning_rate: 0.1000 - val_custom_mse: 0.1157 - val_custom_mae: 0.2346\n",
            "Epoch 21/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0255 - mae: 0.0731 - mse: 0.0255 - val_loss: 0.0177 - val_mae: 0.0720 - val_mse: 0.0177 - learning_rate: 0.1000 - val_custom_mse: 0.1160 - val_custom_mae: 0.2351\n",
            "Epoch 22/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0249 - mae: 0.0710 - mse: 0.0249 - val_loss: 0.0171 - val_mae: 0.0700 - val_mse: 0.0171 - learning_rate: 0.1000 - val_custom_mse: 0.1161 - val_custom_mae: 0.2352\n",
            "Epoch 23/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0243 - mae: 0.0691 - mse: 0.0243 - val_loss: 0.0166 - val_mae: 0.0681 - val_mse: 0.0166 - learning_rate: 0.1000 - val_custom_mse: 0.1152 - val_custom_mae: 0.2339\n",
            "Epoch 24/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0238 - mae: 0.0671 - mse: 0.0238 - val_loss: 0.0162 - val_mae: 0.0662 - val_mse: 0.0162 - learning_rate: 0.1000 - val_custom_mse: 0.1156 - val_custom_mae: 0.2345\n",
            "Epoch 25/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0233 - mae: 0.0654 - mse: 0.0233 - val_loss: 0.0157 - val_mae: 0.0648 - val_mse: 0.0157 - learning_rate: 0.1000 - val_custom_mse: 0.1147 - val_custom_mae: 0.2332\n",
            "Epoch 26/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0229 - mae: 0.0637 - mse: 0.0229 - val_loss: 0.0153 - val_mae: 0.0631 - val_mse: 0.0153 - learning_rate: 0.1000 - val_custom_mse: 0.1149 - val_custom_mae: 0.2333\n",
            "Epoch 27/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0225 - mae: 0.0622 - mse: 0.0225 - val_loss: 0.0149 - val_mae: 0.0613 - val_mse: 0.0149 - learning_rate: 0.1000 - val_custom_mse: 0.1150 - val_custom_mae: 0.2335\n",
            "Epoch 28/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0222 - mae: 0.0607 - mse: 0.0222 - val_loss: 0.0146 - val_mae: 0.0597 - val_mse: 0.0146 - learning_rate: 0.1000 - val_custom_mse: 0.1149 - val_custom_mae: 0.2334\n",
            "Epoch 29/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0218 - mae: 0.0593 - mse: 0.0218 - val_loss: 0.0143 - val_mae: 0.0585 - val_mse: 0.0143 - learning_rate: 0.1000 - val_custom_mse: 0.1147 - val_custom_mae: 0.2331\n",
            "Epoch 30/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0215 - mae: 0.0580 - mse: 0.0215 - val_loss: 0.0140 - val_mae: 0.0570 - val_mse: 0.0140 - learning_rate: 0.1000 - val_custom_mse: 0.1150 - val_custom_mae: 0.2337\n",
            "Epoch 31/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0212 - mae: 0.0568 - mse: 0.0212 - val_loss: 0.0137 - val_mae: 0.0559 - val_mse: 0.0137 - learning_rate: 0.1000 - val_custom_mse: 0.1145 - val_custom_mae: 0.2328\n",
            "Epoch 32/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0209 - mae: 0.0556 - mse: 0.0209 - val_loss: 0.0135 - val_mae: 0.0545 - val_mse: 0.0135 - learning_rate: 0.1000 - val_custom_mse: 0.1148 - val_custom_mae: 0.2334\n",
            "Epoch 33/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0207 - mae: 0.0544 - mse: 0.0207 - val_loss: 0.0133 - val_mae: 0.0534 - val_mse: 0.0133 - learning_rate: 0.1000 - val_custom_mse: 0.1149 - val_custom_mae: 0.2335\n",
            "Epoch 34/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0204 - mae: 0.0533 - mse: 0.0204 - val_loss: 0.0130 - val_mae: 0.0522 - val_mse: 0.0130 - learning_rate: 0.1000 - val_custom_mse: 0.1144 - val_custom_mae: 0.2328\n",
            "Epoch 35/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0202 - mae: 0.0523 - mse: 0.0202 - val_loss: 0.0129 - val_mae: 0.0514 - val_mse: 0.0129 - learning_rate: 0.1000 - val_custom_mse: 0.1149 - val_custom_mae: 0.2336\n",
            "Epoch 36/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0200 - mae: 0.0513 - mse: 0.0200 - val_loss: 0.0127 - val_mae: 0.0510 - val_mse: 0.0127 - learning_rate: 0.1000 - val_custom_mse: 0.1152 - val_custom_mae: 0.2340\n",
            "Epoch 37/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0198 - mae: 0.0504 - mse: 0.0198 - val_loss: 0.0124 - val_mae: 0.0492 - val_mse: 0.0124 - learning_rate: 0.1000 - val_custom_mse: 0.1144 - val_custom_mae: 0.2327\n",
            "Epoch 38/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0196 - mae: 0.0495 - mse: 0.0196 - val_loss: 0.0123 - val_mae: 0.0485 - val_mse: 0.0123 - learning_rate: 0.1000 - val_custom_mse: 0.1145 - val_custom_mae: 0.2331\n",
            "Epoch 39/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0195 - mae: 0.0487 - mse: 0.0195 - val_loss: 0.0121 - val_mae: 0.0475 - val_mse: 0.0121 - learning_rate: 0.1000 - val_custom_mse: 0.1145 - val_custom_mae: 0.2329\n",
            "Epoch 40/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0193 - mae: 0.0479 - mse: 0.0193 - val_loss: 0.0119 - val_mae: 0.0467 - val_mse: 0.0119 - learning_rate: 0.1000 - val_custom_mse: 0.1141 - val_custom_mae: 0.2322\n",
            "Epoch 41/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0192 - mae: 0.0472 - mse: 0.0192 - val_loss: 0.0119 - val_mae: 0.0462 - val_mse: 0.0119 - learning_rate: 0.1000 - val_custom_mse: 0.1146 - val_custom_mae: 0.2331\n",
            "Epoch 42/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0190 - mae: 0.0464 - mse: 0.0190 - val_loss: 0.0117 - val_mae: 0.0454 - val_mse: 0.0117 - learning_rate: 0.1000 - val_custom_mse: 0.1146 - val_custom_mae: 0.2331\n",
            "Epoch 43/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0189 - mae: 0.0457 - mse: 0.0189 - val_loss: 0.0115 - val_mae: 0.0444 - val_mse: 0.0115 - learning_rate: 0.1000 - val_custom_mse: 0.1141 - val_custom_mae: 0.2323\n",
            "Epoch 44/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0188 - mae: 0.0451 - mse: 0.0188 - val_loss: 0.0114 - val_mae: 0.0439 - val_mse: 0.0114 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2319\n",
            "Epoch 45/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0186 - mae: 0.0444 - mse: 0.0186 - val_loss: 0.0113 - val_mae: 0.0436 - val_mse: 0.0113 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2316\n",
            "Epoch 46/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0185 - mae: 0.0439 - mse: 0.0185 - val_loss: 0.0112 - val_mae: 0.0425 - val_mse: 0.0112 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2320\n",
            "Epoch 47/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0184 - mae: 0.0433 - mse: 0.0184 - val_loss: 0.0112 - val_mae: 0.0421 - val_mse: 0.0112 - learning_rate: 0.1000 - val_custom_mse: 0.1144 - val_custom_mae: 0.2327\n",
            "Epoch 48/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0183 - mae: 0.0428 - mse: 0.0183 - val_loss: 0.0110 - val_mae: 0.0414 - val_mse: 0.0110 - learning_rate: 0.1000 - val_custom_mse: 0.1140 - val_custom_mae: 0.2322\n",
            "Epoch 49/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0182 - mae: 0.0422 - mse: 0.0182 - val_loss: 0.0109 - val_mae: 0.0408 - val_mse: 0.0109 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2321\n",
            "Epoch 50/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0182 - mae: 0.0418 - mse: 0.0182 - val_loss: 0.0108 - val_mae: 0.0403 - val_mse: 0.0108 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2318\n",
            "Epoch 51/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0181 - mae: 0.0412 - mse: 0.0181 - val_loss: 0.0108 - val_mae: 0.0400 - val_mse: 0.0108 - learning_rate: 0.1000 - val_custom_mse: 0.1143 - val_custom_mae: 0.2326\n",
            "Epoch 52/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0180 - mae: 0.0407 - mse: 0.0180 - val_loss: 0.0107 - val_mae: 0.0394 - val_mse: 0.0107 - learning_rate: 0.1000 - val_custom_mse: 0.1141 - val_custom_mae: 0.2323\n",
            "Epoch 53/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0179 - mae: 0.0404 - mse: 0.0179 - val_loss: 0.0106 - val_mae: 0.0389 - val_mse: 0.0106 - learning_rate: 0.1000 - val_custom_mse: 0.1140 - val_custom_mae: 0.2322\n",
            "Epoch 54/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0178 - mae: 0.0399 - mse: 0.0178 - val_loss: 0.0106 - val_mae: 0.0389 - val_mse: 0.0106 - learning_rate: 0.1000 - val_custom_mse: 0.1142 - val_custom_mae: 0.2326\n",
            "Epoch 55/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0178 - mae: 0.0395 - mse: 0.0178 - val_loss: 0.0105 - val_mae: 0.0379 - val_mse: 0.0105 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2317\n",
            "Epoch 56/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0177 - mae: 0.0391 - mse: 0.0177 - val_loss: 0.0104 - val_mae: 0.0377 - val_mse: 0.0104 - learning_rate: 0.1000 - val_custom_mse: 0.1141 - val_custom_mae: 0.2323\n",
            "Epoch 57/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0177 - mae: 0.0388 - mse: 0.0177 - val_loss: 0.0103 - val_mae: 0.0371 - val_mse: 0.0103 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2317\n",
            "Epoch 58/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0176 - mae: 0.0384 - mse: 0.0176 - val_loss: 0.0103 - val_mae: 0.0367 - val_mse: 0.0103 - learning_rate: 0.1000 - val_custom_mse: 0.1135 - val_custom_mae: 0.2316\n",
            "Epoch 59/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0175 - mae: 0.0380 - mse: 0.0175 - val_loss: 0.0103 - val_mae: 0.0364 - val_mse: 0.0103 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2319\n",
            "Epoch 60/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0175 - mae: 0.0377 - mse: 0.0175 - val_loss: 0.0102 - val_mae: 0.0361 - val_mse: 0.0102 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2319\n",
            "Epoch 61/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0174 - mae: 0.0374 - mse: 0.0174 - val_loss: 0.0102 - val_mae: 0.0357 - val_mse: 0.0102 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2317\n",
            "Epoch 62/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0174 - mae: 0.0371 - mse: 0.0174 - val_loss: 0.0101 - val_mae: 0.0356 - val_mse: 0.0101 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2313\n",
            "Epoch 63/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0173 - mae: 0.0368 - mse: 0.0173 - val_loss: 0.0101 - val_mae: 0.0353 - val_mse: 0.0101 - learning_rate: 0.1000 - val_custom_mse: 0.1140 - val_custom_mae: 0.2321\n",
            "Epoch 64/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0173 - mae: 0.0365 - mse: 0.0173 - val_loss: 0.0100 - val_mae: 0.0349 - val_mse: 0.0100 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2320\n",
            "Epoch 65/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0173 - mae: 0.0362 - mse: 0.0173 - val_loss: 0.0100 - val_mae: 0.0346 - val_mse: 0.0100 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2316\n",
            "Epoch 66/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0172 - mae: 0.0359 - mse: 0.0172 - val_loss: 0.0099 - val_mae: 0.0341 - val_mse: 0.0099 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2316\n",
            "Epoch 67/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0172 - mae: 0.0357 - mse: 0.0172 - val_loss: 0.0099 - val_mae: 0.0339 - val_mse: 0.0099 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2317\n",
            "Epoch 68/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0171 - mae: 0.0354 - mse: 0.0171 - val_loss: 0.0099 - val_mae: 0.0336 - val_mse: 0.0099 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2316\n",
            "Epoch 69/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0171 - mae: 0.0352 - mse: 0.0171 - val_loss: 0.0098 - val_mae: 0.0334 - val_mse: 0.0098 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2316\n",
            "Epoch 70/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0171 - mae: 0.0350 - mse: 0.0171 - val_loss: 0.0098 - val_mae: 0.0332 - val_mse: 0.0098 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2315\n",
            "Epoch 71/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0170 - mae: 0.0348 - mse: 0.0170 - val_loss: 0.0098 - val_mae: 0.0331 - val_mse: 0.0098 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2319\n",
            "Epoch 72/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0170 - mae: 0.0346 - mse: 0.0170 - val_loss: 0.0098 - val_mae: 0.0327 - val_mse: 0.0098 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2315\n",
            "Epoch 73/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0170 - mae: 0.0343 - mse: 0.0170 - val_loss: 0.0097 - val_mae: 0.0325 - val_mse: 0.0097 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2317\n",
            "Epoch 74/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0170 - mae: 0.0340 - mse: 0.0170 - val_loss: 0.0097 - val_mae: 0.0325 - val_mse: 0.0097 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2319\n",
            "Epoch 75/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0169 - mae: 0.0339 - mse: 0.0169 - val_loss: 0.0097 - val_mae: 0.0321 - val_mse: 0.0097 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2318\n",
            "Epoch 76/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0169 - mae: 0.0338 - mse: 0.0169 - val_loss: 0.0098 - val_mae: 0.0334 - val_mse: 0.0098 - learning_rate: 0.1000 - val_custom_mse: 0.1145 - val_custom_mae: 0.2328\n",
            "Epoch 77/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0169 - mae: 0.0335 - mse: 0.0169 - val_loss: 0.0097 - val_mae: 0.0317 - val_mse: 0.0097 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2318\n",
            "Epoch 78/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0169 - mae: 0.0333 - mse: 0.0169 - val_loss: 0.0096 - val_mae: 0.0317 - val_mse: 0.0096 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2320\n",
            "Epoch 79/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0168 - mae: 0.0331 - mse: 0.0168 - val_loss: 0.0096 - val_mae: 0.0314 - val_mse: 0.0096 - learning_rate: 0.1000 - val_custom_mse: 0.1135 - val_custom_mae: 0.2314\n",
            "Epoch 80/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0168 - mae: 0.0331 - mse: 0.0168 - val_loss: 0.0097 - val_mae: 0.0325 - val_mse: 0.0097 - learning_rate: 0.1000 - val_custom_mse: 0.1142 - val_custom_mae: 0.2325\n",
            "Epoch 81/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0168 - mae: 0.0328 - mse: 0.0168 - val_loss: 0.0096 - val_mae: 0.0311 - val_mse: 0.0096 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2318\n",
            "Epoch 82/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0168 - mae: 0.0327 - mse: 0.0168 - val_loss: 0.0096 - val_mae: 0.0308 - val_mse: 0.0096 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2317\n",
            "Epoch 83/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0168 - mae: 0.0325 - mse: 0.0168 - val_loss: 0.0096 - val_mae: 0.0308 - val_mse: 0.0096 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2319\n",
            "Epoch 84/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0167 - mae: 0.0323 - mse: 0.0167 - val_loss: 0.0095 - val_mae: 0.0304 - val_mse: 0.0095 - learning_rate: 0.1000 - val_custom_mse: 0.1135 - val_custom_mae: 0.2315\n",
            "Epoch 85/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0167 - mae: 0.0322 - mse: 0.0167 - val_loss: 0.0095 - val_mae: 0.0303 - val_mse: 0.0095 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2316\n",
            "Epoch 86/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0167 - mae: 0.0321 - mse: 0.0167 - val_loss: 0.0095 - val_mae: 0.0304 - val_mse: 0.0095 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2311\n",
            "Epoch 87/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0167 - mae: 0.0319 - mse: 0.0167 - val_loss: 0.0095 - val_mae: 0.0301 - val_mse: 0.0095 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2317\n",
            "Epoch 88/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0167 - mae: 0.0317 - mse: 0.0167 - val_loss: 0.0094 - val_mae: 0.0300 - val_mse: 0.0094 - learning_rate: 0.1000 - val_custom_mse: 0.1133 - val_custom_mae: 0.2311\n",
            "Epoch 89/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0167 - mae: 0.0317 - mse: 0.0167 - val_loss: 0.0095 - val_mae: 0.0301 - val_mse: 0.0095 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2311\n",
            "Epoch 90/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0315 - mse: 0.0166 - val_loss: 0.0095 - val_mae: 0.0300 - val_mse: 0.0095 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2319\n",
            "Epoch 91/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0314 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0296 - val_mse: 0.0094 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2318\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0313 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0295 - val_mse: 0.0094 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2317\n",
            "Epoch 93/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0309 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0291 - val_mse: 0.0094 - learning_rate: 0.0200 - val_custom_mse: 0.1134 - val_custom_mae: 0.2314\n",
            "Epoch 94/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0308 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0291 - val_mse: 0.0094 - learning_rate: 0.0200 - val_custom_mse: 0.1133 - val_custom_mae: 0.2312\n",
            "Epoch 95/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0308 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0291 - val_mse: 0.0094 - learning_rate: 0.0200 - val_custom_mse: 0.1134 - val_custom_mae: 0.2313\n",
            "Epoch 96/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0307 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0291 - val_mse: 0.0094 - learning_rate: 0.0200 - val_custom_mse: 0.1134 - val_custom_mae: 0.2313\n",
            "Epoch 97/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0307 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0290 - val_mse: 0.0094 - learning_rate: 0.0200 - val_custom_mse: 0.1134 - val_custom_mae: 0.2313\n",
            "Epoch 98/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0307 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0292 - val_mse: 0.0094 - learning_rate: 0.0200 - val_custom_mse: 0.1135 - val_custom_mae: 0.2316\n",
            "Epoch 99/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0307 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0289 - val_mse: 0.0094 - learning_rate: 0.0200 - val_custom_mse: 0.1133 - val_custom_mae: 0.2313\n",
            "Epoch 100/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0166 - mae: 0.0306 - mse: 0.0166 - val_loss: 0.0094 - val_mae: 0.0290 - val_mse: 0.0094 - learning_rate: 0.0200 - val_custom_mse: 0.1135 - val_custom_mae: 0.2314\n",
            "Running experiment: horizon=96, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_17', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1040/1040 - 13s - 13ms/step - loss: 0.2486 - mae: 0.2907 - mse: 0.2486 - val_loss: 0.1077 - val_mae: 0.2359 - val_mse: 0.1077 - learning_rate: 0.1000 - val_custom_mse: 0.1642 - val_custom_mae: 0.2888\n",
            "Epoch 2/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.1681 - mae: 0.2470 - mse: 0.1681 - val_loss: 0.0762 - val_mae: 0.1961 - val_mse: 0.0762 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2771\n",
            "Epoch 3/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.1051 - mae: 0.2010 - mse: 0.1051 - val_loss: 0.0576 - val_mae: 0.1668 - val_mse: 0.0576 - learning_rate: 0.1000 - val_custom_mse: 0.1418 - val_custom_mae: 0.2681\n",
            "Epoch 4/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0757 - mae: 0.1704 - mse: 0.0757 - val_loss: 0.0486 - val_mae: 0.1497 - val_mse: 0.0486 - learning_rate: 0.1000 - val_custom_mse: 0.1353 - val_custom_mae: 0.2608\n",
            "Epoch 5/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0635 - mae: 0.1531 - mse: 0.0635 - val_loss: 0.0432 - val_mae: 0.1387 - val_mse: 0.0432 - learning_rate: 0.1000 - val_custom_mse: 0.1314 - val_custom_mae: 0.2561\n",
            "Epoch 6/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0570 - mae: 0.1422 - mse: 0.0570 - val_loss: 0.0393 - val_mae: 0.1302 - val_mse: 0.0393 - learning_rate: 0.1000 - val_custom_mse: 0.1283 - val_custom_mae: 0.2520\n",
            "Epoch 7/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1343 - mse: 0.0525 - val_loss: 0.0361 - val_mae: 0.1232 - val_mse: 0.0361 - learning_rate: 0.1000 - val_custom_mse: 0.1244 - val_custom_mae: 0.2468\n",
            "Epoch 8/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0493 - mae: 0.1284 - mse: 0.0493 - val_loss: 0.0337 - val_mae: 0.1178 - val_mse: 0.0337 - learning_rate: 0.1000 - val_custom_mse: 0.1227 - val_custom_mae: 0.2446\n",
            "Epoch 9/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0468 - mae: 0.1236 - mse: 0.0468 - val_loss: 0.0318 - val_mae: 0.1133 - val_mse: 0.0318 - learning_rate: 0.1000 - val_custom_mse: 0.1213 - val_custom_mae: 0.2428\n",
            "Epoch 10/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0449 - mae: 0.1198 - mse: 0.0449 - val_loss: 0.0300 - val_mae: 0.1093 - val_mse: 0.0300 - learning_rate: 0.1000 - val_custom_mse: 0.1194 - val_custom_mae: 0.2400\n",
            "Epoch 11/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0433 - mae: 0.1166 - mse: 0.0433 - val_loss: 0.0286 - val_mae: 0.1060 - val_mse: 0.0286 - learning_rate: 0.1000 - val_custom_mse: 0.1180 - val_custom_mae: 0.2381\n",
            "Epoch 12/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0420 - mae: 0.1138 - mse: 0.0420 - val_loss: 0.0274 - val_mae: 0.1029 - val_mse: 0.0274 - learning_rate: 0.1000 - val_custom_mse: 0.1175 - val_custom_mae: 0.2374\n",
            "Epoch 13/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0409 - mae: 0.1114 - mse: 0.0409 - val_loss: 0.0264 - val_mae: 0.1002 - val_mse: 0.0264 - learning_rate: 0.1000 - val_custom_mse: 0.1175 - val_custom_mae: 0.2375\n",
            "Epoch 14/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0399 - mae: 0.1093 - mse: 0.0399 - val_loss: 0.0253 - val_mae: 0.0978 - val_mse: 0.0253 - learning_rate: 0.1000 - val_custom_mse: 0.1163 - val_custom_mae: 0.2357\n",
            "Epoch 15/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0391 - mae: 0.1074 - mse: 0.0391 - val_loss: 0.0245 - val_mae: 0.0956 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.1158 - val_custom_mae: 0.2349\n",
            "Epoch 16/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0384 - mae: 0.1056 - mse: 0.0384 - val_loss: 0.0237 - val_mae: 0.0936 - val_mse: 0.0237 - learning_rate: 0.1000 - val_custom_mse: 0.1155 - val_custom_mae: 0.2346\n",
            "Epoch 17/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0378 - mae: 0.1041 - mse: 0.0378 - val_loss: 0.0231 - val_mae: 0.0916 - val_mse: 0.0231 - learning_rate: 0.1000 - val_custom_mse: 0.1156 - val_custom_mae: 0.2348\n",
            "Epoch 18/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0372 - mae: 0.1027 - mse: 0.0372 - val_loss: 0.0225 - val_mae: 0.0899 - val_mse: 0.0225 - learning_rate: 0.1000 - val_custom_mse: 0.1155 - val_custom_mae: 0.2346\n",
            "Epoch 19/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0367 - mae: 0.1014 - mse: 0.0367 - val_loss: 0.0219 - val_mae: 0.0884 - val_mse: 0.0219 - learning_rate: 0.1000 - val_custom_mse: 0.1150 - val_custom_mae: 0.2339\n",
            "Epoch 20/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0363 - mae: 0.1002 - mse: 0.0363 - val_loss: 0.0214 - val_mae: 0.0871 - val_mse: 0.0214 - learning_rate: 0.1000 - val_custom_mse: 0.1147 - val_custom_mae: 0.2334\n",
            "Epoch 21/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0359 - mae: 0.0992 - mse: 0.0359 - val_loss: 0.0209 - val_mae: 0.0856 - val_mse: 0.0209 - learning_rate: 0.1000 - val_custom_mse: 0.1148 - val_custom_mae: 0.2335\n",
            "Epoch 22/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0356 - mae: 0.0982 - mse: 0.0356 - val_loss: 0.0205 - val_mae: 0.0844 - val_mse: 0.0205 - learning_rate: 0.1000 - val_custom_mse: 0.1147 - val_custom_mae: 0.2335\n",
            "Epoch 23/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0352 - mae: 0.0973 - mse: 0.0352 - val_loss: 0.0201 - val_mae: 0.0834 - val_mse: 0.0201 - learning_rate: 0.1000 - val_custom_mse: 0.1144 - val_custom_mae: 0.2330\n",
            "Epoch 24/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0350 - mae: 0.0964 - mse: 0.0350 - val_loss: 0.0198 - val_mae: 0.0823 - val_mse: 0.0198 - learning_rate: 0.1000 - val_custom_mse: 0.1143 - val_custom_mae: 0.2329\n",
            "Epoch 25/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0347 - mae: 0.0956 - mse: 0.0347 - val_loss: 0.0195 - val_mae: 0.0813 - val_mse: 0.0195 - learning_rate: 0.1000 - val_custom_mse: 0.1143 - val_custom_mae: 0.2329\n",
            "Epoch 26/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0345 - mae: 0.0949 - mse: 0.0345 - val_loss: 0.0192 - val_mae: 0.0805 - val_mse: 0.0192 - learning_rate: 0.1000 - val_custom_mse: 0.1141 - val_custom_mae: 0.2326\n",
            "Epoch 27/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0343 - mae: 0.0943 - mse: 0.0343 - val_loss: 0.0190 - val_mae: 0.0796 - val_mse: 0.0190 - learning_rate: 0.1000 - val_custom_mse: 0.1143 - val_custom_mae: 0.2329\n",
            "Epoch 28/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0340 - mae: 0.0935 - mse: 0.0340 - val_loss: 0.0188 - val_mae: 0.0791 - val_mse: 0.0188 - learning_rate: 0.1000 - val_custom_mse: 0.1142 - val_custom_mae: 0.2329\n",
            "Epoch 29/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0338 - mae: 0.0930 - mse: 0.0338 - val_loss: 0.0185 - val_mae: 0.0784 - val_mse: 0.0185 - learning_rate: 0.1000 - val_custom_mse: 0.1140 - val_custom_mae: 0.2325\n",
            "Epoch 30/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0337 - mae: 0.0926 - mse: 0.0337 - val_loss: 0.0183 - val_mae: 0.0777 - val_mse: 0.0183 - learning_rate: 0.1000 - val_custom_mse: 0.1141 - val_custom_mae: 0.2327\n",
            "Epoch 31/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0335 - mae: 0.0921 - mse: 0.0335 - val_loss: 0.0181 - val_mae: 0.0770 - val_mse: 0.0181 - learning_rate: 0.1000 - val_custom_mse: 0.1140 - val_custom_mae: 0.2325\n",
            "Epoch 32/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0334 - mae: 0.0917 - mse: 0.0334 - val_loss: 0.0179 - val_mae: 0.0767 - val_mse: 0.0179 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2317\n",
            "Epoch 33/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0332 - mae: 0.0913 - mse: 0.0332 - val_loss: 0.0178 - val_mae: 0.0760 - val_mse: 0.0178 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2324\n",
            "Epoch 34/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0331 - mae: 0.0909 - mse: 0.0331 - val_loss: 0.0176 - val_mae: 0.0756 - val_mse: 0.0176 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2322\n",
            "Epoch 35/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0331 - mae: 0.0905 - mse: 0.0331 - val_loss: 0.0175 - val_mae: 0.0751 - val_mse: 0.0175 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2323\n",
            "Epoch 36/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0329 - mae: 0.0902 - mse: 0.0329 - val_loss: 0.0174 - val_mae: 0.0747 - val_mse: 0.0174 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2321\n",
            "Epoch 37/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0328 - mae: 0.0899 - mse: 0.0328 - val_loss: 0.0173 - val_mae: 0.0743 - val_mse: 0.0173 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2321\n",
            "Epoch 38/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0327 - mae: 0.0896 - mse: 0.0327 - val_loss: 0.0171 - val_mae: 0.0739 - val_mse: 0.0171 - learning_rate: 0.1000 - val_custom_mse: 0.1135 - val_custom_mae: 0.2319\n",
            "Epoch 39/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0326 - mae: 0.0894 - mse: 0.0326 - val_loss: 0.0170 - val_mae: 0.0738 - val_mse: 0.0170 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2313\n",
            "Epoch 40/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0325 - mae: 0.0891 - mse: 0.0325 - val_loss: 0.0169 - val_mae: 0.0734 - val_mse: 0.0169 - learning_rate: 0.1000 - val_custom_mse: 0.1132 - val_custom_mae: 0.2315\n",
            "Epoch 41/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0325 - mae: 0.0889 - mse: 0.0325 - val_loss: 0.0169 - val_mae: 0.0731 - val_mse: 0.0169 - learning_rate: 0.1000 - val_custom_mse: 0.1133 - val_custom_mae: 0.2315\n",
            "Epoch 42/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0324 - mae: 0.0887 - mse: 0.0324 - val_loss: 0.0168 - val_mae: 0.0729 - val_mse: 0.0168 - learning_rate: 0.1000 - val_custom_mse: 0.1132 - val_custom_mae: 0.2315\n",
            "Epoch 43/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0324 - mae: 0.0885 - mse: 0.0324 - val_loss: 0.0167 - val_mae: 0.0726 - val_mse: 0.0167 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2314\n",
            "Epoch 44/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0323 - mae: 0.0883 - mse: 0.0323 - val_loss: 0.0167 - val_mae: 0.0724 - val_mse: 0.0167 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2317\n",
            "Epoch 45/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0323 - mae: 0.0882 - mse: 0.0323 - val_loss: 0.0166 - val_mae: 0.0721 - val_mse: 0.0166 - learning_rate: 0.1000 - val_custom_mse: 0.1132 - val_custom_mae: 0.2314\n",
            "Epoch 46/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0322 - mae: 0.0880 - mse: 0.0322 - val_loss: 0.0165 - val_mae: 0.0719 - val_mse: 0.0165 - learning_rate: 0.1000 - val_custom_mse: 0.1130 - val_custom_mae: 0.2312\n",
            "Epoch 47/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0321 - mae: 0.0879 - mse: 0.0321 - val_loss: 0.0165 - val_mae: 0.0718 - val_mse: 0.0165 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2312\n",
            "Epoch 48/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0321 - mae: 0.0877 - mse: 0.0321 - val_loss: 0.0165 - val_mae: 0.0717 - val_mse: 0.0165 - learning_rate: 0.1000 - val_custom_mse: 0.1132 - val_custom_mae: 0.2313\n",
            "Epoch 49/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0320 - mae: 0.0876 - mse: 0.0320 - val_loss: 0.0164 - val_mae: 0.0714 - val_mse: 0.0164 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2309\n",
            "Epoch 50/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0320 - mae: 0.0875 - mse: 0.0320 - val_loss: 0.0163 - val_mae: 0.0713 - val_mse: 0.0163 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2310\n",
            "Epoch 51/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0320 - mae: 0.0874 - mse: 0.0320 - val_loss: 0.0163 - val_mae: 0.0712 - val_mse: 0.0163 - learning_rate: 0.1000 - val_custom_mse: 0.1130 - val_custom_mae: 0.2311\n",
            "Epoch 52/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0320 - mae: 0.0873 - mse: 0.0320 - val_loss: 0.0163 - val_mae: 0.0711 - val_mse: 0.0163 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2310\n",
            "Epoch 53/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0319 - mae: 0.0872 - mse: 0.0319 - val_loss: 0.0162 - val_mae: 0.0709 - val_mse: 0.0162 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2309\n",
            "Epoch 54/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0319 - mae: 0.0871 - mse: 0.0319 - val_loss: 0.0162 - val_mae: 0.0709 - val_mse: 0.0162 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2307\n",
            "Epoch 55/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0318 - mae: 0.0870 - mse: 0.0318 - val_loss: 0.0162 - val_mae: 0.0709 - val_mse: 0.0162 - learning_rate: 0.1000 - val_custom_mse: 0.1124 - val_custom_mae: 0.2304\n",
            "Epoch 56/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0318 - mae: 0.0870 - mse: 0.0318 - val_loss: 0.0162 - val_mae: 0.0708 - val_mse: 0.0162 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2315\n",
            "Epoch 57/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0318 - mae: 0.0869 - mse: 0.0318 - val_loss: 0.0161 - val_mae: 0.0706 - val_mse: 0.0161 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2310\n",
            "Epoch 58/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0317 - mae: 0.0868 - mse: 0.0317 - val_loss: 0.0161 - val_mae: 0.0704 - val_mse: 0.0161 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2309\n",
            "Epoch 59/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0317 - mae: 0.0868 - mse: 0.0317 - val_loss: 0.0161 - val_mae: 0.0704 - val_mse: 0.0161 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2310\n",
            "Epoch 60/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0317 - mae: 0.0867 - mse: 0.0317 - val_loss: 0.0161 - val_mae: 0.0704 - val_mse: 0.0161 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2308\n",
            "Epoch 61/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0317 - mae: 0.0867 - mse: 0.0317 - val_loss: 0.0160 - val_mae: 0.0703 - val_mse: 0.0160 - learning_rate: 0.1000 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 62/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0317 - mae: 0.0866 - mse: 0.0317 - val_loss: 0.0161 - val_mae: 0.0703 - val_mse: 0.0161 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2309\n",
            "Epoch 63/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0317 - mae: 0.0866 - mse: 0.0317 - val_loss: 0.0161 - val_mae: 0.0703 - val_mse: 0.0161 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2313\n",
            "Epoch 64/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0316 - mae: 0.0865 - mse: 0.0316 - val_loss: 0.0160 - val_mae: 0.0702 - val_mse: 0.0160 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2310\n",
            "Epoch 65/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0316 - mae: 0.0865 - mse: 0.0316 - val_loss: 0.0160 - val_mae: 0.0701 - val_mse: 0.0160 - learning_rate: 0.1000 - val_custom_mse: 0.1126 - val_custom_mae: 0.2306\n",
            "Epoch 66/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0316 - mae: 0.0865 - mse: 0.0316 - val_loss: 0.0160 - val_mae: 0.0700 - val_mse: 0.0160 - learning_rate: 0.1000 - val_custom_mse: 0.1127 - val_custom_mae: 0.2308\n",
            "Epoch 67/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0316 - mae: 0.0864 - mse: 0.0316 - val_loss: 0.0160 - val_mae: 0.0700 - val_mse: 0.0160 - learning_rate: 0.1000 - val_custom_mse: 0.1125 - val_custom_mae: 0.2305\n",
            "Epoch 68/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0316 - mae: 0.0864 - mse: 0.0316 - val_loss: 0.0160 - val_mae: 0.0700 - val_mse: 0.0160 - learning_rate: 0.1000 - val_custom_mse: 0.1126 - val_custom_mae: 0.2305\n",
            "Epoch 69/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0315 - mae: 0.0864 - mse: 0.0315 - val_loss: 0.0160 - val_mae: 0.0700 - val_mse: 0.0160 - learning_rate: 0.1000 - val_custom_mse: 0.1127 - val_custom_mae: 0.2306\n",
            "Epoch 70/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0315 - mae: 0.0864 - mse: 0.0315 - val_loss: 0.0160 - val_mae: 0.0700 - val_mse: 0.0160 - learning_rate: 0.1000 - val_custom_mse: 0.1126 - val_custom_mae: 0.2305\n",
            "Epoch 71/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0315 - mae: 0.0863 - mse: 0.0315 - val_loss: 0.0159 - val_mae: 0.0701 - val_mse: 0.0159 - learning_rate: 0.1000 - val_custom_mse: 0.1123 - val_custom_mae: 0.2301\n",
            "Epoch 72/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0315 - mae: 0.0863 - mse: 0.0315 - val_loss: 0.0159 - val_mae: 0.0700 - val_mse: 0.0159 - learning_rate: 0.1000 - val_custom_mse: 0.1122 - val_custom_mae: 0.2301\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0315 - mae: 0.0863 - mse: 0.0315 - val_loss: 0.0160 - val_mae: 0.0700 - val_mse: 0.0160 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2311\n",
            "Epoch 74/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0862 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0200 - val_custom_mse: 0.1125 - val_custom_mae: 0.2305\n",
            "Epoch 75/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0862 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0200 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 76/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0862 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0698 - val_mse: 0.0159 - learning_rate: 0.0200 - val_custom_mse: 0.1124 - val_custom_mae: 0.2303\n",
            "Epoch 77/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0862 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0200 - val_custom_mse: 0.1125 - val_custom_mae: 0.2305\n",
            "Epoch 78/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0200 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 79/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0200 - val_custom_mse: 0.1124 - val_custom_mae: 0.2305\n",
            "Epoch 80/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0862 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0698 - val_mse: 0.0159 - learning_rate: 0.0200 - val_custom_mse: 0.1125 - val_custom_mae: 0.2305\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0698 - val_mse: 0.0159 - learning_rate: 0.0200 - val_custom_mse: 0.1124 - val_custom_mae: 0.2305\n",
            "Epoch 82/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 83/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 84/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 85/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 86/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 87/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 88/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 90/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 91/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 92/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 93/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0698 - val_mse: 0.0159 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 94/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0698 - val_mse: 0.0159 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 95/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 96/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 98/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 99/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Epoch 100/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0314 - mae: 0.0861 - mse: 0.0314 - val_loss: 0.0159 - val_mae: 0.0699 - val_mse: 0.0159 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2306\n",
            "Running experiment: horizon=96, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_18', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1040/1040 - 13s - 13ms/step - loss: 0.2547 - mae: 0.2962 - mse: 0.2547 - val_loss: 0.1081 - val_mae: 0.2368 - val_mse: 0.1081 - learning_rate: 0.1000 - val_custom_mse: 0.1663 - val_custom_mae: 0.2911\n",
            "Epoch 2/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.1685 - mae: 0.2491 - mse: 0.1685 - val_loss: 0.0764 - val_mae: 0.1969 - val_mse: 0.0764 - learning_rate: 0.1000 - val_custom_mse: 0.1524 - val_custom_mae: 0.2788\n",
            "Epoch 3/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.1051 - mae: 0.2022 - mse: 0.1051 - val_loss: 0.0578 - val_mae: 0.1673 - val_mse: 0.0578 - learning_rate: 0.1000 - val_custom_mse: 0.1405 - val_custom_mae: 0.2668\n",
            "Epoch 4/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0776 - mae: 0.1731 - mse: 0.0776 - val_loss: 0.0495 - val_mae: 0.1516 - val_mse: 0.0495 - learning_rate: 0.1000 - val_custom_mse: 0.1355 - val_custom_mae: 0.2611\n",
            "Epoch 5/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0667 - mae: 0.1577 - mse: 0.0667 - val_loss: 0.0444 - val_mae: 0.1411 - val_mse: 0.0444 - learning_rate: 0.1000 - val_custom_mse: 0.1316 - val_custom_mae: 0.2563\n",
            "Epoch 6/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0608 - mae: 0.1482 - mse: 0.0608 - val_loss: 0.0400 - val_mae: 0.1320 - val_mse: 0.0400 - learning_rate: 0.1000 - val_custom_mse: 0.1255 - val_custom_mae: 0.2483\n",
            "Epoch 7/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0568 - mae: 0.1414 - mse: 0.0568 - val_loss: 0.0372 - val_mae: 0.1261 - val_mse: 0.0372 - learning_rate: 0.1000 - val_custom_mse: 0.1235 - val_custom_mae: 0.2457\n",
            "Epoch 8/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0540 - mae: 0.1364 - mse: 0.0540 - val_loss: 0.0350 - val_mae: 0.1211 - val_mse: 0.0350 - learning_rate: 0.1000 - val_custom_mse: 0.1214 - val_custom_mae: 0.2430\n",
            "Epoch 9/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0519 - mae: 0.1326 - mse: 0.0519 - val_loss: 0.0334 - val_mae: 0.1173 - val_mse: 0.0334 - learning_rate: 0.1000 - val_custom_mse: 0.1212 - val_custom_mae: 0.2427\n",
            "Epoch 10/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0503 - mae: 0.1296 - mse: 0.0503 - val_loss: 0.0319 - val_mae: 0.1140 - val_mse: 0.0319 - learning_rate: 0.1000 - val_custom_mse: 0.1201 - val_custom_mae: 0.2411\n",
            "Epoch 11/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0489 - mae: 0.1272 - mse: 0.0489 - val_loss: 0.0305 - val_mae: 0.1109 - val_mse: 0.0305 - learning_rate: 0.1000 - val_custom_mse: 0.1182 - val_custom_mae: 0.2385\n",
            "Epoch 12/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0478 - mae: 0.1251 - mse: 0.0478 - val_loss: 0.0295 - val_mae: 0.1086 - val_mse: 0.0295 - learning_rate: 0.1000 - val_custom_mse: 0.1180 - val_custom_mae: 0.2383\n",
            "Epoch 13/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0470 - mae: 0.1233 - mse: 0.0470 - val_loss: 0.0284 - val_mae: 0.1062 - val_mse: 0.0284 - learning_rate: 0.1000 - val_custom_mse: 0.1168 - val_custom_mae: 0.2364\n",
            "Epoch 14/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0461 - mae: 0.1218 - mse: 0.0461 - val_loss: 0.0276 - val_mae: 0.1043 - val_mse: 0.0276 - learning_rate: 0.1000 - val_custom_mse: 0.1161 - val_custom_mae: 0.2355\n",
            "Epoch 15/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0455 - mae: 0.1204 - mse: 0.0455 - val_loss: 0.0271 - val_mae: 0.1030 - val_mse: 0.0271 - learning_rate: 0.1000 - val_custom_mse: 0.1169 - val_custom_mae: 0.2367\n",
            "Epoch 16/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0450 - mae: 0.1193 - mse: 0.0450 - val_loss: 0.0263 - val_mae: 0.1011 - val_mse: 0.0263 - learning_rate: 0.1000 - val_custom_mse: 0.1157 - val_custom_mae: 0.2351\n",
            "Epoch 17/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0443 - mae: 0.1182 - mse: 0.0443 - val_loss: 0.0257 - val_mae: 0.0997 - val_mse: 0.0257 - learning_rate: 0.1000 - val_custom_mse: 0.1148 - val_custom_mae: 0.2337\n",
            "Epoch 18/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0439 - mae: 0.1173 - mse: 0.0439 - val_loss: 0.0253 - val_mae: 0.0985 - val_mse: 0.0253 - learning_rate: 0.1000 - val_custom_mse: 0.1151 - val_custom_mae: 0.2342\n",
            "Epoch 19/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0436 - mae: 0.1164 - mse: 0.0436 - val_loss: 0.0248 - val_mae: 0.0973 - val_mse: 0.0248 - learning_rate: 0.1000 - val_custom_mse: 0.1148 - val_custom_mae: 0.2336\n",
            "Epoch 20/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0432 - mae: 0.1157 - mse: 0.0432 - val_loss: 0.0245 - val_mae: 0.0964 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.1149 - val_custom_mae: 0.2339\n",
            "Epoch 21/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0429 - mae: 0.1150 - mse: 0.0429 - val_loss: 0.0241 - val_mae: 0.0955 - val_mse: 0.0241 - learning_rate: 0.1000 - val_custom_mse: 0.1141 - val_custom_mae: 0.2327\n",
            "Epoch 22/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0426 - mae: 0.1143 - mse: 0.0426 - val_loss: 0.0239 - val_mae: 0.0949 - val_mse: 0.0239 - learning_rate: 0.1000 - val_custom_mse: 0.1142 - val_custom_mae: 0.2329\n",
            "Epoch 23/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0423 - mae: 0.1137 - mse: 0.0423 - val_loss: 0.0236 - val_mae: 0.0942 - val_mse: 0.0236 - learning_rate: 0.1000 - val_custom_mse: 0.1148 - val_custom_mae: 0.2337\n",
            "Epoch 24/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0420 - mae: 0.1132 - mse: 0.0420 - val_loss: 0.0232 - val_mae: 0.0935 - val_mse: 0.0232 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2322\n",
            "Epoch 25/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0418 - mae: 0.1127 - mse: 0.0418 - val_loss: 0.0230 - val_mae: 0.0928 - val_mse: 0.0230 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2324\n",
            "Epoch 26/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0416 - mae: 0.1123 - mse: 0.0416 - val_loss: 0.0228 - val_mae: 0.0923 - val_mse: 0.0228 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2322\n",
            "Epoch 27/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0415 - mae: 0.1119 - mse: 0.0415 - val_loss: 0.0226 - val_mae: 0.0917 - val_mse: 0.0226 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2326\n",
            "Epoch 28/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0413 - mae: 0.1115 - mse: 0.0413 - val_loss: 0.0224 - val_mae: 0.0912 - val_mse: 0.0224 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2321\n",
            "Epoch 29/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0411 - mae: 0.1112 - mse: 0.0411 - val_loss: 0.0223 - val_mae: 0.0907 - val_mse: 0.0223 - learning_rate: 0.1000 - val_custom_mse: 0.1135 - val_custom_mae: 0.2320\n",
            "Epoch 30/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0410 - mae: 0.1109 - mse: 0.0410 - val_loss: 0.0222 - val_mae: 0.0904 - val_mse: 0.0222 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2325\n",
            "Epoch 31/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0409 - mae: 0.1106 - mse: 0.0409 - val_loss: 0.0221 - val_mae: 0.0900 - val_mse: 0.0221 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2326\n",
            "Epoch 32/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0408 - mae: 0.1103 - mse: 0.0408 - val_loss: 0.0220 - val_mae: 0.0897 - val_mse: 0.0220 - learning_rate: 0.1000 - val_custom_mse: 0.1141 - val_custom_mae: 0.2329\n",
            "Epoch 33/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0406 - mae: 0.1100 - mse: 0.0406 - val_loss: 0.0218 - val_mae: 0.0892 - val_mse: 0.0218 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2322\n",
            "Epoch 34/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0405 - mae: 0.1098 - mse: 0.0405 - val_loss: 0.0217 - val_mae: 0.0890 - val_mse: 0.0217 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2320\n",
            "Epoch 35/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0404 - mae: 0.1096 - mse: 0.0404 - val_loss: 0.0217 - val_mae: 0.0888 - val_mse: 0.0217 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2326\n",
            "Epoch 36/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0403 - mae: 0.1094 - mse: 0.0403 - val_loss: 0.0215 - val_mae: 0.0885 - val_mse: 0.0215 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2319\n",
            "Epoch 37/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0403 - mae: 0.1092 - mse: 0.0403 - val_loss: 0.0215 - val_mae: 0.0883 - val_mse: 0.0215 - learning_rate: 0.1000 - val_custom_mse: 0.1133 - val_custom_mae: 0.2319\n",
            "Epoch 38/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0402 - mae: 0.1091 - mse: 0.0402 - val_loss: 0.0214 - val_mae: 0.0881 - val_mse: 0.0214 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2319\n",
            "Epoch 39/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0401 - mae: 0.1089 - mse: 0.0401 - val_loss: 0.0213 - val_mae: 0.0879 - val_mse: 0.0213 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2311\n",
            "Epoch 40/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0400 - mae: 0.1088 - mse: 0.0400 - val_loss: 0.0215 - val_mae: 0.0883 - val_mse: 0.0215 - learning_rate: 0.1000 - val_custom_mse: 0.1140 - val_custom_mae: 0.2328\n",
            "Epoch 41/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0400 - mae: 0.1086 - mse: 0.0400 - val_loss: 0.0212 - val_mae: 0.0875 - val_mse: 0.0212 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2311\n",
            "Epoch 42/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0400 - mae: 0.1085 - mse: 0.0400 - val_loss: 0.0211 - val_mae: 0.0873 - val_mse: 0.0211 - learning_rate: 0.1000 - val_custom_mse: 0.1132 - val_custom_mae: 0.2316\n",
            "Epoch 43/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0399 - mae: 0.1084 - mse: 0.0399 - val_loss: 0.0212 - val_mae: 0.0874 - val_mse: 0.0212 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2323\n",
            "Epoch 44/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0399 - mae: 0.1083 - mse: 0.0399 - val_loss: 0.0211 - val_mae: 0.0871 - val_mse: 0.0211 - learning_rate: 0.1000 - val_custom_mse: 0.1130 - val_custom_mae: 0.2313\n",
            "Epoch 45/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0398 - mae: 0.1082 - mse: 0.0398 - val_loss: 0.0211 - val_mae: 0.0873 - val_mse: 0.0211 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2321\n",
            "Epoch 46/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0397 - mae: 0.1081 - mse: 0.0397 - val_loss: 0.0210 - val_mae: 0.0868 - val_mse: 0.0210 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2315\n",
            "Epoch 47/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0397 - mae: 0.1080 - mse: 0.0397 - val_loss: 0.0209 - val_mae: 0.0868 - val_mse: 0.0209 - learning_rate: 0.1000 - val_custom_mse: 0.1126 - val_custom_mae: 0.2308\n",
            "Epoch 48/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0396 - mae: 0.1079 - mse: 0.0396 - val_loss: 0.0209 - val_mae: 0.0866 - val_mse: 0.0209 - learning_rate: 0.1000 - val_custom_mse: 0.1130 - val_custom_mae: 0.2313\n",
            "Epoch 49/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0396 - mae: 0.1079 - mse: 0.0396 - val_loss: 0.0209 - val_mae: 0.0865 - val_mse: 0.0209 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2310\n",
            "Epoch 50/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0395 - mae: 0.1078 - mse: 0.0395 - val_loss: 0.0208 - val_mae: 0.0865 - val_mse: 0.0208 - learning_rate: 0.1000 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 51/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0396 - mae: 0.1077 - mse: 0.0396 - val_loss: 0.0209 - val_mae: 0.0865 - val_mse: 0.0209 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2315\n",
            "Epoch 52/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0395 - mae: 0.1076 - mse: 0.0395 - val_loss: 0.0209 - val_mae: 0.0867 - val_mse: 0.0209 - learning_rate: 0.1000 - val_custom_mse: 0.1133 - val_custom_mae: 0.2319\n",
            "Epoch 53/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0395 - mae: 0.1076 - mse: 0.0395 - val_loss: 0.0210 - val_mae: 0.0869 - val_mse: 0.0210 - learning_rate: 0.1000 - val_custom_mse: 0.1135 - val_custom_mae: 0.2323\n",
            "Epoch 54/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0394 - mae: 0.1075 - mse: 0.0394 - val_loss: 0.0208 - val_mae: 0.0862 - val_mse: 0.0208 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2312\n",
            "Epoch 55/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0394 - mae: 0.1075 - mse: 0.0394 - val_loss: 0.0207 - val_mae: 0.0863 - val_mse: 0.0207 - learning_rate: 0.1000 - val_custom_mse: 0.1122 - val_custom_mae: 0.2303\n",
            "Epoch 56/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0394 - mae: 0.1074 - mse: 0.0394 - val_loss: 0.0208 - val_mae: 0.0862 - val_mse: 0.0208 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2316\n",
            "Epoch 57/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0393 - mae: 0.1074 - mse: 0.0393 - val_loss: 0.0207 - val_mae: 0.0861 - val_mse: 0.0207 - learning_rate: 0.1000 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 58/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0393 - mae: 0.1073 - mse: 0.0393 - val_loss: 0.0207 - val_mae: 0.0859 - val_mse: 0.0207 - learning_rate: 0.1000 - val_custom_mse: 0.1124 - val_custom_mae: 0.2306\n",
            "Epoch 59/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0393 - mae: 0.1073 - mse: 0.0393 - val_loss: 0.0207 - val_mae: 0.0860 - val_mse: 0.0207 - learning_rate: 0.1000 - val_custom_mse: 0.1127 - val_custom_mae: 0.2309\n",
            "Epoch 60/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0393 - mae: 0.1072 - mse: 0.0393 - val_loss: 0.0208 - val_mae: 0.0861 - val_mse: 0.0208 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2314\n",
            "Epoch 61/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0393 - mae: 0.1072 - mse: 0.0393 - val_loss: 0.0207 - val_mae: 0.0860 - val_mse: 0.0207 - learning_rate: 0.1000 - val_custom_mse: 0.1124 - val_custom_mae: 0.2307\n",
            "Epoch 62/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0392 - mae: 0.1071 - mse: 0.0392 - val_loss: 0.0207 - val_mae: 0.0860 - val_mse: 0.0207 - learning_rate: 0.1000 - val_custom_mse: 0.1123 - val_custom_mae: 0.2306\n",
            "Epoch 63/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0392 - mae: 0.1071 - mse: 0.0392 - val_loss: 0.0206 - val_mae: 0.0858 - val_mse: 0.0206 - learning_rate: 0.1000 - val_custom_mse: 0.1123 - val_custom_mae: 0.2304\n",
            "Epoch 64/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0392 - mae: 0.1071 - mse: 0.0392 - val_loss: 0.0207 - val_mae: 0.0859 - val_mse: 0.0207 - learning_rate: 0.1000 - val_custom_mse: 0.1127 - val_custom_mae: 0.2311\n",
            "Epoch 65/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0392 - mae: 0.1071 - mse: 0.0392 - val_loss: 0.0207 - val_mae: 0.0859 - val_mse: 0.0207 - learning_rate: 0.1000 - val_custom_mse: 0.1127 - val_custom_mae: 0.2311\n",
            "Epoch 66/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0391 - mae: 0.1070 - mse: 0.0391 - val_loss: 0.0208 - val_mae: 0.0863 - val_mse: 0.0208 - learning_rate: 0.1000 - val_custom_mse: 0.1132 - val_custom_mae: 0.2319\n",
            "Epoch 67/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0391 - mae: 0.1070 - mse: 0.0391 - val_loss: 0.0206 - val_mae: 0.0857 - val_mse: 0.0206 - learning_rate: 0.1000 - val_custom_mse: 0.1124 - val_custom_mae: 0.2306\n",
            "Epoch 68/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0391 - mae: 0.1070 - mse: 0.0391 - val_loss: 0.0206 - val_mae: 0.0857 - val_mse: 0.0206 - learning_rate: 0.1000 - val_custom_mse: 0.1122 - val_custom_mae: 0.2304\n",
            "Epoch 69/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0391 - mae: 0.1069 - mse: 0.0391 - val_loss: 0.0206 - val_mae: 0.0857 - val_mse: 0.0206 - learning_rate: 0.1000 - val_custom_mse: 0.1124 - val_custom_mae: 0.2305\n",
            "Epoch 70/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0391 - mae: 0.1069 - mse: 0.0391 - val_loss: 0.0206 - val_mae: 0.0857 - val_mse: 0.0206 - learning_rate: 0.1000 - val_custom_mse: 0.1124 - val_custom_mae: 0.2308\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0391 - mae: 0.1069 - mse: 0.0391 - val_loss: 0.0207 - val_mae: 0.0858 - val_mse: 0.0207 - learning_rate: 0.1000 - val_custom_mse: 0.1127 - val_custom_mae: 0.2311\n",
            "Epoch 72/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0207 - val_mae: 0.0857 - val_mse: 0.0207 - learning_rate: 0.0200 - val_custom_mse: 0.1127 - val_custom_mae: 0.2310\n",
            "Epoch 73/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0207 - val_mae: 0.0858 - val_mse: 0.0207 - learning_rate: 0.0200 - val_custom_mse: 0.1128 - val_custom_mae: 0.2312\n",
            "Epoch 74/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0207 - val_mae: 0.0857 - val_mse: 0.0207 - learning_rate: 0.0200 - val_custom_mse: 0.1127 - val_custom_mae: 0.2310\n",
            "Epoch 75/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0856 - val_mse: 0.0206 - learning_rate: 0.0200 - val_custom_mse: 0.1126 - val_custom_mae: 0.2309\n",
            "Epoch 76/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0207 - val_mae: 0.0857 - val_mse: 0.0207 - learning_rate: 0.0200 - val_custom_mse: 0.1126 - val_custom_mae: 0.2310\n",
            "Epoch 77/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0856 - val_mse: 0.0206 - learning_rate: 0.0200 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 78/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0391 - mae: 0.1068 - mse: 0.0391 - val_loss: 0.0206 - val_mae: 0.0856 - val_mse: 0.0206 - learning_rate: 0.0200 - val_custom_mse: 0.1126 - val_custom_mae: 0.2309\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0856 - val_mse: 0.0206 - learning_rate: 0.0200 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 80/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 81/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 82/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 83/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 84/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 85/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 86/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1067 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 0.0040 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 88/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1067 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 89/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0389 - mae: 0.1067 - mse: 0.0389 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 90/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1067 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 91/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1067 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 92/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1067 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 93/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 94/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2307\n",
            "Epoch 96/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1067 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 97/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0390 - mae: 0.1067 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 98/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0390 - mae: 0.1068 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 99/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0390 - mae: 0.1067 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 100/100\n",
            "1040/1040 - 9s - 9ms/step - loss: 0.0390 - mae: 0.1067 - mse: 0.0390 - val_loss: 0.0206 - val_mae: 0.0855 - val_mse: 0.0206 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Running experiment: horizon=96, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_19', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1040/1040 - 14s - 14ms/step - loss: 0.2601 - mae: 0.3009 - mse: 0.2601 - val_loss: 0.1088 - val_mae: 0.2381 - val_mse: 0.1088 - learning_rate: 0.1000 - val_custom_mse: 0.1698 - val_custom_mae: 0.2947\n",
            "Epoch 2/100\n",
            "1040/1040 - 9s - 9ms/step - loss: 0.1678 - mae: 0.2502 - mse: 0.1678 - val_loss: 0.0763 - val_mae: 0.1969 - val_mse: 0.0763 - learning_rate: 0.1000 - val_custom_mse: 0.1536 - val_custom_mae: 0.2803\n",
            "Epoch 3/100\n",
            "1040/1040 - 9s - 9ms/step - loss: 0.1048 - mae: 0.2026 - mse: 0.1048 - val_loss: 0.0583 - val_mae: 0.1681 - val_mse: 0.0583 - learning_rate: 0.1000 - val_custom_mse: 0.1418 - val_custom_mae: 0.2683\n",
            "Epoch 4/100\n",
            "1040/1040 - 9s - 9ms/step - loss: 0.0799 - mae: 0.1758 - mse: 0.0799 - val_loss: 0.0500 - val_mae: 0.1525 - val_mse: 0.0500 - learning_rate: 0.1000 - val_custom_mse: 0.1352 - val_custom_mae: 0.2606\n",
            "Epoch 5/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0704 - mae: 0.1623 - mse: 0.0704 - val_loss: 0.0447 - val_mae: 0.1418 - val_mse: 0.0447 - learning_rate: 0.1000 - val_custom_mse: 0.1299 - val_custom_mae: 0.2540\n",
            "Epoch 6/100\n",
            "1040/1040 - 9s - 9ms/step - loss: 0.0650 - mae: 0.1540 - mse: 0.0650 - val_loss: 0.0411 - val_mae: 0.1344 - val_mse: 0.0411 - learning_rate: 0.1000 - val_custom_mse: 0.1265 - val_custom_mae: 0.2497\n",
            "Epoch 7/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0615 - mae: 0.1481 - mse: 0.0615 - val_loss: 0.0386 - val_mae: 0.1289 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.1245 - val_custom_mae: 0.2471\n",
            "Epoch 8/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0589 - mae: 0.1438 - mse: 0.0589 - val_loss: 0.0364 - val_mae: 0.1244 - val_mse: 0.0364 - learning_rate: 0.1000 - val_custom_mse: 0.1223 - val_custom_mae: 0.2441\n",
            "Epoch 9/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0570 - mae: 0.1406 - mse: 0.0570 - val_loss: 0.0348 - val_mae: 0.1209 - val_mse: 0.0348 - learning_rate: 0.1000 - val_custom_mse: 0.1208 - val_custom_mae: 0.2421\n",
            "Epoch 10/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0555 - mae: 0.1380 - mse: 0.0555 - val_loss: 0.0334 - val_mae: 0.1179 - val_mse: 0.0334 - learning_rate: 0.1000 - val_custom_mse: 0.1193 - val_custom_mae: 0.2400\n",
            "Epoch 11/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0543 - mae: 0.1359 - mse: 0.0543 - val_loss: 0.0323 - val_mae: 0.1154 - val_mse: 0.0323 - learning_rate: 0.1000 - val_custom_mse: 0.1184 - val_custom_mae: 0.2389\n",
            "Epoch 12/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0533 - mae: 0.1342 - mse: 0.0533 - val_loss: 0.0313 - val_mae: 0.1133 - val_mse: 0.0313 - learning_rate: 0.1000 - val_custom_mse: 0.1173 - val_custom_mae: 0.2372\n",
            "Epoch 13/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0525 - mae: 0.1328 - mse: 0.0525 - val_loss: 0.0305 - val_mae: 0.1115 - val_mse: 0.0305 - learning_rate: 0.1000 - val_custom_mse: 0.1167 - val_custom_mae: 0.2364\n",
            "Epoch 14/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0518 - mae: 0.1315 - mse: 0.0518 - val_loss: 0.0299 - val_mae: 0.1101 - val_mse: 0.0299 - learning_rate: 0.1000 - val_custom_mse: 0.1168 - val_custom_mae: 0.2366\n",
            "Epoch 15/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0512 - mae: 0.1304 - mse: 0.0512 - val_loss: 0.0293 - val_mae: 0.1086 - val_mse: 0.0293 - learning_rate: 0.1000 - val_custom_mse: 0.1163 - val_custom_mae: 0.2359\n",
            "Epoch 16/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0507 - mae: 0.1295 - mse: 0.0507 - val_loss: 0.0288 - val_mae: 0.1075 - val_mse: 0.0288 - learning_rate: 0.1000 - val_custom_mse: 0.1161 - val_custom_mae: 0.2356\n",
            "Epoch 17/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0502 - mae: 0.1286 - mse: 0.0502 - val_loss: 0.0285 - val_mae: 0.1068 - val_mse: 0.0285 - learning_rate: 0.1000 - val_custom_mse: 0.1163 - val_custom_mae: 0.2360\n",
            "Epoch 18/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0498 - mae: 0.1279 - mse: 0.0498 - val_loss: 0.0278 - val_mae: 0.1054 - val_mse: 0.0278 - learning_rate: 0.1000 - val_custom_mse: 0.1149 - val_custom_mae: 0.2339\n",
            "Epoch 19/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0494 - mae: 0.1272 - mse: 0.0494 - val_loss: 0.0277 - val_mae: 0.1050 - val_mse: 0.0277 - learning_rate: 0.1000 - val_custom_mse: 0.1156 - val_custom_mae: 0.2349\n",
            "Epoch 20/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0490 - mae: 0.1265 - mse: 0.0490 - val_loss: 0.0273 - val_mae: 0.1040 - val_mse: 0.0273 - learning_rate: 0.1000 - val_custom_mse: 0.1152 - val_custom_mae: 0.2343\n",
            "Epoch 21/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0487 - mae: 0.1260 - mse: 0.0487 - val_loss: 0.0269 - val_mae: 0.1032 - val_mse: 0.0269 - learning_rate: 0.1000 - val_custom_mse: 0.1148 - val_custom_mae: 0.2338\n",
            "Epoch 22/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0485 - mae: 0.1255 - mse: 0.0485 - val_loss: 0.0268 - val_mae: 0.1027 - val_mse: 0.0268 - learning_rate: 0.1000 - val_custom_mse: 0.1151 - val_custom_mae: 0.2343\n",
            "Epoch 23/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0483 - mae: 0.1251 - mse: 0.0483 - val_loss: 0.0265 - val_mae: 0.1021 - val_mse: 0.0265 - learning_rate: 0.1000 - val_custom_mse: 0.1148 - val_custom_mae: 0.2339\n",
            "Epoch 24/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0481 - mae: 0.1247 - mse: 0.0481 - val_loss: 0.0262 - val_mae: 0.1014 - val_mse: 0.0262 - learning_rate: 0.1000 - val_custom_mse: 0.1142 - val_custom_mae: 0.2330\n",
            "Epoch 25/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0479 - mae: 0.1243 - mse: 0.0479 - val_loss: 0.0261 - val_mae: 0.1012 - val_mse: 0.0261 - learning_rate: 0.1000 - val_custom_mse: 0.1146 - val_custom_mae: 0.2337\n",
            "Epoch 26/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0477 - mae: 0.1240 - mse: 0.0477 - val_loss: 0.0259 - val_mae: 0.1006 - val_mse: 0.0259 - learning_rate: 0.1000 - val_custom_mse: 0.1142 - val_custom_mae: 0.2330\n",
            "Epoch 27/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0476 - mae: 0.1237 - mse: 0.0476 - val_loss: 0.0257 - val_mae: 0.1002 - val_mse: 0.0257 - learning_rate: 0.1000 - val_custom_mse: 0.1140 - val_custom_mae: 0.2328\n",
            "Epoch 28/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0474 - mae: 0.1234 - mse: 0.0474 - val_loss: 0.0257 - val_mae: 0.1001 - val_mse: 0.0257 - learning_rate: 0.1000 - val_custom_mse: 0.1143 - val_custom_mae: 0.2332\n",
            "Epoch 29/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0473 - mae: 0.1232 - mse: 0.0473 - val_loss: 0.0255 - val_mae: 0.0996 - val_mse: 0.0255 - learning_rate: 0.1000 - val_custom_mse: 0.1139 - val_custom_mae: 0.2326\n",
            "Epoch 30/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0472 - mae: 0.1230 - mse: 0.0472 - val_loss: 0.0254 - val_mae: 0.0993 - val_mse: 0.0254 - learning_rate: 0.1000 - val_custom_mse: 0.1137 - val_custom_mae: 0.2323\n",
            "Epoch 31/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0470 - mae: 0.1227 - mse: 0.0470 - val_loss: 0.0254 - val_mae: 0.0991 - val_mse: 0.0254 - learning_rate: 0.1000 - val_custom_mse: 0.1141 - val_custom_mae: 0.2330\n",
            "Epoch 32/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0469 - mae: 0.1225 - mse: 0.0469 - val_loss: 0.0253 - val_mae: 0.0989 - val_mse: 0.0253 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2325\n",
            "Epoch 33/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0469 - mae: 0.1224 - mse: 0.0469 - val_loss: 0.0251 - val_mae: 0.0987 - val_mse: 0.0251 - learning_rate: 0.1000 - val_custom_mse: 0.1133 - val_custom_mae: 0.2319\n",
            "Epoch 34/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0467 - mae: 0.1222 - mse: 0.0467 - val_loss: 0.0252 - val_mae: 0.0985 - val_mse: 0.0252 - learning_rate: 0.1000 - val_custom_mse: 0.1140 - val_custom_mae: 0.2328\n",
            "Epoch 35/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0467 - mae: 0.1220 - mse: 0.0467 - val_loss: 0.0250 - val_mae: 0.0983 - val_mse: 0.0250 - learning_rate: 0.1000 - val_custom_mse: 0.1132 - val_custom_mae: 0.2317\n",
            "Epoch 36/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0466 - mae: 0.1219 - mse: 0.0466 - val_loss: 0.0249 - val_mae: 0.0980 - val_mse: 0.0249 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2319\n",
            "Epoch 37/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0466 - mae: 0.1218 - mse: 0.0466 - val_loss: 0.0250 - val_mae: 0.0980 - val_mse: 0.0250 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2320\n",
            "Epoch 38/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0465 - mae: 0.1217 - mse: 0.0465 - val_loss: 0.0249 - val_mae: 0.0978 - val_mse: 0.0249 - learning_rate: 0.1000 - val_custom_mse: 0.1134 - val_custom_mae: 0.2320\n",
            "Epoch 39/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0464 - mae: 0.1215 - mse: 0.0464 - val_loss: 0.0248 - val_mae: 0.0977 - val_mse: 0.0248 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2316\n",
            "Epoch 40/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0463 - mae: 0.1214 - mse: 0.0463 - val_loss: 0.0249 - val_mae: 0.0978 - val_mse: 0.0249 - learning_rate: 0.1000 - val_custom_mse: 0.1138 - val_custom_mae: 0.2327\n",
            "Epoch 41/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0463 - mae: 0.1213 - mse: 0.0463 - val_loss: 0.0248 - val_mae: 0.0976 - val_mse: 0.0248 - learning_rate: 0.1000 - val_custom_mse: 0.1136 - val_custom_mae: 0.2322\n",
            "Epoch 42/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0462 - mae: 0.1212 - mse: 0.0462 - val_loss: 0.0247 - val_mae: 0.0974 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.1133 - val_custom_mae: 0.2319\n",
            "Epoch 43/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0461 - mae: 0.1211 - mse: 0.0461 - val_loss: 0.0247 - val_mae: 0.0973 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.1135 - val_custom_mae: 0.2321\n",
            "Epoch 44/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0461 - mae: 0.1210 - mse: 0.0461 - val_loss: 0.0246 - val_mae: 0.0972 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2315\n",
            "Epoch 45/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0460 - mae: 0.1209 - mse: 0.0460 - val_loss: 0.0247 - val_mae: 0.0971 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.1132 - val_custom_mae: 0.2317\n",
            "Epoch 46/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0460 - mae: 0.1208 - mse: 0.0460 - val_loss: 0.0246 - val_mae: 0.0970 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2315\n",
            "Epoch 47/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0460 - mae: 0.1207 - mse: 0.0460 - val_loss: 0.0246 - val_mae: 0.0971 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2317\n",
            "Epoch 48/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0459 - mae: 0.1207 - mse: 0.0459 - val_loss: 0.0246 - val_mae: 0.0971 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1133 - val_custom_mae: 0.2320\n",
            "Epoch 49/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0458 - mae: 0.1206 - mse: 0.0458 - val_loss: 0.0246 - val_mae: 0.0971 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1133 - val_custom_mae: 0.2320\n",
            "Epoch 50/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0458 - mae: 0.1205 - mse: 0.0458 - val_loss: 0.0245 - val_mae: 0.0968 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 51/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0458 - mae: 0.1205 - mse: 0.0458 - val_loss: 0.0246 - val_mae: 0.0968 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1132 - val_custom_mae: 0.2318\n",
            "Epoch 52/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0458 - mae: 0.1204 - mse: 0.0458 - val_loss: 0.0245 - val_mae: 0.0968 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2316\n",
            "Epoch 53/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0457 - mae: 0.1204 - mse: 0.0457 - val_loss: 0.0245 - val_mae: 0.0967 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.1130 - val_custom_mae: 0.2315\n",
            "Epoch 54/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0457 - mae: 0.1203 - mse: 0.0457 - val_loss: 0.0244 - val_mae: 0.0966 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2312\n",
            "Epoch 55/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0457 - mae: 0.1202 - mse: 0.0457 - val_loss: 0.0244 - val_mae: 0.0965 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2314\n",
            "Epoch 56/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0457 - mae: 0.1202 - mse: 0.0457 - val_loss: 0.0245 - val_mae: 0.0965 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.1131 - val_custom_mae: 0.2316\n",
            "Epoch 57/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0456 - mae: 0.1201 - mse: 0.0456 - val_loss: 0.0245 - val_mae: 0.0965 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2312\n",
            "Epoch 58/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0456 - mae: 0.1201 - mse: 0.0456 - val_loss: 0.0245 - val_mae: 0.0966 - val_mse: 0.0245 - learning_rate: 0.1000 - val_custom_mse: 0.1129 - val_custom_mae: 0.2315\n",
            "Epoch 59/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0456 - mae: 0.1200 - mse: 0.0456 - val_loss: 0.0244 - val_mae: 0.0965 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 60/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0455 - mae: 0.1200 - mse: 0.0455 - val_loss: 0.0244 - val_mae: 0.0965 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1127 - val_custom_mae: 0.2312\n",
            "Epoch 61/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0455 - mae: 0.1200 - mse: 0.0455 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1126 - val_custom_mae: 0.2310\n",
            "Epoch 62/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0455 - mae: 0.1199 - mse: 0.0455 - val_loss: 0.0244 - val_mae: 0.0964 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1125 - val_custom_mae: 0.2309\n",
            "Epoch 63/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0455 - mae: 0.1199 - mse: 0.0455 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1125 - val_custom_mae: 0.2309\n",
            "Epoch 64/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0454 - mae: 0.1199 - mse: 0.0454 - val_loss: 0.0244 - val_mae: 0.0964 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1130 - val_custom_mae: 0.2315\n",
            "Epoch 65/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0454 - mae: 0.1198 - mse: 0.0454 - val_loss: 0.0243 - val_mae: 0.0962 - val_mse: 0.0243 - learning_rate: 0.1000 - val_custom_mse: 0.1125 - val_custom_mae: 0.2308\n",
            "Epoch 66/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0453 - mae: 0.1198 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0964 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1130 - val_custom_mae: 0.2314\n",
            "Epoch 67/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1198 - mse: 0.0453 - val_loss: 0.0243 - val_mae: 0.0963 - val_mse: 0.0243 - learning_rate: 0.1000 - val_custom_mse: 0.1122 - val_custom_mae: 0.2304\n",
            "Epoch 68/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0453 - mae: 0.1197 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0964 - val_mse: 0.0244 - learning_rate: 0.1000 - val_custom_mse: 0.1130 - val_custom_mae: 0.2316\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0454 - mae: 0.1197 - mse: 0.0454 - val_loss: 0.0243 - val_mae: 0.0961 - val_mse: 0.0243 - learning_rate: 0.1000 - val_custom_mse: 0.1123 - val_custom_mae: 0.2306\n",
            "Epoch 70/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0962 - val_mse: 0.0244 - learning_rate: 0.0200 - val_custom_mse: 0.1127 - val_custom_mae: 0.2311\n",
            "Epoch 71/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.0200 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 72/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0962 - val_mse: 0.0244 - learning_rate: 0.0200 - val_custom_mse: 0.1128 - val_custom_mae: 0.2312\n",
            "Epoch 73/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.0200 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 74/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.0200 - val_custom_mse: 0.1129 - val_custom_mae: 0.2313\n",
            "Epoch 75/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.0200 - val_custom_mse: 0.1129 - val_custom_mae: 0.2314\n",
            "Epoch 76/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0243 - val_mae: 0.0962 - val_mse: 0.0243 - learning_rate: 0.0200 - val_custom_mse: 0.1126 - val_custom_mae: 0.2310\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0962 - val_mse: 0.0244 - learning_rate: 0.0200 - val_custom_mse: 0.1127 - val_custom_mae: 0.2311\n",
            "Epoch 78/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.0040 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 79/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.0040 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 80/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0962 - val_mse: 0.0244 - learning_rate: 0.0040 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 81/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0962 - val_mse: 0.0244 - learning_rate: 0.0040 - val_custom_mse: 0.1128 - val_custom_mae: 0.2312\n",
            "Epoch 82/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1195 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0962 - val_mse: 0.0244 - learning_rate: 0.0040 - val_custom_mse: 0.1127 - val_custom_mae: 0.2312\n",
            "Epoch 83/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0452 - mae: 0.1195 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.0040 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 84/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.0040 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1195 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 0.0040 - val_custom_mse: 0.1128 - val_custom_mae: 0.2312\n",
            "Epoch 86/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 87/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 88/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1129 - val_custom_mae: 0.2313\n",
            "Epoch 89/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1195 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1129 - val_custom_mae: 0.2313\n",
            "Epoch 90/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1195 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 91/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1129 - val_custom_mae: 0.2313\n",
            "Epoch 92/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 94/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 95/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 96/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 97/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0453 - mae: 0.1196 - mse: 0.0453 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 98/100\n",
            "1040/1040 - 9s - 8ms/step - loss: 0.0452 - mae: 0.1195 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Epoch 99/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0452 - mae: 0.1195 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1129 - val_custom_mae: 0.2313\n",
            "Epoch 100/100\n",
            "1040/1040 - 8s - 8ms/step - loss: 0.0452 - mae: 0.1196 - mse: 0.0452 - val_loss: 0.0244 - val_mae: 0.0963 - val_mse: 0.0244 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1128 - val_custom_mae: 0.2313\n",
            "Running experiment: horizon=192, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_20', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1037/1037 - 12s - 11ms/step - loss: 0.2948 - mae: 0.3227 - mse: 0.2948 - val_loss: 0.1311 - val_mae: 0.2594 - val_mse: 0.1311 - learning_rate: 0.1000 - val_custom_mse: 0.2162 - val_custom_mae: 0.3303\n",
            "Epoch 2/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.1895 - mae: 0.2626 - mse: 0.1895 - val_loss: 0.0900 - val_mae: 0.2109 - val_mse: 0.0900 - learning_rate: 0.1000 - val_custom_mse: 0.1904 - val_custom_mae: 0.3101\n",
            "Epoch 3/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.1266 - mae: 0.2131 - mse: 0.1266 - val_loss: 0.0709 - val_mae: 0.1811 - val_mse: 0.0709 - learning_rate: 0.1000 - val_custom_mse: 0.1756 - val_custom_mae: 0.2960\n",
            "Epoch 4/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.1017 - mae: 0.1853 - mse: 0.1017 - val_loss: 0.0618 - val_mae: 0.1644 - val_mse: 0.0618 - learning_rate: 0.1000 - val_custom_mse: 0.1690 - val_custom_mae: 0.2889\n",
            "Epoch 5/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0898 - mae: 0.1684 - mse: 0.0898 - val_loss: 0.0560 - val_mae: 0.1532 - val_mse: 0.0560 - learning_rate: 0.1000 - val_custom_mse: 0.1647 - val_custom_mae: 0.2840\n",
            "Epoch 6/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0825 - mae: 0.1565 - mse: 0.0825 - val_loss: 0.0520 - val_mae: 0.1446 - val_mse: 0.0520 - learning_rate: 0.1000 - val_custom_mse: 0.1627 - val_custom_mae: 0.2817\n",
            "Epoch 7/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0774 - mae: 0.1475 - mse: 0.0774 - val_loss: 0.0489 - val_mae: 0.1379 - val_mse: 0.0489 - learning_rate: 0.1000 - val_custom_mse: 0.1609 - val_custom_mae: 0.2796\n",
            "Epoch 8/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0735 - mae: 0.1404 - mse: 0.0735 - val_loss: 0.0464 - val_mae: 0.1322 - val_mse: 0.0464 - learning_rate: 0.1000 - val_custom_mse: 0.1592 - val_custom_mae: 0.2775\n",
            "Epoch 9/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0704 - mae: 0.1345 - mse: 0.0704 - val_loss: 0.0444 - val_mae: 0.1280 - val_mse: 0.0444 - learning_rate: 0.1000 - val_custom_mse: 0.1573 - val_custom_mae: 0.2748\n",
            "Epoch 10/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0680 - mae: 0.1296 - mse: 0.0680 - val_loss: 0.0426 - val_mae: 0.1235 - val_mse: 0.0426 - learning_rate: 0.1000 - val_custom_mse: 0.1566 - val_custom_mae: 0.2739\n",
            "Epoch 11/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0658 - mae: 0.1252 - mse: 0.0658 - val_loss: 0.0411 - val_mae: 0.1192 - val_mse: 0.0411 - learning_rate: 0.1000 - val_custom_mse: 0.1563 - val_custom_mae: 0.2737\n",
            "Epoch 12/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0641 - mae: 0.1213 - mse: 0.0641 - val_loss: 0.0398 - val_mae: 0.1157 - val_mse: 0.0398 - learning_rate: 0.1000 - val_custom_mse: 0.1559 - val_custom_mae: 0.2732\n",
            "Epoch 13/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0625 - mae: 0.1178 - mse: 0.0625 - val_loss: 0.0387 - val_mae: 0.1125 - val_mse: 0.0387 - learning_rate: 0.1000 - val_custom_mse: 0.1558 - val_custom_mae: 0.2732\n",
            "Epoch 14/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0612 - mae: 0.1147 - mse: 0.0612 - val_loss: 0.0376 - val_mae: 0.1095 - val_mse: 0.0376 - learning_rate: 0.1000 - val_custom_mse: 0.1553 - val_custom_mae: 0.2725\n",
            "Epoch 15/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0600 - mae: 0.1117 - mse: 0.0600 - val_loss: 0.0365 - val_mae: 0.1070 - val_mse: 0.0365 - learning_rate: 0.1000 - val_custom_mse: 0.1543 - val_custom_mae: 0.2711\n",
            "Epoch 16/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0589 - mae: 0.1090 - mse: 0.0589 - val_loss: 0.0357 - val_mae: 0.1042 - val_mse: 0.0357 - learning_rate: 0.1000 - val_custom_mse: 0.1542 - val_custom_mae: 0.2711\n",
            "Epoch 17/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0579 - mae: 0.1064 - mse: 0.0579 - val_loss: 0.0350 - val_mae: 0.1027 - val_mse: 0.0350 - learning_rate: 0.1000 - val_custom_mse: 0.1535 - val_custom_mae: 0.2700\n",
            "Epoch 18/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0571 - mae: 0.1041 - mse: 0.0571 - val_loss: 0.0342 - val_mae: 0.0993 - val_mse: 0.0342 - learning_rate: 0.1000 - val_custom_mse: 0.1540 - val_custom_mae: 0.2708\n",
            "Epoch 19/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0563 - mae: 0.1017 - mse: 0.0563 - val_loss: 0.0336 - val_mae: 0.0974 - val_mse: 0.0336 - learning_rate: 0.1000 - val_custom_mse: 0.1534 - val_custom_mae: 0.2701\n",
            "Epoch 20/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0556 - mae: 0.0997 - mse: 0.0556 - val_loss: 0.0330 - val_mae: 0.0961 - val_mse: 0.0330 - learning_rate: 0.1000 - val_custom_mse: 0.1530 - val_custom_mae: 0.2693\n",
            "Epoch 21/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0550 - mae: 0.0977 - mse: 0.0550 - val_loss: 0.0325 - val_mae: 0.0942 - val_mse: 0.0325 - learning_rate: 0.1000 - val_custom_mse: 0.1528 - val_custom_mae: 0.2692\n",
            "Epoch 22/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0544 - mae: 0.0958 - mse: 0.0544 - val_loss: 0.0320 - val_mae: 0.0930 - val_mse: 0.0320 - learning_rate: 0.1000 - val_custom_mse: 0.1526 - val_custom_mae: 0.2688\n",
            "Epoch 23/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0538 - mae: 0.0940 - mse: 0.0538 - val_loss: 0.0315 - val_mae: 0.0904 - val_mse: 0.0315 - learning_rate: 0.1000 - val_custom_mse: 0.1527 - val_custom_mae: 0.2690\n",
            "Epoch 24/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0533 - mae: 0.0923 - mse: 0.0533 - val_loss: 0.0310 - val_mae: 0.0882 - val_mse: 0.0310 - learning_rate: 0.1000 - val_custom_mse: 0.1528 - val_custom_mae: 0.2692\n",
            "Epoch 25/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0529 - mae: 0.0907 - mse: 0.0529 - val_loss: 0.0309 - val_mae: 0.0891 - val_mse: 0.0309 - learning_rate: 0.1000 - val_custom_mse: 0.1522 - val_custom_mae: 0.2681\n",
            "Epoch 26/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0524 - mae: 0.0892 - mse: 0.0524 - val_loss: 0.0303 - val_mae: 0.0857 - val_mse: 0.0303 - learning_rate: 0.1000 - val_custom_mse: 0.1524 - val_custom_mae: 0.2687\n",
            "Epoch 27/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0521 - mae: 0.0878 - mse: 0.0521 - val_loss: 0.0299 - val_mae: 0.0834 - val_mse: 0.0299 - learning_rate: 0.1000 - val_custom_mse: 0.1527 - val_custom_mae: 0.2691\n",
            "Epoch 28/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0517 - mae: 0.0864 - mse: 0.0517 - val_loss: 0.0296 - val_mae: 0.0825 - val_mse: 0.0296 - learning_rate: 0.1000 - val_custom_mse: 0.1524 - val_custom_mae: 0.2687\n",
            "Epoch 29/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0514 - mae: 0.0852 - mse: 0.0514 - val_loss: 0.0293 - val_mae: 0.0813 - val_mse: 0.0293 - learning_rate: 0.1000 - val_custom_mse: 0.1522 - val_custom_mae: 0.2685\n",
            "Epoch 30/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0511 - mae: 0.0838 - mse: 0.0511 - val_loss: 0.0290 - val_mae: 0.0797 - val_mse: 0.0290 - learning_rate: 0.1000 - val_custom_mse: 0.1523 - val_custom_mae: 0.2686\n",
            "Epoch 31/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0508 - mae: 0.0828 - mse: 0.0508 - val_loss: 0.0288 - val_mae: 0.0784 - val_mse: 0.0288 - learning_rate: 0.1000 - val_custom_mse: 0.1523 - val_custom_mae: 0.2687\n",
            "Epoch 32/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0505 - mae: 0.0817 - mse: 0.0505 - val_loss: 0.0286 - val_mae: 0.0772 - val_mse: 0.0286 - learning_rate: 0.1000 - val_custom_mse: 0.1529 - val_custom_mae: 0.2697\n",
            "Epoch 33/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0502 - mae: 0.0804 - mse: 0.0502 - val_loss: 0.0284 - val_mae: 0.0784 - val_mse: 0.0284 - learning_rate: 0.1000 - val_custom_mse: 0.1517 - val_custom_mae: 0.2677\n",
            "Epoch 34/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0500 - mae: 0.0796 - mse: 0.0500 - val_loss: 0.0281 - val_mae: 0.0763 - val_mse: 0.0281 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2679\n",
            "Epoch 35/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0498 - mae: 0.0786 - mse: 0.0498 - val_loss: 0.0279 - val_mae: 0.0740 - val_mse: 0.0279 - learning_rate: 0.1000 - val_custom_mse: 0.1522 - val_custom_mae: 0.2686\n",
            "Epoch 36/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0496 - mae: 0.0777 - mse: 0.0496 - val_loss: 0.0277 - val_mae: 0.0729 - val_mse: 0.0277 - learning_rate: 0.1000 - val_custom_mse: 0.1523 - val_custom_mae: 0.2687\n",
            "Epoch 37/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0494 - mae: 0.0769 - mse: 0.0494 - val_loss: 0.0276 - val_mae: 0.0721 - val_mse: 0.0276 - learning_rate: 0.1000 - val_custom_mse: 0.1523 - val_custom_mae: 0.2688\n",
            "Epoch 38/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0492 - mae: 0.0760 - mse: 0.0492 - val_loss: 0.0274 - val_mae: 0.0712 - val_mse: 0.0274 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2686\n",
            "Epoch 39/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0490 - mae: 0.0753 - mse: 0.0490 - val_loss: 0.0272 - val_mae: 0.0710 - val_mse: 0.0272 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2680\n",
            "Epoch 40/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0489 - mae: 0.0746 - mse: 0.0489 - val_loss: 0.0271 - val_mae: 0.0696 - val_mse: 0.0271 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2686\n",
            "Epoch 41/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0487 - mae: 0.0740 - mse: 0.0487 - val_loss: 0.0270 - val_mae: 0.0713 - val_mse: 0.0270 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2674\n",
            "Epoch 42/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0486 - mae: 0.0732 - mse: 0.0486 - val_loss: 0.0269 - val_mae: 0.0703 - val_mse: 0.0269 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2674\n",
            "Epoch 43/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0484 - mae: 0.0726 - mse: 0.0484 - val_loss: 0.0267 - val_mae: 0.0687 - val_mse: 0.0267 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2676\n",
            "Epoch 44/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0483 - mae: 0.0720 - mse: 0.0483 - val_loss: 0.0266 - val_mae: 0.0682 - val_mse: 0.0266 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2675\n",
            "Epoch 45/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0482 - mae: 0.0714 - mse: 0.0482 - val_loss: 0.0264 - val_mae: 0.0663 - val_mse: 0.0264 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2682\n",
            "Epoch 46/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0481 - mae: 0.0707 - mse: 0.0481 - val_loss: 0.0263 - val_mae: 0.0663 - val_mse: 0.0263 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2676\n",
            "Epoch 47/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0479 - mae: 0.0701 - mse: 0.0479 - val_loss: 0.0262 - val_mae: 0.0660 - val_mse: 0.0262 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2675\n",
            "Epoch 48/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0479 - mae: 0.0698 - mse: 0.0479 - val_loss: 0.0262 - val_mae: 0.0657 - val_mse: 0.0262 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2674\n",
            "Epoch 49/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0477 - mae: 0.0690 - mse: 0.0477 - val_loss: 0.0261 - val_mae: 0.0642 - val_mse: 0.0261 - learning_rate: 0.1000 - val_custom_mse: 0.1520 - val_custom_mae: 0.2685\n",
            "Epoch 50/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0476 - mae: 0.0687 - mse: 0.0476 - val_loss: 0.0260 - val_mae: 0.0646 - val_mse: 0.0260 - learning_rate: 0.1000 - val_custom_mse: 0.1512 - val_custom_mae: 0.2674\n",
            "Epoch 51/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0476 - mae: 0.0682 - mse: 0.0476 - val_loss: 0.0259 - val_mae: 0.0631 - val_mse: 0.0259 - learning_rate: 0.1000 - val_custom_mse: 0.1517 - val_custom_mae: 0.2680\n",
            "Epoch 52/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0475 - mae: 0.0679 - mse: 0.0475 - val_loss: 0.0258 - val_mae: 0.0633 - val_mse: 0.0258 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2675\n",
            "Epoch 53/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0474 - mae: 0.0675 - mse: 0.0474 - val_loss: 0.0257 - val_mae: 0.0622 - val_mse: 0.0257 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2678\n",
            "Epoch 54/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0473 - mae: 0.0669 - mse: 0.0473 - val_loss: 0.0257 - val_mae: 0.0638 - val_mse: 0.0257 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2671\n",
            "Epoch 55/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0472 - mae: 0.0665 - mse: 0.0472 - val_loss: 0.0256 - val_mae: 0.0616 - val_mse: 0.0256 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2676\n",
            "Epoch 56/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0472 - mae: 0.0663 - mse: 0.0472 - val_loss: 0.0256 - val_mae: 0.0627 - val_mse: 0.0256 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2671\n",
            "Epoch 57/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0471 - mae: 0.0660 - mse: 0.0471 - val_loss: 0.0255 - val_mae: 0.0606 - val_mse: 0.0255 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2678\n",
            "Epoch 58/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0471 - mae: 0.0656 - mse: 0.0471 - val_loss: 0.0254 - val_mae: 0.0615 - val_mse: 0.0254 - learning_rate: 0.1000 - val_custom_mse: 0.1510 - val_custom_mae: 0.2671\n",
            "Epoch 59/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0470 - mae: 0.0653 - mse: 0.0470 - val_loss: 0.0254 - val_mae: 0.0619 - val_mse: 0.0254 - learning_rate: 0.1000 - val_custom_mse: 0.1510 - val_custom_mae: 0.2670\n",
            "Epoch 60/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0469 - mae: 0.0649 - mse: 0.0469 - val_loss: 0.0253 - val_mae: 0.0597 - val_mse: 0.0253 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2675\n",
            "Epoch 61/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0469 - mae: 0.0646 - mse: 0.0469 - val_loss: 0.0253 - val_mae: 0.0592 - val_mse: 0.0253 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2679\n",
            "Epoch 62/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0468 - mae: 0.0644 - mse: 0.0468 - val_loss: 0.0253 - val_mae: 0.0610 - val_mse: 0.0253 - learning_rate: 0.1000 - val_custom_mse: 0.1510 - val_custom_mae: 0.2669\n",
            "Epoch 63/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0468 - mae: 0.0642 - mse: 0.0468 - val_loss: 0.0253 - val_mae: 0.0617 - val_mse: 0.0253 - learning_rate: 0.1000 - val_custom_mse: 0.1508 - val_custom_mae: 0.2667\n",
            "Epoch 64/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0467 - mae: 0.0637 - mse: 0.0467 - val_loss: 0.0252 - val_mae: 0.0588 - val_mse: 0.0252 - learning_rate: 0.1000 - val_custom_mse: 0.1512 - val_custom_mae: 0.2673\n",
            "Epoch 65/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0467 - mae: 0.0635 - mse: 0.0467 - val_loss: 0.0251 - val_mae: 0.0597 - val_mse: 0.0251 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2669\n",
            "Epoch 66/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0466 - mae: 0.0631 - mse: 0.0466 - val_loss: 0.0251 - val_mae: 0.0577 - val_mse: 0.0251 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2676\n",
            "Epoch 67/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0466 - mae: 0.0629 - mse: 0.0466 - val_loss: 0.0250 - val_mae: 0.0592 - val_mse: 0.0250 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2669\n",
            "Epoch 68/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0465 - mae: 0.0627 - mse: 0.0465 - val_loss: 0.0250 - val_mae: 0.0572 - val_mse: 0.0250 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2679\n",
            "Epoch 69/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0465 - mae: 0.0626 - mse: 0.0465 - val_loss: 0.0251 - val_mae: 0.0601 - val_mse: 0.0251 - learning_rate: 0.1000 - val_custom_mse: 0.1508 - val_custom_mae: 0.2667\n",
            "Epoch 70/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0465 - mae: 0.0623 - mse: 0.0465 - val_loss: 0.0249 - val_mae: 0.0571 - val_mse: 0.0249 - learning_rate: 0.1000 - val_custom_mse: 0.1510 - val_custom_mae: 0.2673\n",
            "Epoch 71/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0464 - mae: 0.0621 - mse: 0.0464 - val_loss: 0.0249 - val_mae: 0.0564 - val_mse: 0.0249 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2676\n",
            "Epoch 72/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0464 - mae: 0.0618 - mse: 0.0464 - val_loss: 0.0249 - val_mae: 0.0565 - val_mse: 0.0249 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2673\n",
            "Epoch 73/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0464 - mae: 0.0615 - mse: 0.0464 - val_loss: 0.0248 - val_mae: 0.0562 - val_mse: 0.0248 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2674\n",
            "Epoch 74/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0464 - mae: 0.0615 - mse: 0.0464 - val_loss: 0.0248 - val_mae: 0.0559 - val_mse: 0.0248 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2678\n",
            "Epoch 75/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0463 - mae: 0.0612 - mse: 0.0463 - val_loss: 0.0248 - val_mae: 0.0572 - val_mse: 0.0248 - learning_rate: 0.1000 - val_custom_mse: 0.1508 - val_custom_mae: 0.2669\n",
            "Epoch 76/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0463 - mae: 0.0611 - mse: 0.0463 - val_loss: 0.0248 - val_mae: 0.0571 - val_mse: 0.0248 - learning_rate: 0.1000 - val_custom_mse: 0.1508 - val_custom_mae: 0.2669\n",
            "Epoch 77/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0462 - mae: 0.0609 - mse: 0.0462 - val_loss: 0.0247 - val_mae: 0.0557 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.1510 - val_custom_mae: 0.2672\n",
            "Epoch 78/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0462 - mae: 0.0609 - mse: 0.0462 - val_loss: 0.0247 - val_mae: 0.0565 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.1508 - val_custom_mae: 0.2669\n",
            "Epoch 79/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0462 - mae: 0.0606 - mse: 0.0462 - val_loss: 0.0247 - val_mae: 0.0556 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2671\n",
            "Epoch 80/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0462 - mae: 0.0603 - mse: 0.0462 - val_loss: 0.0247 - val_mae: 0.0553 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2670\n",
            "Epoch 81/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0462 - mae: 0.0605 - mse: 0.0462 - val_loss: 0.0247 - val_mae: 0.0555 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2670\n",
            "Epoch 82/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0461 - mae: 0.0600 - mse: 0.0461 - val_loss: 0.0247 - val_mae: 0.0566 - val_mse: 0.0247 - learning_rate: 0.1000 - val_custom_mse: 0.1507 - val_custom_mae: 0.2667\n",
            "Epoch 83/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0461 - mae: 0.0600 - mse: 0.0461 - val_loss: 0.0246 - val_mae: 0.0548 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2671\n",
            "Epoch 84/100\n",
            "1037/1037 - 10s - 10ms/step - loss: 0.0461 - mae: 0.0598 - mse: 0.0461 - val_loss: 0.0246 - val_mae: 0.0543 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2678\n",
            "Epoch 85/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0461 - mae: 0.0598 - mse: 0.0461 - val_loss: 0.0246 - val_mae: 0.0541 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2677\n",
            "Epoch 86/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0461 - mae: 0.0598 - mse: 0.0461 - val_loss: 0.0246 - val_mae: 0.0560 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1508 - val_custom_mae: 0.2668\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0460 - mae: 0.0595 - mse: 0.0460 - val_loss: 0.0246 - val_mae: 0.0570 - val_mse: 0.0246 - learning_rate: 0.1000 - val_custom_mse: 0.1507 - val_custom_mae: 0.2665\n",
            "Epoch 88/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0460 - mae: 0.0585 - mse: 0.0460 - val_loss: 0.0246 - val_mae: 0.0538 - val_mse: 0.0246 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2674\n",
            "Epoch 89/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0584 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0539 - val_mse: 0.0245 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2674\n",
            "Epoch 90/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0584 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0538 - val_mse: 0.0245 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2674\n",
            "Epoch 91/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0459 - mae: 0.0584 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0538 - val_mse: 0.0245 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2674\n",
            "Epoch 92/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0459 - mae: 0.0584 - mse: 0.0459 - val_loss: 0.0246 - val_mae: 0.0539 - val_mse: 0.0246 - learning_rate: 0.0200 - val_custom_mse: 0.1514 - val_custom_mae: 0.2678\n",
            "Epoch 93/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0583 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0537 - val_mse: 0.0245 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2674\n",
            "Epoch 94/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0583 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0536 - val_mse: 0.0245 - learning_rate: 0.0200 - val_custom_mse: 0.1512 - val_custom_mae: 0.2675\n",
            "Epoch 95/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0583 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0536 - val_mse: 0.0245 - learning_rate: 0.0200 - val_custom_mse: 0.1512 - val_custom_mae: 0.2676\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0583 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0536 - val_mse: 0.0245 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2675\n",
            "Epoch 97/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0580 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0535 - val_mse: 0.0245 - learning_rate: 0.0040 - val_custom_mse: 0.1512 - val_custom_mae: 0.2676\n",
            "Epoch 98/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0581 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0535 - val_mse: 0.0245 - learning_rate: 0.0040 - val_custom_mse: 0.1512 - val_custom_mae: 0.2676\n",
            "Epoch 99/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0581 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0535 - val_mse: 0.0245 - learning_rate: 0.0040 - val_custom_mse: 0.1512 - val_custom_mae: 0.2676\n",
            "Epoch 100/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0459 - mae: 0.0580 - mse: 0.0459 - val_loss: 0.0245 - val_mae: 0.0535 - val_mse: 0.0245 - learning_rate: 0.0040 - val_custom_mse: 0.1512 - val_custom_mae: 0.2676\n",
            "Running experiment: horizon=192, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_21', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1037/1037 - 13s - 12ms/step - loss: 0.2966 - mae: 0.3223 - mse: 0.2966 - val_loss: 0.1281 - val_mae: 0.2570 - val_mse: 0.1281 - learning_rate: 0.1000 - val_custom_mse: 0.2145 - val_custom_mae: 0.3294\n",
            "Epoch 2/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.1837 - mae: 0.2586 - mse: 0.1837 - val_loss: 0.0877 - val_mae: 0.2073 - val_mse: 0.0877 - learning_rate: 0.1000 - val_custom_mse: 0.1929 - val_custom_mae: 0.3118\n",
            "Epoch 3/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.1223 - mae: 0.2089 - mse: 0.1223 - val_loss: 0.0701 - val_mae: 0.1794 - val_mse: 0.0701 - learning_rate: 0.1000 - val_custom_mse: 0.1791 - val_custom_mae: 0.2991\n",
            "Epoch 4/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.1010 - mae: 0.1841 - mse: 0.1010 - val_loss: 0.0616 - val_mae: 0.1639 - val_mse: 0.0616 - learning_rate: 0.1000 - val_custom_mse: 0.1708 - val_custom_mae: 0.2907\n",
            "Epoch 5/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0915 - mae: 0.1701 - mse: 0.0915 - val_loss: 0.0573 - val_mae: 0.1550 - val_mse: 0.0573 - learning_rate: 0.1000 - val_custom_mse: 0.1687 - val_custom_mae: 0.2886\n",
            "Epoch 6/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0854 - mae: 0.1605 - mse: 0.0854 - val_loss: 0.0529 - val_mae: 0.1463 - val_mse: 0.0529 - learning_rate: 0.1000 - val_custom_mse: 0.1643 - val_custom_mae: 0.2836\n",
            "Epoch 7/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0811 - mae: 0.1533 - mse: 0.0811 - val_loss: 0.0498 - val_mae: 0.1397 - val_mse: 0.0498 - learning_rate: 0.1000 - val_custom_mse: 0.1616 - val_custom_mae: 0.2802\n",
            "Epoch 8/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0779 - mae: 0.1479 - mse: 0.0779 - val_loss: 0.0478 - val_mae: 0.1352 - val_mse: 0.0478 - learning_rate: 0.1000 - val_custom_mse: 0.1608 - val_custom_mae: 0.2794\n",
            "Epoch 9/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0754 - mae: 0.1435 - mse: 0.0754 - val_loss: 0.0456 - val_mae: 0.1306 - val_mse: 0.0456 - learning_rate: 0.1000 - val_custom_mse: 0.1584 - val_custom_mae: 0.2764\n",
            "Epoch 10/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0735 - mae: 0.1400 - mse: 0.0735 - val_loss: 0.0442 - val_mae: 0.1271 - val_mse: 0.0442 - learning_rate: 0.1000 - val_custom_mse: 0.1579 - val_custom_mae: 0.2758\n",
            "Epoch 11/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0719 - mae: 0.1370 - mse: 0.0719 - val_loss: 0.0430 - val_mae: 0.1245 - val_mse: 0.0430 - learning_rate: 0.1000 - val_custom_mse: 0.1577 - val_custom_mae: 0.2756\n",
            "Epoch 12/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0706 - mae: 0.1344 - mse: 0.0706 - val_loss: 0.0418 - val_mae: 0.1216 - val_mse: 0.0418 - learning_rate: 0.1000 - val_custom_mse: 0.1570 - val_custom_mae: 0.2747\n",
            "Epoch 13/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0695 - mae: 0.1321 - mse: 0.0695 - val_loss: 0.0408 - val_mae: 0.1191 - val_mse: 0.0408 - learning_rate: 0.1000 - val_custom_mse: 0.1566 - val_custom_mae: 0.2742\n",
            "Epoch 14/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0685 - mae: 0.1301 - mse: 0.0685 - val_loss: 0.0395 - val_mae: 0.1161 - val_mse: 0.0395 - learning_rate: 0.1000 - val_custom_mse: 0.1551 - val_custom_mae: 0.2722\n",
            "Epoch 15/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0677 - mae: 0.1283 - mse: 0.0677 - val_loss: 0.0386 - val_mae: 0.1141 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.1541 - val_custom_mae: 0.2710\n",
            "Epoch 16/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1266 - mse: 0.0669 - val_loss: 0.0381 - val_mae: 0.1122 - val_mse: 0.0381 - learning_rate: 0.1000 - val_custom_mse: 0.1548 - val_custom_mae: 0.2719\n",
            "Epoch 17/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0663 - mae: 0.1251 - mse: 0.0663 - val_loss: 0.0375 - val_mae: 0.1106 - val_mse: 0.0375 - learning_rate: 0.1000 - val_custom_mse: 0.1548 - val_custom_mae: 0.2719\n",
            "Epoch 18/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0658 - mae: 0.1238 - mse: 0.0658 - val_loss: 0.0369 - val_mae: 0.1090 - val_mse: 0.0369 - learning_rate: 0.1000 - val_custom_mse: 0.1543 - val_custom_mae: 0.2714\n",
            "Epoch 19/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0653 - mae: 0.1226 - mse: 0.0653 - val_loss: 0.0363 - val_mae: 0.1073 - val_mse: 0.0363 - learning_rate: 0.1000 - val_custom_mse: 0.1538 - val_custom_mae: 0.2707\n",
            "Epoch 20/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0648 - mae: 0.1214 - mse: 0.0648 - val_loss: 0.0359 - val_mae: 0.1061 - val_mse: 0.0359 - learning_rate: 0.1000 - val_custom_mse: 0.1539 - val_custom_mae: 0.2709\n",
            "Epoch 21/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0644 - mae: 0.1204 - mse: 0.0644 - val_loss: 0.0356 - val_mae: 0.1052 - val_mse: 0.0356 - learning_rate: 0.1000 - val_custom_mse: 0.1544 - val_custom_mae: 0.2713\n",
            "Epoch 22/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0641 - mae: 0.1195 - mse: 0.0641 - val_loss: 0.0352 - val_mae: 0.1043 - val_mse: 0.0352 - learning_rate: 0.1000 - val_custom_mse: 0.1541 - val_custom_mae: 0.2713\n",
            "Epoch 23/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0637 - mae: 0.1186 - mse: 0.0637 - val_loss: 0.0348 - val_mae: 0.1030 - val_mse: 0.0348 - learning_rate: 0.1000 - val_custom_mse: 0.1539 - val_custom_mae: 0.2708\n",
            "Epoch 24/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0635 - mae: 0.1178 - mse: 0.0635 - val_loss: 0.0343 - val_mae: 0.1017 - val_mse: 0.0343 - learning_rate: 0.1000 - val_custom_mse: 0.1533 - val_custom_mae: 0.2701\n",
            "Epoch 25/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0632 - mae: 0.1170 - mse: 0.0632 - val_loss: 0.0344 - val_mae: 0.1018 - val_mse: 0.0344 - learning_rate: 0.1000 - val_custom_mse: 0.1541 - val_custom_mae: 0.2710\n",
            "Epoch 26/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0629 - mae: 0.1163 - mse: 0.0629 - val_loss: 0.0339 - val_mae: 0.1008 - val_mse: 0.0339 - learning_rate: 0.1000 - val_custom_mse: 0.1534 - val_custom_mae: 0.2705\n",
            "Epoch 27/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0627 - mae: 0.1158 - mse: 0.0627 - val_loss: 0.0335 - val_mae: 0.0995 - val_mse: 0.0335 - learning_rate: 0.1000 - val_custom_mse: 0.1529 - val_custom_mae: 0.2696\n",
            "Epoch 28/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0625 - mae: 0.1153 - mse: 0.0625 - val_loss: 0.0335 - val_mae: 0.0997 - val_mse: 0.0335 - learning_rate: 0.1000 - val_custom_mse: 0.1533 - val_custom_mae: 0.2704\n",
            "Epoch 29/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0623 - mae: 0.1147 - mse: 0.0623 - val_loss: 0.0331 - val_mae: 0.0984 - val_mse: 0.0331 - learning_rate: 0.1000 - val_custom_mse: 0.1527 - val_custom_mae: 0.2696\n",
            "Epoch 30/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0621 - mae: 0.1142 - mse: 0.0621 - val_loss: 0.0329 - val_mae: 0.0977 - val_mse: 0.0329 - learning_rate: 0.1000 - val_custom_mse: 0.1527 - val_custom_mae: 0.2694\n",
            "Epoch 31/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0620 - mae: 0.1137 - mse: 0.0620 - val_loss: 0.0328 - val_mae: 0.0974 - val_mse: 0.0328 - learning_rate: 0.1000 - val_custom_mse: 0.1527 - val_custom_mae: 0.2696\n",
            "Epoch 32/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0618 - mae: 0.1133 - mse: 0.0618 - val_loss: 0.0326 - val_mae: 0.0967 - val_mse: 0.0326 - learning_rate: 0.1000 - val_custom_mse: 0.1527 - val_custom_mae: 0.2694\n",
            "Epoch 33/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0617 - mae: 0.1129 - mse: 0.0617 - val_loss: 0.0326 - val_mae: 0.0972 - val_mse: 0.0326 - learning_rate: 0.1000 - val_custom_mse: 0.1531 - val_custom_mae: 0.2701\n",
            "Epoch 34/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0616 - mae: 0.1125 - mse: 0.0616 - val_loss: 0.0322 - val_mae: 0.0957 - val_mse: 0.0322 - learning_rate: 0.1000 - val_custom_mse: 0.1523 - val_custom_mae: 0.2690\n",
            "Epoch 35/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0614 - mae: 0.1122 - mse: 0.0614 - val_loss: 0.0320 - val_mae: 0.0952 - val_mse: 0.0320 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2687\n",
            "Epoch 36/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0614 - mae: 0.1119 - mse: 0.0614 - val_loss: 0.0320 - val_mae: 0.0949 - val_mse: 0.0320 - learning_rate: 0.1000 - val_custom_mse: 0.1526 - val_custom_mae: 0.2693\n",
            "Epoch 37/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0612 - mae: 0.1116 - mse: 0.0612 - val_loss: 0.0319 - val_mae: 0.0948 - val_mse: 0.0319 - learning_rate: 0.1000 - val_custom_mse: 0.1523 - val_custom_mae: 0.2691\n",
            "Epoch 38/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0611 - mae: 0.1113 - mse: 0.0611 - val_loss: 0.0322 - val_mae: 0.0960 - val_mse: 0.0322 - learning_rate: 0.1000 - val_custom_mse: 0.1532 - val_custom_mae: 0.2702\n",
            "Epoch 39/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0611 - mae: 0.1111 - mse: 0.0611 - val_loss: 0.0320 - val_mae: 0.0951 - val_mse: 0.0320 - learning_rate: 0.1000 - val_custom_mse: 0.1532 - val_custom_mae: 0.2701\n",
            "Epoch 40/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0609 - mae: 0.1107 - mse: 0.0609 - val_loss: 0.0315 - val_mae: 0.0934 - val_mse: 0.0315 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2683\n",
            "Epoch 41/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0609 - mae: 0.1106 - mse: 0.0609 - val_loss: 0.0314 - val_mae: 0.0932 - val_mse: 0.0314 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2685\n",
            "Epoch 42/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0608 - mae: 0.1104 - mse: 0.0608 - val_loss: 0.0313 - val_mae: 0.0929 - val_mse: 0.0313 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2681\n",
            "Epoch 43/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0607 - mae: 0.1102 - mse: 0.0607 - val_loss: 0.0314 - val_mae: 0.0931 - val_mse: 0.0314 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2688\n",
            "Epoch 44/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0607 - mae: 0.1101 - mse: 0.0607 - val_loss: 0.0312 - val_mae: 0.0924 - val_mse: 0.0312 - learning_rate: 0.1000 - val_custom_mse: 0.1516 - val_custom_mae: 0.2681\n",
            "Epoch 45/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0606 - mae: 0.1098 - mse: 0.0606 - val_loss: 0.0313 - val_mae: 0.0926 - val_mse: 0.0313 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2687\n",
            "Epoch 46/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1098 - mse: 0.0605 - val_loss: 0.0312 - val_mae: 0.0924 - val_mse: 0.0312 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2685\n",
            "Epoch 47/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0605 - mae: 0.1096 - mse: 0.0605 - val_loss: 0.0313 - val_mae: 0.0927 - val_mse: 0.0313 - learning_rate: 0.1000 - val_custom_mse: 0.1523 - val_custom_mae: 0.2690\n",
            "Epoch 48/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0604 - mae: 0.1095 - mse: 0.0604 - val_loss: 0.0311 - val_mae: 0.0924 - val_mse: 0.0311 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2686\n",
            "Epoch 49/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0604 - mae: 0.1094 - mse: 0.0604 - val_loss: 0.0312 - val_mae: 0.0925 - val_mse: 0.0312 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2689\n",
            "Epoch 50/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0604 - mae: 0.1093 - mse: 0.0604 - val_loss: 0.0312 - val_mae: 0.0926 - val_mse: 0.0312 - learning_rate: 0.1000 - val_custom_mse: 0.1522 - val_custom_mae: 0.2691\n",
            "Epoch 51/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0603 - mae: 0.1091 - mse: 0.0603 - val_loss: 0.0309 - val_mae: 0.0915 - val_mse: 0.0309 - learning_rate: 0.1000 - val_custom_mse: 0.1517 - val_custom_mae: 0.2683\n",
            "Epoch 52/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0603 - mae: 0.1091 - mse: 0.0603 - val_loss: 0.0310 - val_mae: 0.0917 - val_mse: 0.0310 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2686\n",
            "Epoch 53/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0603 - mae: 0.1090 - mse: 0.0603 - val_loss: 0.0310 - val_mae: 0.0921 - val_mse: 0.0310 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2687\n",
            "Epoch 54/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0603 - mae: 0.1089 - mse: 0.0603 - val_loss: 0.0309 - val_mae: 0.0912 - val_mse: 0.0309 - learning_rate: 0.1000 - val_custom_mse: 0.1517 - val_custom_mae: 0.2680\n",
            "Epoch 55/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0602 - mae: 0.1089 - mse: 0.0602 - val_loss: 0.0310 - val_mae: 0.0916 - val_mse: 0.0310 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2686\n",
            "Epoch 56/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0601 - mae: 0.1087 - mse: 0.0601 - val_loss: 0.0309 - val_mae: 0.0915 - val_mse: 0.0309 - learning_rate: 0.1000 - val_custom_mse: 0.1516 - val_custom_mae: 0.2684\n",
            "Epoch 57/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0601 - mae: 0.1087 - mse: 0.0601 - val_loss: 0.0308 - val_mae: 0.0912 - val_mse: 0.0308 - learning_rate: 0.1000 - val_custom_mse: 0.1517 - val_custom_mae: 0.2683\n",
            "Epoch 58/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0601 - mae: 0.1087 - mse: 0.0601 - val_loss: 0.0307 - val_mae: 0.0909 - val_mse: 0.0307 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2679\n",
            "Epoch 59/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0601 - mae: 0.1086 - mse: 0.0601 - val_loss: 0.0308 - val_mae: 0.0914 - val_mse: 0.0308 - learning_rate: 0.1000 - val_custom_mse: 0.1516 - val_custom_mae: 0.2684\n",
            "Epoch 60/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0600 - mae: 0.1085 - mse: 0.0600 - val_loss: 0.0308 - val_mae: 0.0910 - val_mse: 0.0308 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2680\n",
            "Epoch 61/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0600 - mae: 0.1085 - mse: 0.0600 - val_loss: 0.0307 - val_mae: 0.0909 - val_mse: 0.0307 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 62/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0600 - mae: 0.1083 - mse: 0.0600 - val_loss: 0.0309 - val_mae: 0.0917 - val_mse: 0.0309 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2687\n",
            "Epoch 63/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0599 - mae: 0.1083 - mse: 0.0599 - val_loss: 0.0308 - val_mae: 0.0908 - val_mse: 0.0308 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2683\n",
            "Epoch 64/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0600 - mae: 0.1083 - mse: 0.0600 - val_loss: 0.0307 - val_mae: 0.0906 - val_mse: 0.0307 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2679\n",
            "Epoch 65/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0599 - mae: 0.1083 - mse: 0.0599 - val_loss: 0.0308 - val_mae: 0.0912 - val_mse: 0.0308 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2683\n",
            "Epoch 66/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0599 - mae: 0.1083 - mse: 0.0599 - val_loss: 0.0306 - val_mae: 0.0904 - val_mse: 0.0306 - learning_rate: 0.1000 - val_custom_mse: 0.1512 - val_custom_mae: 0.2677\n",
            "Epoch 67/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0599 - mae: 0.1082 - mse: 0.0599 - val_loss: 0.0308 - val_mae: 0.0912 - val_mse: 0.0308 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2686\n",
            "Epoch 68/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0599 - mae: 0.1082 - mse: 0.0599 - val_loss: 0.0306 - val_mae: 0.0903 - val_mse: 0.0306 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2677\n",
            "Epoch 69/100\n",
            "1037/1037 - 9s - 9ms/step - loss: 0.0598 - mae: 0.1081 - mse: 0.0598 - val_loss: 0.0305 - val_mae: 0.0903 - val_mse: 0.0305 - learning_rate: 0.1000 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 70/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0598 - mae: 0.1081 - mse: 0.0598 - val_loss: 0.0307 - val_mae: 0.0908 - val_mse: 0.0307 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 71/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0598 - mae: 0.1081 - mse: 0.0598 - val_loss: 0.0309 - val_mae: 0.0914 - val_mse: 0.0309 - learning_rate: 0.1000 - val_custom_mse: 0.1520 - val_custom_mae: 0.2688\n",
            "Epoch 72/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0598 - mae: 0.1081 - mse: 0.0598 - val_loss: 0.0306 - val_mae: 0.0904 - val_mse: 0.0306 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2679\n",
            "Epoch 73/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0598 - mae: 0.1081 - mse: 0.0598 - val_loss: 0.0306 - val_mae: 0.0904 - val_mse: 0.0306 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2677\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0598 - mae: 0.1081 - mse: 0.0598 - val_loss: 0.0306 - val_mae: 0.0902 - val_mse: 0.0306 - learning_rate: 0.1000 - val_custom_mse: 0.1512 - val_custom_mae: 0.2676\n",
            "Epoch 75/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0597 - mae: 0.1078 - mse: 0.0597 - val_loss: 0.0305 - val_mae: 0.0897 - val_mse: 0.0305 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2675\n",
            "Epoch 76/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0597 - mae: 0.1078 - mse: 0.0597 - val_loss: 0.0305 - val_mae: 0.0897 - val_mse: 0.0305 - learning_rate: 0.0200 - val_custom_mse: 0.1510 - val_custom_mae: 0.2674\n",
            "Epoch 77/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0596 - mae: 0.1078 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0200 - val_custom_mse: 0.1510 - val_custom_mae: 0.2674\n",
            "Epoch 78/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0597 - mae: 0.1078 - mse: 0.0597 - val_loss: 0.0305 - val_mae: 0.0898 - val_mse: 0.0305 - learning_rate: 0.0200 - val_custom_mse: 0.1512 - val_custom_mae: 0.2676\n",
            "Epoch 79/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0596 - mae: 0.1078 - mse: 0.0596 - val_loss: 0.0305 - val_mae: 0.0898 - val_mse: 0.0305 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2676\n",
            "Epoch 80/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0200 - val_custom_mse: 0.1510 - val_custom_mae: 0.2674\n",
            "Epoch 81/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0597 - mae: 0.1078 - mse: 0.0597 - val_loss: 0.0305 - val_mae: 0.0898 - val_mse: 0.0305 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2676\n",
            "Epoch 82/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0305 - val_mae: 0.0898 - val_mse: 0.0305 - learning_rate: 0.0200 - val_custom_mse: 0.1512 - val_custom_mae: 0.2676\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0305 - val_mae: 0.0898 - val_mse: 0.0305 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2675\n",
            "Epoch 84/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0040 - val_custom_mse: 0.1509 - val_custom_mae: 0.2673\n",
            "Epoch 85/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0040 - val_custom_mse: 0.1509 - val_custom_mae: 0.2673\n",
            "Epoch 86/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0595 - mae: 0.1076 - mse: 0.0595 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0040 - val_custom_mse: 0.1508 - val_custom_mae: 0.2673\n",
            "Epoch 87/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0040 - val_custom_mse: 0.1508 - val_custom_mae: 0.2672\n",
            "Epoch 88/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0040 - val_custom_mse: 0.1509 - val_custom_mae: 0.2673\n",
            "Epoch 89/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0040 - val_custom_mse: 0.1509 - val_custom_mae: 0.2673\n",
            "Epoch 90/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0040 - val_custom_mse: 0.1508 - val_custom_mae: 0.2673\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 0.0040 - val_custom_mse: 0.1509 - val_custom_mae: 0.2673\n",
            "Epoch 92/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0596 - mae: 0.1076 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1508 - val_custom_mae: 0.2672\n",
            "Epoch 93/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1508 - val_custom_mae: 0.2672\n",
            "Epoch 94/100\n",
            "1037/1037 - 9s - 9ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1508 - val_custom_mae: 0.2672\n",
            "Epoch 95/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1508 - val_custom_mae: 0.2672\n",
            "Epoch 96/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0597 - mae: 0.1077 - mse: 0.0597 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1507 - val_custom_mae: 0.2672\n",
            "Epoch 97/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1508 - val_custom_mae: 0.2672\n",
            "Epoch 98/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1508 - val_custom_mae: 0.2672\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1508 - val_custom_mae: 0.2672\n",
            "Epoch 100/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0596 - mae: 0.1077 - mse: 0.0596 - val_loss: 0.0304 - val_mae: 0.0897 - val_mse: 0.0304 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1508 - val_custom_mae: 0.2672\n",
            "Running experiment: horizon=192, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_22', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1037/1037 - 13s - 13ms/step - loss: 0.2973 - mae: 0.3233 - mse: 0.2973 - val_loss: 0.1302 - val_mae: 0.2596 - val_mse: 0.1302 - learning_rate: 0.1000 - val_custom_mse: 0.2213 - val_custom_mae: 0.3349\n",
            "Epoch 2/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.1834 - mae: 0.2591 - mse: 0.1834 - val_loss: 0.0875 - val_mae: 0.2071 - val_mse: 0.0875 - learning_rate: 0.1000 - val_custom_mse: 0.1920 - val_custom_mae: 0.3110\n",
            "Epoch 3/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.1235 - mae: 0.2104 - mse: 0.1235 - val_loss: 0.0697 - val_mae: 0.1789 - val_mse: 0.0697 - learning_rate: 0.1000 - val_custom_mse: 0.1770 - val_custom_mae: 0.2969\n",
            "Epoch 4/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.1035 - mae: 0.1870 - mse: 0.1035 - val_loss: 0.0624 - val_mae: 0.1652 - val_mse: 0.0624 - learning_rate: 0.1000 - val_custom_mse: 0.1716 - val_custom_mae: 0.2916\n",
            "Epoch 5/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0944 - mae: 0.1741 - mse: 0.0944 - val_loss: 0.0571 - val_mae: 0.1550 - val_mse: 0.0571 - learning_rate: 0.1000 - val_custom_mse: 0.1666 - val_custom_mae: 0.2861\n",
            "Epoch 6/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0889 - mae: 0.1656 - mse: 0.0889 - val_loss: 0.0534 - val_mae: 0.1474 - val_mse: 0.0534 - learning_rate: 0.1000 - val_custom_mse: 0.1634 - val_custom_mae: 0.2823\n",
            "Epoch 7/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0850 - mae: 0.1595 - mse: 0.0850 - val_loss: 0.0508 - val_mae: 0.1419 - val_mse: 0.0508 - learning_rate: 0.1000 - val_custom_mse: 0.1615 - val_custom_mae: 0.2801\n",
            "Epoch 8/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0823 - mae: 0.1550 - mse: 0.0823 - val_loss: 0.0487 - val_mae: 0.1375 - val_mse: 0.0487 - learning_rate: 0.1000 - val_custom_mse: 0.1599 - val_custom_mae: 0.2782\n",
            "Epoch 9/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0802 - mae: 0.1514 - mse: 0.0802 - val_loss: 0.0469 - val_mae: 0.1336 - val_mse: 0.0469 - learning_rate: 0.1000 - val_custom_mse: 0.1582 - val_custom_mae: 0.2759\n",
            "Epoch 10/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0785 - mae: 0.1486 - mse: 0.0785 - val_loss: 0.0455 - val_mae: 0.1306 - val_mse: 0.0455 - learning_rate: 0.1000 - val_custom_mse: 0.1573 - val_custom_mae: 0.2749\n",
            "Epoch 11/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0771 - mae: 0.1463 - mse: 0.0771 - val_loss: 0.0442 - val_mae: 0.1279 - val_mse: 0.0442 - learning_rate: 0.1000 - val_custom_mse: 0.1564 - val_custom_mae: 0.2738\n",
            "Epoch 12/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0760 - mae: 0.1443 - mse: 0.0760 - val_loss: 0.0432 - val_mae: 0.1257 - val_mse: 0.0432 - learning_rate: 0.1000 - val_custom_mse: 0.1559 - val_custom_mae: 0.2733\n",
            "Epoch 13/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0751 - mae: 0.1427 - mse: 0.0751 - val_loss: 0.0423 - val_mae: 0.1236 - val_mse: 0.0423 - learning_rate: 0.1000 - val_custom_mse: 0.1554 - val_custom_mae: 0.2726\n",
            "Epoch 14/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0742 - mae: 0.1412 - mse: 0.0742 - val_loss: 0.0414 - val_mae: 0.1219 - val_mse: 0.0414 - learning_rate: 0.1000 - val_custom_mse: 0.1542 - val_custom_mae: 0.2711\n",
            "Epoch 15/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0736 - mae: 0.1399 - mse: 0.0736 - val_loss: 0.0408 - val_mae: 0.1201 - val_mse: 0.0408 - learning_rate: 0.1000 - val_custom_mse: 0.1545 - val_custom_mae: 0.2714\n",
            "Epoch 16/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0729 - mae: 0.1388 - mse: 0.0729 - val_loss: 0.0402 - val_mae: 0.1186 - val_mse: 0.0402 - learning_rate: 0.1000 - val_custom_mse: 0.1543 - val_custom_mae: 0.2712\n",
            "Epoch 17/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0724 - mae: 0.1377 - mse: 0.0724 - val_loss: 0.0396 - val_mae: 0.1174 - val_mse: 0.0396 - learning_rate: 0.1000 - val_custom_mse: 0.1536 - val_custom_mae: 0.2703\n",
            "Epoch 18/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0719 - mae: 0.1368 - mse: 0.0719 - val_loss: 0.0393 - val_mae: 0.1161 - val_mse: 0.0393 - learning_rate: 0.1000 - val_custom_mse: 0.1540 - val_custom_mae: 0.2709\n",
            "Epoch 19/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0715 - mae: 0.1359 - mse: 0.0715 - val_loss: 0.0388 - val_mae: 0.1150 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.1535 - val_custom_mae: 0.2704\n",
            "Epoch 20/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0712 - mae: 0.1352 - mse: 0.0712 - val_loss: 0.0383 - val_mae: 0.1141 - val_mse: 0.0383 - learning_rate: 0.1000 - val_custom_mse: 0.1531 - val_custom_mae: 0.2699\n",
            "Epoch 21/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0709 - mae: 0.1346 - mse: 0.0709 - val_loss: 0.0380 - val_mae: 0.1132 - val_mse: 0.0380 - learning_rate: 0.1000 - val_custom_mse: 0.1529 - val_custom_mae: 0.2695\n",
            "Epoch 22/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0706 - mae: 0.1339 - mse: 0.0706 - val_loss: 0.0377 - val_mae: 0.1129 - val_mse: 0.0377 - learning_rate: 0.1000 - val_custom_mse: 0.1525 - val_custom_mae: 0.2691\n",
            "Epoch 23/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0702 - mae: 0.1333 - mse: 0.0702 - val_loss: 0.0375 - val_mae: 0.1119 - val_mse: 0.0375 - learning_rate: 0.1000 - val_custom_mse: 0.1528 - val_custom_mae: 0.2693\n",
            "Epoch 24/100\n",
            "1037/1037 - 9s - 9ms/step - loss: 0.0700 - mae: 0.1329 - mse: 0.0700 - val_loss: 0.0372 - val_mae: 0.1114 - val_mse: 0.0372 - learning_rate: 0.1000 - val_custom_mse: 0.1524 - val_custom_mae: 0.2690\n",
            "Epoch 25/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0697 - mae: 0.1324 - mse: 0.0697 - val_loss: 0.0369 - val_mae: 0.1107 - val_mse: 0.0369 - learning_rate: 0.1000 - val_custom_mse: 0.1523 - val_custom_mae: 0.2689\n",
            "Epoch 26/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0696 - mae: 0.1320 - mse: 0.0696 - val_loss: 0.0369 - val_mae: 0.1101 - val_mse: 0.0369 - learning_rate: 0.1000 - val_custom_mse: 0.1529 - val_custom_mae: 0.2698\n",
            "Epoch 27/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0694 - mae: 0.1316 - mse: 0.0694 - val_loss: 0.0368 - val_mae: 0.1098 - val_mse: 0.0368 - learning_rate: 0.1000 - val_custom_mse: 0.1531 - val_custom_mae: 0.2700\n",
            "Epoch 28/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0692 - mae: 0.1313 - mse: 0.0692 - val_loss: 0.0365 - val_mae: 0.1090 - val_mse: 0.0365 - learning_rate: 0.1000 - val_custom_mse: 0.1526 - val_custom_mae: 0.2694\n",
            "Epoch 29/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0691 - mae: 0.1309 - mse: 0.0691 - val_loss: 0.0367 - val_mae: 0.1093 - val_mse: 0.0367 - learning_rate: 0.1000 - val_custom_mse: 0.1537 - val_custom_mae: 0.2706\n",
            "Epoch 30/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0689 - mae: 0.1306 - mse: 0.0689 - val_loss: 0.0361 - val_mae: 0.1083 - val_mse: 0.0361 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2689\n",
            "Epoch 31/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0687 - mae: 0.1304 - mse: 0.0687 - val_loss: 0.0361 - val_mae: 0.1079 - val_mse: 0.0361 - learning_rate: 0.1000 - val_custom_mse: 0.1524 - val_custom_mae: 0.2692\n",
            "Epoch 32/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0687 - mae: 0.1302 - mse: 0.0687 - val_loss: 0.0358 - val_mae: 0.1078 - val_mse: 0.0358 - learning_rate: 0.1000 - val_custom_mse: 0.1517 - val_custom_mae: 0.2681\n",
            "Epoch 33/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0685 - mae: 0.1299 - mse: 0.0685 - val_loss: 0.0358 - val_mae: 0.1073 - val_mse: 0.0358 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2687\n",
            "Epoch 34/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0684 - mae: 0.1297 - mse: 0.0684 - val_loss: 0.0356 - val_mae: 0.1069 - val_mse: 0.0356 - learning_rate: 0.1000 - val_custom_mse: 0.1517 - val_custom_mae: 0.2684\n",
            "Epoch 35/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0684 - mae: 0.1295 - mse: 0.0684 - val_loss: 0.0356 - val_mae: 0.1067 - val_mse: 0.0356 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2688\n",
            "Epoch 36/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0683 - mae: 0.1293 - mse: 0.0683 - val_loss: 0.0355 - val_mae: 0.1064 - val_mse: 0.0355 - learning_rate: 0.1000 - val_custom_mse: 0.1520 - val_custom_mae: 0.2687\n",
            "Epoch 37/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0682 - mae: 0.1292 - mse: 0.0682 - val_loss: 0.0354 - val_mae: 0.1062 - val_mse: 0.0354 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2683\n",
            "Epoch 38/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0681 - mae: 0.1290 - mse: 0.0681 - val_loss: 0.0354 - val_mae: 0.1061 - val_mse: 0.0354 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2684\n",
            "Epoch 39/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0680 - mae: 0.1288 - mse: 0.0680 - val_loss: 0.0353 - val_mae: 0.1059 - val_mse: 0.0353 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2684\n",
            "Epoch 40/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0679 - mae: 0.1287 - mse: 0.0679 - val_loss: 0.0353 - val_mae: 0.1059 - val_mse: 0.0353 - learning_rate: 0.1000 - val_custom_mse: 0.1516 - val_custom_mae: 0.2681\n",
            "Epoch 41/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0680 - mae: 0.1287 - mse: 0.0680 - val_loss: 0.0353 - val_mae: 0.1058 - val_mse: 0.0353 - learning_rate: 0.1000 - val_custom_mse: 0.1520 - val_custom_mae: 0.2686\n",
            "Epoch 42/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0678 - mae: 0.1285 - mse: 0.0678 - val_loss: 0.0351 - val_mae: 0.1056 - val_mse: 0.0351 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2677\n",
            "Epoch 43/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0678 - mae: 0.1284 - mse: 0.0678 - val_loss: 0.0351 - val_mae: 0.1054 - val_mse: 0.0351 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2677\n",
            "Epoch 44/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0678 - mae: 0.1283 - mse: 0.0678 - val_loss: 0.0351 - val_mae: 0.1052 - val_mse: 0.0351 - learning_rate: 0.1000 - val_custom_mse: 0.1516 - val_custom_mae: 0.2682\n",
            "Epoch 45/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0677 - mae: 0.1282 - mse: 0.0677 - val_loss: 0.0350 - val_mae: 0.1052 - val_mse: 0.0350 - learning_rate: 0.1000 - val_custom_mse: 0.1512 - val_custom_mae: 0.2677\n",
            "Epoch 46/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0676 - mae: 0.1281 - mse: 0.0676 - val_loss: 0.0350 - val_mae: 0.1051 - val_mse: 0.0350 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2679\n",
            "Epoch 47/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0677 - mae: 0.1281 - mse: 0.0677 - val_loss: 0.0351 - val_mae: 0.1051 - val_mse: 0.0351 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2685\n",
            "Epoch 48/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0675 - mae: 0.1279 - mse: 0.0675 - val_loss: 0.0349 - val_mae: 0.1047 - val_mse: 0.0349 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 49/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0675 - mae: 0.1279 - mse: 0.0675 - val_loss: 0.0349 - val_mae: 0.1049 - val_mse: 0.0349 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2679\n",
            "Epoch 50/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0674 - mae: 0.1278 - mse: 0.0674 - val_loss: 0.0349 - val_mae: 0.1047 - val_mse: 0.0349 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2676\n",
            "Epoch 51/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0675 - mae: 0.1278 - mse: 0.0675 - val_loss: 0.0349 - val_mae: 0.1046 - val_mse: 0.0349 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2679\n",
            "Epoch 52/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0673 - mae: 0.1277 - mse: 0.0673 - val_loss: 0.0348 - val_mae: 0.1046 - val_mse: 0.0348 - learning_rate: 0.1000 - val_custom_mse: 0.1510 - val_custom_mae: 0.2674\n",
            "Epoch 53/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0674 - mae: 0.1277 - mse: 0.0674 - val_loss: 0.0349 - val_mae: 0.1045 - val_mse: 0.0349 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2679\n",
            "Epoch 54/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0673 - mae: 0.1275 - mse: 0.0673 - val_loss: 0.0348 - val_mae: 0.1045 - val_mse: 0.0348 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2678\n",
            "Epoch 55/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0673 - mae: 0.1275 - mse: 0.0673 - val_loss: 0.0348 - val_mae: 0.1045 - val_mse: 0.0348 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2678\n",
            "Epoch 56/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0672 - mae: 0.1274 - mse: 0.0672 - val_loss: 0.0348 - val_mae: 0.1044 - val_mse: 0.0348 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2673\n",
            "Epoch 57/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0672 - mae: 0.1274 - mse: 0.0672 - val_loss: 0.0348 - val_mae: 0.1043 - val_mse: 0.0348 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2674\n",
            "Epoch 58/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0671 - mae: 0.1273 - mse: 0.0671 - val_loss: 0.0349 - val_mae: 0.1046 - val_mse: 0.0349 - learning_rate: 0.1000 - val_custom_mse: 0.1516 - val_custom_mae: 0.2683\n",
            "Epoch 59/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0671 - mae: 0.1273 - mse: 0.0671 - val_loss: 0.0348 - val_mae: 0.1042 - val_mse: 0.0348 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2678\n",
            "Epoch 60/100\n",
            "1037/1037 - 9s - 9ms/step - loss: 0.0672 - mae: 0.1273 - mse: 0.0672 - val_loss: 0.0347 - val_mae: 0.1043 - val_mse: 0.0347 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2673\n",
            "Epoch 61/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0672 - mae: 0.1272 - mse: 0.0672 - val_loss: 0.0347 - val_mae: 0.1044 - val_mse: 0.0347 - learning_rate: 0.1000 - val_custom_mse: 0.1504 - val_custom_mae: 0.2668\n",
            "Epoch 62/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0671 - mae: 0.1272 - mse: 0.0671 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.1000 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 63/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0671 - mae: 0.1272 - mse: 0.0671 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2674\n",
            "Epoch 64/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0670 - mae: 0.1271 - mse: 0.0670 - val_loss: 0.0349 - val_mae: 0.1044 - val_mse: 0.0349 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2682\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0670 - mae: 0.1271 - mse: 0.0670 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.1000 - val_custom_mse: 0.1512 - val_custom_mae: 0.2677\n",
            "Epoch 66/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0669 - mae: 0.1269 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2676\n",
            "Epoch 67/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0669 - mae: 0.1269 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2677\n",
            "Epoch 68/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0670 - mae: 0.1269 - mse: 0.0670 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0200 - val_custom_mse: 0.1512 - val_custom_mae: 0.2678\n",
            "Epoch 69/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0669 - mae: 0.1269 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1040 - val_mse: 0.0347 - learning_rate: 0.0200 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 70/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2677\n",
            "Epoch 71/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1040 - val_mse: 0.0347 - learning_rate: 0.0200 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 72/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0348 - val_mae: 0.1041 - val_mse: 0.0348 - learning_rate: 0.0200 - val_custom_mse: 0.1512 - val_custom_mae: 0.2678\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1269 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1040 - val_mse: 0.0347 - learning_rate: 0.0200 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 74/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0040 - val_custom_mse: 0.1511 - val_custom_mae: 0.2676\n",
            "Epoch 75/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1040 - val_mse: 0.0347 - learning_rate: 0.0040 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 76/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0040 - val_custom_mse: 0.1511 - val_custom_mae: 0.2676\n",
            "Epoch 77/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0040 - val_custom_mse: 0.1511 - val_custom_mae: 0.2676\n",
            "Epoch 78/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0040 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 79/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0040 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 80/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0040 - val_custom_mse: 0.1511 - val_custom_mae: 0.2676\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 0.0040 - val_custom_mse: 0.1511 - val_custom_mae: 0.2676\n",
            "Epoch 82/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 83/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 84/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 85/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 86/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 87/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 88/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2675\n",
            "Epoch 90/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 91/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 92/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 93/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 94/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1268 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 95/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1267 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 96/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 98/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0668 - mae: 0.1267 - mse: 0.0668 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 99/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Epoch 100/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0669 - mae: 0.1268 - mse: 0.0669 - val_loss: 0.0347 - val_mae: 0.1041 - val_mse: 0.0347 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1510 - val_custom_mae: 0.2676\n",
            "Running experiment: horizon=192, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_23', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1037/1037 - 13s - 12ms/step - loss: 0.2986 - mae: 0.3253 - mse: 0.2986 - val_loss: 0.1281 - val_mae: 0.2574 - val_mse: 0.1281 - learning_rate: 0.1000 - val_custom_mse: 0.2170 - val_custom_mae: 0.3317\n",
            "Epoch 2/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.1837 - mae: 0.2602 - mse: 0.1837 - val_loss: 0.0866 - val_mae: 0.2061 - val_mse: 0.0866 - learning_rate: 0.1000 - val_custom_mse: 0.1894 - val_custom_mae: 0.3088\n",
            "Epoch 3/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.1254 - mae: 0.2124 - mse: 0.1254 - val_loss: 0.0702 - val_mae: 0.1798 - val_mse: 0.0702 - learning_rate: 0.1000 - val_custom_mse: 0.1771 - val_custom_mae: 0.2973\n",
            "Epoch 4/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.1062 - mae: 0.1903 - mse: 0.1062 - val_loss: 0.0629 - val_mae: 0.1665 - val_mse: 0.0629 - learning_rate: 0.1000 - val_custom_mse: 0.1717 - val_custom_mae: 0.2919\n",
            "Epoch 5/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0979 - mae: 0.1786 - mse: 0.0979 - val_loss: 0.0577 - val_mae: 0.1564 - val_mse: 0.0577 - learning_rate: 0.1000 - val_custom_mse: 0.1666 - val_custom_mae: 0.2861\n",
            "Epoch 6/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0929 - mae: 0.1709 - mse: 0.0929 - val_loss: 0.0541 - val_mae: 0.1492 - val_mse: 0.0541 - learning_rate: 0.1000 - val_custom_mse: 0.1633 - val_custom_mae: 0.2822\n",
            "Epoch 7/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0893 - mae: 0.1656 - mse: 0.0893 - val_loss: 0.0516 - val_mae: 0.1441 - val_mse: 0.0516 - learning_rate: 0.1000 - val_custom_mse: 0.1610 - val_custom_mae: 0.2795\n",
            "Epoch 8/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0868 - mae: 0.1616 - mse: 0.0868 - val_loss: 0.0500 - val_mae: 0.1406 - val_mse: 0.0500 - learning_rate: 0.1000 - val_custom_mse: 0.1606 - val_custom_mae: 0.2792\n",
            "Epoch 9/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0849 - mae: 0.1586 - mse: 0.0849 - val_loss: 0.0483 - val_mae: 0.1371 - val_mse: 0.0483 - learning_rate: 0.1000 - val_custom_mse: 0.1588 - val_custom_mae: 0.2769\n",
            "Epoch 10/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0835 - mae: 0.1564 - mse: 0.0835 - val_loss: 0.0475 - val_mae: 0.1352 - val_mse: 0.0475 - learning_rate: 0.1000 - val_custom_mse: 0.1591 - val_custom_mae: 0.2772\n",
            "Epoch 11/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0822 - mae: 0.1544 - mse: 0.0822 - val_loss: 0.0462 - val_mae: 0.1325 - val_mse: 0.0462 - learning_rate: 0.1000 - val_custom_mse: 0.1576 - val_custom_mae: 0.2755\n",
            "Epoch 12/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0813 - mae: 0.1528 - mse: 0.0813 - val_loss: 0.0450 - val_mae: 0.1301 - val_mse: 0.0450 - learning_rate: 0.1000 - val_custom_mse: 0.1558 - val_custom_mae: 0.2731\n",
            "Epoch 13/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0804 - mae: 0.1514 - mse: 0.0804 - val_loss: 0.0444 - val_mae: 0.1288 - val_mse: 0.0444 - learning_rate: 0.1000 - val_custom_mse: 0.1555 - val_custom_mae: 0.2730\n",
            "Epoch 14/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0797 - mae: 0.1502 - mse: 0.0797 - val_loss: 0.0435 - val_mae: 0.1271 - val_mse: 0.0435 - learning_rate: 0.1000 - val_custom_mse: 0.1544 - val_custom_mae: 0.2713\n",
            "Epoch 15/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0791 - mae: 0.1492 - mse: 0.0791 - val_loss: 0.0432 - val_mae: 0.1261 - val_mse: 0.0432 - learning_rate: 0.1000 - val_custom_mse: 0.1550 - val_custom_mae: 0.2723\n",
            "Epoch 16/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0785 - mae: 0.1483 - mse: 0.0785 - val_loss: 0.0429 - val_mae: 0.1255 - val_mse: 0.0429 - learning_rate: 0.1000 - val_custom_mse: 0.1550 - val_custom_mae: 0.2725\n",
            "Epoch 17/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0781 - mae: 0.1475 - mse: 0.0781 - val_loss: 0.0420 - val_mae: 0.1237 - val_mse: 0.0420 - learning_rate: 0.1000 - val_custom_mse: 0.1534 - val_custom_mae: 0.2702\n",
            "Epoch 18/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0777 - mae: 0.1468 - mse: 0.0777 - val_loss: 0.0416 - val_mae: 0.1228 - val_mse: 0.0416 - learning_rate: 0.1000 - val_custom_mse: 0.1532 - val_custom_mae: 0.2699\n",
            "Epoch 19/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0773 - mae: 0.1461 - mse: 0.0773 - val_loss: 0.0413 - val_mae: 0.1224 - val_mse: 0.0413 - learning_rate: 0.1000 - val_custom_mse: 0.1525 - val_custom_mae: 0.2690\n",
            "Epoch 20/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0769 - mae: 0.1455 - mse: 0.0769 - val_loss: 0.0411 - val_mae: 0.1216 - val_mse: 0.0411 - learning_rate: 0.1000 - val_custom_mse: 0.1531 - val_custom_mae: 0.2699\n",
            "Epoch 21/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0765 - mae: 0.1450 - mse: 0.0765 - val_loss: 0.0409 - val_mae: 0.1209 - val_mse: 0.0409 - learning_rate: 0.1000 - val_custom_mse: 0.1532 - val_custom_mae: 0.2701\n",
            "Epoch 22/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0763 - mae: 0.1446 - mse: 0.0763 - val_loss: 0.0406 - val_mae: 0.1204 - val_mse: 0.0406 - learning_rate: 0.1000 - val_custom_mse: 0.1528 - val_custom_mae: 0.2697\n",
            "Epoch 23/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0761 - mae: 0.1441 - mse: 0.0761 - val_loss: 0.0405 - val_mae: 0.1201 - val_mse: 0.0405 - learning_rate: 0.1000 - val_custom_mse: 0.1533 - val_custom_mae: 0.2703\n",
            "Epoch 24/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0758 - mae: 0.1437 - mse: 0.0758 - val_loss: 0.0402 - val_mae: 0.1197 - val_mse: 0.0402 - learning_rate: 0.1000 - val_custom_mse: 0.1522 - val_custom_mae: 0.2691\n",
            "Epoch 25/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0757 - mae: 0.1434 - mse: 0.0757 - val_loss: 0.0399 - val_mae: 0.1189 - val_mse: 0.0399 - learning_rate: 0.1000 - val_custom_mse: 0.1524 - val_custom_mae: 0.2690\n",
            "Epoch 26/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0754 - mae: 0.1430 - mse: 0.0754 - val_loss: 0.0398 - val_mae: 0.1188 - val_mse: 0.0398 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2689\n",
            "Epoch 27/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0752 - mae: 0.1427 - mse: 0.0752 - val_loss: 0.0398 - val_mae: 0.1181 - val_mse: 0.0398 - learning_rate: 0.1000 - val_custom_mse: 0.1529 - val_custom_mae: 0.2698\n",
            "Epoch 28/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0752 - mae: 0.1424 - mse: 0.0752 - val_loss: 0.0396 - val_mae: 0.1180 - val_mse: 0.0396 - learning_rate: 0.1000 - val_custom_mse: 0.1522 - val_custom_mae: 0.2691\n",
            "Epoch 29/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0749 - mae: 0.1422 - mse: 0.0749 - val_loss: 0.0394 - val_mae: 0.1179 - val_mse: 0.0394 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2685\n",
            "Epoch 30/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0749 - mae: 0.1420 - mse: 0.0749 - val_loss: 0.0393 - val_mae: 0.1176 - val_mse: 0.0393 - learning_rate: 0.1000 - val_custom_mse: 0.1516 - val_custom_mae: 0.2682\n",
            "Epoch 31/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0747 - mae: 0.1417 - mse: 0.0747 - val_loss: 0.0392 - val_mae: 0.1175 - val_mse: 0.0392 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2680\n",
            "Epoch 32/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0747 - mae: 0.1415 - mse: 0.0747 - val_loss: 0.0392 - val_mae: 0.1169 - val_mse: 0.0392 - learning_rate: 0.1000 - val_custom_mse: 0.1520 - val_custom_mae: 0.2688\n",
            "Epoch 33/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0745 - mae: 0.1414 - mse: 0.0745 - val_loss: 0.0390 - val_mae: 0.1167 - val_mse: 0.0390 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2679\n",
            "Epoch 34/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0744 - mae: 0.1412 - mse: 0.0744 - val_loss: 0.0393 - val_mae: 0.1167 - val_mse: 0.0393 - learning_rate: 0.1000 - val_custom_mse: 0.1529 - val_custom_mae: 0.2698\n",
            "Epoch 35/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0742 - mae: 0.1410 - mse: 0.0742 - val_loss: 0.0390 - val_mae: 0.1164 - val_mse: 0.0390 - learning_rate: 0.1000 - val_custom_mse: 0.1519 - val_custom_mae: 0.2686\n",
            "Epoch 36/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0742 - mae: 0.1409 - mse: 0.0742 - val_loss: 0.0391 - val_mae: 0.1163 - val_mse: 0.0391 - learning_rate: 0.1000 - val_custom_mse: 0.1525 - val_custom_mae: 0.2695\n",
            "Epoch 37/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0741 - mae: 0.1407 - mse: 0.0741 - val_loss: 0.0388 - val_mae: 0.1160 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.1517 - val_custom_mae: 0.2686\n",
            "Epoch 38/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0741 - mae: 0.1406 - mse: 0.0741 - val_loss: 0.0388 - val_mae: 0.1159 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.1520 - val_custom_mae: 0.2688\n",
            "Epoch 39/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0739 - mae: 0.1405 - mse: 0.0739 - val_loss: 0.0388 - val_mae: 0.1157 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.1521 - val_custom_mae: 0.2690\n",
            "Epoch 40/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0739 - mae: 0.1403 - mse: 0.0739 - val_loss: 0.0392 - val_mae: 0.1169 - val_mse: 0.0392 - learning_rate: 0.1000 - val_custom_mse: 0.1528 - val_custom_mae: 0.2701\n",
            "Epoch 41/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0738 - mae: 0.1402 - mse: 0.0738 - val_loss: 0.0386 - val_mae: 0.1156 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.1513 - val_custom_mae: 0.2681\n",
            "Epoch 42/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0738 - mae: 0.1401 - mse: 0.0738 - val_loss: 0.0385 - val_mae: 0.1153 - val_mse: 0.0385 - learning_rate: 0.1000 - val_custom_mse: 0.1512 - val_custom_mae: 0.2680\n",
            "Epoch 43/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0738 - mae: 0.1401 - mse: 0.0738 - val_loss: 0.0388 - val_mae: 0.1157 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.1522 - val_custom_mae: 0.2692\n",
            "Epoch 44/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0738 - mae: 0.1400 - mse: 0.0738 - val_loss: 0.0387 - val_mae: 0.1157 - val_mse: 0.0387 - learning_rate: 0.1000 - val_custom_mse: 0.1515 - val_custom_mae: 0.2685\n",
            "Epoch 45/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0736 - mae: 0.1399 - mse: 0.0736 - val_loss: 0.0385 - val_mae: 0.1151 - val_mse: 0.0385 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2680\n",
            "Epoch 46/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0736 - mae: 0.1398 - mse: 0.0736 - val_loss: 0.0384 - val_mae: 0.1151 - val_mse: 0.0384 - learning_rate: 0.1000 - val_custom_mse: 0.1512 - val_custom_mae: 0.2678\n",
            "Epoch 47/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0735 - mae: 0.1397 - mse: 0.0735 - val_loss: 0.0386 - val_mae: 0.1154 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.1516 - val_custom_mae: 0.2686\n",
            "Epoch 48/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0735 - mae: 0.1396 - mse: 0.0735 - val_loss: 0.0387 - val_mae: 0.1156 - val_mse: 0.0387 - learning_rate: 0.1000 - val_custom_mse: 0.1520 - val_custom_mae: 0.2691\n",
            "Epoch 49/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0735 - mae: 0.1396 - mse: 0.0735 - val_loss: 0.0384 - val_mae: 0.1152 - val_mse: 0.0384 - learning_rate: 0.1000 - val_custom_mse: 0.1507 - val_custom_mae: 0.2674\n",
            "Epoch 50/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0734 - mae: 0.1395 - mse: 0.0734 - val_loss: 0.0384 - val_mae: 0.1148 - val_mse: 0.0384 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2677\n",
            "Epoch 51/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0733 - mae: 0.1395 - mse: 0.0733 - val_loss: 0.0383 - val_mae: 0.1149 - val_mse: 0.0383 - learning_rate: 0.1000 - val_custom_mse: 0.1508 - val_custom_mae: 0.2673\n",
            "Epoch 52/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0734 - mae: 0.1395 - mse: 0.0734 - val_loss: 0.0383 - val_mae: 0.1147 - val_mse: 0.0383 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2677\n",
            "Epoch 53/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0733 - mae: 0.1393 - mse: 0.0733 - val_loss: 0.0384 - val_mae: 0.1148 - val_mse: 0.0384 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2680\n",
            "Epoch 54/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0733 - mae: 0.1393 - mse: 0.0733 - val_loss: 0.0383 - val_mae: 0.1147 - val_mse: 0.0383 - learning_rate: 0.1000 - val_custom_mse: 0.1509 - val_custom_mae: 0.2675\n",
            "Epoch 55/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0733 - mae: 0.1393 - mse: 0.0733 - val_loss: 0.0388 - val_mae: 0.1154 - val_mse: 0.0388 - learning_rate: 0.1000 - val_custom_mse: 0.1525 - val_custom_mae: 0.2695\n",
            "Epoch 56/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0732 - mae: 0.1392 - mse: 0.0732 - val_loss: 0.0382 - val_mae: 0.1147 - val_mse: 0.0382 - learning_rate: 0.1000 - val_custom_mse: 0.1505 - val_custom_mae: 0.2670\n",
            "Epoch 57/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0732 - mae: 0.1391 - mse: 0.0732 - val_loss: 0.0384 - val_mae: 0.1148 - val_mse: 0.0384 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2681\n",
            "Epoch 58/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0731 - mae: 0.1391 - mse: 0.0731 - val_loss: 0.0386 - val_mae: 0.1154 - val_mse: 0.0386 - learning_rate: 0.1000 - val_custom_mse: 0.1518 - val_custom_mae: 0.2690\n",
            "Epoch 59/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0731 - mae: 0.1391 - mse: 0.0731 - val_loss: 0.0383 - val_mae: 0.1145 - val_mse: 0.0383 - learning_rate: 0.1000 - val_custom_mse: 0.1510 - val_custom_mae: 0.2678\n",
            "Epoch 60/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0732 - mae: 0.1391 - mse: 0.0732 - val_loss: 0.0383 - val_mae: 0.1149 - val_mse: 0.0383 - learning_rate: 0.1000 - val_custom_mse: 0.1506 - val_custom_mae: 0.2675\n",
            "Epoch 61/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0730 - mae: 0.1390 - mse: 0.0730 - val_loss: 0.0384 - val_mae: 0.1146 - val_mse: 0.0384 - learning_rate: 0.1000 - val_custom_mse: 0.1514 - val_custom_mae: 0.2682\n",
            "Epoch 62/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0730 - mae: 0.1390 - mse: 0.0730 - val_loss: 0.0383 - val_mae: 0.1146 - val_mse: 0.0383 - learning_rate: 0.1000 - val_custom_mse: 0.1508 - val_custom_mae: 0.2677\n",
            "Epoch 63/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0730 - mae: 0.1390 - mse: 0.0730 - val_loss: 0.0382 - val_mae: 0.1144 - val_mse: 0.0382 - learning_rate: 0.1000 - val_custom_mse: 0.1505 - val_custom_mae: 0.2672\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0731 - mae: 0.1390 - mse: 0.0731 - val_loss: 0.0383 - val_mae: 0.1145 - val_mse: 0.0383 - learning_rate: 0.1000 - val_custom_mse: 0.1511 - val_custom_mae: 0.2680\n",
            "Epoch 65/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0729 - mae: 0.1387 - mse: 0.0729 - val_loss: 0.0382 - val_mae: 0.1141 - val_mse: 0.0382 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2679\n",
            "Epoch 66/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0729 - mae: 0.1387 - mse: 0.0729 - val_loss: 0.0382 - val_mae: 0.1142 - val_mse: 0.0382 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2678\n",
            "Epoch 67/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1387 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1141 - val_mse: 0.0382 - learning_rate: 0.0200 - val_custom_mse: 0.1512 - val_custom_mae: 0.2679\n",
            "Epoch 68/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0729 - mae: 0.1387 - mse: 0.0729 - val_loss: 0.0382 - val_mae: 0.1141 - val_mse: 0.0382 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2680\n",
            "Epoch 69/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0729 - mae: 0.1387 - mse: 0.0729 - val_loss: 0.0382 - val_mae: 0.1141 - val_mse: 0.0382 - learning_rate: 0.0200 - val_custom_mse: 0.1510 - val_custom_mae: 0.2677\n",
            "Epoch 70/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0729 - mae: 0.1387 - mse: 0.0729 - val_loss: 0.0382 - val_mae: 0.1142 - val_mse: 0.0382 - learning_rate: 0.0200 - val_custom_mse: 0.1512 - val_custom_mae: 0.2680\n",
            "Epoch 71/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1387 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1142 - val_mse: 0.0382 - learning_rate: 0.0200 - val_custom_mse: 0.1511 - val_custom_mae: 0.2680\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1387 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1141 - val_mse: 0.0382 - learning_rate: 0.0200 - val_custom_mse: 0.1510 - val_custom_mae: 0.2677\n",
            "Epoch 73/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1387 - mse: 0.0728 - val_loss: 0.0383 - val_mae: 0.1141 - val_mse: 0.0383 - learning_rate: 0.0040 - val_custom_mse: 0.1514 - val_custom_mae: 0.2682\n",
            "Epoch 74/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0729 - mae: 0.1386 - mse: 0.0729 - val_loss: 0.0383 - val_mae: 0.1141 - val_mse: 0.0383 - learning_rate: 0.0040 - val_custom_mse: 0.1515 - val_custom_mae: 0.2682\n",
            "Epoch 75/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1140 - val_mse: 0.0382 - learning_rate: 0.0040 - val_custom_mse: 0.1514 - val_custom_mae: 0.2681\n",
            "Epoch 76/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0383 - val_mae: 0.1141 - val_mse: 0.0383 - learning_rate: 0.0040 - val_custom_mse: 0.1514 - val_custom_mae: 0.2681\n",
            "Epoch 77/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0729 - mae: 0.1387 - mse: 0.0729 - val_loss: 0.0382 - val_mae: 0.1140 - val_mse: 0.0382 - learning_rate: 0.0040 - val_custom_mse: 0.1514 - val_custom_mae: 0.2681\n",
            "Epoch 78/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0727 - mae: 0.1386 - mse: 0.0727 - val_loss: 0.0382 - val_mae: 0.1140 - val_mse: 0.0382 - learning_rate: 0.0040 - val_custom_mse: 0.1513 - val_custom_mae: 0.2681\n",
            "Epoch 79/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1140 - val_mse: 0.0382 - learning_rate: 0.0040 - val_custom_mse: 0.1513 - val_custom_mae: 0.2681\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 80: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0727 - mae: 0.1386 - mse: 0.0727 - val_loss: 0.0382 - val_mae: 0.1140 - val_mse: 0.0382 - learning_rate: 0.0040 - val_custom_mse: 0.1514 - val_custom_mae: 0.2681\n",
            "Epoch 81/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 82/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 83/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 84/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 85/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 86/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 87/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 89/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 90/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 91/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 92/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0727 - mae: 0.1386 - mse: 0.0727 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 93/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 94/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 95/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0727 - mae: 0.1386 - mse: 0.0727 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 97/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0729 - mae: 0.1386 - mse: 0.0729 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 98/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0728 - mae: 0.1386 - mse: 0.0728 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 99/100\n",
            "1037/1037 - 9s - 8ms/step - loss: 0.0729 - mae: 0.1386 - mse: 0.0729 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Epoch 100/100\n",
            "1037/1037 - 8s - 8ms/step - loss: 0.0727 - mae: 0.1386 - mse: 0.0727 - val_loss: 0.0382 - val_mae: 0.1139 - val_mse: 0.0382 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1513 - val_custom_mae: 0.2680\n",
            "Running experiment: horizon=336, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_24', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1032/1032 - 13s - 12ms/step - loss: 0.3439 - mae: 0.3549 - mse: 0.3439 - val_loss: 0.1649 - val_mae: 0.2865 - val_mse: 0.1649 - learning_rate: 0.1000 - val_custom_mse: 0.2791 - val_custom_mae: 0.3716\n",
            "Epoch 2/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.2343 - mae: 0.2893 - mse: 0.2343 - val_loss: 0.1152 - val_mae: 0.2319 - val_mse: 0.1152 - learning_rate: 0.1000 - val_custom_mse: 0.2333 - val_custom_mae: 0.3400\n",
            "Epoch 3/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1758 - mae: 0.2388 - mse: 0.1758 - val_loss: 0.0971 - val_mae: 0.2045 - val_mse: 0.0971 - learning_rate: 0.1000 - val_custom_mse: 0.2195 - val_custom_mae: 0.3282\n",
            "Epoch 4/100\n",
            "1032/1032 - 9s - 9ms/step - loss: 0.1549 - mae: 0.2140 - mse: 0.1549 - val_loss: 0.0894 - val_mae: 0.1915 - val_mse: 0.0894 - learning_rate: 0.1000 - val_custom_mse: 0.2134 - val_custom_mae: 0.3222\n",
            "Epoch 5/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1451 - mae: 0.1998 - mse: 0.1451 - val_loss: 0.0839 - val_mae: 0.1815 - val_mse: 0.0839 - learning_rate: 0.1000 - val_custom_mse: 0.2093 - val_custom_mae: 0.3185\n",
            "Epoch 6/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1384 - mae: 0.1894 - mse: 0.1384 - val_loss: 0.0801 - val_mae: 0.1739 - val_mse: 0.0801 - learning_rate: 0.1000 - val_custom_mse: 0.2064 - val_custom_mae: 0.3157\n",
            "Epoch 7/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1335 - mae: 0.1812 - mse: 0.1335 - val_loss: 0.0770 - val_mae: 0.1672 - val_mse: 0.0770 - learning_rate: 0.1000 - val_custom_mse: 0.2047 - val_custom_mae: 0.3137\n",
            "Epoch 8/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1297 - mae: 0.1746 - mse: 0.1297 - val_loss: 0.0747 - val_mae: 0.1619 - val_mse: 0.0747 - learning_rate: 0.1000 - val_custom_mse: 0.2035 - val_custom_mae: 0.3124\n",
            "Epoch 9/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1267 - mae: 0.1690 - mse: 0.1267 - val_loss: 0.0725 - val_mae: 0.1571 - val_mse: 0.0725 - learning_rate: 0.1000 - val_custom_mse: 0.2017 - val_custom_mae: 0.3105\n",
            "Epoch 10/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1242 - mae: 0.1643 - mse: 0.1242 - val_loss: 0.0712 - val_mae: 0.1542 - val_mse: 0.0712 - learning_rate: 0.1000 - val_custom_mse: 0.2012 - val_custom_mae: 0.3097\n",
            "Epoch 11/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1221 - mae: 0.1604 - mse: 0.1221 - val_loss: 0.0694 - val_mae: 0.1497 - val_mse: 0.0694 - learning_rate: 0.1000 - val_custom_mse: 0.1999 - val_custom_mae: 0.3083\n",
            "Epoch 12/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1203 - mae: 0.1568 - mse: 0.1203 - val_loss: 0.0682 - val_mae: 0.1471 - val_mse: 0.0682 - learning_rate: 0.1000 - val_custom_mse: 0.1990 - val_custom_mae: 0.3075\n",
            "Epoch 13/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1188 - mae: 0.1537 - mse: 0.1188 - val_loss: 0.0670 - val_mae: 0.1439 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1984 - val_custom_mae: 0.3067\n",
            "Epoch 14/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1176 - mae: 0.1510 - mse: 0.1176 - val_loss: 0.0662 - val_mae: 0.1417 - val_mse: 0.0662 - learning_rate: 0.1000 - val_custom_mse: 0.1984 - val_custom_mae: 0.3066\n",
            "Epoch 15/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1164 - mae: 0.1484 - mse: 0.1164 - val_loss: 0.0651 - val_mae: 0.1392 - val_mse: 0.0651 - learning_rate: 0.1000 - val_custom_mse: 0.1973 - val_custom_mae: 0.3057\n",
            "Epoch 16/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1154 - mae: 0.1461 - mse: 0.1154 - val_loss: 0.0645 - val_mae: 0.1368 - val_mse: 0.0645 - learning_rate: 0.1000 - val_custom_mse: 0.1974 - val_custom_mae: 0.3056\n",
            "Epoch 17/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1145 - mae: 0.1439 - mse: 0.1145 - val_loss: 0.0637 - val_mae: 0.1348 - val_mse: 0.0637 - learning_rate: 0.1000 - val_custom_mse: 0.1965 - val_custom_mae: 0.3048\n",
            "Epoch 18/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1137 - mae: 0.1420 - mse: 0.1137 - val_loss: 0.0633 - val_mae: 0.1332 - val_mse: 0.0633 - learning_rate: 0.1000 - val_custom_mse: 0.1974 - val_custom_mae: 0.3054\n",
            "Epoch 19/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1130 - mae: 0.1402 - mse: 0.1130 - val_loss: 0.0626 - val_mae: 0.1311 - val_mse: 0.0626 - learning_rate: 0.1000 - val_custom_mse: 0.1967 - val_custom_mae: 0.3048\n",
            "Epoch 20/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1124 - mae: 0.1384 - mse: 0.1124 - val_loss: 0.0621 - val_mae: 0.1296 - val_mse: 0.0621 - learning_rate: 0.1000 - val_custom_mse: 0.1962 - val_custom_mae: 0.3044\n",
            "Epoch 21/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1118 - mae: 0.1368 - mse: 0.1118 - val_loss: 0.0618 - val_mae: 0.1283 - val_mse: 0.0618 - learning_rate: 0.1000 - val_custom_mse: 0.1968 - val_custom_mae: 0.3047\n",
            "Epoch 22/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1112 - mae: 0.1353 - mse: 0.1112 - val_loss: 0.0614 - val_mae: 0.1268 - val_mse: 0.0614 - learning_rate: 0.1000 - val_custom_mse: 0.1969 - val_custom_mae: 0.3049\n",
            "Epoch 23/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1108 - mae: 0.1338 - mse: 0.1108 - val_loss: 0.0608 - val_mae: 0.1249 - val_mse: 0.0608 - learning_rate: 0.1000 - val_custom_mse: 0.1963 - val_custom_mae: 0.3044\n",
            "Epoch 24/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1103 - mae: 0.1323 - mse: 0.1103 - val_loss: 0.0605 - val_mae: 0.1236 - val_mse: 0.0605 - learning_rate: 0.1000 - val_custom_mse: 0.1965 - val_custom_mae: 0.3045\n",
            "Epoch 25/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1099 - mae: 0.1311 - mse: 0.1099 - val_loss: 0.0602 - val_mae: 0.1224 - val_mse: 0.0602 - learning_rate: 0.1000 - val_custom_mse: 0.1964 - val_custom_mae: 0.3045\n",
            "Epoch 26/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1095 - mae: 0.1298 - mse: 0.1095 - val_loss: 0.0597 - val_mae: 0.1210 - val_mse: 0.0597 - learning_rate: 0.1000 - val_custom_mse: 0.1961 - val_custom_mae: 0.3042\n",
            "Epoch 27/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1091 - mae: 0.1286 - mse: 0.1091 - val_loss: 0.0595 - val_mae: 0.1202 - val_mse: 0.0595 - learning_rate: 0.1000 - val_custom_mse: 0.1961 - val_custom_mae: 0.3043\n",
            "Epoch 28/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1088 - mae: 0.1275 - mse: 0.1088 - val_loss: 0.0593 - val_mae: 0.1188 - val_mse: 0.0593 - learning_rate: 0.1000 - val_custom_mse: 0.1963 - val_custom_mae: 0.3043\n",
            "Epoch 29/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1084 - mae: 0.1264 - mse: 0.1084 - val_loss: 0.0590 - val_mae: 0.1183 - val_mse: 0.0590 - learning_rate: 0.1000 - val_custom_mse: 0.1957 - val_custom_mae: 0.3037\n",
            "Epoch 30/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1082 - mae: 0.1255 - mse: 0.1082 - val_loss: 0.0588 - val_mae: 0.1167 - val_mse: 0.0588 - learning_rate: 0.1000 - val_custom_mse: 0.1964 - val_custom_mae: 0.3043\n",
            "Epoch 31/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1079 - mae: 0.1243 - mse: 0.1079 - val_loss: 0.0586 - val_mae: 0.1162 - val_mse: 0.0586 - learning_rate: 0.1000 - val_custom_mse: 0.1962 - val_custom_mae: 0.3043\n",
            "Epoch 32/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1077 - mae: 0.1236 - mse: 0.1077 - val_loss: 0.0583 - val_mae: 0.1147 - val_mse: 0.0583 - learning_rate: 0.1000 - val_custom_mse: 0.1962 - val_custom_mae: 0.3042\n",
            "Epoch 33/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1074 - mae: 0.1227 - mse: 0.1074 - val_loss: 0.0580 - val_mae: 0.1138 - val_mse: 0.0580 - learning_rate: 0.1000 - val_custom_mse: 0.1957 - val_custom_mae: 0.3038\n",
            "Epoch 34/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1072 - mae: 0.1218 - mse: 0.1072 - val_loss: 0.0579 - val_mae: 0.1137 - val_mse: 0.0579 - learning_rate: 0.1000 - val_custom_mse: 0.1956 - val_custom_mae: 0.3036\n",
            "Epoch 35/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1070 - mae: 0.1210 - mse: 0.1070 - val_loss: 0.0579 - val_mae: 0.1124 - val_mse: 0.0579 - learning_rate: 0.1000 - val_custom_mse: 0.1964 - val_custom_mae: 0.3042\n",
            "Epoch 36/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1068 - mae: 0.1203 - mse: 0.1068 - val_loss: 0.0574 - val_mae: 0.1113 - val_mse: 0.0574 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3036\n",
            "Epoch 37/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1066 - mae: 0.1197 - mse: 0.1066 - val_loss: 0.0574 - val_mae: 0.1105 - val_mse: 0.0574 - learning_rate: 0.1000 - val_custom_mse: 0.1957 - val_custom_mae: 0.3036\n",
            "Epoch 38/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1064 - mae: 0.1191 - mse: 0.1064 - val_loss: 0.0574 - val_mae: 0.1106 - val_mse: 0.0574 - learning_rate: 0.1000 - val_custom_mse: 0.1963 - val_custom_mae: 0.3042\n",
            "Epoch 39/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1062 - mae: 0.1182 - mse: 0.1062 - val_loss: 0.0575 - val_mae: 0.1107 - val_mse: 0.0575 - learning_rate: 0.1000 - val_custom_mse: 0.1968 - val_custom_mae: 0.3046\n",
            "Epoch 40/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1061 - mae: 0.1176 - mse: 0.1061 - val_loss: 0.0571 - val_mae: 0.1092 - val_mse: 0.0571 - learning_rate: 0.1000 - val_custom_mse: 0.1961 - val_custom_mae: 0.3040\n",
            "Epoch 41/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1060 - mae: 0.1171 - mse: 0.1060 - val_loss: 0.0569 - val_mae: 0.1082 - val_mse: 0.0569 - learning_rate: 0.1000 - val_custom_mse: 0.1959 - val_custom_mae: 0.3039\n",
            "Epoch 42/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1058 - mae: 0.1165 - mse: 0.1058 - val_loss: 0.0566 - val_mae: 0.1070 - val_mse: 0.0566 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3033\n",
            "Epoch 43/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1057 - mae: 0.1158 - mse: 0.1057 - val_loss: 0.0565 - val_mae: 0.1074 - val_mse: 0.0565 - learning_rate: 0.1000 - val_custom_mse: 0.1948 - val_custom_mae: 0.3030\n",
            "Epoch 44/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1056 - mae: 0.1155 - mse: 0.1056 - val_loss: 0.0565 - val_mae: 0.1074 - val_mse: 0.0565 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3031\n",
            "Epoch 45/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1055 - mae: 0.1148 - mse: 0.1055 - val_loss: 0.0564 - val_mae: 0.1055 - val_mse: 0.0564 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3035\n",
            "Epoch 46/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1054 - mae: 0.1145 - mse: 0.1054 - val_loss: 0.0565 - val_mae: 0.1059 - val_mse: 0.0565 - learning_rate: 0.1000 - val_custom_mse: 0.1960 - val_custom_mae: 0.3039\n",
            "Epoch 47/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1053 - mae: 0.1140 - mse: 0.1053 - val_loss: 0.0564 - val_mae: 0.1059 - val_mse: 0.0564 - learning_rate: 0.1000 - val_custom_mse: 0.1957 - val_custom_mae: 0.3038\n",
            "Epoch 48/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1052 - mae: 0.1135 - mse: 0.1052 - val_loss: 0.0563 - val_mae: 0.1052 - val_mse: 0.0563 - learning_rate: 0.1000 - val_custom_mse: 0.1959 - val_custom_mae: 0.3038\n",
            "Epoch 49/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1051 - mae: 0.1131 - mse: 0.1051 - val_loss: 0.0561 - val_mae: 0.1036 - val_mse: 0.0561 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3034\n",
            "Epoch 50/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1050 - mae: 0.1129 - mse: 0.1050 - val_loss: 0.0559 - val_mae: 0.1033 - val_mse: 0.0559 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3033\n",
            "Epoch 51/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1049 - mae: 0.1122 - mse: 0.1049 - val_loss: 0.0559 - val_mae: 0.1029 - val_mse: 0.0559 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3031\n",
            "Epoch 52/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1048 - mae: 0.1118 - mse: 0.1048 - val_loss: 0.0558 - val_mae: 0.1024 - val_mse: 0.0558 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3031\n",
            "Epoch 53/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1047 - mae: 0.1115 - mse: 0.1047 - val_loss: 0.0557 - val_mae: 0.1021 - val_mse: 0.0557 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3031\n",
            "Epoch 54/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1047 - mae: 0.1113 - mse: 0.1047 - val_loss: 0.0557 - val_mae: 0.1023 - val_mse: 0.0557 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3033\n",
            "Epoch 55/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1046 - mae: 0.1111 - mse: 0.1046 - val_loss: 0.0557 - val_mae: 0.1022 - val_mse: 0.0557 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3033\n",
            "Epoch 56/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1045 - mae: 0.1107 - mse: 0.1045 - val_loss: 0.0555 - val_mae: 0.1021 - val_mse: 0.0555 - learning_rate: 0.1000 - val_custom_mse: 0.1945 - val_custom_mae: 0.3027\n",
            "Epoch 57/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1045 - mae: 0.1104 - mse: 0.1045 - val_loss: 0.0556 - val_mae: 0.1019 - val_mse: 0.0556 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3029\n",
            "Epoch 58/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1044 - mae: 0.1101 - mse: 0.1044 - val_loss: 0.0557 - val_mae: 0.1024 - val_mse: 0.0557 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3031\n",
            "Epoch 59/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1044 - mae: 0.1098 - mse: 0.1044 - val_loss: 0.0557 - val_mae: 0.1018 - val_mse: 0.0557 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3036\n",
            "Epoch 60/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1043 - mae: 0.1096 - mse: 0.1043 - val_loss: 0.0556 - val_mae: 0.1009 - val_mse: 0.0556 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3035\n",
            "Epoch 61/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1043 - mae: 0.1092 - mse: 0.1043 - val_loss: 0.0554 - val_mae: 0.0998 - val_mse: 0.0554 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3031\n",
            "Epoch 62/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1042 - mae: 0.1092 - mse: 0.1042 - val_loss: 0.0553 - val_mae: 0.1000 - val_mse: 0.0553 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3029\n",
            "Epoch 63/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1041 - mae: 0.1088 - mse: 0.1041 - val_loss: 0.0553 - val_mae: 0.0994 - val_mse: 0.0553 - learning_rate: 0.1000 - val_custom_mse: 0.1948 - val_custom_mae: 0.3030\n",
            "Epoch 64/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1041 - mae: 0.1087 - mse: 0.1041 - val_loss: 0.0552 - val_mae: 0.0993 - val_mse: 0.0552 - learning_rate: 0.1000 - val_custom_mse: 0.1946 - val_custom_mae: 0.3028\n",
            "Epoch 65/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1041 - mae: 0.1084 - mse: 0.1041 - val_loss: 0.0553 - val_mae: 0.0991 - val_mse: 0.0553 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3032\n",
            "Epoch 66/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1041 - mae: 0.1082 - mse: 0.1041 - val_loss: 0.0554 - val_mae: 0.0992 - val_mse: 0.0554 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3035\n",
            "Epoch 67/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1040 - mae: 0.1080 - mse: 0.1040 - val_loss: 0.0553 - val_mae: 0.0990 - val_mse: 0.0553 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3033\n",
            "Epoch 68/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1040 - mae: 0.1077 - mse: 0.1040 - val_loss: 0.0551 - val_mae: 0.0983 - val_mse: 0.0551 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3031\n",
            "Epoch 69/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1039 - mae: 0.1075 - mse: 0.1039 - val_loss: 0.0551 - val_mae: 0.0983 - val_mse: 0.0551 - learning_rate: 0.1000 - val_custom_mse: 0.1948 - val_custom_mae: 0.3030\n",
            "Epoch 70/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1039 - mae: 0.1072 - mse: 0.1039 - val_loss: 0.0551 - val_mae: 0.0976 - val_mse: 0.0551 - learning_rate: 0.1000 - val_custom_mse: 0.1948 - val_custom_mae: 0.3030\n",
            "Epoch 71/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1039 - mae: 0.1073 - mse: 0.1039 - val_loss: 0.0551 - val_mae: 0.0979 - val_mse: 0.0551 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3032\n",
            "Epoch 72/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1038 - mae: 0.1072 - mse: 0.1038 - val_loss: 0.0552 - val_mae: 0.0982 - val_mse: 0.0552 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3033\n",
            "Epoch 73/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1038 - mae: 0.1070 - mse: 0.1038 - val_loss: 0.0550 - val_mae: 0.0971 - val_mse: 0.0550 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3030\n",
            "Epoch 74/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1038 - mae: 0.1066 - mse: 0.1038 - val_loss: 0.0549 - val_mae: 0.0975 - val_mse: 0.0549 - learning_rate: 0.1000 - val_custom_mse: 0.1943 - val_custom_mae: 0.3026\n",
            "Epoch 75/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1038 - mae: 0.1067 - mse: 0.1038 - val_loss: 0.0551 - val_mae: 0.0977 - val_mse: 0.0551 - learning_rate: 0.1000 - val_custom_mse: 0.1953 - val_custom_mae: 0.3033\n",
            "Epoch 76/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1037 - mae: 0.1063 - mse: 0.1037 - val_loss: 0.0549 - val_mae: 0.0966 - val_mse: 0.0549 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3028\n",
            "Epoch 77/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1037 - mae: 0.1062 - mse: 0.1037 - val_loss: 0.0549 - val_mae: 0.0979 - val_mse: 0.0549 - learning_rate: 0.1000 - val_custom_mse: 0.1944 - val_custom_mae: 0.3026\n",
            "Epoch 78/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1037 - mae: 0.1062 - mse: 0.1037 - val_loss: 0.0550 - val_mae: 0.0966 - val_mse: 0.0550 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3031\n",
            "Epoch 79/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1037 - mae: 0.1060 - mse: 0.1037 - val_loss: 0.0549 - val_mae: 0.0967 - val_mse: 0.0549 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3031\n",
            "Epoch 80/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1037 - mae: 0.1063 - mse: 0.1037 - val_loss: 0.0549 - val_mae: 0.0971 - val_mse: 0.0549 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3031\n",
            "Epoch 81/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1061 - mse: 0.1036 - val_loss: 0.0549 - val_mae: 0.0963 - val_mse: 0.0549 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3030\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 82: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1036 - mae: 0.1057 - mse: 0.1036 - val_loss: 0.0548 - val_mae: 0.0965 - val_mse: 0.0548 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3027\n",
            "Epoch 83/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1046 - mse: 0.1034 - val_loss: 0.0545 - val_mae: 0.0954 - val_mse: 0.0545 - learning_rate: 0.0200 - val_custom_mse: 0.1938 - val_custom_mae: 0.3022\n",
            "Epoch 84/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1046 - mse: 0.1034 - val_loss: 0.0545 - val_mae: 0.0952 - val_mse: 0.0545 - learning_rate: 0.0200 - val_custom_mse: 0.1939 - val_custom_mae: 0.3023\n",
            "Epoch 85/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1045 - mse: 0.1034 - val_loss: 0.0545 - val_mae: 0.0952 - val_mse: 0.0545 - learning_rate: 0.0200 - val_custom_mse: 0.1938 - val_custom_mae: 0.3022\n",
            "Epoch 86/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1045 - mse: 0.1034 - val_loss: 0.0545 - val_mae: 0.0953 - val_mse: 0.0545 - learning_rate: 0.0200 - val_custom_mse: 0.1938 - val_custom_mae: 0.3022\n",
            "Epoch 87/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1045 - mse: 0.1034 - val_loss: 0.0545 - val_mae: 0.0953 - val_mse: 0.0545 - learning_rate: 0.0200 - val_custom_mse: 0.1937 - val_custom_mae: 0.3022\n",
            "Epoch 88/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1045 - mse: 0.1034 - val_loss: 0.0545 - val_mae: 0.0953 - val_mse: 0.0545 - learning_rate: 0.0200 - val_custom_mse: 0.1938 - val_custom_mae: 0.3022\n",
            "Epoch 89/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1045 - mse: 0.1034 - val_loss: 0.0545 - val_mae: 0.0953 - val_mse: 0.0545 - learning_rate: 0.0200 - val_custom_mse: 0.1937 - val_custom_mae: 0.3022\n",
            "Epoch 90/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1044 - mse: 0.1034 - val_loss: 0.0545 - val_mae: 0.0950 - val_mse: 0.0545 - learning_rate: 0.0200 - val_custom_mse: 0.1939 - val_custom_mae: 0.3023\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1034 - mae: 0.1044 - mse: 0.1034 - val_loss: 0.0545 - val_mae: 0.0958 - val_mse: 0.0545 - learning_rate: 0.0200 - val_custom_mse: 0.1936 - val_custom_mae: 0.3021\n",
            "Epoch 92/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1040 - mse: 0.1034 - val_loss: 0.0546 - val_mae: 0.0952 - val_mse: 0.0546 - learning_rate: 0.0040 - val_custom_mse: 0.1939 - val_custom_mae: 0.3023\n",
            "Epoch 93/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1041 - mse: 0.1034 - val_loss: 0.0546 - val_mae: 0.0952 - val_mse: 0.0546 - learning_rate: 0.0040 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 94/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1041 - mse: 0.1034 - val_loss: 0.0546 - val_mae: 0.0952 - val_mse: 0.0546 - learning_rate: 0.0040 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 95/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1042 - mse: 0.1034 - val_loss: 0.0546 - val_mae: 0.0952 - val_mse: 0.0546 - learning_rate: 0.0040 - val_custom_mse: 0.1939 - val_custom_mae: 0.3023\n",
            "Epoch 96/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1042 - mse: 0.1034 - val_loss: 0.0546 - val_mae: 0.0952 - val_mse: 0.0546 - learning_rate: 0.0040 - val_custom_mse: 0.1940 - val_custom_mae: 0.3023\n",
            "Epoch 97/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1042 - mse: 0.1034 - val_loss: 0.0546 - val_mae: 0.0952 - val_mse: 0.0546 - learning_rate: 0.0040 - val_custom_mse: 0.1940 - val_custom_mae: 0.3023\n",
            "Epoch 98/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1041 - mse: 0.1034 - val_loss: 0.0546 - val_mae: 0.0952 - val_mse: 0.0546 - learning_rate: 0.0040 - val_custom_mse: 0.1939 - val_custom_mae: 0.3023\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1034 - mae: 0.1041 - mse: 0.1034 - val_loss: 0.0546 - val_mae: 0.0952 - val_mse: 0.0546 - learning_rate: 0.0040 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 100/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1033 - mae: 0.1040 - mse: 0.1033 - val_loss: 0.0546 - val_mae: 0.0952 - val_mse: 0.0546 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Running experiment: horizon=336, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_25', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1032/1032 - 13s - 13ms/step - loss: 0.3439 - mae: 0.3547 - mse: 0.3439 - val_loss: 0.1613 - val_mae: 0.2846 - val_mse: 0.1613 - learning_rate: 0.1000 - val_custom_mse: 0.2712 - val_custom_mae: 0.3678\n",
            "Epoch 2/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.2342 - mae: 0.2892 - mse: 0.2342 - val_loss: 0.1146 - val_mae: 0.2317 - val_mse: 0.1146 - learning_rate: 0.1000 - val_custom_mse: 0.2316 - val_custom_mae: 0.3389\n",
            "Epoch 3/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1771 - mae: 0.2401 - mse: 0.1771 - val_loss: 0.0973 - val_mae: 0.2053 - val_mse: 0.0973 - learning_rate: 0.1000 - val_custom_mse: 0.2185 - val_custom_mae: 0.3274\n",
            "Epoch 4/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1571 - mae: 0.2165 - mse: 0.1571 - val_loss: 0.0896 - val_mae: 0.1921 - val_mse: 0.0896 - learning_rate: 0.1000 - val_custom_mse: 0.2130 - val_custom_mae: 0.3224\n",
            "Epoch 5/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1478 - mae: 0.2036 - mse: 0.1478 - val_loss: 0.0847 - val_mae: 0.1830 - val_mse: 0.0847 - learning_rate: 0.1000 - val_custom_mse: 0.2095 - val_custom_mae: 0.3188\n",
            "Epoch 6/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1418 - mae: 0.1945 - mse: 0.1418 - val_loss: 0.0809 - val_mae: 0.1756 - val_mse: 0.0809 - learning_rate: 0.1000 - val_custom_mse: 0.2068 - val_custom_mae: 0.3161\n",
            "Epoch 7/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1375 - mae: 0.1877 - mse: 0.1375 - val_loss: 0.0780 - val_mae: 0.1699 - val_mse: 0.0780 - learning_rate: 0.1000 - val_custom_mse: 0.2045 - val_custom_mae: 0.3139\n",
            "Epoch 8/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1342 - mae: 0.1823 - mse: 0.1342 - val_loss: 0.0758 - val_mae: 0.1651 - val_mse: 0.0758 - learning_rate: 0.1000 - val_custom_mse: 0.2030 - val_custom_mae: 0.3122\n",
            "Epoch 9/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1316 - mae: 0.1780 - mse: 0.1316 - val_loss: 0.0740 - val_mae: 0.1612 - val_mse: 0.0740 - learning_rate: 0.1000 - val_custom_mse: 0.2019 - val_custom_mae: 0.3108\n",
            "Epoch 10/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1297 - mae: 0.1747 - mse: 0.1297 - val_loss: 0.0722 - val_mae: 0.1576 - val_mse: 0.0722 - learning_rate: 0.1000 - val_custom_mse: 0.2001 - val_custom_mae: 0.3091\n",
            "Epoch 11/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1279 - mae: 0.1717 - mse: 0.1279 - val_loss: 0.0708 - val_mae: 0.1545 - val_mse: 0.0708 - learning_rate: 0.1000 - val_custom_mse: 0.1990 - val_custom_mae: 0.3080\n",
            "Epoch 12/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1265 - mae: 0.1693 - mse: 0.1265 - val_loss: 0.0697 - val_mae: 0.1520 - val_mse: 0.0697 - learning_rate: 0.1000 - val_custom_mse: 0.1987 - val_custom_mae: 0.3074\n",
            "Epoch 13/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1254 - mae: 0.1671 - mse: 0.1254 - val_loss: 0.0687 - val_mae: 0.1495 - val_mse: 0.0687 - learning_rate: 0.1000 - val_custom_mse: 0.1981 - val_custom_mae: 0.3067\n",
            "Epoch 14/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1244 - mae: 0.1654 - mse: 0.1244 - val_loss: 0.0678 - val_mae: 0.1475 - val_mse: 0.0678 - learning_rate: 0.1000 - val_custom_mse: 0.1974 - val_custom_mae: 0.3060\n",
            "Epoch 15/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1236 - mae: 0.1637 - mse: 0.1236 - val_loss: 0.0670 - val_mae: 0.1460 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1969 - val_custom_mae: 0.3056\n",
            "Epoch 16/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1229 - mae: 0.1623 - mse: 0.1229 - val_loss: 0.0664 - val_mae: 0.1445 - val_mse: 0.0664 - learning_rate: 0.1000 - val_custom_mse: 0.1968 - val_custom_mae: 0.3055\n",
            "Epoch 17/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1610 - mse: 0.1222 - val_loss: 0.0659 - val_mae: 0.1428 - val_mse: 0.0659 - learning_rate: 0.1000 - val_custom_mse: 0.1968 - val_custom_mae: 0.3052\n",
            "Epoch 18/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1217 - mae: 0.1598 - mse: 0.1217 - val_loss: 0.0653 - val_mae: 0.1412 - val_mse: 0.0653 - learning_rate: 0.1000 - val_custom_mse: 0.1964 - val_custom_mae: 0.3049\n",
            "Epoch 19/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1212 - mae: 0.1587 - mse: 0.1212 - val_loss: 0.0650 - val_mae: 0.1408 - val_mse: 0.0650 - learning_rate: 0.1000 - val_custom_mse: 0.1963 - val_custom_mae: 0.3049\n",
            "Epoch 20/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1207 - mae: 0.1579 - mse: 0.1207 - val_loss: 0.0642 - val_mae: 0.1388 - val_mse: 0.0642 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3040\n",
            "Epoch 21/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1204 - mae: 0.1569 - mse: 0.1204 - val_loss: 0.0639 - val_mae: 0.1377 - val_mse: 0.0639 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3041\n",
            "Epoch 22/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1200 - mae: 0.1561 - mse: 0.1200 - val_loss: 0.0638 - val_mae: 0.1377 - val_mse: 0.0638 - learning_rate: 0.1000 - val_custom_mse: 0.1960 - val_custom_mae: 0.3045\n",
            "Epoch 23/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1198 - mae: 0.1555 - mse: 0.1198 - val_loss: 0.0632 - val_mae: 0.1358 - val_mse: 0.0632 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3038\n",
            "Epoch 24/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1194 - mae: 0.1547 - mse: 0.1194 - val_loss: 0.0630 - val_mae: 0.1354 - val_mse: 0.0630 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3040\n",
            "Epoch 25/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1191 - mae: 0.1541 - mse: 0.1191 - val_loss: 0.0626 - val_mae: 0.1344 - val_mse: 0.0626 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3034\n",
            "Epoch 26/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1189 - mae: 0.1535 - mse: 0.1189 - val_loss: 0.0624 - val_mae: 0.1337 - val_mse: 0.0624 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3036\n",
            "Epoch 27/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1187 - mae: 0.1530 - mse: 0.1187 - val_loss: 0.0621 - val_mae: 0.1334 - val_mse: 0.0621 - learning_rate: 0.1000 - val_custom_mse: 0.1943 - val_custom_mae: 0.3031\n",
            "Epoch 28/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1185 - mae: 0.1525 - mse: 0.1185 - val_loss: 0.0621 - val_mae: 0.1325 - val_mse: 0.0621 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3038\n",
            "Epoch 29/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1184 - mae: 0.1521 - mse: 0.1184 - val_loss: 0.0618 - val_mae: 0.1317 - val_mse: 0.0618 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3034\n",
            "Epoch 30/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1182 - mae: 0.1517 - mse: 0.1182 - val_loss: 0.0617 - val_mae: 0.1313 - val_mse: 0.0617 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3034\n",
            "Epoch 31/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1180 - mae: 0.1513 - mse: 0.1180 - val_loss: 0.0615 - val_mae: 0.1308 - val_mse: 0.0615 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3033\n",
            "Epoch 32/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1179 - mae: 0.1509 - mse: 0.1179 - val_loss: 0.0614 - val_mae: 0.1306 - val_mse: 0.0614 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3033\n",
            "Epoch 33/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1177 - mae: 0.1506 - mse: 0.1177 - val_loss: 0.0612 - val_mae: 0.1300 - val_mse: 0.0612 - learning_rate: 0.1000 - val_custom_mse: 0.1943 - val_custom_mae: 0.3029\n",
            "Epoch 34/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1175 - mae: 0.1502 - mse: 0.1175 - val_loss: 0.0610 - val_mae: 0.1296 - val_mse: 0.0610 - learning_rate: 0.1000 - val_custom_mse: 0.1944 - val_custom_mae: 0.3030\n",
            "Epoch 35/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1175 - mae: 0.1500 - mse: 0.1175 - val_loss: 0.0614 - val_mae: 0.1307 - val_mse: 0.0614 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3040\n",
            "Epoch 36/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1175 - mae: 0.1498 - mse: 0.1175 - val_loss: 0.0612 - val_mae: 0.1299 - val_mse: 0.0612 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3038\n",
            "Epoch 37/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1173 - mae: 0.1495 - mse: 0.1173 - val_loss: 0.0607 - val_mae: 0.1288 - val_mse: 0.0607 - learning_rate: 0.1000 - val_custom_mse: 0.1942 - val_custom_mae: 0.3029\n",
            "Epoch 38/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1173 - mae: 0.1493 - mse: 0.1173 - val_loss: 0.0607 - val_mae: 0.1286 - val_mse: 0.0607 - learning_rate: 0.1000 - val_custom_mse: 0.1944 - val_custom_mae: 0.3030\n",
            "Epoch 39/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1171 - mae: 0.1491 - mse: 0.1171 - val_loss: 0.0606 - val_mae: 0.1286 - val_mse: 0.0606 - learning_rate: 0.1000 - val_custom_mse: 0.1940 - val_custom_mae: 0.3026\n",
            "Epoch 40/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1170 - mae: 0.1489 - mse: 0.1170 - val_loss: 0.0605 - val_mae: 0.1279 - val_mse: 0.0605 - learning_rate: 0.1000 - val_custom_mse: 0.1943 - val_custom_mae: 0.3029\n",
            "Epoch 41/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1170 - mae: 0.1487 - mse: 0.1170 - val_loss: 0.0607 - val_mae: 0.1286 - val_mse: 0.0607 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3035\n",
            "Epoch 42/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1169 - mae: 0.1485 - mse: 0.1169 - val_loss: 0.0603 - val_mae: 0.1275 - val_mse: 0.0603 - learning_rate: 0.1000 - val_custom_mse: 0.1940 - val_custom_mae: 0.3026\n",
            "Epoch 43/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1169 - mae: 0.1484 - mse: 0.1169 - val_loss: 0.0605 - val_mae: 0.1279 - val_mse: 0.0605 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3033\n",
            "Epoch 44/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1167 - mae: 0.1482 - mse: 0.1167 - val_loss: 0.0602 - val_mae: 0.1271 - val_mse: 0.0602 - learning_rate: 0.1000 - val_custom_mse: 0.1941 - val_custom_mae: 0.3027\n",
            "Epoch 45/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1168 - mae: 0.1481 - mse: 0.1168 - val_loss: 0.0602 - val_mae: 0.1269 - val_mse: 0.0602 - learning_rate: 0.1000 - val_custom_mse: 0.1940 - val_custom_mae: 0.3026\n",
            "Epoch 46/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1166 - mae: 0.1479 - mse: 0.1166 - val_loss: 0.0601 - val_mae: 0.1268 - val_mse: 0.0601 - learning_rate: 0.1000 - val_custom_mse: 0.1940 - val_custom_mae: 0.3026\n",
            "Epoch 47/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1166 - mae: 0.1479 - mse: 0.1166 - val_loss: 0.0600 - val_mae: 0.1269 - val_mse: 0.0600 - learning_rate: 0.1000 - val_custom_mse: 0.1936 - val_custom_mae: 0.3022\n",
            "Epoch 48/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1166 - mae: 0.1477 - mse: 0.1166 - val_loss: 0.0601 - val_mae: 0.1267 - val_mse: 0.0601 - learning_rate: 0.1000 - val_custom_mse: 0.1939 - val_custom_mae: 0.3027\n",
            "Epoch 49/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1164 - mae: 0.1476 - mse: 0.1164 - val_loss: 0.0601 - val_mae: 0.1267 - val_mse: 0.0601 - learning_rate: 0.1000 - val_custom_mse: 0.1940 - val_custom_mae: 0.3028\n",
            "Epoch 50/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1164 - mae: 0.1475 - mse: 0.1164 - val_loss: 0.0600 - val_mae: 0.1263 - val_mse: 0.0600 - learning_rate: 0.1000 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 51/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1163 - mae: 0.1474 - mse: 0.1163 - val_loss: 0.0600 - val_mae: 0.1264 - val_mse: 0.0600 - learning_rate: 0.1000 - val_custom_mse: 0.1939 - val_custom_mae: 0.3027\n",
            "Epoch 52/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1164 - mae: 0.1474 - mse: 0.1164 - val_loss: 0.0600 - val_mae: 0.1261 - val_mse: 0.0600 - learning_rate: 0.1000 - val_custom_mse: 0.1942 - val_custom_mae: 0.3028\n",
            "Epoch 53/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1162 - mae: 0.1472 - mse: 0.1162 - val_loss: 0.0598 - val_mae: 0.1259 - val_mse: 0.0598 - learning_rate: 0.1000 - val_custom_mse: 0.1935 - val_custom_mae: 0.3022\n",
            "Epoch 54/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1162 - mae: 0.1472 - mse: 0.1162 - val_loss: 0.0598 - val_mae: 0.1258 - val_mse: 0.0598 - learning_rate: 0.1000 - val_custom_mse: 0.1938 - val_custom_mae: 0.3024\n",
            "Epoch 55/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1162 - mae: 0.1471 - mse: 0.1162 - val_loss: 0.0600 - val_mae: 0.1262 - val_mse: 0.0600 - learning_rate: 0.1000 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 56/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1162 - mae: 0.1471 - mse: 0.1162 - val_loss: 0.0598 - val_mae: 0.1257 - val_mse: 0.0598 - learning_rate: 0.1000 - val_custom_mse: 0.1939 - val_custom_mae: 0.3025\n",
            "Epoch 57/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1161 - mae: 0.1470 - mse: 0.1161 - val_loss: 0.0597 - val_mae: 0.1258 - val_mse: 0.0597 - learning_rate: 0.1000 - val_custom_mse: 0.1935 - val_custom_mae: 0.3022\n",
            "Epoch 58/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1161 - mae: 0.1469 - mse: 0.1161 - val_loss: 0.0597 - val_mae: 0.1257 - val_mse: 0.0597 - learning_rate: 0.1000 - val_custom_mse: 0.1935 - val_custom_mae: 0.3023\n",
            "Epoch 59/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1160 - mae: 0.1469 - mse: 0.1160 - val_loss: 0.0602 - val_mae: 0.1273 - val_mse: 0.0602 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3033\n",
            "Epoch 60/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1160 - mae: 0.1468 - mse: 0.1160 - val_loss: 0.0597 - val_mae: 0.1254 - val_mse: 0.0597 - learning_rate: 0.1000 - val_custom_mse: 0.1938 - val_custom_mae: 0.3023\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1161 - mae: 0.1468 - mse: 0.1161 - val_loss: 0.0598 - val_mae: 0.1255 - val_mse: 0.0598 - learning_rate: 0.1000 - val_custom_mse: 0.1941 - val_custom_mae: 0.3026\n",
            "Epoch 62/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1465 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 63/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1464 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1257 - val_mse: 0.0598 - learning_rate: 0.0200 - val_custom_mse: 0.1939 - val_custom_mae: 0.3024\n",
            "Epoch 64/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1464 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0200 - val_custom_mse: 0.1939 - val_custom_mae: 0.3024\n",
            "Epoch 65/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1464 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 66/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1464 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1257 - val_mse: 0.0598 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 67/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1464 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 68/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1464 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0200 - val_custom_mse: 0.1941 - val_custom_mae: 0.3025\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1464 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 70/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3025\n",
            "Epoch 71/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3025\n",
            "Epoch 72/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3024\n",
            "Epoch 73/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3024\n",
            "Epoch 74/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3025\n",
            "Epoch 75/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3025\n",
            "Epoch 76/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3024\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3025\n",
            "Epoch 78/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 79/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1941 - val_custom_mae: 0.3025\n",
            "Epoch 80/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 81/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3025\n",
            "Epoch 82/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 83/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 84/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 86/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3025\n",
            "Epoch 87/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1159 - mae: 0.1463 - mse: 0.1159 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 88/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3025\n",
            "Epoch 89/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 90/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 91/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 92/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 94/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 95/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 96/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 97/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 98/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1157 - mae: 0.1463 - mse: 0.1157 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 99/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Epoch 100/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1158 - mae: 0.1463 - mse: 0.1158 - val_loss: 0.0598 - val_mae: 0.1256 - val_mse: 0.0598 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1940 - val_custom_mae: 0.3024\n",
            "Running experiment: horizon=336, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_26', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1032/1032 - 13s - 13ms/step - loss: 0.3448 - mae: 0.3544 - mse: 0.3448 - val_loss: 0.1606 - val_mae: 0.2836 - val_mse: 0.1606 - learning_rate: 0.1000 - val_custom_mse: 0.2712 - val_custom_mae: 0.3678\n",
            "Epoch 2/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.2343 - mae: 0.2890 - mse: 0.2343 - val_loss: 0.1152 - val_mae: 0.2314 - val_mse: 0.1152 - learning_rate: 0.1000 - val_custom_mse: 0.2344 - val_custom_mae: 0.3406\n",
            "Epoch 3/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1781 - mae: 0.2408 - mse: 0.1781 - val_loss: 0.0974 - val_mae: 0.2062 - val_mse: 0.0974 - learning_rate: 0.1000 - val_custom_mse: 0.2183 - val_custom_mae: 0.3277\n",
            "Epoch 4/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1589 - mae: 0.2186 - mse: 0.1589 - val_loss: 0.0898 - val_mae: 0.1927 - val_mse: 0.0898 - learning_rate: 0.1000 - val_custom_mse: 0.2127 - val_custom_mae: 0.3222\n",
            "Epoch 5/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1503 - mae: 0.2067 - mse: 0.1503 - val_loss: 0.0850 - val_mae: 0.1838 - val_mse: 0.0850 - learning_rate: 0.1000 - val_custom_mse: 0.2094 - val_custom_mae: 0.3188\n",
            "Epoch 6/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1448 - mae: 0.1986 - mse: 0.1448 - val_loss: 0.0814 - val_mae: 0.1773 - val_mse: 0.0814 - learning_rate: 0.1000 - val_custom_mse: 0.2063 - val_custom_mae: 0.3160\n",
            "Epoch 7/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1409 - mae: 0.1927 - mse: 0.1409 - val_loss: 0.0789 - val_mae: 0.1720 - val_mse: 0.0789 - learning_rate: 0.1000 - val_custom_mse: 0.2050 - val_custom_mae: 0.3143\n",
            "Epoch 8/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1379 - mae: 0.1881 - mse: 0.1379 - val_loss: 0.0765 - val_mae: 0.1671 - val_mse: 0.0765 - learning_rate: 0.1000 - val_custom_mse: 0.2031 - val_custom_mae: 0.3123\n",
            "Epoch 9/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1357 - mae: 0.1847 - mse: 0.1357 - val_loss: 0.0748 - val_mae: 0.1636 - val_mse: 0.0748 - learning_rate: 0.1000 - val_custom_mse: 0.2018 - val_custom_mae: 0.3108\n",
            "Epoch 10/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1339 - mae: 0.1819 - mse: 0.1339 - val_loss: 0.0735 - val_mae: 0.1606 - val_mse: 0.0735 - learning_rate: 0.1000 - val_custom_mse: 0.2009 - val_custom_mae: 0.3098\n",
            "Epoch 11/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1326 - mae: 0.1797 - mse: 0.1326 - val_loss: 0.0721 - val_mae: 0.1578 - val_mse: 0.0721 - learning_rate: 0.1000 - val_custom_mse: 0.1998 - val_custom_mae: 0.3087\n",
            "Epoch 12/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1313 - mae: 0.1777 - mse: 0.1313 - val_loss: 0.0711 - val_mae: 0.1555 - val_mse: 0.0711 - learning_rate: 0.1000 - val_custom_mse: 0.1991 - val_custom_mae: 0.3078\n",
            "Epoch 13/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1303 - mae: 0.1762 - mse: 0.1303 - val_loss: 0.0702 - val_mae: 0.1538 - val_mse: 0.0702 - learning_rate: 0.1000 - val_custom_mse: 0.1984 - val_custom_mae: 0.3071\n",
            "Epoch 14/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1294 - mae: 0.1748 - mse: 0.1294 - val_loss: 0.0699 - val_mae: 0.1534 - val_mse: 0.0699 - learning_rate: 0.1000 - val_custom_mse: 0.1985 - val_custom_mae: 0.3070\n",
            "Epoch 15/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1288 - mae: 0.1737 - mse: 0.1288 - val_loss: 0.0691 - val_mae: 0.1514 - val_mse: 0.0691 - learning_rate: 0.1000 - val_custom_mse: 0.1980 - val_custom_mae: 0.3065\n",
            "Epoch 16/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1282 - mae: 0.1727 - mse: 0.1282 - val_loss: 0.0685 - val_mae: 0.1498 - val_mse: 0.0685 - learning_rate: 0.1000 - val_custom_mse: 0.1977 - val_custom_mae: 0.3061\n",
            "Epoch 17/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1277 - mae: 0.1718 - mse: 0.1277 - val_loss: 0.0679 - val_mae: 0.1485 - val_mse: 0.0679 - learning_rate: 0.1000 - val_custom_mse: 0.1973 - val_custom_mae: 0.3057\n",
            "Epoch 18/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1272 - mae: 0.1710 - mse: 0.1272 - val_loss: 0.0673 - val_mae: 0.1473 - val_mse: 0.0673 - learning_rate: 0.1000 - val_custom_mse: 0.1967 - val_custom_mae: 0.3051\n",
            "Epoch 19/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1268 - mae: 0.1703 - mse: 0.1268 - val_loss: 0.0668 - val_mae: 0.1462 - val_mse: 0.0668 - learning_rate: 0.1000 - val_custom_mse: 0.1961 - val_custom_mae: 0.3047\n",
            "Epoch 20/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1263 - mae: 0.1696 - mse: 0.1263 - val_loss: 0.0666 - val_mae: 0.1456 - val_mse: 0.0666 - learning_rate: 0.1000 - val_custom_mse: 0.1963 - val_custom_mae: 0.3048\n",
            "Epoch 21/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1261 - mae: 0.1691 - mse: 0.1261 - val_loss: 0.0664 - val_mae: 0.1448 - val_mse: 0.0664 - learning_rate: 0.1000 - val_custom_mse: 0.1966 - val_custom_mae: 0.3048\n",
            "Epoch 22/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1259 - mae: 0.1687 - mse: 0.1259 - val_loss: 0.0660 - val_mae: 0.1442 - val_mse: 0.0660 - learning_rate: 0.1000 - val_custom_mse: 0.1959 - val_custom_mae: 0.3043\n",
            "Epoch 23/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1255 - mae: 0.1681 - mse: 0.1255 - val_loss: 0.0658 - val_mae: 0.1434 - val_mse: 0.0658 - learning_rate: 0.1000 - val_custom_mse: 0.1958 - val_custom_mae: 0.3043\n",
            "Epoch 24/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1253 - mae: 0.1677 - mse: 0.1253 - val_loss: 0.0655 - val_mae: 0.1429 - val_mse: 0.0655 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3039\n",
            "Epoch 25/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1251 - mae: 0.1673 - mse: 0.1251 - val_loss: 0.0655 - val_mae: 0.1426 - val_mse: 0.0655 - learning_rate: 0.1000 - val_custom_mse: 0.1960 - val_custom_mae: 0.3043\n",
            "Epoch 26/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1249 - mae: 0.1669 - mse: 0.1249 - val_loss: 0.0653 - val_mae: 0.1422 - val_mse: 0.0653 - learning_rate: 0.1000 - val_custom_mse: 0.1959 - val_custom_mae: 0.3042\n",
            "Epoch 27/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1248 - mae: 0.1667 - mse: 0.1248 - val_loss: 0.0651 - val_mae: 0.1417 - val_mse: 0.0651 - learning_rate: 0.1000 - val_custom_mse: 0.1958 - val_custom_mae: 0.3041\n",
            "Epoch 28/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1245 - mae: 0.1663 - mse: 0.1245 - val_loss: 0.0649 - val_mae: 0.1412 - val_mse: 0.0649 - learning_rate: 0.1000 - val_custom_mse: 0.1953 - val_custom_mae: 0.3037\n",
            "Epoch 29/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1244 - mae: 0.1661 - mse: 0.1244 - val_loss: 0.0649 - val_mae: 0.1409 - val_mse: 0.0649 - learning_rate: 0.1000 - val_custom_mse: 0.1957 - val_custom_mae: 0.3040\n",
            "Epoch 30/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1244 - mae: 0.1659 - mse: 0.1244 - val_loss: 0.0647 - val_mae: 0.1407 - val_mse: 0.0647 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3039\n",
            "Epoch 31/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1242 - mae: 0.1656 - mse: 0.1242 - val_loss: 0.0648 - val_mae: 0.1405 - val_mse: 0.0648 - learning_rate: 0.1000 - val_custom_mse: 0.1959 - val_custom_mae: 0.3041\n",
            "Epoch 32/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1240 - mae: 0.1653 - mse: 0.1240 - val_loss: 0.0646 - val_mae: 0.1402 - val_mse: 0.0646 - learning_rate: 0.1000 - val_custom_mse: 0.1956 - val_custom_mae: 0.3039\n",
            "Epoch 33/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1239 - mae: 0.1652 - mse: 0.1239 - val_loss: 0.0644 - val_mae: 0.1398 - val_mse: 0.0644 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3037\n",
            "Epoch 34/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1238 - mae: 0.1651 - mse: 0.1238 - val_loss: 0.0644 - val_mae: 0.1398 - val_mse: 0.0644 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3038\n",
            "Epoch 35/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1237 - mae: 0.1649 - mse: 0.1237 - val_loss: 0.0642 - val_mae: 0.1394 - val_mse: 0.0642 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3033\n",
            "Epoch 36/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1236 - mae: 0.1647 - mse: 0.1236 - val_loss: 0.0642 - val_mae: 0.1392 - val_mse: 0.0642 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3035\n",
            "Epoch 37/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1234 - mae: 0.1645 - mse: 0.1234 - val_loss: 0.0641 - val_mae: 0.1390 - val_mse: 0.0641 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3033\n",
            "Epoch 38/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1235 - mae: 0.1644 - mse: 0.1235 - val_loss: 0.0641 - val_mae: 0.1389 - val_mse: 0.0641 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3035\n",
            "Epoch 39/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1233 - mae: 0.1643 - mse: 0.1233 - val_loss: 0.0640 - val_mae: 0.1390 - val_mse: 0.0640 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3031\n",
            "Epoch 40/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1233 - mae: 0.1642 - mse: 0.1233 - val_loss: 0.0640 - val_mae: 0.1387 - val_mse: 0.0640 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3034\n",
            "Epoch 41/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1231 - mae: 0.1640 - mse: 0.1231 - val_loss: 0.0640 - val_mae: 0.1385 - val_mse: 0.0640 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3034\n",
            "Epoch 42/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1231 - mae: 0.1639 - mse: 0.1231 - val_loss: 0.0640 - val_mae: 0.1388 - val_mse: 0.0640 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3036\n",
            "Epoch 43/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1231 - mae: 0.1639 - mse: 0.1231 - val_loss: 0.0642 - val_mae: 0.1391 - val_mse: 0.0642 - learning_rate: 0.1000 - val_custom_mse: 0.1958 - val_custom_mae: 0.3039\n",
            "Epoch 44/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1230 - mae: 0.1638 - mse: 0.1230 - val_loss: 0.0639 - val_mae: 0.1383 - val_mse: 0.0639 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3034\n",
            "Epoch 45/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1230 - mae: 0.1637 - mse: 0.1230 - val_loss: 0.0638 - val_mae: 0.1384 - val_mse: 0.0638 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3033\n",
            "Epoch 46/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1228 - mae: 0.1636 - mse: 0.1228 - val_loss: 0.0642 - val_mae: 0.1392 - val_mse: 0.0642 - learning_rate: 0.1000 - val_custom_mse: 0.1957 - val_custom_mae: 0.3039\n",
            "Epoch 47/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1228 - mae: 0.1635 - mse: 0.1228 - val_loss: 0.0637 - val_mae: 0.1384 - val_mse: 0.0637 - learning_rate: 0.1000 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 48/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1228 - mae: 0.1635 - mse: 0.1228 - val_loss: 0.0636 - val_mae: 0.1380 - val_mse: 0.0636 - learning_rate: 0.1000 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 49/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1227 - mae: 0.1634 - mse: 0.1227 - val_loss: 0.0637 - val_mae: 0.1379 - val_mse: 0.0637 - learning_rate: 0.1000 - val_custom_mse: 0.1948 - val_custom_mae: 0.3031\n",
            "Epoch 50/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1228 - mae: 0.1634 - mse: 0.1228 - val_loss: 0.0635 - val_mae: 0.1379 - val_mse: 0.0635 - learning_rate: 0.1000 - val_custom_mse: 0.1943 - val_custom_mae: 0.3027\n",
            "Epoch 51/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1227 - mae: 0.1633 - mse: 0.1227 - val_loss: 0.0637 - val_mae: 0.1381 - val_mse: 0.0637 - learning_rate: 0.1000 - val_custom_mse: 0.1946 - val_custom_mae: 0.3028\n",
            "Epoch 52/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1227 - mae: 0.1633 - mse: 0.1227 - val_loss: 0.0637 - val_mae: 0.1378 - val_mse: 0.0637 - learning_rate: 0.1000 - val_custom_mse: 0.1948 - val_custom_mae: 0.3030\n",
            "Epoch 53/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1226 - mae: 0.1632 - mse: 0.1226 - val_loss: 0.0637 - val_mae: 0.1383 - val_mse: 0.0637 - learning_rate: 0.1000 - val_custom_mse: 0.1946 - val_custom_mae: 0.3028\n",
            "Epoch 54/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1227 - mae: 0.1632 - mse: 0.1227 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.1000 - val_custom_mse: 0.1941 - val_custom_mae: 0.3025\n",
            "Epoch 55/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1225 - mae: 0.1630 - mse: 0.1225 - val_loss: 0.0635 - val_mae: 0.1376 - val_mse: 0.0635 - learning_rate: 0.1000 - val_custom_mse: 0.1945 - val_custom_mae: 0.3028\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1226 - mae: 0.1631 - mse: 0.1226 - val_loss: 0.0635 - val_mae: 0.1375 - val_mse: 0.0635 - learning_rate: 0.1000 - val_custom_mse: 0.1945 - val_custom_mae: 0.3028\n",
            "Epoch 57/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0200 - val_custom_mse: 0.1941 - val_custom_mae: 0.3026\n",
            "Epoch 58/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1628 - mse: 0.1223 - val_loss: 0.0634 - val_mae: 0.1377 - val_mse: 0.0634 - learning_rate: 0.0200 - val_custom_mse: 0.1941 - val_custom_mae: 0.3026\n",
            "Epoch 59/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1627 - mse: 0.1222 - val_loss: 0.0634 - val_mae: 0.1377 - val_mse: 0.0634 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3026\n",
            "Epoch 60/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0634 - val_mae: 0.1378 - val_mse: 0.0634 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3025\n",
            "Epoch 61/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1222 - mae: 0.1627 - mse: 0.1222 - val_loss: 0.0634 - val_mae: 0.1378 - val_mse: 0.0634 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3026\n",
            "Epoch 62/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1224 - mae: 0.1627 - mse: 0.1224 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0200 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 63/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1224 - mae: 0.1628 - mse: 0.1224 - val_loss: 0.0634 - val_mae: 0.1378 - val_mse: 0.0634 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3026\n",
            "Epoch 64/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0634 - val_mae: 0.1378 - val_mse: 0.0634 - learning_rate: 0.0200 - val_custom_mse: 0.1940 - val_custom_mae: 0.3025\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1627 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1377 - val_mse: 0.0635 - learning_rate: 0.0200 - val_custom_mse: 0.1941 - val_custom_mae: 0.3027\n",
            "Epoch 66/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1626 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3026\n",
            "Epoch 67/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1222 - mae: 0.1627 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3026\n",
            "Epoch 68/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0040 - val_custom_mse: 0.1942 - val_custom_mae: 0.3026\n",
            "Epoch 69/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0040 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 70/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0040 - val_custom_mse: 0.1942 - val_custom_mae: 0.3026\n",
            "Epoch 71/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3026\n",
            "Epoch 72/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0040 - val_custom_mse: 0.1941 - val_custom_mae: 0.3026\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 0.0040 - val_custom_mse: 0.1942 - val_custom_mae: 0.3026\n",
            "Epoch 74/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 75/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 76/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1626 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 77/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 78/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 79/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 80/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 82/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 83/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1627 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 84/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1626 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 85/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1626 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 86/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1627 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 87/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1221 - mae: 0.1626 - mse: 0.1221 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 88/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1627 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 90/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 91/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 92/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1627 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 93/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 94/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 95/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1626 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 96/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1626 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 98/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1222 - mae: 0.1626 - mse: 0.1222 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 6.4000e-06 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 99/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1626 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 6.4000e-06 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Epoch 100/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1223 - mae: 0.1626 - mse: 0.1223 - val_loss: 0.0635 - val_mae: 0.1378 - val_mse: 0.0635 - learning_rate: 6.4000e-06 - val_custom_mse: 0.1942 - val_custom_mae: 0.3027\n",
            "Running experiment: horizon=336, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_27', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1032/1032 - 13s - 13ms/step - loss: 0.3469 - mae: 0.3547 - mse: 0.3469 - val_loss: 0.1603 - val_mae: 0.2839 - val_mse: 0.1603 - learning_rate: 0.1000 - val_custom_mse: 0.2697 - val_custom_mae: 0.3671\n",
            "Epoch 2/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.2349 - mae: 0.2888 - mse: 0.2349 - val_loss: 0.1148 - val_mae: 0.2313 - val_mse: 0.1148 - learning_rate: 0.1000 - val_custom_mse: 0.2322 - val_custom_mae: 0.3394\n",
            "Epoch 3/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1793 - mae: 0.2416 - mse: 0.1793 - val_loss: 0.0977 - val_mae: 0.2054 - val_mse: 0.0977 - learning_rate: 0.1000 - val_custom_mse: 0.2194 - val_custom_mae: 0.3279\n",
            "Epoch 4/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1612 - mae: 0.2210 - mse: 0.1612 - val_loss: 0.0902 - val_mae: 0.1938 - val_mse: 0.0902 - learning_rate: 0.1000 - val_custom_mse: 0.2129 - val_custom_mae: 0.3225\n",
            "Epoch 5/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1534 - mae: 0.2103 - mse: 0.1534 - val_loss: 0.0857 - val_mae: 0.1851 - val_mse: 0.0857 - learning_rate: 0.1000 - val_custom_mse: 0.2098 - val_custom_mae: 0.3192\n",
            "Epoch 6/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1483 - mae: 0.2032 - mse: 0.1483 - val_loss: 0.0823 - val_mae: 0.1788 - val_mse: 0.0823 - learning_rate: 0.1000 - val_custom_mse: 0.2074 - val_custom_mae: 0.3169\n",
            "Epoch 7/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1447 - mae: 0.1979 - mse: 0.1447 - val_loss: 0.0797 - val_mae: 0.1740 - val_mse: 0.0797 - learning_rate: 0.1000 - val_custom_mse: 0.2050 - val_custom_mae: 0.3145\n",
            "Epoch 8/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1422 - mae: 0.1941 - mse: 0.1422 - val_loss: 0.0777 - val_mae: 0.1699 - val_mse: 0.0777 - learning_rate: 0.1000 - val_custom_mse: 0.2033 - val_custom_mae: 0.3126\n",
            "Epoch 9/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1401 - mae: 0.1911 - mse: 0.1401 - val_loss: 0.0761 - val_mae: 0.1666 - val_mse: 0.0761 - learning_rate: 0.1000 - val_custom_mse: 0.2021 - val_custom_mae: 0.3113\n",
            "Epoch 10/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1386 - mae: 0.1888 - mse: 0.1386 - val_loss: 0.0748 - val_mae: 0.1641 - val_mse: 0.0748 - learning_rate: 0.1000 - val_custom_mse: 0.2009 - val_custom_mae: 0.3100\n",
            "Epoch 11/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1371 - mae: 0.1868 - mse: 0.1371 - val_loss: 0.0737 - val_mae: 0.1621 - val_mse: 0.0737 - learning_rate: 0.1000 - val_custom_mse: 0.1998 - val_custom_mae: 0.3088\n",
            "Epoch 12/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1360 - mae: 0.1852 - mse: 0.1360 - val_loss: 0.0728 - val_mae: 0.1601 - val_mse: 0.0728 - learning_rate: 0.1000 - val_custom_mse: 0.1991 - val_custom_mae: 0.3079\n",
            "Epoch 13/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1353 - mae: 0.1840 - mse: 0.1353 - val_loss: 0.0720 - val_mae: 0.1584 - val_mse: 0.0720 - learning_rate: 0.1000 - val_custom_mse: 0.1984 - val_custom_mae: 0.3073\n",
            "Epoch 14/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1345 - mae: 0.1828 - mse: 0.1345 - val_loss: 0.0715 - val_mae: 0.1572 - val_mse: 0.0715 - learning_rate: 0.1000 - val_custom_mse: 0.1982 - val_custom_mae: 0.3069\n",
            "Epoch 15/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1338 - mae: 0.1819 - mse: 0.1338 - val_loss: 0.0709 - val_mae: 0.1561 - val_mse: 0.0709 - learning_rate: 0.1000 - val_custom_mse: 0.1978 - val_custom_mae: 0.3064\n",
            "Epoch 16/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1333 - mae: 0.1811 - mse: 0.1333 - val_loss: 0.0703 - val_mae: 0.1551 - val_mse: 0.0703 - learning_rate: 0.1000 - val_custom_mse: 0.1970 - val_custom_mae: 0.3057\n",
            "Epoch 17/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1327 - mae: 0.1804 - mse: 0.1327 - val_loss: 0.0699 - val_mae: 0.1542 - val_mse: 0.0699 - learning_rate: 0.1000 - val_custom_mse: 0.1966 - val_custom_mae: 0.3053\n",
            "Epoch 18/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1324 - mae: 0.1797 - mse: 0.1324 - val_loss: 0.0698 - val_mae: 0.1537 - val_mse: 0.0698 - learning_rate: 0.1000 - val_custom_mse: 0.1973 - val_custom_mae: 0.3057\n",
            "Epoch 19/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1320 - mae: 0.1792 - mse: 0.1320 - val_loss: 0.0693 - val_mae: 0.1528 - val_mse: 0.0693 - learning_rate: 0.1000 - val_custom_mse: 0.1965 - val_custom_mae: 0.3051\n",
            "Epoch 20/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1317 - mae: 0.1787 - mse: 0.1317 - val_loss: 0.0690 - val_mae: 0.1522 - val_mse: 0.0690 - learning_rate: 0.1000 - val_custom_mse: 0.1961 - val_custom_mae: 0.3047\n",
            "Epoch 21/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1313 - mae: 0.1783 - mse: 0.1313 - val_loss: 0.0689 - val_mae: 0.1516 - val_mse: 0.0689 - learning_rate: 0.1000 - val_custom_mse: 0.1965 - val_custom_mae: 0.3050\n",
            "Epoch 22/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1312 - mae: 0.1778 - mse: 0.1312 - val_loss: 0.0687 - val_mae: 0.1511 - val_mse: 0.0687 - learning_rate: 0.1000 - val_custom_mse: 0.1965 - val_custom_mae: 0.3049\n",
            "Epoch 23/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1309 - mae: 0.1775 - mse: 0.1309 - val_loss: 0.0684 - val_mae: 0.1507 - val_mse: 0.0684 - learning_rate: 0.1000 - val_custom_mse: 0.1961 - val_custom_mae: 0.3045\n",
            "Epoch 24/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1307 - mae: 0.1772 - mse: 0.1307 - val_loss: 0.0685 - val_mae: 0.1507 - val_mse: 0.0685 - learning_rate: 0.1000 - val_custom_mse: 0.1964 - val_custom_mae: 0.3048\n",
            "Epoch 25/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1305 - mae: 0.1769 - mse: 0.1305 - val_loss: 0.0682 - val_mae: 0.1501 - val_mse: 0.0682 - learning_rate: 0.1000 - val_custom_mse: 0.1960 - val_custom_mae: 0.3045\n",
            "Epoch 26/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1303 - mae: 0.1766 - mse: 0.1303 - val_loss: 0.0679 - val_mae: 0.1497 - val_mse: 0.0679 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3041\n",
            "Epoch 27/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1301 - mae: 0.1764 - mse: 0.1301 - val_loss: 0.0678 - val_mae: 0.1493 - val_mse: 0.0678 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3040\n",
            "Epoch 28/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1301 - mae: 0.1761 - mse: 0.1301 - val_loss: 0.0679 - val_mae: 0.1493 - val_mse: 0.0679 - learning_rate: 0.1000 - val_custom_mse: 0.1961 - val_custom_mae: 0.3044\n",
            "Epoch 29/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1297 - mae: 0.1758 - mse: 0.1297 - val_loss: 0.0676 - val_mae: 0.1487 - val_mse: 0.0676 - learning_rate: 0.1000 - val_custom_mse: 0.1954 - val_custom_mae: 0.3039\n",
            "Epoch 30/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1297 - mae: 0.1757 - mse: 0.1297 - val_loss: 0.0675 - val_mae: 0.1486 - val_mse: 0.0675 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3037\n",
            "Epoch 31/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1295 - mae: 0.1755 - mse: 0.1295 - val_loss: 0.0676 - val_mae: 0.1489 - val_mse: 0.0676 - learning_rate: 0.1000 - val_custom_mse: 0.1956 - val_custom_mae: 0.3041\n",
            "Epoch 32/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1295 - mae: 0.1753 - mse: 0.1295 - val_loss: 0.0673 - val_mae: 0.1482 - val_mse: 0.0673 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3036\n",
            "Epoch 33/100\n",
            "1032/1032 - 9s - 8ms/step - loss: 0.1293 - mae: 0.1752 - mse: 0.1293 - val_loss: 0.0674 - val_mae: 0.1482 - val_mse: 0.0674 - learning_rate: 0.1000 - val_custom_mse: 0.1955 - val_custom_mae: 0.3040\n",
            "Epoch 34/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1292 - mae: 0.1751 - mse: 0.1292 - val_loss: 0.0672 - val_mae: 0.1480 - val_mse: 0.0672 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3035\n",
            "Epoch 35/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1291 - mae: 0.1749 - mse: 0.1291 - val_loss: 0.0672 - val_mae: 0.1478 - val_mse: 0.0672 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3037\n",
            "Epoch 36/100\n",
            "1032/1032 - 9s - 9ms/step - loss: 0.1290 - mae: 0.1748 - mse: 0.1290 - val_loss: 0.0671 - val_mae: 0.1478 - val_mse: 0.0671 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3036\n",
            "Epoch 37/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1290 - mae: 0.1747 - mse: 0.1290 - val_loss: 0.0670 - val_mae: 0.1475 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3033\n",
            "Epoch 38/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1288 - mae: 0.1745 - mse: 0.1288 - val_loss: 0.0671 - val_mae: 0.1477 - val_mse: 0.0671 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3037\n",
            "Epoch 39/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1287 - mae: 0.1744 - mse: 0.1287 - val_loss: 0.0670 - val_mae: 0.1473 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3035\n",
            "Epoch 40/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1288 - mae: 0.1743 - mse: 0.1288 - val_loss: 0.0670 - val_mae: 0.1473 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3035\n",
            "Epoch 41/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1287 - mae: 0.1742 - mse: 0.1287 - val_loss: 0.0668 - val_mae: 0.1471 - val_mse: 0.0668 - learning_rate: 0.1000 - val_custom_mse: 0.1946 - val_custom_mae: 0.3032\n",
            "Epoch 42/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1285 - mae: 0.1741 - mse: 0.1285 - val_loss: 0.0670 - val_mae: 0.1473 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1950 - val_custom_mae: 0.3036\n",
            "Epoch 43/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1285 - mae: 0.1741 - mse: 0.1285 - val_loss: 0.0669 - val_mae: 0.1470 - val_mse: 0.0669 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3035\n",
            "Epoch 44/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1286 - mae: 0.1741 - mse: 0.1286 - val_loss: 0.0670 - val_mae: 0.1473 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1951 - val_custom_mae: 0.3036\n",
            "Epoch 45/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1284 - mae: 0.1739 - mse: 0.1284 - val_loss: 0.0670 - val_mae: 0.1476 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3037\n",
            "Epoch 46/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1283 - mae: 0.1738 - mse: 0.1283 - val_loss: 0.0667 - val_mae: 0.1468 - val_mse: 0.0667 - learning_rate: 0.1000 - val_custom_mse: 0.1945 - val_custom_mae: 0.3030\n",
            "Epoch 47/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1282 - mae: 0.1737 - mse: 0.1282 - val_loss: 0.0666 - val_mae: 0.1468 - val_mse: 0.0666 - learning_rate: 0.1000 - val_custom_mse: 0.1943 - val_custom_mae: 0.3030\n",
            "Epoch 48/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1282 - mae: 0.1738 - mse: 0.1282 - val_loss: 0.0667 - val_mae: 0.1470 - val_mse: 0.0667 - learning_rate: 0.1000 - val_custom_mse: 0.1944 - val_custom_mae: 0.3032\n",
            "Epoch 49/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1282 - mae: 0.1737 - mse: 0.1282 - val_loss: 0.0667 - val_mae: 0.1471 - val_mse: 0.0667 - learning_rate: 0.1000 - val_custom_mse: 0.1942 - val_custom_mae: 0.3030\n",
            "Epoch 50/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1281 - mae: 0.1736 - mse: 0.1281 - val_loss: 0.0666 - val_mae: 0.1466 - val_mse: 0.0666 - learning_rate: 0.1000 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 51/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1282 - mae: 0.1736 - mse: 0.1282 - val_loss: 0.0667 - val_mae: 0.1467 - val_mse: 0.0667 - learning_rate: 0.1000 - val_custom_mse: 0.1947 - val_custom_mae: 0.3033\n",
            "Epoch 52/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1281 - mae: 0.1735 - mse: 0.1281 - val_loss: 0.0665 - val_mae: 0.1465 - val_mse: 0.0665 - learning_rate: 0.1000 - val_custom_mse: 0.1942 - val_custom_mae: 0.3028\n",
            "Epoch 53/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1280 - mae: 0.1734 - mse: 0.1280 - val_loss: 0.0666 - val_mae: 0.1465 - val_mse: 0.0666 - learning_rate: 0.1000 - val_custom_mse: 0.1942 - val_custom_mae: 0.3030\n",
            "Epoch 54/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1280 - mae: 0.1734 - mse: 0.1280 - val_loss: 0.0665 - val_mae: 0.1464 - val_mse: 0.0665 - learning_rate: 0.1000 - val_custom_mse: 0.1942 - val_custom_mae: 0.3029\n",
            "Epoch 55/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1279 - mae: 0.1733 - mse: 0.1279 - val_loss: 0.0664 - val_mae: 0.1463 - val_mse: 0.0664 - learning_rate: 0.1000 - val_custom_mse: 0.1939 - val_custom_mae: 0.3027\n",
            "Epoch 56/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1279 - mae: 0.1733 - mse: 0.1279 - val_loss: 0.0670 - val_mae: 0.1474 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1953 - val_custom_mae: 0.3037\n",
            "Epoch 57/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1279 - mae: 0.1732 - mse: 0.1279 - val_loss: 0.0666 - val_mae: 0.1465 - val_mse: 0.0666 - learning_rate: 0.1000 - val_custom_mse: 0.1944 - val_custom_mae: 0.3030\n",
            "Epoch 58/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1278 - mae: 0.1732 - mse: 0.1278 - val_loss: 0.0665 - val_mae: 0.1462 - val_mse: 0.0665 - learning_rate: 0.1000 - val_custom_mse: 0.1944 - val_custom_mae: 0.3030\n",
            "Epoch 59/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1278 - mae: 0.1732 - mse: 0.1278 - val_loss: 0.0668 - val_mae: 0.1468 - val_mse: 0.0668 - learning_rate: 0.1000 - val_custom_mse: 0.1949 - val_custom_mae: 0.3034\n",
            "Epoch 60/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1278 - mae: 0.1732 - mse: 0.1278 - val_loss: 0.0666 - val_mae: 0.1464 - val_mse: 0.0666 - learning_rate: 0.1000 - val_custom_mse: 0.1946 - val_custom_mae: 0.3032\n",
            "Epoch 61/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1277 - mae: 0.1731 - mse: 0.1277 - val_loss: 0.0670 - val_mae: 0.1473 - val_mse: 0.0670 - learning_rate: 0.1000 - val_custom_mse: 0.1952 - val_custom_mae: 0.3037\n",
            "Epoch 62/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1278 - mae: 0.1731 - mse: 0.1278 - val_loss: 0.0665 - val_mae: 0.1464 - val_mse: 0.0665 - learning_rate: 0.1000 - val_custom_mse: 0.1942 - val_custom_mae: 0.3028\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1277 - mae: 0.1731 - mse: 0.1277 - val_loss: 0.0664 - val_mae: 0.1463 - val_mse: 0.0664 - learning_rate: 0.1000 - val_custom_mse: 0.1940 - val_custom_mae: 0.3028\n",
            "Epoch 64/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1728 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1461 - val_mse: 0.0665 - learning_rate: 0.0200 - val_custom_mse: 0.1945 - val_custom_mae: 0.3029\n",
            "Epoch 65/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 0.0200 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 66/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1276 - mae: 0.1728 - mse: 0.1276 - val_loss: 0.0665 - val_mae: 0.1461 - val_mse: 0.0665 - learning_rate: 0.0200 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 67/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1461 - val_mse: 0.0665 - learning_rate: 0.0200 - val_custom_mse: 0.1945 - val_custom_mae: 0.3029\n",
            "Epoch 68/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1276 - mae: 0.1727 - mse: 0.1276 - val_loss: 0.0665 - val_mae: 0.1461 - val_mse: 0.0665 - learning_rate: 0.0200 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 69/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1461 - val_mse: 0.0665 - learning_rate: 0.0200 - val_custom_mse: 0.1943 - val_custom_mae: 0.3028\n",
            "Epoch 70/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1276 - mae: 0.1728 - mse: 0.1276 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 0.0200 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0666 - val_mae: 0.1461 - val_mse: 0.0666 - learning_rate: 0.0200 - val_custom_mse: 0.1947 - val_custom_mae: 0.3030\n",
            "Epoch 72/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 0.0040 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 73/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1274 - mae: 0.1727 - mse: 0.1274 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 0.0040 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 74/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1273 - mae: 0.1726 - mse: 0.1273 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 0.0040 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 75/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1276 - mae: 0.1727 - mse: 0.1276 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 0.0040 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 76/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 0.0040 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 77/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1726 - mse: 0.1275 - val_loss: 0.0664 - val_mae: 0.1460 - val_mse: 0.0664 - learning_rate: 0.0040 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 78/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1726 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 0.0040 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0664 - val_mae: 0.1460 - val_mse: 0.0664 - learning_rate: 0.0040 - val_custom_mse: 0.1944 - val_custom_mae: 0.3028\n",
            "Epoch 80/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1274 - mae: 0.1726 - mse: 0.1274 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 81/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1726 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 82/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 83/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1274 - mae: 0.1726 - mse: 0.1274 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 84/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1273 - mae: 0.1726 - mse: 0.1273 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 85/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1273 - mae: 0.1726 - mse: 0.1273 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 86/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1274 - mae: 0.1726 - mse: 0.1274 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 8.0000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 88/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1274 - mae: 0.1726 - mse: 0.1274 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 89/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1727 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 90/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1274 - mae: 0.1726 - mse: 0.1274 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 91/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1274 - mae: 0.1727 - mse: 0.1274 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 92/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1726 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 93/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1726 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 94/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1276 - mae: 0.1727 - mse: 0.1276 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1726 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 1.6000e-04 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 96/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1273 - mae: 0.1726 - mse: 0.1273 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 97/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1276 - mae: 0.1727 - mse: 0.1276 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 98/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1274 - mae: 0.1726 - mse: 0.1274 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 99/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1275 - mae: 0.1726 - mse: 0.1275 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Epoch 100/100\n",
            "1032/1032 - 8s - 8ms/step - loss: 0.1274 - mae: 0.1726 - mse: 0.1274 - val_loss: 0.0665 - val_mae: 0.1460 - val_mse: 0.0665 - learning_rate: 3.2000e-05 - val_custom_mse: 0.1944 - val_custom_mae: 0.3029\n",
            "Running experiment: horizon=720, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_28', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1020/1020 - 12s - 11ms/step - loss: 0.4642 - mae: 0.4050 - mse: 0.4642 - val_loss: 0.2571 - val_mae: 0.3485 - val_mse: 0.2571 - learning_rate: 0.1000 - val_custom_mse: 0.3496 - val_custom_mae: 0.4129\n",
            "Epoch 2/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3899 - mae: 0.3643 - mse: 0.3899 - val_loss: 0.2173 - val_mae: 0.3162 - val_mse: 0.2173 - learning_rate: 0.1000 - val_custom_mse: 0.3080 - val_custom_mae: 0.3880\n",
            "Epoch 3/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3529 - mae: 0.3343 - mse: 0.3529 - val_loss: 0.1966 - val_mae: 0.2917 - val_mse: 0.1966 - learning_rate: 0.1000 - val_custom_mse: 0.2926 - val_custom_mae: 0.3764\n",
            "Epoch 4/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3352 - mae: 0.3157 - mse: 0.3352 - val_loss: 0.1885 - val_mae: 0.2812 - val_mse: 0.1885 - learning_rate: 0.1000 - val_custom_mse: 0.2853 - val_custom_mae: 0.3705\n",
            "Epoch 5/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3270 - mae: 0.3057 - mse: 0.3270 - val_loss: 0.1833 - val_mae: 0.2742 - val_mse: 0.1833 - learning_rate: 0.1000 - val_custom_mse: 0.2803 - val_custom_mae: 0.3664\n",
            "Epoch 6/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3214 - mae: 0.2988 - mse: 0.3214 - val_loss: 0.1792 - val_mae: 0.2682 - val_mse: 0.1792 - learning_rate: 0.1000 - val_custom_mse: 0.2767 - val_custom_mae: 0.3633\n",
            "Epoch 7/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3172 - mae: 0.2936 - mse: 0.3172 - val_loss: 0.1777 - val_mae: 0.2652 - val_mse: 0.1777 - learning_rate: 0.1000 - val_custom_mse: 0.2759 - val_custom_mae: 0.3624\n",
            "Epoch 8/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3138 - mae: 0.2893 - mse: 0.3138 - val_loss: 0.1754 - val_mae: 0.2621 - val_mse: 0.1754 - learning_rate: 0.1000 - val_custom_mse: 0.2735 - val_custom_mae: 0.3605\n",
            "Epoch 9/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3110 - mae: 0.2859 - mse: 0.3110 - val_loss: 0.1732 - val_mae: 0.2583 - val_mse: 0.1732 - learning_rate: 0.1000 - val_custom_mse: 0.2720 - val_custom_mae: 0.3592\n",
            "Epoch 10/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3086 - mae: 0.2828 - mse: 0.3086 - val_loss: 0.1725 - val_mae: 0.2572 - val_mse: 0.1725 - learning_rate: 0.1000 - val_custom_mse: 0.2714 - val_custom_mae: 0.3586\n",
            "Epoch 11/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3066 - mae: 0.2802 - mse: 0.3066 - val_loss: 0.1715 - val_mae: 0.2547 - val_mse: 0.1715 - learning_rate: 0.1000 - val_custom_mse: 0.2712 - val_custom_mae: 0.3584\n",
            "Epoch 12/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2778 - mse: 0.3049 - val_loss: 0.1700 - val_mae: 0.2524 - val_mse: 0.1700 - learning_rate: 0.1000 - val_custom_mse: 0.2698 - val_custom_mae: 0.3570\n",
            "Epoch 13/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3034 - mae: 0.2758 - mse: 0.3034 - val_loss: 0.1693 - val_mae: 0.2505 - val_mse: 0.1693 - learning_rate: 0.1000 - val_custom_mse: 0.2696 - val_custom_mae: 0.3567\n",
            "Epoch 14/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3020 - mae: 0.2738 - mse: 0.3020 - val_loss: 0.1689 - val_mae: 0.2496 - val_mse: 0.1689 - learning_rate: 0.1000 - val_custom_mse: 0.2696 - val_custom_mae: 0.3568\n",
            "Epoch 15/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3009 - mae: 0.2721 - mse: 0.3009 - val_loss: 0.1686 - val_mae: 0.2487 - val_mse: 0.1686 - learning_rate: 0.1000 - val_custom_mse: 0.2699 - val_custom_mae: 0.3574\n",
            "Epoch 16/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3000 - mae: 0.2706 - mse: 0.3000 - val_loss: 0.1678 - val_mae: 0.2465 - val_mse: 0.1678 - learning_rate: 0.1000 - val_custom_mse: 0.2697 - val_custom_mae: 0.3568\n",
            "Epoch 17/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2991 - mae: 0.2691 - mse: 0.2991 - val_loss: 0.1672 - val_mae: 0.2452 - val_mse: 0.1672 - learning_rate: 0.1000 - val_custom_mse: 0.2691 - val_custom_mae: 0.3560\n",
            "Epoch 18/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2984 - mae: 0.2678 - mse: 0.2984 - val_loss: 0.1662 - val_mae: 0.2435 - val_mse: 0.1662 - learning_rate: 0.1000 - val_custom_mse: 0.2683 - val_custom_mae: 0.3553\n",
            "Epoch 19/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2976 - mae: 0.2666 - mse: 0.2976 - val_loss: 0.1659 - val_mae: 0.2423 - val_mse: 0.1659 - learning_rate: 0.1000 - val_custom_mse: 0.2684 - val_custom_mae: 0.3554\n",
            "Epoch 20/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2971 - mae: 0.2656 - mse: 0.2971 - val_loss: 0.1653 - val_mae: 0.2411 - val_mse: 0.1653 - learning_rate: 0.1000 - val_custom_mse: 0.2677 - val_custom_mae: 0.3549\n",
            "Epoch 21/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2965 - mae: 0.2645 - mse: 0.2965 - val_loss: 0.1651 - val_mae: 0.2401 - val_mse: 0.1651 - learning_rate: 0.1000 - val_custom_mse: 0.2680 - val_custom_mae: 0.3552\n",
            "Epoch 22/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2960 - mae: 0.2636 - mse: 0.2960 - val_loss: 0.1655 - val_mae: 0.2395 - val_mse: 0.1655 - learning_rate: 0.1000 - val_custom_mse: 0.2693 - val_custom_mae: 0.3562\n",
            "Epoch 23/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2955 - mae: 0.2626 - mse: 0.2955 - val_loss: 0.1649 - val_mae: 0.2387 - val_mse: 0.1649 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3554\n",
            "Epoch 24/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2951 - mae: 0.2619 - mse: 0.2951 - val_loss: 0.1645 - val_mae: 0.2371 - val_mse: 0.1645 - learning_rate: 0.1000 - val_custom_mse: 0.2686 - val_custom_mae: 0.3556\n",
            "Epoch 25/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2947 - mae: 0.2611 - mse: 0.2947 - val_loss: 0.1641 - val_mae: 0.2364 - val_mse: 0.1641 - learning_rate: 0.1000 - val_custom_mse: 0.2680 - val_custom_mae: 0.3550\n",
            "Epoch 26/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2943 - mae: 0.2603 - mse: 0.2943 - val_loss: 0.1641 - val_mae: 0.2363 - val_mse: 0.1641 - learning_rate: 0.1000 - val_custom_mse: 0.2683 - val_custom_mae: 0.3556\n",
            "Epoch 27/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2940 - mae: 0.2597 - mse: 0.2940 - val_loss: 0.1640 - val_mae: 0.2357 - val_mse: 0.1640 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3553\n",
            "Epoch 28/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2936 - mae: 0.2589 - mse: 0.2936 - val_loss: 0.1636 - val_mae: 0.2353 - val_mse: 0.1636 - learning_rate: 0.1000 - val_custom_mse: 0.2679 - val_custom_mae: 0.3548\n",
            "Epoch 29/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2933 - mae: 0.2584 - mse: 0.2933 - val_loss: 0.1637 - val_mae: 0.2356 - val_mse: 0.1637 - learning_rate: 0.1000 - val_custom_mse: 0.2681 - val_custom_mae: 0.3548\n",
            "Epoch 30/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2931 - mae: 0.2578 - mse: 0.2931 - val_loss: 0.1635 - val_mae: 0.2334 - val_mse: 0.1635 - learning_rate: 0.1000 - val_custom_mse: 0.2686 - val_custom_mae: 0.3555\n",
            "Epoch 31/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2927 - mae: 0.2573 - mse: 0.2927 - val_loss: 0.1633 - val_mae: 0.2334 - val_mse: 0.1633 - learning_rate: 0.1000 - val_custom_mse: 0.2683 - val_custom_mae: 0.3553\n",
            "Epoch 32/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2925 - mae: 0.2567 - mse: 0.2925 - val_loss: 0.1629 - val_mae: 0.2330 - val_mse: 0.1629 - learning_rate: 0.1000 - val_custom_mse: 0.2678 - val_custom_mae: 0.3551\n",
            "Epoch 33/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2922 - mae: 0.2563 - mse: 0.2922 - val_loss: 0.1633 - val_mae: 0.2327 - val_mse: 0.1633 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3551\n",
            "Epoch 34/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2920 - mae: 0.2559 - mse: 0.2920 - val_loss: 0.1627 - val_mae: 0.2317 - val_mse: 0.1627 - learning_rate: 0.1000 - val_custom_mse: 0.2679 - val_custom_mae: 0.3550\n",
            "Epoch 35/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2918 - mae: 0.2556 - mse: 0.2918 - val_loss: 0.1632 - val_mae: 0.2321 - val_mse: 0.1632 - learning_rate: 0.1000 - val_custom_mse: 0.2688 - val_custom_mae: 0.3560\n",
            "Epoch 36/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2916 - mae: 0.2552 - mse: 0.2916 - val_loss: 0.1627 - val_mae: 0.2306 - val_mse: 0.1627 - learning_rate: 0.1000 - val_custom_mse: 0.2683 - val_custom_mae: 0.3554\n",
            "Epoch 37/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2913 - mae: 0.2548 - mse: 0.2913 - val_loss: 0.1627 - val_mae: 0.2303 - val_mse: 0.1627 - learning_rate: 0.1000 - val_custom_mse: 0.2686 - val_custom_mae: 0.3555\n",
            "Epoch 38/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2912 - mae: 0.2543 - mse: 0.2912 - val_loss: 0.1627 - val_mae: 0.2303 - val_mse: 0.1627 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3555\n",
            "Epoch 39/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2909 - mae: 0.2542 - mse: 0.2909 - val_loss: 0.1626 - val_mae: 0.2300 - val_mse: 0.1626 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3554\n",
            "Epoch 40/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2907 - mae: 0.2539 - mse: 0.2907 - val_loss: 0.1626 - val_mae: 0.2298 - val_mse: 0.1626 - learning_rate: 0.1000 - val_custom_mse: 0.2686 - val_custom_mae: 0.3554\n",
            "Epoch 41/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2906 - mae: 0.2537 - mse: 0.2906 - val_loss: 0.1621 - val_mae: 0.2288 - val_mse: 0.1621 - learning_rate: 0.1000 - val_custom_mse: 0.2680 - val_custom_mae: 0.3550\n",
            "Epoch 42/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2905 - mae: 0.2536 - mse: 0.2905 - val_loss: 0.1622 - val_mae: 0.2294 - val_mse: 0.1622 - learning_rate: 0.1000 - val_custom_mse: 0.2682 - val_custom_mae: 0.3553\n",
            "Epoch 43/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2902 - mae: 0.2534 - mse: 0.2902 - val_loss: 0.1625 - val_mae: 0.2301 - val_mse: 0.1625 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3554\n",
            "Epoch 44/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2901 - mae: 0.2533 - mse: 0.2901 - val_loss: 0.1628 - val_mae: 0.2290 - val_mse: 0.1628 - learning_rate: 0.1000 - val_custom_mse: 0.2694 - val_custom_mae: 0.3564\n",
            "Epoch 45/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2899 - mae: 0.2531 - mse: 0.2899 - val_loss: 0.1625 - val_mae: 0.2291 - val_mse: 0.1625 - learning_rate: 0.1000 - val_custom_mse: 0.2690 - val_custom_mae: 0.3558\n",
            "Epoch 46/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2897 - mae: 0.2529 - mse: 0.2897 - val_loss: 0.1626 - val_mae: 0.2288 - val_mse: 0.1626 - learning_rate: 0.1000 - val_custom_mse: 0.2691 - val_custom_mae: 0.3558\n",
            "Epoch 47/100\n",
            "1020/1020 - 10s - 10ms/step - loss: 0.2895 - mae: 0.2528 - mse: 0.2895 - val_loss: 0.1622 - val_mae: 0.2289 - val_mse: 0.1622 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3557\n",
            "Epoch 48/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2894 - mae: 0.2528 - mse: 0.2894 - val_loss: 0.1622 - val_mae: 0.2289 - val_mse: 0.1622 - learning_rate: 0.1000 - val_custom_mse: 0.2683 - val_custom_mae: 0.3550\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2893 - mae: 0.2526 - mse: 0.2893 - val_loss: 0.1621 - val_mae: 0.2286 - val_mse: 0.1621 - learning_rate: 0.1000 - val_custom_mse: 0.2684 - val_custom_mae: 0.3554\n",
            "Epoch 50/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2889 - mae: 0.2523 - mse: 0.2889 - val_loss: 0.1630 - val_mae: 0.2299 - val_mse: 0.1630 - learning_rate: 0.0200 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 51/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2888 - mae: 0.2523 - mse: 0.2888 - val_loss: 0.1629 - val_mae: 0.2294 - val_mse: 0.1629 - learning_rate: 0.0200 - val_custom_mse: 0.2697 - val_custom_mae: 0.3561\n",
            "Epoch 52/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2888 - mae: 0.2522 - mse: 0.2888 - val_loss: 0.1630 - val_mae: 0.2299 - val_mse: 0.1630 - learning_rate: 0.0200 - val_custom_mse: 0.2697 - val_custom_mae: 0.3561\n",
            "Epoch 53/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2887 - mae: 0.2522 - mse: 0.2887 - val_loss: 0.1630 - val_mae: 0.2296 - val_mse: 0.1630 - learning_rate: 0.0200 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 54/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2887 - mae: 0.2522 - mse: 0.2887 - val_loss: 0.1628 - val_mae: 0.2294 - val_mse: 0.1628 - learning_rate: 0.0200 - val_custom_mse: 0.2696 - val_custom_mae: 0.3560\n",
            "Epoch 55/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2887 - mae: 0.2522 - mse: 0.2887 - val_loss: 0.1630 - val_mae: 0.2297 - val_mse: 0.1630 - learning_rate: 0.0200 - val_custom_mse: 0.2698 - val_custom_mae: 0.3563\n",
            "Epoch 56/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2887 - mae: 0.2522 - mse: 0.2887 - val_loss: 0.1628 - val_mae: 0.2294 - val_mse: 0.1628 - learning_rate: 0.0200 - val_custom_mse: 0.2695 - val_custom_mae: 0.3560\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2886 - mae: 0.2522 - mse: 0.2886 - val_loss: 0.1630 - val_mae: 0.2298 - val_mse: 0.1630 - learning_rate: 0.0200 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 58/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2296 - val_mse: 0.1630 - learning_rate: 0.0040 - val_custom_mse: 0.2699 - val_custom_mae: 0.3563\n",
            "Epoch 59/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2296 - val_mse: 0.1630 - learning_rate: 0.0040 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 60/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2296 - val_mse: 0.1630 - learning_rate: 0.0040 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 61/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2296 - val_mse: 0.1630 - learning_rate: 0.0040 - val_custom_mse: 0.2698 - val_custom_mae: 0.3563\n",
            "Epoch 62/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2296 - val_mse: 0.1630 - learning_rate: 0.0040 - val_custom_mse: 0.2698 - val_custom_mae: 0.3563\n",
            "Epoch 63/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1629 - val_mae: 0.2295 - val_mse: 0.1629 - learning_rate: 0.0040 - val_custom_mse: 0.2698 - val_custom_mae: 0.3563\n",
            "Epoch 64/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1629 - val_mae: 0.2295 - val_mse: 0.1629 - learning_rate: 0.0040 - val_custom_mse: 0.2698 - val_custom_mae: 0.3563\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2296 - val_mse: 0.1630 - learning_rate: 0.0040 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 66/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2522 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2295 - val_mse: 0.1630 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 67/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2295 - val_mse: 0.1630 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3563\n",
            "Epoch 68/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3563\n",
            "Epoch 69/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3563\n",
            "Epoch 70/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 71/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 72/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3563\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3563\n",
            "Epoch 74/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 75/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 76/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 77/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 78/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 79/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 80/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2885 - mae: 0.2521 - mse: 0.2885 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 82/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 83/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 84/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 85/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 86/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 87/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 88/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 90/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 91/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 92/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 93/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 94/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 95/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 96/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.2799998330592645e-06.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 98/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 99/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 100/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2884 - mae: 0.2521 - mse: 0.2884 - val_loss: 0.1630 - val_mae: 0.2294 - val_mse: 0.1630 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Running experiment: horizon=720, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_29', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1020/1020 - 13s - 12ms/step - loss: 0.4666 - mae: 0.4057 - mse: 0.4666 - val_loss: 0.2524 - val_mae: 0.3454 - val_mse: 0.2524 - learning_rate: 0.1000 - val_custom_mse: 0.3429 - val_custom_mae: 0.4084\n",
            "Epoch 2/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3911 - mae: 0.3642 - mse: 0.3911 - val_loss: 0.2148 - val_mae: 0.3130 - val_mse: 0.2148 - learning_rate: 0.1000 - val_custom_mse: 0.3086 - val_custom_mae: 0.3885\n",
            "Epoch 3/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3522 - mae: 0.3329 - mse: 0.3522 - val_loss: 0.1961 - val_mae: 0.2912 - val_mse: 0.1961 - learning_rate: 0.1000 - val_custom_mse: 0.2918 - val_custom_mae: 0.3758\n",
            "Epoch 4/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3360 - mae: 0.3162 - mse: 0.3360 - val_loss: 0.1882 - val_mae: 0.2812 - val_mse: 0.1882 - learning_rate: 0.1000 - val_custom_mse: 0.2844 - val_custom_mae: 0.3699\n",
            "Epoch 5/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3282 - mae: 0.3070 - mse: 0.3282 - val_loss: 0.1838 - val_mae: 0.2741 - val_mse: 0.1838 - learning_rate: 0.1000 - val_custom_mse: 0.2813 - val_custom_mae: 0.3668\n",
            "Epoch 6/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3230 - mae: 0.3008 - mse: 0.3230 - val_loss: 0.1805 - val_mae: 0.2699 - val_mse: 0.1805 - learning_rate: 0.1000 - val_custom_mse: 0.2778 - val_custom_mae: 0.3641\n",
            "Epoch 7/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3192 - mae: 0.2962 - mse: 0.3192 - val_loss: 0.1781 - val_mae: 0.2661 - val_mse: 0.1781 - learning_rate: 0.1000 - val_custom_mse: 0.2760 - val_custom_mae: 0.3624\n",
            "Epoch 8/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3162 - mae: 0.2927 - mse: 0.3162 - val_loss: 0.1770 - val_mae: 0.2642 - val_mse: 0.1770 - learning_rate: 0.1000 - val_custom_mse: 0.2751 - val_custom_mae: 0.3616\n",
            "Epoch 9/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3136 - mae: 0.2897 - mse: 0.3136 - val_loss: 0.1747 - val_mae: 0.2608 - val_mse: 0.1747 - learning_rate: 0.1000 - val_custom_mse: 0.2731 - val_custom_mae: 0.3597\n",
            "Epoch 10/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3116 - mae: 0.2873 - mse: 0.3116 - val_loss: 0.1739 - val_mae: 0.2594 - val_mse: 0.1739 - learning_rate: 0.1000 - val_custom_mse: 0.2728 - val_custom_mae: 0.3594\n",
            "Epoch 11/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3097 - mae: 0.2852 - mse: 0.3097 - val_loss: 0.1733 - val_mae: 0.2572 - val_mse: 0.1733 - learning_rate: 0.1000 - val_custom_mse: 0.2731 - val_custom_mae: 0.3591\n",
            "Epoch 12/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3082 - mae: 0.2833 - mse: 0.3082 - val_loss: 0.1721 - val_mae: 0.2560 - val_mse: 0.1721 - learning_rate: 0.1000 - val_custom_mse: 0.2717 - val_custom_mae: 0.3582\n",
            "Epoch 13/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3070 - mae: 0.2818 - mse: 0.3070 - val_loss: 0.1720 - val_mae: 0.2548 - val_mse: 0.1720 - learning_rate: 0.1000 - val_custom_mse: 0.2725 - val_custom_mae: 0.3589\n",
            "Epoch 14/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3060 - mae: 0.2805 - mse: 0.3060 - val_loss: 0.1719 - val_mae: 0.2543 - val_mse: 0.1719 - learning_rate: 0.1000 - val_custom_mse: 0.2727 - val_custom_mae: 0.3586\n",
            "Epoch 15/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3050 - mae: 0.2792 - mse: 0.3050 - val_loss: 0.1708 - val_mae: 0.2528 - val_mse: 0.1708 - learning_rate: 0.1000 - val_custom_mse: 0.2715 - val_custom_mae: 0.3579\n",
            "Epoch 16/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3042 - mae: 0.2782 - mse: 0.3042 - val_loss: 0.1703 - val_mae: 0.2520 - val_mse: 0.1703 - learning_rate: 0.1000 - val_custom_mse: 0.2711 - val_custom_mae: 0.3573\n",
            "Epoch 17/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3036 - mae: 0.2773 - mse: 0.3036 - val_loss: 0.1689 - val_mae: 0.2488 - val_mse: 0.1689 - learning_rate: 0.1000 - val_custom_mse: 0.2705 - val_custom_mae: 0.3570\n",
            "Epoch 18/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3031 - mae: 0.2765 - mse: 0.3031 - val_loss: 0.1694 - val_mae: 0.2500 - val_mse: 0.1694 - learning_rate: 0.1000 - val_custom_mse: 0.2709 - val_custom_mae: 0.3571\n",
            "Epoch 19/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3025 - mae: 0.2757 - mse: 0.3025 - val_loss: 0.1699 - val_mae: 0.2499 - val_mse: 0.1699 - learning_rate: 0.1000 - val_custom_mse: 0.2722 - val_custom_mae: 0.3584\n",
            "Epoch 20/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3020 - mae: 0.2750 - mse: 0.3020 - val_loss: 0.1677 - val_mae: 0.2462 - val_mse: 0.1677 - learning_rate: 0.1000 - val_custom_mse: 0.2698 - val_custom_mae: 0.3558\n",
            "Epoch 21/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3015 - mae: 0.2744 - mse: 0.3015 - val_loss: 0.1685 - val_mae: 0.2475 - val_mse: 0.1685 - learning_rate: 0.1000 - val_custom_mse: 0.2710 - val_custom_mae: 0.3573\n",
            "Epoch 22/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3012 - mae: 0.2738 - mse: 0.3012 - val_loss: 0.1689 - val_mae: 0.2479 - val_mse: 0.1689 - learning_rate: 0.1000 - val_custom_mse: 0.2716 - val_custom_mae: 0.3579\n",
            "Epoch 23/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3008 - mae: 0.2734 - mse: 0.3008 - val_loss: 0.1679 - val_mae: 0.2461 - val_mse: 0.1679 - learning_rate: 0.1000 - val_custom_mse: 0.2706 - val_custom_mae: 0.3571\n",
            "Epoch 24/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3005 - mae: 0.2729 - mse: 0.3005 - val_loss: 0.1677 - val_mae: 0.2455 - val_mse: 0.1677 - learning_rate: 0.1000 - val_custom_mse: 0.2707 - val_custom_mae: 0.3567\n",
            "Epoch 25/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3002 - mae: 0.2726 - mse: 0.3002 - val_loss: 0.1678 - val_mae: 0.2455 - val_mse: 0.1678 - learning_rate: 0.1000 - val_custom_mse: 0.2709 - val_custom_mae: 0.3567\n",
            "Epoch 26/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2999 - mae: 0.2722 - mse: 0.2999 - val_loss: 0.1678 - val_mae: 0.2443 - val_mse: 0.1678 - learning_rate: 0.1000 - val_custom_mse: 0.2717 - val_custom_mae: 0.3575\n",
            "Epoch 27/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2996 - mae: 0.2719 - mse: 0.2996 - val_loss: 0.1671 - val_mae: 0.2438 - val_mse: 0.1671 - learning_rate: 0.1000 - val_custom_mse: 0.2705 - val_custom_mae: 0.3565\n",
            "Epoch 28/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2993 - mae: 0.2716 - mse: 0.2993 - val_loss: 0.1671 - val_mae: 0.2427 - val_mse: 0.1671 - learning_rate: 0.1000 - val_custom_mse: 0.2713 - val_custom_mae: 0.3572\n",
            "Epoch 29/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2990 - mae: 0.2714 - mse: 0.2990 - val_loss: 0.1666 - val_mae: 0.2421 - val_mse: 0.1666 - learning_rate: 0.1000 - val_custom_mse: 0.2705 - val_custom_mae: 0.3565\n",
            "Epoch 30/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2989 - mae: 0.2711 - mse: 0.2989 - val_loss: 0.1684 - val_mae: 0.2457 - val_mse: 0.1684 - learning_rate: 0.1000 - val_custom_mse: 0.2725 - val_custom_mae: 0.3584\n",
            "Epoch 31/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2986 - mae: 0.2709 - mse: 0.2986 - val_loss: 0.1682 - val_mae: 0.2453 - val_mse: 0.1682 - learning_rate: 0.1000 - val_custom_mse: 0.2723 - val_custom_mae: 0.3581\n",
            "Epoch 32/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2985 - mae: 0.2708 - mse: 0.2985 - val_loss: 0.1667 - val_mae: 0.2433 - val_mse: 0.1667 - learning_rate: 0.1000 - val_custom_mse: 0.2704 - val_custom_mae: 0.3566\n",
            "Epoch 33/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2982 - mae: 0.2705 - mse: 0.2982 - val_loss: 0.1683 - val_mae: 0.2465 - val_mse: 0.1683 - learning_rate: 0.1000 - val_custom_mse: 0.2721 - val_custom_mae: 0.3583\n",
            "Epoch 34/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2979 - mae: 0.2704 - mse: 0.2979 - val_loss: 0.1670 - val_mae: 0.2437 - val_mse: 0.1670 - learning_rate: 0.1000 - val_custom_mse: 0.2709 - val_custom_mae: 0.3570\n",
            "Epoch 35/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2978 - mae: 0.2703 - mse: 0.2978 - val_loss: 0.1669 - val_mae: 0.2429 - val_mse: 0.1669 - learning_rate: 0.1000 - val_custom_mse: 0.2712 - val_custom_mae: 0.3573\n",
            "Epoch 36/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2976 - mae: 0.2702 - mse: 0.2976 - val_loss: 0.1672 - val_mae: 0.2439 - val_mse: 0.1672 - learning_rate: 0.1000 - val_custom_mse: 0.2713 - val_custom_mae: 0.3575\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2976 - mae: 0.2701 - mse: 0.2976 - val_loss: 0.1679 - val_mae: 0.2445 - val_mse: 0.1679 - learning_rate: 0.1000 - val_custom_mse: 0.2724 - val_custom_mae: 0.3583\n",
            "Epoch 38/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2971 - mae: 0.2697 - mse: 0.2971 - val_loss: 0.1669 - val_mae: 0.2434 - val_mse: 0.1669 - learning_rate: 0.0200 - val_custom_mse: 0.2711 - val_custom_mae: 0.3575\n",
            "Epoch 39/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2971 - mae: 0.2697 - mse: 0.2971 - val_loss: 0.1669 - val_mae: 0.2431 - val_mse: 0.1669 - learning_rate: 0.0200 - val_custom_mse: 0.2711 - val_custom_mae: 0.3574\n",
            "Epoch 40/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2971 - mae: 0.2696 - mse: 0.2971 - val_loss: 0.1668 - val_mae: 0.2432 - val_mse: 0.1668 - learning_rate: 0.0200 - val_custom_mse: 0.2709 - val_custom_mae: 0.3573\n",
            "Epoch 41/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2971 - mae: 0.2696 - mse: 0.2971 - val_loss: 0.1670 - val_mae: 0.2435 - val_mse: 0.1670 - learning_rate: 0.0200 - val_custom_mse: 0.2713 - val_custom_mae: 0.3576\n",
            "Epoch 42/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2971 - mae: 0.2696 - mse: 0.2971 - val_loss: 0.1669 - val_mae: 0.2428 - val_mse: 0.1669 - learning_rate: 0.0200 - val_custom_mse: 0.2713 - val_custom_mae: 0.3576\n",
            "Epoch 43/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2969 - mae: 0.2695 - mse: 0.2969 - val_loss: 0.1669 - val_mae: 0.2431 - val_mse: 0.1669 - learning_rate: 0.0200 - val_custom_mse: 0.2712 - val_custom_mae: 0.3575\n",
            "Epoch 44/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2970 - mae: 0.2695 - mse: 0.2970 - val_loss: 0.1670 - val_mae: 0.2432 - val_mse: 0.1670 - learning_rate: 0.0200 - val_custom_mse: 0.2713 - val_custom_mae: 0.3576\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2969 - mae: 0.2695 - mse: 0.2969 - val_loss: 0.1670 - val_mae: 0.2432 - val_mse: 0.1670 - learning_rate: 0.0200 - val_custom_mse: 0.2713 - val_custom_mae: 0.3576\n",
            "Epoch 46/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2696 - mse: 0.2968 - val_loss: 0.1661 - val_mae: 0.2418 - val_mse: 0.1661 - learning_rate: 0.0040 - val_custom_mse: 0.2703 - val_custom_mae: 0.3567\n",
            "Epoch 47/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2969 - mae: 0.2695 - mse: 0.2969 - val_loss: 0.1661 - val_mae: 0.2418 - val_mse: 0.1661 - learning_rate: 0.0040 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 48/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1661 - val_mae: 0.2417 - val_mse: 0.1661 - learning_rate: 0.0040 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 49/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2969 - mae: 0.2694 - mse: 0.2969 - val_loss: 0.1660 - val_mae: 0.2417 - val_mse: 0.1660 - learning_rate: 0.0040 - val_custom_mse: 0.2701 - val_custom_mae: 0.3567\n",
            "Epoch 50/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1661 - val_mae: 0.2417 - val_mse: 0.1661 - learning_rate: 0.0040 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 51/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2969 - mae: 0.2694 - mse: 0.2969 - val_loss: 0.1661 - val_mae: 0.2418 - val_mse: 0.1661 - learning_rate: 0.0040 - val_custom_mse: 0.2702 - val_custom_mae: 0.3568\n",
            "Epoch 52/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1661 - val_mae: 0.2418 - val_mse: 0.1661 - learning_rate: 0.0040 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 53/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2969 - mae: 0.2695 - mse: 0.2969 - val_loss: 0.1660 - val_mae: 0.2416 - val_mse: 0.1660 - learning_rate: 0.0040 - val_custom_mse: 0.2701 - val_custom_mae: 0.3566\n",
            "Epoch 54/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1660 - val_mae: 0.2417 - val_mse: 0.1660 - learning_rate: 0.0040 - val_custom_mse: 0.2701 - val_custom_mae: 0.3567\n",
            "Epoch 55/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1660 - val_mae: 0.2417 - val_mse: 0.1660 - learning_rate: 0.0040 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 56/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1660 - val_mae: 0.2417 - val_mse: 0.1660 - learning_rate: 0.0040 - val_custom_mse: 0.2701 - val_custom_mae: 0.3567\n",
            "Epoch 57/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2969 - mae: 0.2694 - mse: 0.2969 - val_loss: 0.1660 - val_mae: 0.2417 - val_mse: 0.1660 - learning_rate: 0.0040 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 58/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1660 - val_mae: 0.2416 - val_mse: 0.1660 - learning_rate: 0.0040 - val_custom_mse: 0.2701 - val_custom_mae: 0.3566\n",
            "Epoch 59/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1661 - val_mae: 0.2417 - val_mse: 0.1661 - learning_rate: 0.0040 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 60/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1661 - val_mae: 0.2418 - val_mse: 0.1661 - learning_rate: 0.0040 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1660 - val_mae: 0.2417 - val_mse: 0.1660 - learning_rate: 0.0040 - val_custom_mse: 0.2701 - val_custom_mae: 0.3566\n",
            "Epoch 62/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2696 - mse: 0.2968 - val_loss: 0.1659 - val_mae: 0.2415 - val_mse: 0.1659 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2700 - val_custom_mae: 0.3566\n",
            "Epoch 63/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1659 - val_mae: 0.2414 - val_mse: 0.1659 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 64/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 65/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1659 - val_mae: 0.2413 - val_mse: 0.1659 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 66/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 67/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 68/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 69/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2693 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 70/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2693 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 72/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 73/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 74/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 75/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 76/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2693 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 77/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 78/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2693 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 80/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 81/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 82/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2966 - mae: 0.2694 - mse: 0.2966 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 83/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 84/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2693 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 85/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 86/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 88/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2966 - mae: 0.2694 - mse: 0.2966 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 89/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 90/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 91/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2693 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 92/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 93/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2966 - mae: 0.2694 - mse: 0.2966 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 94/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.2799998330592645e-06.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2966 - mae: 0.2693 - mse: 0.2966 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 96/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 97/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2966 - mae: 0.2693 - mse: 0.2966 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 98/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2968 - mae: 0.2694 - mse: 0.2968 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 99/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2967 - mae: 0.2694 - mse: 0.2967 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 100/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.2966 - mae: 0.2694 - mse: 0.2966 - val_loss: 0.1658 - val_mae: 0.2413 - val_mse: 0.1658 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Running experiment: horizon=720, dropout_rate=0.2\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_30', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1020/1020 - 13s - 13ms/step - loss: 0.4679 - mae: 0.4060 - mse: 0.4679 - val_loss: 0.2494 - val_mae: 0.3450 - val_mse: 0.2494 - learning_rate: 0.1000 - val_custom_mse: 0.3363 - val_custom_mae: 0.4058\n",
            "Epoch 2/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3914 - mae: 0.3640 - mse: 0.3914 - val_loss: 0.2125 - val_mae: 0.3102 - val_mse: 0.2125 - learning_rate: 0.1000 - val_custom_mse: 0.3068 - val_custom_mae: 0.3871\n",
            "Epoch 3/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3522 - mae: 0.3324 - mse: 0.3522 - val_loss: 0.1965 - val_mae: 0.2909 - val_mse: 0.1965 - learning_rate: 0.1000 - val_custom_mse: 0.2930 - val_custom_mae: 0.3765\n",
            "Epoch 4/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3371 - mae: 0.3168 - mse: 0.3371 - val_loss: 0.1878 - val_mae: 0.2803 - val_mse: 0.1878 - learning_rate: 0.1000 - val_custom_mse: 0.2845 - val_custom_mae: 0.3698\n",
            "Epoch 5/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3299 - mae: 0.3083 - mse: 0.3299 - val_loss: 0.1831 - val_mae: 0.2748 - val_mse: 0.1831 - learning_rate: 0.1000 - val_custom_mse: 0.2792 - val_custom_mae: 0.3657\n",
            "Epoch 6/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3252 - mae: 0.3029 - mse: 0.3252 - val_loss: 0.1802 - val_mae: 0.2698 - val_mse: 0.1802 - learning_rate: 0.1000 - val_custom_mse: 0.2774 - val_custom_mae: 0.3640\n",
            "Epoch 7/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3216 - mae: 0.2987 - mse: 0.3216 - val_loss: 0.1777 - val_mae: 0.2664 - val_mse: 0.1777 - learning_rate: 0.1000 - val_custom_mse: 0.2749 - val_custom_mae: 0.3619\n",
            "Epoch 8/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3188 - mae: 0.2956 - mse: 0.3188 - val_loss: 0.1760 - val_mae: 0.2635 - val_mse: 0.1760 - learning_rate: 0.1000 - val_custom_mse: 0.2736 - val_custom_mae: 0.3606\n",
            "Epoch 9/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3165 - mae: 0.2931 - mse: 0.3165 - val_loss: 0.1747 - val_mae: 0.2622 - val_mse: 0.1747 - learning_rate: 0.1000 - val_custom_mse: 0.2720 - val_custom_mae: 0.3595\n",
            "Epoch 10/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3146 - mae: 0.2911 - mse: 0.3146 - val_loss: 0.1741 - val_mae: 0.2600 - val_mse: 0.1741 - learning_rate: 0.1000 - val_custom_mse: 0.2726 - val_custom_mae: 0.3595\n",
            "Epoch 11/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3131 - mae: 0.2895 - mse: 0.3131 - val_loss: 0.1731 - val_mae: 0.2583 - val_mse: 0.1731 - learning_rate: 0.1000 - val_custom_mse: 0.2718 - val_custom_mae: 0.3588\n",
            "Epoch 12/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3117 - mae: 0.2880 - mse: 0.3117 - val_loss: 0.1719 - val_mae: 0.2565 - val_mse: 0.1719 - learning_rate: 0.1000 - val_custom_mse: 0.2707 - val_custom_mae: 0.3579\n",
            "Epoch 13/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3106 - mae: 0.2868 - mse: 0.3106 - val_loss: 0.1709 - val_mae: 0.2554 - val_mse: 0.1709 - learning_rate: 0.1000 - val_custom_mse: 0.2695 - val_custom_mae: 0.3567\n",
            "Epoch 14/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3096 - mae: 0.2858 - mse: 0.3096 - val_loss: 0.1707 - val_mae: 0.2540 - val_mse: 0.1707 - learning_rate: 0.1000 - val_custom_mse: 0.2703 - val_custom_mae: 0.3573\n",
            "Epoch 15/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3089 - mae: 0.2849 - mse: 0.3089 - val_loss: 0.1709 - val_mae: 0.2540 - val_mse: 0.1709 - learning_rate: 0.1000 - val_custom_mse: 0.2708 - val_custom_mae: 0.3578\n",
            "Epoch 16/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3081 - mae: 0.2841 - mse: 0.3081 - val_loss: 0.1695 - val_mae: 0.2517 - val_mse: 0.1695 - learning_rate: 0.1000 - val_custom_mse: 0.2693 - val_custom_mae: 0.3563\n",
            "Epoch 17/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3076 - mae: 0.2835 - mse: 0.3076 - val_loss: 0.1698 - val_mae: 0.2519 - val_mse: 0.1698 - learning_rate: 0.1000 - val_custom_mse: 0.2702 - val_custom_mae: 0.3570\n",
            "Epoch 18/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3071 - mae: 0.2829 - mse: 0.3071 - val_loss: 0.1686 - val_mae: 0.2505 - val_mse: 0.1686 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3555\n",
            "Epoch 19/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3064 - mae: 0.2823 - mse: 0.3064 - val_loss: 0.1683 - val_mae: 0.2494 - val_mse: 0.1683 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3554\n",
            "Epoch 20/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3060 - mae: 0.2819 - mse: 0.3060 - val_loss: 0.1688 - val_mae: 0.2496 - val_mse: 0.1688 - learning_rate: 0.1000 - val_custom_mse: 0.2697 - val_custom_mae: 0.3563\n",
            "Epoch 21/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3055 - mae: 0.2814 - mse: 0.3055 - val_loss: 0.1683 - val_mae: 0.2492 - val_mse: 0.1683 - learning_rate: 0.1000 - val_custom_mse: 0.2690 - val_custom_mae: 0.3560\n",
            "Epoch 22/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3053 - mae: 0.2811 - mse: 0.3053 - val_loss: 0.1687 - val_mae: 0.2493 - val_mse: 0.1687 - learning_rate: 0.1000 - val_custom_mse: 0.2699 - val_custom_mae: 0.3566\n",
            "Epoch 23/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3050 - mae: 0.2808 - mse: 0.3050 - val_loss: 0.1682 - val_mae: 0.2483 - val_mse: 0.1682 - learning_rate: 0.1000 - val_custom_mse: 0.2694 - val_custom_mae: 0.3560\n",
            "Epoch 24/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2805 - mse: 0.3048 - val_loss: 0.1678 - val_mae: 0.2478 - val_mse: 0.1678 - learning_rate: 0.1000 - val_custom_mse: 0.2689 - val_custom_mae: 0.3557\n",
            "Epoch 25/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2803 - mse: 0.3046 - val_loss: 0.1679 - val_mae: 0.2475 - val_mse: 0.1679 - learning_rate: 0.1000 - val_custom_mse: 0.2693 - val_custom_mae: 0.3558\n",
            "Epoch 26/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3041 - mae: 0.2800 - mse: 0.3041 - val_loss: 0.1680 - val_mae: 0.2478 - val_mse: 0.1680 - learning_rate: 0.1000 - val_custom_mse: 0.2695 - val_custom_mae: 0.3563\n",
            "Epoch 27/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3038 - mae: 0.2797 - mse: 0.3038 - val_loss: 0.1673 - val_mae: 0.2468 - val_mse: 0.1673 - learning_rate: 0.1000 - val_custom_mse: 0.2685 - val_custom_mae: 0.3553\n",
            "Epoch 28/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3038 - mae: 0.2796 - mse: 0.3038 - val_loss: 0.1673 - val_mae: 0.2465 - val_mse: 0.1673 - learning_rate: 0.1000 - val_custom_mse: 0.2688 - val_custom_mae: 0.3555\n",
            "Epoch 29/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3035 - mae: 0.2794 - mse: 0.3035 - val_loss: 0.1676 - val_mae: 0.2472 - val_mse: 0.1676 - learning_rate: 0.1000 - val_custom_mse: 0.2691 - val_custom_mae: 0.3557\n",
            "Epoch 30/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3031 - mae: 0.2792 - mse: 0.3031 - val_loss: 0.1674 - val_mae: 0.2468 - val_mse: 0.1674 - learning_rate: 0.1000 - val_custom_mse: 0.2691 - val_custom_mae: 0.3559\n",
            "Epoch 31/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3029 - mae: 0.2790 - mse: 0.3029 - val_loss: 0.1674 - val_mae: 0.2464 - val_mse: 0.1674 - learning_rate: 0.1000 - val_custom_mse: 0.2694 - val_custom_mae: 0.3562\n",
            "Epoch 32/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3029 - mae: 0.2790 - mse: 0.3029 - val_loss: 0.1667 - val_mae: 0.2459 - val_mse: 0.1667 - learning_rate: 0.1000 - val_custom_mse: 0.2680 - val_custom_mae: 0.3551\n",
            "Epoch 33/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3027 - mae: 0.2788 - mse: 0.3027 - val_loss: 0.1668 - val_mae: 0.2456 - val_mse: 0.1668 - learning_rate: 0.1000 - val_custom_mse: 0.2683 - val_custom_mae: 0.3549\n",
            "Epoch 34/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3024 - mae: 0.2786 - mse: 0.3024 - val_loss: 0.1672 - val_mae: 0.2455 - val_mse: 0.1672 - learning_rate: 0.1000 - val_custom_mse: 0.2693 - val_custom_mae: 0.3558\n",
            "Epoch 35/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3021 - mae: 0.2785 - mse: 0.3021 - val_loss: 0.1672 - val_mae: 0.2463 - val_mse: 0.1672 - learning_rate: 0.1000 - val_custom_mse: 0.2691 - val_custom_mae: 0.3561\n",
            "Epoch 36/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3022 - mae: 0.2785 - mse: 0.3022 - val_loss: 0.1670 - val_mae: 0.2456 - val_mse: 0.1670 - learning_rate: 0.1000 - val_custom_mse: 0.2689 - val_custom_mae: 0.3557\n",
            "Epoch 37/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3023 - mae: 0.2784 - mse: 0.3023 - val_loss: 0.1666 - val_mae: 0.2455 - val_mse: 0.1666 - learning_rate: 0.1000 - val_custom_mse: 0.2683 - val_custom_mae: 0.3554\n",
            "Epoch 38/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3019 - mae: 0.2783 - mse: 0.3019 - val_loss: 0.1669 - val_mae: 0.2451 - val_mse: 0.1669 - learning_rate: 0.1000 - val_custom_mse: 0.2690 - val_custom_mae: 0.3556\n",
            "Epoch 39/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3019 - mae: 0.2783 - mse: 0.3019 - val_loss: 0.1667 - val_mae: 0.2452 - val_mse: 0.1667 - learning_rate: 0.1000 - val_custom_mse: 0.2687 - val_custom_mae: 0.3554\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3018 - mae: 0.2782 - mse: 0.3018 - val_loss: 0.1669 - val_mae: 0.2451 - val_mse: 0.1669 - learning_rate: 0.1000 - val_custom_mse: 0.2691 - val_custom_mae: 0.3557\n",
            "Epoch 41/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3013 - mae: 0.2778 - mse: 0.3013 - val_loss: 0.1669 - val_mae: 0.2460 - val_mse: 0.1669 - learning_rate: 0.0200 - val_custom_mse: 0.2687 - val_custom_mae: 0.3556\n",
            "Epoch 42/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3014 - mae: 0.2778 - mse: 0.3014 - val_loss: 0.1671 - val_mae: 0.2463 - val_mse: 0.1671 - learning_rate: 0.0200 - val_custom_mse: 0.2689 - val_custom_mae: 0.3557\n",
            "Epoch 43/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3013 - mae: 0.2778 - mse: 0.3013 - val_loss: 0.1668 - val_mae: 0.2461 - val_mse: 0.1668 - learning_rate: 0.0200 - val_custom_mse: 0.2686 - val_custom_mae: 0.3555\n",
            "Epoch 44/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3012 - mae: 0.2778 - mse: 0.3012 - val_loss: 0.1668 - val_mae: 0.2457 - val_mse: 0.1668 - learning_rate: 0.0200 - val_custom_mse: 0.2686 - val_custom_mae: 0.3554\n",
            "Epoch 45/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3012 - mae: 0.2777 - mse: 0.3012 - val_loss: 0.1668 - val_mae: 0.2460 - val_mse: 0.1668 - learning_rate: 0.0200 - val_custom_mse: 0.2685 - val_custom_mae: 0.3554\n",
            "Epoch 46/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2777 - mse: 0.3010 - val_loss: 0.1669 - val_mae: 0.2460 - val_mse: 0.1669 - learning_rate: 0.0200 - val_custom_mse: 0.2686 - val_custom_mae: 0.3555\n",
            "Epoch 47/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3013 - mae: 0.2778 - mse: 0.3013 - val_loss: 0.1669 - val_mae: 0.2461 - val_mse: 0.1669 - learning_rate: 0.0200 - val_custom_mse: 0.2687 - val_custom_mae: 0.3555\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2777 - mse: 0.3010 - val_loss: 0.1668 - val_mae: 0.2459 - val_mse: 0.1668 - learning_rate: 0.0200 - val_custom_mse: 0.2686 - val_custom_mae: 0.3555\n",
            "Epoch 49/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2777 - mse: 0.3011 - val_loss: 0.1672 - val_mae: 0.2461 - val_mse: 0.1672 - learning_rate: 0.0040 - val_custom_mse: 0.2693 - val_custom_mae: 0.3559\n",
            "Epoch 50/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2777 - mse: 0.3011 - val_loss: 0.1674 - val_mae: 0.2463 - val_mse: 0.1674 - learning_rate: 0.0040 - val_custom_mse: 0.2696 - val_custom_mae: 0.3561\n",
            "Epoch 51/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1674 - val_mae: 0.2462 - val_mse: 0.1674 - learning_rate: 0.0040 - val_custom_mse: 0.2695 - val_custom_mae: 0.3561\n",
            "Epoch 52/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1674 - val_mae: 0.2462 - val_mse: 0.1674 - learning_rate: 0.0040 - val_custom_mse: 0.2695 - val_custom_mae: 0.3560\n",
            "Epoch 53/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1674 - val_mae: 0.2462 - val_mse: 0.1674 - learning_rate: 0.0040 - val_custom_mse: 0.2695 - val_custom_mae: 0.3561\n",
            "Epoch 54/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1674 - val_mae: 0.2462 - val_mse: 0.1674 - learning_rate: 0.0040 - val_custom_mse: 0.2695 - val_custom_mae: 0.3560\n",
            "Epoch 55/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3012 - mae: 0.2777 - mse: 0.3012 - val_loss: 0.1674 - val_mae: 0.2462 - val_mse: 0.1674 - learning_rate: 0.0040 - val_custom_mse: 0.2695 - val_custom_mae: 0.3560\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2777 - mse: 0.3011 - val_loss: 0.1674 - val_mae: 0.2462 - val_mse: 0.1674 - learning_rate: 0.0040 - val_custom_mse: 0.2696 - val_custom_mae: 0.3561\n",
            "Epoch 57/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1674 - val_mae: 0.2463 - val_mse: 0.1674 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2696 - val_custom_mae: 0.3561\n",
            "Epoch 58/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1674 - val_mae: 0.2463 - val_mse: 0.1674 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2696 - val_custom_mae: 0.3561\n",
            "Epoch 59/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1674 - val_mae: 0.2463 - val_mse: 0.1674 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2696 - val_custom_mae: 0.3561\n",
            "Epoch 60/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3009 - mae: 0.2776 - mse: 0.3009 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2696 - val_custom_mae: 0.3561\n",
            "Epoch 61/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2696 - val_custom_mae: 0.3561\n",
            "Epoch 62/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2696 - val_custom_mae: 0.3561\n",
            "Epoch 63/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3008 - mae: 0.2776 - mse: 0.3008 - val_loss: 0.1675 - val_mae: 0.2464 - val_mse: 0.1675 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2696 - val_custom_mae: 0.3561\n",
            "Epoch 65/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 66/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 67/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3009 - mae: 0.2776 - mse: 0.3009 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 68/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 69/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 70/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 71/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 73/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 74/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 75/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 76/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 77/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3012 - mae: 0.2777 - mse: 0.3012 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 78/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3008 - mae: 0.2775 - mse: 0.3008 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 79/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 81/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 82/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2777 - mse: 0.3011 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 83/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 84/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 85/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3009 - mae: 0.2776 - mse: 0.3009 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 86/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 87/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 1.2799998330592645e-06.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 89/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3009 - mae: 0.2776 - mse: 0.3009 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 90/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3009 - mae: 0.2776 - mse: 0.3009 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 91/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3008 - mae: 0.2775 - mse: 0.3008 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 92/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3009 - mae: 0.2776 - mse: 0.3009 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 93/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 94/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 95/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3009 - mae: 0.2776 - mse: 0.3009 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 2.559999757067999e-07.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 1.2800e-06 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 97/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 2.5600e-07 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 98/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3011 - mae: 0.2776 - mse: 0.3011 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 2.5600e-07 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 99/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3009 - mae: 0.2776 - mse: 0.3009 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 2.5600e-07 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Epoch 100/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3010 - mae: 0.2776 - mse: 0.3010 - val_loss: 0.1675 - val_mae: 0.2463 - val_mse: 0.1675 - learning_rate: 2.5600e-07 - val_custom_mse: 0.2697 - val_custom_mae: 0.3562\n",
            "Running experiment: horizon=720, dropout_rate=0.3\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_31', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1020/1020 - 13s - 12ms/step - loss: 0.4692 - mae: 0.4071 - mse: 0.4692 - val_loss: 0.2509 - val_mae: 0.3455 - val_mse: 0.2509 - learning_rate: 0.1000 - val_custom_mse: 0.3391 - val_custom_mae: 0.4071\n",
            "Epoch 2/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3921 - mae: 0.3646 - mse: 0.3921 - val_loss: 0.2136 - val_mae: 0.3113 - val_mse: 0.2136 - learning_rate: 0.1000 - val_custom_mse: 0.3082 - val_custom_mae: 0.3881\n",
            "Epoch 3/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3537 - mae: 0.3336 - mse: 0.3537 - val_loss: 0.1968 - val_mae: 0.2914 - val_mse: 0.1968 - learning_rate: 0.1000 - val_custom_mse: 0.2935 - val_custom_mae: 0.3770\n",
            "Epoch 4/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3388 - mae: 0.3185 - mse: 0.3388 - val_loss: 0.1894 - val_mae: 0.2816 - val_mse: 0.1894 - learning_rate: 0.1000 - val_custom_mse: 0.2866 - val_custom_mae: 0.3713\n",
            "Epoch 5/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3321 - mae: 0.3106 - mse: 0.3321 - val_loss: 0.1869 - val_mae: 0.2783 - val_mse: 0.1869 - learning_rate: 0.1000 - val_custom_mse: 0.2844 - val_custom_mae: 0.3695\n",
            "Epoch 6/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3277 - mae: 0.3054 - mse: 0.3277 - val_loss: 0.1826 - val_mae: 0.2721 - val_mse: 0.1826 - learning_rate: 0.1000 - val_custom_mse: 0.2806 - val_custom_mae: 0.3661\n",
            "Epoch 7/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3244 - mae: 0.3017 - mse: 0.3244 - val_loss: 0.1813 - val_mae: 0.2699 - val_mse: 0.1813 - learning_rate: 0.1000 - val_custom_mse: 0.2796 - val_custom_mae: 0.3653\n",
            "Epoch 8/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3217 - mae: 0.2989 - mse: 0.3217 - val_loss: 0.1791 - val_mae: 0.2667 - val_mse: 0.1791 - learning_rate: 0.1000 - val_custom_mse: 0.2776 - val_custom_mae: 0.3633\n",
            "Epoch 9/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3197 - mae: 0.2967 - mse: 0.3197 - val_loss: 0.1773 - val_mae: 0.2643 - val_mse: 0.1773 - learning_rate: 0.1000 - val_custom_mse: 0.2757 - val_custom_mae: 0.3615\n",
            "Epoch 10/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3178 - mae: 0.2949 - mse: 0.3178 - val_loss: 0.1779 - val_mae: 0.2643 - val_mse: 0.1779 - learning_rate: 0.1000 - val_custom_mse: 0.2770 - val_custom_mae: 0.3624\n",
            "Epoch 11/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3164 - mae: 0.2935 - mse: 0.3164 - val_loss: 0.1775 - val_mae: 0.2638 - val_mse: 0.1775 - learning_rate: 0.1000 - val_custom_mse: 0.2766 - val_custom_mae: 0.3621\n",
            "Epoch 12/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3151 - mae: 0.2922 - mse: 0.3151 - val_loss: 0.1760 - val_mae: 0.2612 - val_mse: 0.1760 - learning_rate: 0.1000 - val_custom_mse: 0.2758 - val_custom_mae: 0.3612\n",
            "Epoch 13/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3142 - mae: 0.2912 - mse: 0.3142 - val_loss: 0.1748 - val_mae: 0.2596 - val_mse: 0.1748 - learning_rate: 0.1000 - val_custom_mse: 0.2743 - val_custom_mae: 0.3597\n",
            "Epoch 14/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3130 - mae: 0.2902 - mse: 0.3130 - val_loss: 0.1738 - val_mae: 0.2583 - val_mse: 0.1738 - learning_rate: 0.1000 - val_custom_mse: 0.2733 - val_custom_mae: 0.3591\n",
            "Epoch 15/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3126 - mae: 0.2897 - mse: 0.3126 - val_loss: 0.1744 - val_mae: 0.2584 - val_mse: 0.1744 - learning_rate: 0.1000 - val_custom_mse: 0.2745 - val_custom_mae: 0.3599\n",
            "Epoch 16/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3117 - mae: 0.2889 - mse: 0.3117 - val_loss: 0.1739 - val_mae: 0.2578 - val_mse: 0.1739 - learning_rate: 0.1000 - val_custom_mse: 0.2740 - val_custom_mae: 0.3596\n",
            "Epoch 17/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3112 - mae: 0.2883 - mse: 0.3112 - val_loss: 0.1732 - val_mae: 0.2565 - val_mse: 0.1732 - learning_rate: 0.1000 - val_custom_mse: 0.2734 - val_custom_mae: 0.3588\n",
            "Epoch 18/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3108 - mae: 0.2879 - mse: 0.3108 - val_loss: 0.1730 - val_mae: 0.2561 - val_mse: 0.1730 - learning_rate: 0.1000 - val_custom_mse: 0.2734 - val_custom_mae: 0.3587\n",
            "Epoch 19/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3104 - mae: 0.2874 - mse: 0.3104 - val_loss: 0.1741 - val_mae: 0.2581 - val_mse: 0.1741 - learning_rate: 0.1000 - val_custom_mse: 0.2744 - val_custom_mae: 0.3598\n",
            "Epoch 20/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3099 - mae: 0.2870 - mse: 0.3099 - val_loss: 0.1728 - val_mae: 0.2564 - val_mse: 0.1728 - learning_rate: 0.1000 - val_custom_mse: 0.2730 - val_custom_mae: 0.3590\n",
            "Epoch 21/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3096 - mae: 0.2867 - mse: 0.3096 - val_loss: 0.1731 - val_mae: 0.2560 - val_mse: 0.1731 - learning_rate: 0.1000 - val_custom_mse: 0.2740 - val_custom_mae: 0.3594\n",
            "Epoch 22/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3092 - mae: 0.2863 - mse: 0.3092 - val_loss: 0.1720 - val_mae: 0.2545 - val_mse: 0.1720 - learning_rate: 0.1000 - val_custom_mse: 0.2727 - val_custom_mae: 0.3583\n",
            "Epoch 23/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3091 - mae: 0.2861 - mse: 0.3091 - val_loss: 0.1729 - val_mae: 0.2556 - val_mse: 0.1729 - learning_rate: 0.1000 - val_custom_mse: 0.2739 - val_custom_mae: 0.3594\n",
            "Epoch 24/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3088 - mae: 0.2859 - mse: 0.3088 - val_loss: 0.1720 - val_mae: 0.2543 - val_mse: 0.1720 - learning_rate: 0.1000 - val_custom_mse: 0.2729 - val_custom_mae: 0.3586\n",
            "Epoch 25/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3084 - mae: 0.2856 - mse: 0.3084 - val_loss: 0.1721 - val_mae: 0.2546 - val_mse: 0.1721 - learning_rate: 0.1000 - val_custom_mse: 0.2731 - val_custom_mae: 0.3590\n",
            "Epoch 26/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3083 - mae: 0.2855 - mse: 0.3083 - val_loss: 0.1729 - val_mae: 0.2555 - val_mse: 0.1729 - learning_rate: 0.1000 - val_custom_mse: 0.2742 - val_custom_mae: 0.3596\n",
            "Epoch 27/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3080 - mae: 0.2853 - mse: 0.3080 - val_loss: 0.1729 - val_mae: 0.2556 - val_mse: 0.1729 - learning_rate: 0.1000 - val_custom_mse: 0.2741 - val_custom_mae: 0.3596\n",
            "Epoch 28/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3077 - mae: 0.2850 - mse: 0.3077 - val_loss: 0.1713 - val_mae: 0.2535 - val_mse: 0.1713 - learning_rate: 0.1000 - val_custom_mse: 0.2720 - val_custom_mae: 0.3579\n",
            "Epoch 29/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3077 - mae: 0.2849 - mse: 0.3077 - val_loss: 0.1717 - val_mae: 0.2538 - val_mse: 0.1717 - learning_rate: 0.1000 - val_custom_mse: 0.2728 - val_custom_mae: 0.3586\n",
            "Epoch 30/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3074 - mae: 0.2847 - mse: 0.3074 - val_loss: 0.1715 - val_mae: 0.2535 - val_mse: 0.1715 - learning_rate: 0.1000 - val_custom_mse: 0.2726 - val_custom_mae: 0.3582\n",
            "Epoch 31/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3073 - mae: 0.2847 - mse: 0.3073 - val_loss: 0.1705 - val_mae: 0.2520 - val_mse: 0.1705 - learning_rate: 0.1000 - val_custom_mse: 0.2714 - val_custom_mae: 0.3573\n",
            "Epoch 32/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3069 - mae: 0.2844 - mse: 0.3069 - val_loss: 0.1714 - val_mae: 0.2535 - val_mse: 0.1714 - learning_rate: 0.1000 - val_custom_mse: 0.2725 - val_custom_mae: 0.3583\n",
            "Epoch 33/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3069 - mae: 0.2845 - mse: 0.3069 - val_loss: 0.1720 - val_mae: 0.2541 - val_mse: 0.1720 - learning_rate: 0.1000 - val_custom_mse: 0.2735 - val_custom_mae: 0.3592\n",
            "Epoch 34/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3068 - mae: 0.2843 - mse: 0.3068 - val_loss: 0.1721 - val_mae: 0.2544 - val_mse: 0.1721 - learning_rate: 0.1000 - val_custom_mse: 0.2737 - val_custom_mae: 0.3595\n",
            "Epoch 35/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3066 - mae: 0.2842 - mse: 0.3066 - val_loss: 0.1703 - val_mae: 0.2519 - val_mse: 0.1703 - learning_rate: 0.1000 - val_custom_mse: 0.2712 - val_custom_mae: 0.3572\n",
            "Epoch 36/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3063 - mae: 0.2841 - mse: 0.3063 - val_loss: 0.1715 - val_mae: 0.2533 - val_mse: 0.1715 - learning_rate: 0.1000 - val_custom_mse: 0.2729 - val_custom_mae: 0.3584\n",
            "Epoch 37/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3062 - mae: 0.2840 - mse: 0.3062 - val_loss: 0.1724 - val_mae: 0.2551 - val_mse: 0.1724 - learning_rate: 0.1000 - val_custom_mse: 0.2738 - val_custom_mae: 0.3594\n",
            "Epoch 38/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3062 - mae: 0.2841 - mse: 0.3062 - val_loss: 0.1715 - val_mae: 0.2534 - val_mse: 0.1715 - learning_rate: 0.1000 - val_custom_mse: 0.2729 - val_custom_mae: 0.3584\n",
            "Epoch 39/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3058 - mae: 0.2838 - mse: 0.3058 - val_loss: 0.1719 - val_mae: 0.2544 - val_mse: 0.1719 - learning_rate: 0.1000 - val_custom_mse: 0.2732 - val_custom_mae: 0.3591\n",
            "Epoch 40/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3059 - mae: 0.2839 - mse: 0.3059 - val_loss: 0.1715 - val_mae: 0.2537 - val_mse: 0.1715 - learning_rate: 0.1000 - val_custom_mse: 0.2727 - val_custom_mae: 0.3586\n",
            "Epoch 41/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3056 - mae: 0.2838 - mse: 0.3056 - val_loss: 0.1711 - val_mae: 0.2531 - val_mse: 0.1711 - learning_rate: 0.1000 - val_custom_mse: 0.2720 - val_custom_mae: 0.3578\n",
            "Epoch 42/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3056 - mae: 0.2837 - mse: 0.3056 - val_loss: 0.1712 - val_mae: 0.2530 - val_mse: 0.1712 - learning_rate: 0.1000 - val_custom_mse: 0.2727 - val_custom_mae: 0.3588\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3055 - mae: 0.2837 - mse: 0.3055 - val_loss: 0.1710 - val_mae: 0.2526 - val_mse: 0.1710 - learning_rate: 0.1000 - val_custom_mse: 0.2725 - val_custom_mae: 0.3583\n",
            "Epoch 44/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3053 - mae: 0.2834 - mse: 0.3053 - val_loss: 0.1694 - val_mae: 0.2508 - val_mse: 0.1694 - learning_rate: 0.0200 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 45/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3052 - mae: 0.2835 - mse: 0.3052 - val_loss: 0.1694 - val_mae: 0.2509 - val_mse: 0.1694 - learning_rate: 0.0200 - val_custom_mse: 0.2701 - val_custom_mae: 0.3566\n",
            "Epoch 46/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3050 - mae: 0.2834 - mse: 0.3050 - val_loss: 0.1693 - val_mae: 0.2506 - val_mse: 0.1693 - learning_rate: 0.0200 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 47/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3052 - mae: 0.2834 - mse: 0.3052 - val_loss: 0.1695 - val_mae: 0.2509 - val_mse: 0.1695 - learning_rate: 0.0200 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 48/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3051 - mae: 0.2834 - mse: 0.3051 - val_loss: 0.1694 - val_mae: 0.2507 - val_mse: 0.1694 - learning_rate: 0.0200 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 49/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2833 - mse: 0.3049 - val_loss: 0.1694 - val_mae: 0.2508 - val_mse: 0.1694 - learning_rate: 0.0200 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 50/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2833 - mse: 0.3049 - val_loss: 0.1692 - val_mae: 0.2506 - val_mse: 0.1692 - learning_rate: 0.0200 - val_custom_mse: 0.2699 - val_custom_mae: 0.3565\n",
            "Epoch 51/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2833 - mse: 0.3048 - val_loss: 0.1694 - val_mae: 0.2507 - val_mse: 0.1694 - learning_rate: 0.0200 - val_custom_mse: 0.2703 - val_custom_mae: 0.3568\n",
            "Epoch 52/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2833 - mse: 0.3049 - val_loss: 0.1694 - val_mae: 0.2507 - val_mse: 0.1694 - learning_rate: 0.0200 - val_custom_mse: 0.2701 - val_custom_mae: 0.3566\n",
            "Epoch 53/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2833 - mse: 0.3049 - val_loss: 0.1691 - val_mae: 0.2504 - val_mse: 0.1691 - learning_rate: 0.0200 - val_custom_mse: 0.2698 - val_custom_mae: 0.3564\n",
            "Epoch 54/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2833 - mse: 0.3049 - val_loss: 0.1694 - val_mae: 0.2507 - val_mse: 0.1694 - learning_rate: 0.0200 - val_custom_mse: 0.2702 - val_custom_mae: 0.3567\n",
            "Epoch 55/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3050 - mae: 0.2833 - mse: 0.3050 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 0.0200 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 56/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2833 - mse: 0.3049 - val_loss: 0.1693 - val_mae: 0.2506 - val_mse: 0.1693 - learning_rate: 0.0200 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 57/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2833 - mse: 0.3048 - val_loss: 0.1692 - val_mae: 0.2506 - val_mse: 0.1692 - learning_rate: 0.0200 - val_custom_mse: 0.2700 - val_custom_mae: 0.3565\n",
            "Epoch 58/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3050 - mae: 0.2833 - mse: 0.3050 - val_loss: 0.1693 - val_mae: 0.2506 - val_mse: 0.1693 - learning_rate: 0.0200 - val_custom_mse: 0.2701 - val_custom_mae: 0.3567\n",
            "Epoch 59/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3050 - mae: 0.2833 - mse: 0.3050 - val_loss: 0.1691 - val_mae: 0.2504 - val_mse: 0.1691 - learning_rate: 0.0200 - val_custom_mse: 0.2699 - val_custom_mae: 0.3564\n",
            "Epoch 60/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1693 - val_mae: 0.2506 - val_mse: 0.1693 - learning_rate: 0.0200 - val_custom_mse: 0.2701 - val_custom_mae: 0.3565\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2833 - mse: 0.3049 - val_loss: 0.1695 - val_mae: 0.2508 - val_mse: 0.1695 - learning_rate: 0.0200 - val_custom_mse: 0.2703 - val_custom_mae: 0.3567\n",
            "Epoch 62/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2833 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 0.0040 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 63/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2832 - mse: 0.3049 - val_loss: 0.1690 - val_mae: 0.2502 - val_mse: 0.1690 - learning_rate: 0.0040 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 64/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 0.0040 - val_custom_mse: 0.2699 - val_custom_mae: 0.3563\n",
            "Epoch 65/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3045 - mae: 0.2831 - mse: 0.3045 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 0.0040 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 66/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2832 - mse: 0.3049 - val_loss: 0.1692 - val_mae: 0.2503 - val_mse: 0.1692 - learning_rate: 0.0040 - val_custom_mse: 0.2700 - val_custom_mae: 0.3564\n",
            "Epoch 67/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2832 - mse: 0.3048 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 0.0040 - val_custom_mse: 0.2699 - val_custom_mae: 0.3563\n",
            "Epoch 68/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2831 - mse: 0.3046 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 0.0040 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
            "1020/1020 - 9s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 0.0040 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 70/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 71/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 72/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 73/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2832 - mse: 0.3046 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 74/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2832 - mse: 0.3049 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 75/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2832 - mse: 0.3048 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 76/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2832 - mse: 0.3048 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2832 - mse: 0.3048 - val_loss: 0.1691 - val_mae: 0.2502 - val_mse: 0.1691 - learning_rate: 8.0000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 78/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3045 - mae: 0.2831 - mse: 0.3045 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 79/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2831 - mse: 0.3046 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 80/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3049 - mae: 0.2832 - mse: 0.3049 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 81/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2831 - mse: 0.3046 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 82/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2831 - mse: 0.3046 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 83/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2832 - mse: 0.3048 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 84/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2832 - mse: 0.3048 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2832 - mse: 0.3046 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 1.6000e-04 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 86/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3045 - mae: 0.2831 - mse: 0.3045 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 87/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3045 - mae: 0.2831 - mse: 0.3045 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 88/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2831 - mse: 0.3046 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 89/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2832 - mse: 0.3048 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 90/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 91/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3045 - mae: 0.2831 - mse: 0.3045 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 92/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 6.399999256245792e-06.\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 3.2000e-05 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 94/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2831 - mse: 0.3046 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 95/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 96/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 97/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2833 - mse: 0.3048 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 98/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3046 - mae: 0.2832 - mse: 0.3046 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 99/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3048 - mae: 0.2832 - mse: 0.3048 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "Epoch 100/100\n",
            "1020/1020 - 8s - 8ms/step - loss: 0.3047 - mae: 0.2832 - mse: 0.3047 - val_loss: 0.1691 - val_mae: 0.2503 - val_mse: 0.1691 - learning_rate: 6.4000e-06 - val_custom_mse: 0.2698 - val_custom_mae: 0.3562\n",
            "\n",
            " ETTm2 Final Results:\n",
            "\n",
            "MSE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%  DR=20%  DR=30%\n",
            "Horizon                                \n",
            "96       0.1601  0.1592  0.1594  0.1598\n",
            "192      0.2123  0.2113  0.2113  0.2115\n",
            "336      0.2601  0.2597  0.2598  0.2599\n",
            "720      0.3335  0.3329  0.3330  0.3330\n",
            "\n",
            "MAE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%  DR=20%  DR=30%\n",
            "Horizon                                \n",
            "96       0.2532  0.2524  0.2526  0.2530\n",
            "192      0.2906  0.2898  0.2897  0.2899\n",
            "336      0.3244  0.3240  0.3240  0.3240\n",
            "720      0.3774  0.3771  0.3772  0.3770\n",
            "\n",
            "Results saved to: ./flowmixer_results/ETTm2_experiment_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Run the experiments\n",
        "data_name='ETTm2'\n",
        "results = run_experiments(data_name,horizons=[96,192,336,720], dropout_rates=[0.0, 0.1, 0.2, 0.3], revin=1, seq_len_=1204,  learning_rate=1e-1, mopt='sgd')\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\n {data_name} Final Results:\")\n",
        "df = pd.DataFrame(results)\n",
        "# Assuming results is a list of dictionaries with horizon, dropout, MSE, and MAE values\n",
        "display_results_tables(results[0])\n",
        "\n",
        "\n",
        "# The results are already saved in CSV format after each experiment\n",
        "print(f\"\\nResults saved to: ./flowmixer_results/{data_name}_experiment_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weather"
      ],
      "metadata": {
        "id": "h325ZvXK1r7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A4sndE1_lFW",
        "outputId": "fc92d376-8ffe-40cf-9e66-faf978f2d549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: horizon=96, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_32', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1118/1118 - 14s - 12ms/step - loss: 0.3345 - mae: 0.2155 - mse: 0.3345 - val_loss: 0.0834 - val_mae: 0.0900 - val_mse: 0.0834 - learning_rate: 0.0010 - val_custom_mse: 0.4032 - val_custom_mae: 0.2912\n",
            "Epoch 2/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.1150 - mae: 0.0832 - mse: 0.1150 - val_loss: 0.0599 - val_mae: 0.0662 - val_mse: 0.0599 - learning_rate: 0.0010 - val_custom_mse: 0.3883 - val_custom_mae: 0.2828\n",
            "Epoch 3/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0925 - mae: 0.0687 - mse: 0.0925 - val_loss: 0.0525 - val_mae: 0.0582 - val_mse: 0.0525 - learning_rate: 0.0010 - val_custom_mse: 0.3862 - val_custom_mae: 0.2802\n",
            "Epoch 4/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0776 - mae: 0.0606 - mse: 0.0776 - val_loss: 0.0473 - val_mae: 0.0521 - val_mse: 0.0473 - learning_rate: 0.0010 - val_custom_mse: 0.3854 - val_custom_mae: 0.2790\n",
            "Epoch 5/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0656 - mae: 0.0539 - mse: 0.0656 - val_loss: 0.0432 - val_mae: 0.0469 - val_mse: 0.0432 - learning_rate: 0.0010 - val_custom_mse: 0.3843 - val_custom_mae: 0.2775\n",
            "Epoch 6/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0562 - mae: 0.0481 - mse: 0.0562 - val_loss: 0.0401 - val_mae: 0.0421 - val_mse: 0.0401 - learning_rate: 0.0010 - val_custom_mse: 0.3821 - val_custom_mae: 0.2747\n",
            "Epoch 7/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0491 - mae: 0.0432 - mse: 0.0491 - val_loss: 0.0383 - val_mae: 0.0383 - val_mse: 0.0383 - learning_rate: 0.0010 - val_custom_mse: 0.3832 - val_custom_mae: 0.2749\n",
            "Epoch 8/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0444 - mae: 0.0394 - mse: 0.0444 - val_loss: 0.0370 - val_mae: 0.0354 - val_mse: 0.0370 - learning_rate: 0.0010 - val_custom_mse: 0.3817 - val_custom_mae: 0.2731\n",
            "Epoch 9/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0414 - mae: 0.0367 - mse: 0.0414 - val_loss: 0.0363 - val_mae: 0.0334 - val_mse: 0.0363 - learning_rate: 0.0010 - val_custom_mse: 0.3805 - val_custom_mae: 0.2720\n",
            "Epoch 10/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0396 - mae: 0.0349 - mse: 0.0396 - val_loss: 0.0360 - val_mae: 0.0321 - val_mse: 0.0360 - learning_rate: 0.0010 - val_custom_mse: 0.3810 - val_custom_mae: 0.2728\n",
            "Epoch 11/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0386 - mae: 0.0336 - mse: 0.0386 - val_loss: 0.0359 - val_mae: 0.0313 - val_mse: 0.0359 - learning_rate: 0.0010 - val_custom_mse: 0.3810 - val_custom_mae: 0.2727\n",
            "Epoch 12/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0381 - mae: 0.0328 - mse: 0.0381 - val_loss: 0.0356 - val_mae: 0.0305 - val_mse: 0.0356 - learning_rate: 0.0010 - val_custom_mse: 0.3780 - val_custom_mae: 0.2700\n",
            "Epoch 13/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0377 - mae: 0.0322 - mse: 0.0377 - val_loss: 0.0356 - val_mae: 0.0302 - val_mse: 0.0356 - learning_rate: 0.0010 - val_custom_mse: 0.3780 - val_custom_mae: 0.2702\n",
            "Epoch 14/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0375 - mae: 0.0319 - mse: 0.0375 - val_loss: 0.0355 - val_mae: 0.0299 - val_mse: 0.0355 - learning_rate: 0.0010 - val_custom_mse: 0.3769 - val_custom_mae: 0.2695\n",
            "Epoch 15/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0374 - mae: 0.0316 - mse: 0.0374 - val_loss: 0.0354 - val_mae: 0.0297 - val_mse: 0.0354 - learning_rate: 0.0010 - val_custom_mse: 0.3761 - val_custom_mae: 0.2690\n",
            "Epoch 16/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0373 - mae: 0.0314 - mse: 0.0373 - val_loss: 0.0354 - val_mae: 0.0296 - val_mse: 0.0354 - learning_rate: 0.0010 - val_custom_mse: 0.3765 - val_custom_mae: 0.2693\n",
            "Epoch 17/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0372 - mae: 0.0313 - mse: 0.0372 - val_loss: 0.0353 - val_mae: 0.0296 - val_mse: 0.0353 - learning_rate: 0.0010 - val_custom_mse: 0.3755 - val_custom_mae: 0.2682\n",
            "Epoch 18/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0372 - mae: 0.0312 - mse: 0.0372 - val_loss: 0.0353 - val_mae: 0.0294 - val_mse: 0.0353 - learning_rate: 0.0010 - val_custom_mse: 0.3750 - val_custom_mae: 0.2676\n",
            "Epoch 19/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0371 - mae: 0.0311 - mse: 0.0371 - val_loss: 0.0352 - val_mae: 0.0294 - val_mse: 0.0352 - learning_rate: 0.0010 - val_custom_mse: 0.3748 - val_custom_mae: 0.2677\n",
            "Epoch 20/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0371 - mae: 0.0311 - mse: 0.0371 - val_loss: 0.0354 - val_mae: 0.0296 - val_mse: 0.0354 - learning_rate: 0.0010 - val_custom_mse: 0.3769 - val_custom_mae: 0.2697\n",
            "Epoch 21/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0371 - mae: 0.0310 - mse: 0.0371 - val_loss: 0.0353 - val_mae: 0.0294 - val_mse: 0.0353 - learning_rate: 0.0010 - val_custom_mse: 0.3752 - val_custom_mae: 0.2684\n",
            "Epoch 22/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0371 - mae: 0.0310 - mse: 0.0371 - val_loss: 0.0351 - val_mae: 0.0292 - val_mse: 0.0351 - learning_rate: 0.0010 - val_custom_mse: 0.3739 - val_custom_mae: 0.2671\n",
            "Epoch 23/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0371 - mae: 0.0309 - mse: 0.0371 - val_loss: 0.0353 - val_mae: 0.0292 - val_mse: 0.0353 - learning_rate: 0.0010 - val_custom_mse: 0.3752 - val_custom_mae: 0.2679\n",
            "Epoch 24/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0370 - mae: 0.0309 - mse: 0.0370 - val_loss: 0.0353 - val_mae: 0.0292 - val_mse: 0.0353 - learning_rate: 0.0010 - val_custom_mse: 0.3751 - val_custom_mae: 0.2682\n",
            "Epoch 25/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0370 - mae: 0.0308 - mse: 0.0370 - val_loss: 0.0352 - val_mae: 0.0292 - val_mse: 0.0352 - learning_rate: 0.0010 - val_custom_mse: 0.3748 - val_custom_mae: 0.2680\n",
            "Epoch 26/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0370 - mae: 0.0308 - mse: 0.0370 - val_loss: 0.0353 - val_mae: 0.0292 - val_mse: 0.0353 - learning_rate: 0.0010 - val_custom_mse: 0.3755 - val_custom_mae: 0.2683\n",
            "Epoch 27/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0370 - mae: 0.0308 - mse: 0.0370 - val_loss: 0.0352 - val_mae: 0.0291 - val_mse: 0.0352 - learning_rate: 0.0010 - val_custom_mse: 0.3751 - val_custom_mae: 0.2679\n",
            "Epoch 28/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0370 - mae: 0.0307 - mse: 0.0370 - val_loss: 0.0352 - val_mae: 0.0292 - val_mse: 0.0352 - learning_rate: 0.0010 - val_custom_mse: 0.3748 - val_custom_mae: 0.2680\n",
            "Epoch 29/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0370 - mae: 0.0307 - mse: 0.0370 - val_loss: 0.0353 - val_mae: 0.0291 - val_mse: 0.0353 - learning_rate: 0.0010 - val_custom_mse: 0.3751 - val_custom_mae: 0.2682\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0370 - mae: 0.0307 - mse: 0.0370 - val_loss: 0.0352 - val_mae: 0.0289 - val_mse: 0.0352 - learning_rate: 0.0010 - val_custom_mse: 0.3745 - val_custom_mae: 0.2673\n",
            "Epoch 31/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0368 - mae: 0.0299 - mse: 0.0368 - val_loss: 0.0349 - val_mae: 0.0280 - val_mse: 0.0349 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3721 - val_custom_mae: 0.2646\n",
            "Epoch 32/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0368 - mae: 0.0297 - mse: 0.0368 - val_loss: 0.0349 - val_mae: 0.0279 - val_mse: 0.0349 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3720 - val_custom_mae: 0.2645\n",
            "Epoch 33/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0368 - mae: 0.0297 - mse: 0.0368 - val_loss: 0.0349 - val_mae: 0.0279 - val_mse: 0.0349 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3720 - val_custom_mae: 0.2645\n",
            "Epoch 34/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0368 - mae: 0.0297 - mse: 0.0368 - val_loss: 0.0349 - val_mae: 0.0280 - val_mse: 0.0349 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3720 - val_custom_mae: 0.2646\n",
            "Epoch 35/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0368 - mae: 0.0297 - mse: 0.0368 - val_loss: 0.0349 - val_mae: 0.0280 - val_mse: 0.0349 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3717 - val_custom_mae: 0.2644\n",
            "Epoch 36/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0297 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0280 - val_mse: 0.0349 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3719 - val_custom_mae: 0.2645\n",
            "Epoch 37/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0297 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0280 - val_mse: 0.0349 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3721 - val_custom_mae: 0.2647\n",
            "Epoch 38/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0298 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0280 - val_mse: 0.0349 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3716 - val_custom_mae: 0.2643\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0298 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0280 - val_mse: 0.0349 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 40/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0296 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3716 - val_custom_mae: 0.2642\n",
            "Epoch 41/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0296 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3716 - val_custom_mae: 0.2642\n",
            "Epoch 42/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0296 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3715 - val_custom_mae: 0.2642\n",
            "Epoch 43/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0296 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3715 - val_custom_mae: 0.2642\n",
            "Epoch 44/100\n",
            "1118/1118 - 7s - 7ms/step - loss: 0.0367 - mae: 0.0296 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3715 - val_custom_mae: 0.2642\n",
            "Epoch 45/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0296 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3715 - val_custom_mae: 0.2642\n",
            "Epoch 46/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0296 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3715 - val_custom_mae: 0.2642\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0296 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3715 - val_custom_mae: 0.2642\n",
            "Epoch 48/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0296 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3717 - val_custom_mae: 0.2644\n",
            "Epoch 49/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3717 - val_custom_mae: 0.2644\n",
            "Epoch 50/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3717 - val_custom_mae: 0.2644\n",
            "Epoch 51/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3717 - val_custom_mae: 0.2644\n",
            "Epoch 52/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3717 - val_custom_mae: 0.2644\n",
            "Epoch 53/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3717 - val_custom_mae: 0.2644\n",
            "Epoch 54/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3717 - val_custom_mae: 0.2644\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3717 - val_custom_mae: 0.2644\n",
            "Epoch 56/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 57/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 58/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 59/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 60/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 61/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 62/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 64/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 65/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 66/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 67/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 68/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 69/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 70/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 72/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 73/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 74/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 75/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 76/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 77/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 78/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 80/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 81/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 82/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 83/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 84/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 85/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 86/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 88/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 89/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 90/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 91/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 92/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 93/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 94/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 96/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 97/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 98/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 99/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Epoch 100/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0367 - mae: 0.0295 - mse: 0.0367 - val_loss: 0.0349 - val_mae: 0.0278 - val_mse: 0.0349 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3718 - val_custom_mae: 0.2645\n",
            "Running experiment: horizon=96, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_33', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1118/1118 - 14s - 12ms/step - loss: 0.3646 - mae: 0.2474 - mse: 0.3646 - val_loss: 0.0826 - val_mae: 0.0887 - val_mse: 0.0826 - learning_rate: 0.0010 - val_custom_mse: 0.3999 - val_custom_mae: 0.2884\n",
            "Epoch 2/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.1255 - mae: 0.1127 - mse: 0.1255 - val_loss: 0.0618 - val_mae: 0.0671 - val_mse: 0.0618 - learning_rate: 0.0010 - val_custom_mse: 0.3853 - val_custom_mae: 0.2794\n",
            "Epoch 3/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.1068 - mae: 0.0996 - mse: 0.1068 - val_loss: 0.0555 - val_mae: 0.0618 - val_mse: 0.0555 - learning_rate: 0.0010 - val_custom_mse: 0.3854 - val_custom_mae: 0.2792\n",
            "Epoch 4/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0973 - mae: 0.0957 - mse: 0.0973 - val_loss: 0.0507 - val_mae: 0.0580 - val_mse: 0.0507 - learning_rate: 0.0010 - val_custom_mse: 0.3823 - val_custom_mae: 0.2761\n",
            "Epoch 5/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0907 - mae: 0.0936 - mse: 0.0907 - val_loss: 0.0474 - val_mae: 0.0558 - val_mse: 0.0474 - learning_rate: 0.0010 - val_custom_mse: 0.3816 - val_custom_mae: 0.2756\n",
            "Epoch 6/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0858 - mae: 0.0923 - mse: 0.0858 - val_loss: 0.0453 - val_mae: 0.0546 - val_mse: 0.0453 - learning_rate: 0.0010 - val_custom_mse: 0.3827 - val_custom_mae: 0.2765\n",
            "Epoch 7/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0832 - mae: 0.0915 - mse: 0.0832 - val_loss: 0.0442 - val_mae: 0.0562 - val_mse: 0.0442 - learning_rate: 0.0010 - val_custom_mse: 0.3822 - val_custom_mae: 0.2762\n",
            "Epoch 8/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0810 - mae: 0.0922 - mse: 0.0810 - val_loss: 0.0429 - val_mae: 0.0563 - val_mse: 0.0429 - learning_rate: 0.0010 - val_custom_mse: 0.3807 - val_custom_mae: 0.2755\n",
            "Epoch 9/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0790 - mae: 0.0926 - mse: 0.0790 - val_loss: 0.0420 - val_mae: 0.0554 - val_mse: 0.0420 - learning_rate: 0.0010 - val_custom_mse: 0.3800 - val_custom_mae: 0.2739\n",
            "Epoch 10/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0781 - mae: 0.0928 - mse: 0.0781 - val_loss: 0.0418 - val_mae: 0.0556 - val_mse: 0.0418 - learning_rate: 0.0010 - val_custom_mse: 0.3812 - val_custom_mae: 0.2752\n",
            "Epoch 11/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0773 - mae: 0.0930 - mse: 0.0773 - val_loss: 0.0416 - val_mae: 0.0557 - val_mse: 0.0416 - learning_rate: 0.0010 - val_custom_mse: 0.3818 - val_custom_mae: 0.2754\n",
            "Epoch 12/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0769 - mae: 0.0931 - mse: 0.0769 - val_loss: 0.0413 - val_mae: 0.0556 - val_mse: 0.0413 - learning_rate: 0.0010 - val_custom_mse: 0.3798 - val_custom_mae: 0.2739\n",
            "Epoch 13/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0766 - mae: 0.0931 - mse: 0.0766 - val_loss: 0.0413 - val_mae: 0.0558 - val_mse: 0.0413 - learning_rate: 0.0010 - val_custom_mse: 0.3806 - val_custom_mae: 0.2744\n",
            "Epoch 14/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0773 - mae: 0.0931 - mse: 0.0773 - val_loss: 0.0412 - val_mae: 0.0558 - val_mse: 0.0412 - learning_rate: 0.0010 - val_custom_mse: 0.3803 - val_custom_mae: 0.2743\n",
            "Epoch 15/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0771 - mae: 0.0930 - mse: 0.0771 - val_loss: 0.0409 - val_mae: 0.0556 - val_mse: 0.0409 - learning_rate: 0.0010 - val_custom_mse: 0.3779 - val_custom_mae: 0.2726\n",
            "Epoch 16/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0763 - mae: 0.0929 - mse: 0.0763 - val_loss: 0.0411 - val_mae: 0.0557 - val_mse: 0.0411 - learning_rate: 0.0010 - val_custom_mse: 0.3803 - val_custom_mae: 0.2738\n",
            "Epoch 17/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0759 - mae: 0.0929 - mse: 0.0759 - val_loss: 0.0409 - val_mae: 0.0556 - val_mse: 0.0409 - learning_rate: 0.0010 - val_custom_mse: 0.3792 - val_custom_mae: 0.2735\n",
            "Epoch 18/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0766 - mae: 0.0928 - mse: 0.0766 - val_loss: 0.0411 - val_mae: 0.0559 - val_mse: 0.0411 - learning_rate: 0.0010 - val_custom_mse: 0.3808 - val_custom_mae: 0.2745\n",
            "Epoch 19/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0763 - mae: 0.0928 - mse: 0.0763 - val_loss: 0.0411 - val_mae: 0.0560 - val_mse: 0.0411 - learning_rate: 0.0010 - val_custom_mse: 0.3804 - val_custom_mae: 0.2740\n",
            "Epoch 20/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0755 - mae: 0.0928 - mse: 0.0755 - val_loss: 0.0409 - val_mae: 0.0558 - val_mse: 0.0409 - learning_rate: 0.0010 - val_custom_mse: 0.3789 - val_custom_mae: 0.2731\n",
            "Epoch 21/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0758 - mae: 0.0927 - mse: 0.0758 - val_loss: 0.0409 - val_mae: 0.0558 - val_mse: 0.0409 - learning_rate: 0.0010 - val_custom_mse: 0.3789 - val_custom_mae: 0.2726\n",
            "Epoch 22/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0755 - mae: 0.0927 - mse: 0.0755 - val_loss: 0.0410 - val_mae: 0.0560 - val_mse: 0.0410 - learning_rate: 0.0010 - val_custom_mse: 0.3795 - val_custom_mae: 0.2742\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0756 - mae: 0.0926 - mse: 0.0756 - val_loss: 0.0408 - val_mae: 0.0560 - val_mse: 0.0408 - learning_rate: 0.0010 - val_custom_mse: 0.3778 - val_custom_mae: 0.2730\n",
            "Epoch 24/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0743 - mae: 0.0922 - mse: 0.0743 - val_loss: 0.0406 - val_mae: 0.0552 - val_mse: 0.0406 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3769 - val_custom_mae: 0.2708\n",
            "Epoch 25/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0748 - mae: 0.0922 - mse: 0.0748 - val_loss: 0.0406 - val_mae: 0.0551 - val_mse: 0.0406 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3767 - val_custom_mae: 0.2706\n",
            "Epoch 26/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0750 - mae: 0.0922 - mse: 0.0750 - val_loss: 0.0406 - val_mae: 0.0552 - val_mse: 0.0406 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 27/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0745 - mae: 0.0921 - mse: 0.0745 - val_loss: 0.0406 - val_mae: 0.0552 - val_mse: 0.0406 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 28/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0743 - mae: 0.0921 - mse: 0.0743 - val_loss: 0.0407 - val_mae: 0.0553 - val_mse: 0.0407 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3776 - val_custom_mae: 0.2716\n",
            "Epoch 29/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0743 - mae: 0.0920 - mse: 0.0743 - val_loss: 0.0407 - val_mae: 0.0553 - val_mse: 0.0407 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3768 - val_custom_mae: 0.2708\n",
            "Epoch 30/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0750 - mae: 0.0921 - mse: 0.0750 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 31/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0742 - mae: 0.0920 - mse: 0.0742 - val_loss: 0.0406 - val_mae: 0.0553 - val_mse: 0.0406 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3763 - val_custom_mae: 0.2706\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0748 - mae: 0.0920 - mse: 0.0748 - val_loss: 0.0407 - val_mae: 0.0553 - val_mse: 0.0407 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 33/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0742 - mae: 0.0919 - mse: 0.0742 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 34/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0735 - mae: 0.0919 - mse: 0.0735 - val_loss: 0.0406 - val_mae: 0.0552 - val_mse: 0.0406 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 35/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0748 - mae: 0.0919 - mse: 0.0748 - val_loss: 0.0406 - val_mae: 0.0552 - val_mse: 0.0406 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 36/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0747 - mae: 0.0919 - mse: 0.0747 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 37/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0747 - mae: 0.0919 - mse: 0.0747 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 38/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0739 - mae: 0.0919 - mse: 0.0739 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 39/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0743 - mae: 0.0919 - mse: 0.0743 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0739 - mae: 0.0919 - mse: 0.0739 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3764 - val_custom_mae: 0.2704\n",
            "Epoch 41/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0747 - mae: 0.0919 - mse: 0.0747 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 42/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0748 - mae: 0.0919 - mse: 0.0748 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3764 - val_custom_mae: 0.2705\n",
            "Epoch 43/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0740 - mae: 0.0919 - mse: 0.0740 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 44/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0738 - mae: 0.0919 - mse: 0.0738 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 45/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0745 - mae: 0.0919 - mse: 0.0745 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 46/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0744 - mae: 0.0919 - mse: 0.0744 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 47/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0747 - mae: 0.0919 - mse: 0.0747 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0756 - mae: 0.0919 - mse: 0.0756 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 49/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0744 - mae: 0.0919 - mse: 0.0744 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2705\n",
            "Epoch 50/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0741 - mae: 0.0919 - mse: 0.0741 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 51/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0750 - mae: 0.0919 - mse: 0.0750 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 52/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0741 - mae: 0.0919 - mse: 0.0741 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 53/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0741 - mae: 0.0919 - mse: 0.0741 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 54/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0747 - mae: 0.0919 - mse: 0.0747 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 55/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0738 - mae: 0.0919 - mse: 0.0738 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0745 - mae: 0.0919 - mse: 0.0745 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 57/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0743 - mae: 0.0919 - mse: 0.0743 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 58/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0748 - mae: 0.0919 - mse: 0.0748 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 59/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0743 - mae: 0.0919 - mse: 0.0743 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 60/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0746 - mae: 0.0919 - mse: 0.0746 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 61/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0744 - mae: 0.0919 - mse: 0.0744 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 62/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0737 - mae: 0.0919 - mse: 0.0737 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 63/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0741 - mae: 0.0919 - mse: 0.0741 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0742 - mae: 0.0919 - mse: 0.0742 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3765 - val_custom_mae: 0.2706\n",
            "Epoch 65/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0746 - mae: 0.0919 - mse: 0.0746 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 66/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0738 - mae: 0.0919 - mse: 0.0738 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 67/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0745 - mae: 0.0919 - mse: 0.0745 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 68/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0735 - mae: 0.0919 - mse: 0.0735 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 69/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0741 - mae: 0.0919 - mse: 0.0741 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 70/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0744 - mae: 0.0919 - mse: 0.0744 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 71/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0745 - mae: 0.0919 - mse: 0.0745 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0746 - mae: 0.0919 - mse: 0.0746 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 73/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0749 - mae: 0.0919 - mse: 0.0749 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 74/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0739 - mae: 0.0919 - mse: 0.0739 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 75/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0750 - mae: 0.0919 - mse: 0.0750 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 76/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0741 - mae: 0.0919 - mse: 0.0741 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 77/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0744 - mae: 0.0919 - mse: 0.0744 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 78/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0746 - mae: 0.0919 - mse: 0.0746 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 79/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0743 - mae: 0.0919 - mse: 0.0743 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 80: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0739 - mae: 0.0919 - mse: 0.0739 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 81/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0745 - mae: 0.0919 - mse: 0.0745 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 82/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0747 - mae: 0.0919 - mse: 0.0747 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 83/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0748 - mae: 0.0919 - mse: 0.0748 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 84/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0741 - mae: 0.0919 - mse: 0.0741 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 85/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0749 - mae: 0.0919 - mse: 0.0749 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 86/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0752 - mae: 0.0919 - mse: 0.0752 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 87/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0740 - mae: 0.0919 - mse: 0.0740 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0745 - mae: 0.0919 - mse: 0.0745 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 89/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0740 - mae: 0.0919 - mse: 0.0740 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 90/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0741 - mae: 0.0919 - mse: 0.0741 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 91/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0743 - mae: 0.0919 - mse: 0.0743 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 92/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0740 - mae: 0.0919 - mse: 0.0740 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 93/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0753 - mae: 0.0919 - mse: 0.0753 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 94/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0740 - mae: 0.0919 - mse: 0.0740 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 95/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0739 - mae: 0.0919 - mse: 0.0739 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0736 - mae: 0.0919 - mse: 0.0736 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 97/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0743 - mae: 0.0919 - mse: 0.0743 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 98/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0751 - mae: 0.0919 - mse: 0.0751 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 99/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0738 - mae: 0.0919 - mse: 0.0738 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Epoch 100/100\n",
            "1118/1118 - 8s - 7ms/step - loss: 0.0739 - mae: 0.0919 - mse: 0.0739 - val_loss: 0.0407 - val_mae: 0.0552 - val_mse: 0.0407 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3766 - val_custom_mae: 0.2706\n",
            "Running experiment: horizon=192, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_34', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1115/1115 - 13s - 12ms/step - loss: 0.3753 - mae: 0.2445 - mse: 0.3753 - val_loss: 0.1274 - val_mae: 0.1193 - val_mse: 0.1274 - learning_rate: 0.0010 - val_custom_mse: 0.4605 - val_custom_mae: 0.3268\n",
            "Epoch 2/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1542 - mae: 0.1152 - mse: 0.1542 - val_loss: 0.1028 - val_mae: 0.0958 - val_mse: 0.1028 - learning_rate: 0.0010 - val_custom_mse: 0.4415 - val_custom_mae: 0.3188\n",
            "Epoch 3/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1321 - mae: 0.1007 - mse: 0.1321 - val_loss: 0.0958 - val_mae: 0.0879 - val_mse: 0.0958 - learning_rate: 0.0010 - val_custom_mse: 0.4406 - val_custom_mae: 0.3164\n",
            "Epoch 4/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1180 - mae: 0.0926 - mse: 0.1180 - val_loss: 0.0905 - val_mae: 0.0815 - val_mse: 0.0905 - learning_rate: 0.0010 - val_custom_mse: 0.4380 - val_custom_mae: 0.3136\n",
            "Epoch 5/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1069 - mae: 0.0859 - mse: 0.1069 - val_loss: 0.0870 - val_mae: 0.0764 - val_mse: 0.0870 - learning_rate: 0.0010 - val_custom_mse: 0.4378 - val_custom_mae: 0.3139\n",
            "Epoch 6/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0988 - mae: 0.0803 - mse: 0.0988 - val_loss: 0.0849 - val_mae: 0.0723 - val_mse: 0.0849 - learning_rate: 0.0010 - val_custom_mse: 0.4388 - val_custom_mae: 0.3135\n",
            "Epoch 7/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0933 - mae: 0.0761 - mse: 0.0933 - val_loss: 0.0832 - val_mae: 0.0690 - val_mse: 0.0832 - learning_rate: 0.0010 - val_custom_mse: 0.4365 - val_custom_mae: 0.3121\n",
            "Epoch 8/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0899 - mae: 0.0731 - mse: 0.0899 - val_loss: 0.0823 - val_mae: 0.0667 - val_mse: 0.0823 - learning_rate: 0.0010 - val_custom_mse: 0.4355 - val_custom_mae: 0.3112\n",
            "Epoch 9/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0878 - mae: 0.0710 - mse: 0.0878 - val_loss: 0.0817 - val_mae: 0.0650 - val_mse: 0.0817 - learning_rate: 0.0010 - val_custom_mse: 0.4338 - val_custom_mae: 0.3095\n",
            "Epoch 10/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0865 - mae: 0.0696 - mse: 0.0865 - val_loss: 0.0815 - val_mae: 0.0643 - val_mse: 0.0815 - learning_rate: 0.0010 - val_custom_mse: 0.4332 - val_custom_mae: 0.3094\n",
            "Epoch 11/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0857 - mae: 0.0687 - mse: 0.0857 - val_loss: 0.0813 - val_mae: 0.0636 - val_mse: 0.0813 - learning_rate: 0.0010 - val_custom_mse: 0.4322 - val_custom_mae: 0.3091\n",
            "Epoch 12/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0851 - mae: 0.0681 - mse: 0.0851 - val_loss: 0.0810 - val_mae: 0.0629 - val_mse: 0.0810 - learning_rate: 0.0010 - val_custom_mse: 0.4313 - val_custom_mae: 0.3075\n",
            "Epoch 13/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0847 - mae: 0.0677 - mse: 0.0847 - val_loss: 0.0812 - val_mae: 0.0630 - val_mse: 0.0812 - learning_rate: 0.0010 - val_custom_mse: 0.4320 - val_custom_mae: 0.3086\n",
            "Epoch 14/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0844 - mae: 0.0674 - mse: 0.0844 - val_loss: 0.0810 - val_mae: 0.0628 - val_mse: 0.0810 - learning_rate: 0.0010 - val_custom_mse: 0.4314 - val_custom_mae: 0.3083\n",
            "Epoch 15/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0842 - mae: 0.0672 - mse: 0.0842 - val_loss: 0.0808 - val_mae: 0.0625 - val_mse: 0.0808 - learning_rate: 0.0010 - val_custom_mse: 0.4302 - val_custom_mae: 0.3073\n",
            "Epoch 16/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0841 - mae: 0.0670 - mse: 0.0841 - val_loss: 0.0808 - val_mae: 0.0624 - val_mse: 0.0808 - learning_rate: 0.0010 - val_custom_mse: 0.4302 - val_custom_mae: 0.3071\n",
            "Epoch 17/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0840 - mae: 0.0669 - mse: 0.0840 - val_loss: 0.0807 - val_mae: 0.0622 - val_mse: 0.0807 - learning_rate: 0.0010 - val_custom_mse: 0.4295 - val_custom_mae: 0.3064\n",
            "Epoch 18/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0840 - mae: 0.0668 - mse: 0.0840 - val_loss: 0.0807 - val_mae: 0.0621 - val_mse: 0.0807 - learning_rate: 0.0010 - val_custom_mse: 0.4298 - val_custom_mae: 0.3066\n",
            "Epoch 19/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0839 - mae: 0.0667 - mse: 0.0839 - val_loss: 0.0806 - val_mae: 0.0622 - val_mse: 0.0806 - learning_rate: 0.0010 - val_custom_mse: 0.4291 - val_custom_mae: 0.3065\n",
            "Epoch 20/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0839 - mae: 0.0667 - mse: 0.0839 - val_loss: 0.0805 - val_mae: 0.0620 - val_mse: 0.0805 - learning_rate: 0.0010 - val_custom_mse: 0.4289 - val_custom_mae: 0.3064\n",
            "Epoch 21/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0838 - mae: 0.0666 - mse: 0.0838 - val_loss: 0.0809 - val_mae: 0.0621 - val_mse: 0.0809 - learning_rate: 0.0010 - val_custom_mse: 0.4310 - val_custom_mae: 0.3074\n",
            "Epoch 22/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0838 - mae: 0.0665 - mse: 0.0838 - val_loss: 0.0807 - val_mae: 0.0620 - val_mse: 0.0807 - learning_rate: 0.0010 - val_custom_mse: 0.4300 - val_custom_mae: 0.3069\n",
            "Epoch 23/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0838 - mae: 0.0665 - mse: 0.0838 - val_loss: 0.0808 - val_mae: 0.0622 - val_mse: 0.0808 - learning_rate: 0.0010 - val_custom_mse: 0.4301 - val_custom_mae: 0.3074\n",
            "Epoch 24/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0837 - mae: 0.0664 - mse: 0.0837 - val_loss: 0.0806 - val_mae: 0.0618 - val_mse: 0.0806 - learning_rate: 0.0010 - val_custom_mse: 0.4290 - val_custom_mae: 0.3060\n",
            "Epoch 25/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0837 - mae: 0.0664 - mse: 0.0837 - val_loss: 0.0807 - val_mae: 0.0619 - val_mse: 0.0807 - learning_rate: 0.0010 - val_custom_mse: 0.4297 - val_custom_mae: 0.3062\n",
            "Epoch 26/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0837 - mae: 0.0663 - mse: 0.0837 - val_loss: 0.0809 - val_mae: 0.0621 - val_mse: 0.0809 - learning_rate: 0.0010 - val_custom_mse: 0.4308 - val_custom_mae: 0.3075\n",
            "Epoch 27/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0837 - mae: 0.0663 - mse: 0.0837 - val_loss: 0.0806 - val_mae: 0.0617 - val_mse: 0.0806 - learning_rate: 0.0010 - val_custom_mse: 0.4293 - val_custom_mae: 0.3059\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0837 - mae: 0.0662 - mse: 0.0837 - val_loss: 0.0806 - val_mae: 0.0619 - val_mse: 0.0806 - learning_rate: 0.0010 - val_custom_mse: 0.4292 - val_custom_mae: 0.3065\n",
            "Epoch 29/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0834 - mae: 0.0651 - mse: 0.0834 - val_loss: 0.0804 - val_mae: 0.0606 - val_mse: 0.0804 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4284 - val_custom_mae: 0.3046\n",
            "Epoch 30/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0833 - mae: 0.0650 - mse: 0.0833 - val_loss: 0.0804 - val_mae: 0.0605 - val_mse: 0.0804 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4284 - val_custom_mae: 0.3045\n",
            "Epoch 31/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0833 - mae: 0.0649 - mse: 0.0833 - val_loss: 0.0804 - val_mae: 0.0605 - val_mse: 0.0804 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4283 - val_custom_mae: 0.3046\n",
            "Epoch 32/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0833 - mae: 0.0649 - mse: 0.0833 - val_loss: 0.0804 - val_mae: 0.0605 - val_mse: 0.0804 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4282 - val_custom_mae: 0.3044\n",
            "Epoch 33/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0833 - mae: 0.0650 - mse: 0.0833 - val_loss: 0.0804 - val_mae: 0.0606 - val_mse: 0.0804 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4284 - val_custom_mae: 0.3046\n",
            "Epoch 34/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0833 - mae: 0.0650 - mse: 0.0833 - val_loss: 0.0804 - val_mae: 0.0606 - val_mse: 0.0804 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4286 - val_custom_mae: 0.3048\n",
            "Epoch 35/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0833 - mae: 0.0650 - mse: 0.0833 - val_loss: 0.0804 - val_mae: 0.0606 - val_mse: 0.0804 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4283 - val_custom_mae: 0.3046\n",
            "Epoch 36/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0833 - mae: 0.0650 - mse: 0.0833 - val_loss: 0.0803 - val_mae: 0.0605 - val_mse: 0.0803 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4280 - val_custom_mae: 0.3043\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0833 - mae: 0.0650 - mse: 0.0833 - val_loss: 0.0804 - val_mae: 0.0606 - val_mse: 0.0804 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4283 - val_custom_mae: 0.3046\n",
            "Epoch 38/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0832 - mae: 0.0648 - mse: 0.0832 - val_loss: 0.0804 - val_mae: 0.0604 - val_mse: 0.0804 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4282 - val_custom_mae: 0.3045\n",
            "Epoch 39/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0832 - mae: 0.0647 - mse: 0.0832 - val_loss: 0.0804 - val_mae: 0.0604 - val_mse: 0.0804 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4282 - val_custom_mae: 0.3044\n",
            "Epoch 40/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0832 - mae: 0.0647 - mse: 0.0832 - val_loss: 0.0804 - val_mae: 0.0604 - val_mse: 0.0804 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4282 - val_custom_mae: 0.3045\n",
            "Epoch 41/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0832 - mae: 0.0647 - mse: 0.0832 - val_loss: 0.0804 - val_mae: 0.0604 - val_mse: 0.0804 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4282 - val_custom_mae: 0.3045\n",
            "Epoch 42/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0832 - mae: 0.0647 - mse: 0.0832 - val_loss: 0.0804 - val_mae: 0.0604 - val_mse: 0.0804 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4282 - val_custom_mae: 0.3045\n",
            "Epoch 43/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0832 - mae: 0.0647 - mse: 0.0832 - val_loss: 0.0804 - val_mae: 0.0604 - val_mse: 0.0804 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4282 - val_custom_mae: 0.3045\n",
            "Epoch 44/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0832 - mae: 0.0647 - mse: 0.0832 - val_loss: 0.0804 - val_mae: 0.0604 - val_mse: 0.0804 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4282 - val_custom_mae: 0.3045\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0832 - mae: 0.0647 - mse: 0.0832 - val_loss: 0.0804 - val_mae: 0.0604 - val_mse: 0.0804 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4282 - val_custom_mae: 0.3045\n",
            "Epoch 46/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0832 - mae: 0.0647 - mse: 0.0832 - val_loss: 0.0803 - val_mae: 0.0604 - val_mse: 0.0803 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4281 - val_custom_mae: 0.3045\n",
            "Epoch 47/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0604 - val_mse: 0.0803 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4281 - val_custom_mae: 0.3045\n",
            "Epoch 48/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0604 - val_mse: 0.0803 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4281 - val_custom_mae: 0.3045\n",
            "Epoch 49/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0604 - val_mse: 0.0803 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4281 - val_custom_mae: 0.3044\n",
            "Epoch 50/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0604 - val_mse: 0.0803 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4281 - val_custom_mae: 0.3045\n",
            "Epoch 51/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0604 - val_mse: 0.0803 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4281 - val_custom_mae: 0.3045\n",
            "Epoch 52/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0604 - val_mse: 0.0803 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4281 - val_custom_mae: 0.3044\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 53: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0604 - val_mse: 0.0803 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4281 - val_custom_mae: 0.3044\n",
            "Epoch 54/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4281 - val_custom_mae: 0.3044\n",
            "Epoch 55/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 56/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 57/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 58/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 59/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 60/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 62/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 63/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 64/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 65/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 66/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 67/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 68/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 70/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 71/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 72/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 73/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 74/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 75/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 76/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 78/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 79/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 80/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 81/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 82/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 83/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 84/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 86/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 87/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 88/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 89/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 90/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 91/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 92/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 94/100\n",
            "1115/1115 - 7s - 6ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 95/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 96/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 97/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 98/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 99/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Epoch 100/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.0831 - mae: 0.0647 - mse: 0.0831 - val_loss: 0.0803 - val_mae: 0.0603 - val_mse: 0.0803 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4280 - val_custom_mae: 0.3044\n",
            "Running experiment: horizon=192, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_35', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1115/1115 - 12s - 11ms/step - loss: 0.4159 - mae: 0.2758 - mse: 0.4159 - val_loss: 0.1293 - val_mae: 0.1195 - val_mse: 0.1293 - learning_rate: 0.0010 - val_custom_mse: 0.4577 - val_custom_mae: 0.3254\n",
            "Epoch 2/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1683 - mae: 0.1424 - mse: 0.1683 - val_loss: 0.1056 - val_mae: 0.0976 - val_mse: 0.1056 - learning_rate: 0.0010 - val_custom_mse: 0.4383 - val_custom_mae: 0.3158\n",
            "Epoch 3/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1491 - mae: 0.1286 - mse: 0.1491 - val_loss: 0.0990 - val_mae: 0.0923 - val_mse: 0.0990 - learning_rate: 0.0010 - val_custom_mse: 0.4366 - val_custom_mae: 0.3149\n",
            "Epoch 4/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1393 - mae: 0.1246 - mse: 0.1393 - val_loss: 0.0947 - val_mae: 0.0891 - val_mse: 0.0947 - learning_rate: 0.0010 - val_custom_mse: 0.4370 - val_custom_mae: 0.3144\n",
            "Epoch 5/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1332 - mae: 0.1225 - mse: 0.1332 - val_loss: 0.0914 - val_mae: 0.0872 - val_mse: 0.0914 - learning_rate: 0.0010 - val_custom_mse: 0.4350 - val_custom_mae: 0.3136\n",
            "Epoch 6/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1290 - mae: 0.1213 - mse: 0.1290 - val_loss: 0.0896 - val_mae: 0.0860 - val_mse: 0.0896 - learning_rate: 0.0010 - val_custom_mse: 0.4352 - val_custom_mae: 0.3133\n",
            "Epoch 7/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1263 - mae: 0.1206 - mse: 0.1263 - val_loss: 0.0882 - val_mae: 0.0854 - val_mse: 0.0882 - learning_rate: 0.0010 - val_custom_mse: 0.4337 - val_custom_mae: 0.3125\n",
            "Epoch 8/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1248 - mae: 0.1201 - mse: 0.1248 - val_loss: 0.0876 - val_mae: 0.0854 - val_mse: 0.0876 - learning_rate: 0.0010 - val_custom_mse: 0.4334 - val_custom_mae: 0.3123\n",
            "Epoch 9/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1237 - mae: 0.1198 - mse: 0.1237 - val_loss: 0.0880 - val_mae: 0.0875 - val_mse: 0.0880 - learning_rate: 0.0010 - val_custom_mse: 0.4334 - val_custom_mae: 0.3121\n",
            "Epoch 10/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1220 - mae: 0.1209 - mse: 0.1220 - val_loss: 0.0871 - val_mae: 0.0876 - val_mse: 0.0871 - learning_rate: 0.0010 - val_custom_mse: 0.4324 - val_custom_mae: 0.3118\n",
            "Epoch 11/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1214 - mae: 0.1211 - mse: 0.1214 - val_loss: 0.0867 - val_mae: 0.0870 - val_mse: 0.0867 - learning_rate: 0.0010 - val_custom_mse: 0.4320 - val_custom_mae: 0.3114\n",
            "Epoch 12/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1206 - mae: 0.1212 - mse: 0.1206 - val_loss: 0.0865 - val_mae: 0.0870 - val_mse: 0.0865 - learning_rate: 0.0010 - val_custom_mse: 0.4316 - val_custom_mae: 0.3105\n",
            "Epoch 13/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1201 - mae: 0.1213 - mse: 0.1201 - val_loss: 0.0863 - val_mae: 0.0870 - val_mse: 0.0863 - learning_rate: 0.0010 - val_custom_mse: 0.4308 - val_custom_mae: 0.3105\n",
            "Epoch 14/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1204 - mae: 0.1213 - mse: 0.1204 - val_loss: 0.0862 - val_mae: 0.0870 - val_mse: 0.0862 - learning_rate: 0.0010 - val_custom_mse: 0.4307 - val_custom_mae: 0.3102\n",
            "Epoch 15/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1199 - mae: 0.1213 - mse: 0.1199 - val_loss: 0.0861 - val_mae: 0.0869 - val_mse: 0.0861 - learning_rate: 0.0010 - val_custom_mse: 0.4302 - val_custom_mae: 0.3094\n",
            "Epoch 16/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1198 - mae: 0.1212 - mse: 0.1198 - val_loss: 0.0860 - val_mae: 0.0870 - val_mse: 0.0860 - learning_rate: 0.0010 - val_custom_mse: 0.4300 - val_custom_mae: 0.3095\n",
            "Epoch 17/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1186 - mae: 0.1212 - mse: 0.1186 - val_loss: 0.0860 - val_mae: 0.0869 - val_mse: 0.0860 - learning_rate: 0.0010 - val_custom_mse: 0.4299 - val_custom_mae: 0.3096\n",
            "Epoch 18/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1197 - mae: 0.1212 - mse: 0.1197 - val_loss: 0.0860 - val_mae: 0.0870 - val_mse: 0.0860 - learning_rate: 0.0010 - val_custom_mse: 0.4298 - val_custom_mae: 0.3095\n",
            "Epoch 19/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1186 - mae: 0.1211 - mse: 0.1186 - val_loss: 0.0859 - val_mae: 0.0867 - val_mse: 0.0859 - learning_rate: 0.0010 - val_custom_mse: 0.4296 - val_custom_mae: 0.3091\n",
            "Epoch 20/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1196 - mae: 0.1211 - mse: 0.1196 - val_loss: 0.0859 - val_mae: 0.0869 - val_mse: 0.0859 - learning_rate: 0.0010 - val_custom_mse: 0.4296 - val_custom_mae: 0.3094\n",
            "Epoch 21/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1187 - mae: 0.1210 - mse: 0.1187 - val_loss: 0.0859 - val_mae: 0.0868 - val_mse: 0.0859 - learning_rate: 0.0010 - val_custom_mse: 0.4301 - val_custom_mae: 0.3099\n",
            "Epoch 22/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1185 - mae: 0.1210 - mse: 0.1185 - val_loss: 0.0856 - val_mae: 0.0865 - val_mse: 0.0856 - learning_rate: 0.0010 - val_custom_mse: 0.4286 - val_custom_mae: 0.3081\n",
            "Epoch 23/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1191 - mae: 0.1210 - mse: 0.1191 - val_loss: 0.0856 - val_mae: 0.0866 - val_mse: 0.0856 - learning_rate: 0.0010 - val_custom_mse: 0.4289 - val_custom_mae: 0.3089\n",
            "Epoch 24/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1182 - mae: 0.1209 - mse: 0.1182 - val_loss: 0.0855 - val_mae: 0.0866 - val_mse: 0.0855 - learning_rate: 0.0010 - val_custom_mse: 0.4286 - val_custom_mae: 0.3088\n",
            "Epoch 25/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1181 - mae: 0.1209 - mse: 0.1181 - val_loss: 0.0855 - val_mae: 0.0863 - val_mse: 0.0855 - learning_rate: 0.0010 - val_custom_mse: 0.4286 - val_custom_mae: 0.3079\n",
            "Epoch 26/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1186 - mae: 0.1209 - mse: 0.1186 - val_loss: 0.0856 - val_mae: 0.0866 - val_mse: 0.0856 - learning_rate: 0.0010 - val_custom_mse: 0.4290 - val_custom_mae: 0.3086\n",
            "Epoch 27/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1188 - mae: 0.1209 - mse: 0.1188 - val_loss: 0.0856 - val_mae: 0.0865 - val_mse: 0.0856 - learning_rate: 0.0010 - val_custom_mse: 0.4289 - val_custom_mae: 0.3090\n",
            "Epoch 28/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1177 - mae: 0.1208 - mse: 0.1177 - val_loss: 0.0857 - val_mae: 0.0866 - val_mse: 0.0857 - learning_rate: 0.0010 - val_custom_mse: 0.4297 - val_custom_mae: 0.3095\n",
            "Epoch 29/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1178 - mae: 0.1208 - mse: 0.1178 - val_loss: 0.0855 - val_mae: 0.0868 - val_mse: 0.0855 - learning_rate: 0.0010 - val_custom_mse: 0.4288 - val_custom_mae: 0.3098\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1171 - mae: 0.1208 - mse: 0.1171 - val_loss: 0.0856 - val_mae: 0.0865 - val_mse: 0.0856 - learning_rate: 0.0010 - val_custom_mse: 0.4294 - val_custom_mae: 0.3091\n",
            "Epoch 31/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1166 - mae: 0.1204 - mse: 0.1166 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4295 - val_custom_mae: 0.3090\n",
            "Epoch 32/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1172 - mae: 0.1203 - mse: 0.1172 - val_loss: 0.0855 - val_mae: 0.0862 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4296 - val_custom_mae: 0.3092\n",
            "Epoch 33/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1167 - mae: 0.1203 - mse: 0.1167 - val_loss: 0.0856 - val_mae: 0.0862 - val_mse: 0.0856 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4297 - val_custom_mae: 0.3091\n",
            "Epoch 34/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1181 - mae: 0.1203 - mse: 0.1181 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4293 - val_custom_mae: 0.3087\n",
            "Epoch 35/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1172 - mae: 0.1203 - mse: 0.1172 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4293 - val_custom_mae: 0.3089\n",
            "Epoch 36/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1169 - mae: 0.1203 - mse: 0.1169 - val_loss: 0.0855 - val_mae: 0.0862 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4293 - val_custom_mae: 0.3090\n",
            "Epoch 37/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1172 - mae: 0.1203 - mse: 0.1172 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4296 - val_custom_mae: 0.3091\n",
            "Epoch 38/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1167 - mae: 0.1203 - mse: 0.1167 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4293 - val_custom_mae: 0.3089\n",
            "Epoch 39/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1163 - mae: 0.1203 - mse: 0.1163 - val_loss: 0.0855 - val_mae: 0.0862 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4294 - val_custom_mae: 0.3089\n",
            "Epoch 40/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1168 - mae: 0.1203 - mse: 0.1168 - val_loss: 0.0855 - val_mae: 0.0862 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4294 - val_custom_mae: 0.3091\n",
            "Epoch 41/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1167 - mae: 0.1203 - mse: 0.1167 - val_loss: 0.0855 - val_mae: 0.0862 - val_mse: 0.0855 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4293 - val_custom_mae: 0.3088\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1174 - mae: 0.1204 - mse: 0.1174 - val_loss: 0.0854 - val_mae: 0.0862 - val_mse: 0.0854 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4292 - val_custom_mae: 0.3091\n",
            "Epoch 43/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1171 - mae: 0.1202 - mse: 0.1171 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4298 - val_custom_mae: 0.3090\n",
            "Epoch 44/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1165 - mae: 0.1202 - mse: 0.1165 - val_loss: 0.0856 - val_mae: 0.0861 - val_mse: 0.0856 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4298 - val_custom_mae: 0.3091\n",
            "Epoch 45/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1166 - mae: 0.1202 - mse: 0.1166 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4297 - val_custom_mae: 0.3089\n",
            "Epoch 46/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1167 - mae: 0.1202 - mse: 0.1167 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4297 - val_custom_mae: 0.3089\n",
            "Epoch 47/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1171 - mae: 0.1202 - mse: 0.1171 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4297 - val_custom_mae: 0.3090\n",
            "Epoch 48/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1165 - mae: 0.1202 - mse: 0.1165 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4297 - val_custom_mae: 0.3089\n",
            "Epoch 49/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1166 - mae: 0.1202 - mse: 0.1166 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4297 - val_custom_mae: 0.3089\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 50: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1172 - mae: 0.1202 - mse: 0.1172 - val_loss: 0.0855 - val_mae: 0.0861 - val_mse: 0.0855 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4297 - val_custom_mae: 0.3089\n",
            "Epoch 51/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1169 - mae: 0.1202 - mse: 0.1169 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 52/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1167 - mae: 0.1202 - mse: 0.1167 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 53/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1169 - mae: 0.1202 - mse: 0.1169 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 54/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1172 - mae: 0.1202 - mse: 0.1172 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 55/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1168 - mae: 0.1202 - mse: 0.1168 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 56/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1161 - mae: 0.1202 - mse: 0.1161 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 57/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1162 - mae: 0.1202 - mse: 0.1162 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1168 - mae: 0.1202 - mse: 0.1168 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 59/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1168 - mae: 0.1202 - mse: 0.1168 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 60/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1161 - mae: 0.1202 - mse: 0.1161 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 61/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1168 - mae: 0.1202 - mse: 0.1168 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 62/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1172 - mae: 0.1202 - mse: 0.1172 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 63/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1166 - mae: 0.1202 - mse: 0.1166 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 64/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1170 - mae: 0.1202 - mse: 0.1170 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3087\n",
            "Epoch 65/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1173 - mae: 0.1202 - mse: 0.1173 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 66: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1170 - mae: 0.1202 - mse: 0.1170 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 67/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1167 - mae: 0.1202 - mse: 0.1167 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 68/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1172 - mae: 0.1202 - mse: 0.1172 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 69/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1166 - mae: 0.1202 - mse: 0.1166 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 70/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1167 - mae: 0.1202 - mse: 0.1167 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 71/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1169 - mae: 0.1202 - mse: 0.1169 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 72/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1171 - mae: 0.1202 - mse: 0.1171 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 73/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1169 - mae: 0.1202 - mse: 0.1169 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 74: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1170 - mae: 0.1202 - mse: 0.1170 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 75/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1170 - mae: 0.1202 - mse: 0.1170 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 76/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1165 - mae: 0.1202 - mse: 0.1165 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 77/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1165 - mae: 0.1202 - mse: 0.1165 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 78/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1159 - mae: 0.1202 - mse: 0.1159 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 79/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1173 - mae: 0.1202 - mse: 0.1173 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 80/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1171 - mae: 0.1202 - mse: 0.1171 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 81/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1169 - mae: 0.1202 - mse: 0.1169 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1162 - mae: 0.1202 - mse: 0.1162 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 83/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1170 - mae: 0.1202 - mse: 0.1170 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 84/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1162 - mae: 0.1202 - mse: 0.1162 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 85/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1170 - mae: 0.1202 - mse: 0.1170 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 86/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1173 - mae: 0.1202 - mse: 0.1173 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 87/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1169 - mae: 0.1202 - mse: 0.1169 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 88/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1166 - mae: 0.1202 - mse: 0.1166 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 89/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1165 - mae: 0.1202 - mse: 0.1165 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 90: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1169 - mae: 0.1202 - mse: 0.1169 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 91/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1170 - mae: 0.1202 - mse: 0.1170 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 92/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1164 - mae: 0.1202 - mse: 0.1164 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 93/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1166 - mae: 0.1202 - mse: 0.1166 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 94/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1167 - mae: 0.1202 - mse: 0.1167 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 95/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1168 - mae: 0.1202 - mse: 0.1168 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 96/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1168 - mae: 0.1202 - mse: 0.1168 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 97/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1169 - mae: 0.1202 - mse: 0.1169 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 98: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1164 - mae: 0.1202 - mse: 0.1164 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 99/100\n",
            "1115/1115 - 7s - 7ms/step - loss: 0.1166 - mae: 0.1202 - mse: 0.1166 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Epoch 100/100\n",
            "1115/1115 - 8s - 7ms/step - loss: 0.1158 - mae: 0.1202 - mse: 0.1158 - val_loss: 0.0855 - val_mae: 0.0860 - val_mse: 0.0855 - learning_rate: 5.1200e-10 - val_custom_mse: 0.4296 - val_custom_mae: 0.3088\n",
            "Running experiment: horizon=336, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_36', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1111/1111 - 12s - 11ms/step - loss: 0.4442 - mae: 0.2892 - mse: 0.4442 - val_loss: 0.2047 - val_mae: 0.1716 - val_mse: 0.2047 - learning_rate: 0.0010 - val_custom_mse: 0.5187 - val_custom_mae: 0.3662\n",
            "Epoch 2/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.2232 - mae: 0.1680 - mse: 0.2232 - val_loss: 0.1804 - val_mae: 0.1492 - val_mse: 0.1804 - learning_rate: 0.0010 - val_custom_mse: 0.5028 - val_custom_mae: 0.3589\n",
            "Epoch 3/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.2020 - mae: 0.1543 - mse: 0.2020 - val_loss: 0.1735 - val_mae: 0.1418 - val_mse: 0.1735 - learning_rate: 0.0010 - val_custom_mse: 0.5006 - val_custom_mae: 0.3575\n",
            "Epoch 4/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1891 - mae: 0.1465 - mse: 0.1891 - val_loss: 0.1685 - val_mae: 0.1355 - val_mse: 0.1685 - learning_rate: 0.0010 - val_custom_mse: 0.4980 - val_custom_mae: 0.3551\n",
            "Epoch 5/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1797 - mae: 0.1403 - mse: 0.1797 - val_loss: 0.1661 - val_mae: 0.1308 - val_mse: 0.1661 - learning_rate: 0.0010 - val_custom_mse: 0.4985 - val_custom_mae: 0.3550\n",
            "Epoch 6/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1735 - mae: 0.1355 - mse: 0.1735 - val_loss: 0.1642 - val_mae: 0.1271 - val_mse: 0.1642 - learning_rate: 0.0010 - val_custom_mse: 0.4967 - val_custom_mae: 0.3534\n",
            "Epoch 7/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1697 - mae: 0.1321 - mse: 0.1697 - val_loss: 0.1631 - val_mae: 0.1244 - val_mse: 0.1631 - learning_rate: 0.0010 - val_custom_mse: 0.4949 - val_custom_mae: 0.3518\n",
            "Epoch 8/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1674 - mae: 0.1297 - mse: 0.1674 - val_loss: 0.1631 - val_mae: 0.1233 - val_mse: 0.1631 - learning_rate: 0.0010 - val_custom_mse: 0.4960 - val_custom_mae: 0.3527\n",
            "Epoch 9/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1658 - mae: 0.1282 - mse: 0.1658 - val_loss: 0.1630 - val_mae: 0.1224 - val_mse: 0.1630 - learning_rate: 0.0010 - val_custom_mse: 0.4960 - val_custom_mae: 0.3527\n",
            "Epoch 10/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1647 - mae: 0.1271 - mse: 0.1647 - val_loss: 0.1624 - val_mae: 0.1212 - val_mse: 0.1624 - learning_rate: 0.0010 - val_custom_mse: 0.4943 - val_custom_mae: 0.3511\n",
            "Epoch 11/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1640 - mae: 0.1264 - mse: 0.1640 - val_loss: 0.1621 - val_mae: 0.1206 - val_mse: 0.1621 - learning_rate: 0.0010 - val_custom_mse: 0.4936 - val_custom_mae: 0.3506\n",
            "Epoch 12/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1635 - mae: 0.1260 - mse: 0.1635 - val_loss: 0.1622 - val_mae: 0.1206 - val_mse: 0.1622 - learning_rate: 0.0010 - val_custom_mse: 0.4939 - val_custom_mae: 0.3506\n",
            "Epoch 13/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1632 - mae: 0.1257 - mse: 0.1632 - val_loss: 0.1620 - val_mae: 0.1202 - val_mse: 0.1620 - learning_rate: 0.0010 - val_custom_mse: 0.4932 - val_custom_mae: 0.3503\n",
            "Epoch 14/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1630 - mae: 0.1254 - mse: 0.1630 - val_loss: 0.1623 - val_mae: 0.1201 - val_mse: 0.1623 - learning_rate: 0.0010 - val_custom_mse: 0.4940 - val_custom_mae: 0.3504\n",
            "Epoch 15/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1628 - mae: 0.1256 - mse: 0.1628 - val_loss: 0.1620 - val_mae: 0.1213 - val_mse: 0.1620 - learning_rate: 0.0010 - val_custom_mse: 0.4930 - val_custom_mae: 0.3500\n",
            "Epoch 16/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1626 - mae: 0.1268 - mse: 0.1626 - val_loss: 0.1618 - val_mae: 0.1219 - val_mse: 0.1618 - learning_rate: 0.0010 - val_custom_mse: 0.4924 - val_custom_mae: 0.3497\n",
            "Epoch 17/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1626 - mae: 0.1272 - mse: 0.1626 - val_loss: 0.1617 - val_mae: 0.1222 - val_mse: 0.1617 - learning_rate: 0.0010 - val_custom_mse: 0.4919 - val_custom_mae: 0.3499\n",
            "Epoch 18/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1625 - mae: 0.1272 - mse: 0.1625 - val_loss: 0.1621 - val_mae: 0.1226 - val_mse: 0.1621 - learning_rate: 0.0010 - val_custom_mse: 0.4932 - val_custom_mae: 0.3507\n",
            "Epoch 19/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1624 - mae: 0.1273 - mse: 0.1624 - val_loss: 0.1616 - val_mae: 0.1223 - val_mse: 0.1616 - learning_rate: 0.0010 - val_custom_mse: 0.4918 - val_custom_mae: 0.3492\n",
            "Epoch 20/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1624 - mae: 0.1273 - mse: 0.1624 - val_loss: 0.1616 - val_mae: 0.1222 - val_mse: 0.1616 - learning_rate: 0.0010 - val_custom_mse: 0.4917 - val_custom_mae: 0.3491\n",
            "Epoch 21/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1623 - mae: 0.1274 - mse: 0.1623 - val_loss: 0.1618 - val_mae: 0.1224 - val_mse: 0.1618 - learning_rate: 0.0010 - val_custom_mse: 0.4923 - val_custom_mae: 0.3494\n",
            "Epoch 22/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1623 - mae: 0.1273 - mse: 0.1623 - val_loss: 0.1619 - val_mae: 0.1225 - val_mse: 0.1619 - learning_rate: 0.0010 - val_custom_mse: 0.4926 - val_custom_mae: 0.3499\n",
            "Epoch 23/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1622 - mae: 0.1274 - mse: 0.1622 - val_loss: 0.1619 - val_mae: 0.1223 - val_mse: 0.1619 - learning_rate: 0.0010 - val_custom_mse: 0.4927 - val_custom_mae: 0.3495\n",
            "Epoch 24/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1622 - mae: 0.1273 - mse: 0.1622 - val_loss: 0.1616 - val_mae: 0.1222 - val_mse: 0.1616 - learning_rate: 0.0010 - val_custom_mse: 0.4919 - val_custom_mae: 0.3491\n",
            "Epoch 25/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1622 - mae: 0.1273 - mse: 0.1622 - val_loss: 0.1613 - val_mae: 0.1219 - val_mse: 0.1613 - learning_rate: 0.0010 - val_custom_mse: 0.4909 - val_custom_mae: 0.3486\n",
            "Epoch 26/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1621 - mae: 0.1272 - mse: 0.1621 - val_loss: 0.1613 - val_mae: 0.1220 - val_mse: 0.1613 - learning_rate: 0.0010 - val_custom_mse: 0.4909 - val_custom_mae: 0.3485\n",
            "Epoch 27/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1621 - mae: 0.1272 - mse: 0.1621 - val_loss: 0.1613 - val_mae: 0.1219 - val_mse: 0.1613 - learning_rate: 0.0010 - val_custom_mse: 0.4909 - val_custom_mae: 0.3485\n",
            "Epoch 28/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1621 - mae: 0.1271 - mse: 0.1621 - val_loss: 0.1614 - val_mae: 0.1221 - val_mse: 0.1614 - learning_rate: 0.0010 - val_custom_mse: 0.4912 - val_custom_mae: 0.3486\n",
            "Epoch 29/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1621 - mae: 0.1271 - mse: 0.1621 - val_loss: 0.1615 - val_mae: 0.1220 - val_mse: 0.1615 - learning_rate: 0.0010 - val_custom_mse: 0.4914 - val_custom_mae: 0.3487\n",
            "Epoch 30/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1621 - mae: 0.1271 - mse: 0.1621 - val_loss: 0.1616 - val_mae: 0.1221 - val_mse: 0.1616 - learning_rate: 0.0010 - val_custom_mse: 0.4917 - val_custom_mae: 0.3489\n",
            "Epoch 31/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1620 - mae: 0.1270 - mse: 0.1620 - val_loss: 0.1613 - val_mae: 0.1218 - val_mse: 0.1613 - learning_rate: 0.0010 - val_custom_mse: 0.4909 - val_custom_mae: 0.3484\n",
            "Epoch 32/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1620 - mae: 0.1270 - mse: 0.1620 - val_loss: 0.1614 - val_mae: 0.1220 - val_mse: 0.1614 - learning_rate: 0.0010 - val_custom_mse: 0.4911 - val_custom_mae: 0.3487\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1620 - mae: 0.1270 - mse: 0.1620 - val_loss: 0.1613 - val_mae: 0.1215 - val_mse: 0.1613 - learning_rate: 0.0010 - val_custom_mse: 0.4909 - val_custom_mae: 0.3482\n",
            "Epoch 34/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1615 - mae: 0.1259 - mse: 0.1615 - val_loss: 0.1609 - val_mae: 0.1205 - val_mse: 0.1609 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4898 - val_custom_mae: 0.3472\n",
            "Epoch 35/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1614 - mae: 0.1257 - mse: 0.1614 - val_loss: 0.1609 - val_mae: 0.1205 - val_mse: 0.1609 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4898 - val_custom_mae: 0.3472\n",
            "Epoch 36/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1614 - mae: 0.1257 - mse: 0.1614 - val_loss: 0.1610 - val_mae: 0.1206 - val_mse: 0.1610 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4901 - val_custom_mae: 0.3474\n",
            "Epoch 37/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1614 - mae: 0.1257 - mse: 0.1614 - val_loss: 0.1609 - val_mae: 0.1206 - val_mse: 0.1609 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4896 - val_custom_mae: 0.3472\n",
            "Epoch 38/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1614 - mae: 0.1258 - mse: 0.1614 - val_loss: 0.1608 - val_mae: 0.1206 - val_mse: 0.1608 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4895 - val_custom_mae: 0.3471\n",
            "Epoch 39/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1614 - mae: 0.1258 - mse: 0.1614 - val_loss: 0.1609 - val_mae: 0.1206 - val_mse: 0.1609 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4896 - val_custom_mae: 0.3471\n",
            "Epoch 40/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1614 - mae: 0.1258 - mse: 0.1614 - val_loss: 0.1609 - val_mae: 0.1206 - val_mse: 0.1609 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4897 - val_custom_mae: 0.3472\n",
            "Epoch 41/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1614 - mae: 0.1258 - mse: 0.1614 - val_loss: 0.1609 - val_mae: 0.1207 - val_mse: 0.1609 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4898 - val_custom_mae: 0.3473\n",
            "Epoch 42/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1614 - mae: 0.1258 - mse: 0.1614 - val_loss: 0.1609 - val_mae: 0.1206 - val_mse: 0.1609 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4897 - val_custom_mae: 0.3472\n",
            "Epoch 43/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1614 - mae: 0.1258 - mse: 0.1614 - val_loss: 0.1609 - val_mae: 0.1207 - val_mse: 0.1609 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4898 - val_custom_mae: 0.3473\n",
            "Epoch 44/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1613 - mae: 0.1258 - mse: 0.1613 - val_loss: 0.1609 - val_mae: 0.1207 - val_mse: 0.1609 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4899 - val_custom_mae: 0.3473\n",
            "Epoch 45/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1613 - mae: 0.1258 - mse: 0.1613 - val_loss: 0.1608 - val_mae: 0.1206 - val_mse: 0.1608 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4896 - val_custom_mae: 0.3471\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1613 - mae: 0.1258 - mse: 0.1613 - val_loss: 0.1608 - val_mae: 0.1206 - val_mse: 0.1608 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4896 - val_custom_mae: 0.3471\n",
            "Epoch 47/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1256 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 48/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 49/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 50/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 51/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 52/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4893 - val_custom_mae: 0.3469\n",
            "Epoch 53/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4893 - val_custom_mae: 0.3469\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 55/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 56/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 57/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 58/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 59/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 60/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 61/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 63/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 64/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4894 - val_custom_mae: 0.3470\n",
            "Epoch 65/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 66/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 67/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 68/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 69/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 71/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 72/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 73/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 74/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 75/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 76/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 77/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 79/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 80/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 81/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 82/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 83/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 84/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 85/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 87/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 88/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 89/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 90/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 91/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 92/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 93/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 95/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 96/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 97/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 98/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 99/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Epoch 100/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1612 - mae: 0.1255 - mse: 0.1612 - val_loss: 0.1608 - val_mae: 0.1204 - val_mse: 0.1608 - learning_rate: 2.5600e-09 - val_custom_mse: 0.4895 - val_custom_mae: 0.3470\n",
            "Running experiment: horizon=336, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_37', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1111/1111 - 12s - 11ms/step - loss: 0.4791 - mae: 0.3163 - mse: 0.4791 - val_loss: 0.2032 - val_mae: 0.1694 - val_mse: 0.2032 - learning_rate: 0.0010 - val_custom_mse: 0.5148 - val_custom_mae: 0.3646\n",
            "Epoch 2/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.2335 - mae: 0.1892 - mse: 0.2335 - val_loss: 0.1816 - val_mae: 0.1495 - val_mse: 0.1816 - learning_rate: 0.0010 - val_custom_mse: 0.5000 - val_custom_mae: 0.3568\n",
            "Epoch 3/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.2159 - mae: 0.1766 - mse: 0.2159 - val_loss: 0.1757 - val_mae: 0.1449 - val_mse: 0.1757 - learning_rate: 0.0010 - val_custom_mse: 0.4985 - val_custom_mae: 0.3560\n",
            "Epoch 4/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.2071 - mae: 0.1730 - mse: 0.2071 - val_loss: 0.1717 - val_mae: 0.1420 - val_mse: 0.1717 - learning_rate: 0.0010 - val_custom_mse: 0.4966 - val_custom_mae: 0.3542\n",
            "Epoch 5/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.2030 - mae: 0.1712 - mse: 0.2030 - val_loss: 0.1699 - val_mae: 0.1414 - val_mse: 0.1699 - learning_rate: 0.0010 - val_custom_mse: 0.4963 - val_custom_mae: 0.3543\n",
            "Epoch 6/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1999 - mae: 0.1706 - mse: 0.1999 - val_loss: 0.1688 - val_mae: 0.1415 - val_mse: 0.1688 - learning_rate: 0.0010 - val_custom_mse: 0.4953 - val_custom_mae: 0.3535\n",
            "Epoch 7/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1975 - mae: 0.1705 - mse: 0.1975 - val_loss: 0.1679 - val_mae: 0.1413 - val_mse: 0.1679 - learning_rate: 0.0010 - val_custom_mse: 0.4946 - val_custom_mae: 0.3530\n",
            "Epoch 8/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1962 - mae: 0.1705 - mse: 0.1962 - val_loss: 0.1677 - val_mae: 0.1415 - val_mse: 0.1677 - learning_rate: 0.0010 - val_custom_mse: 0.4950 - val_custom_mae: 0.3534\n",
            "Epoch 9/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1960 - mae: 0.1705 - mse: 0.1960 - val_loss: 0.1672 - val_mae: 0.1412 - val_mse: 0.1672 - learning_rate: 0.0010 - val_custom_mse: 0.4940 - val_custom_mae: 0.3524\n",
            "Epoch 10/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1948 - mae: 0.1705 - mse: 0.1948 - val_loss: 0.1668 - val_mae: 0.1409 - val_mse: 0.1668 - learning_rate: 0.0010 - val_custom_mse: 0.4928 - val_custom_mae: 0.3515\n",
            "Epoch 11/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1943 - mae: 0.1704 - mse: 0.1943 - val_loss: 0.1667 - val_mae: 0.1411 - val_mse: 0.1667 - learning_rate: 0.0010 - val_custom_mse: 0.4926 - val_custom_mae: 0.3516\n",
            "Epoch 12/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1934 - mae: 0.1703 - mse: 0.1934 - val_loss: 0.1665 - val_mae: 0.1410 - val_mse: 0.1665 - learning_rate: 0.0010 - val_custom_mse: 0.4923 - val_custom_mae: 0.3511\n",
            "Epoch 13/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1932 - mae: 0.1702 - mse: 0.1932 - val_loss: 0.1666 - val_mae: 0.1410 - val_mse: 0.1666 - learning_rate: 0.0010 - val_custom_mse: 0.4928 - val_custom_mae: 0.3517\n",
            "Epoch 14/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1931 - mae: 0.1701 - mse: 0.1931 - val_loss: 0.1664 - val_mae: 0.1408 - val_mse: 0.1664 - learning_rate: 0.0010 - val_custom_mse: 0.4921 - val_custom_mae: 0.3507\n",
            "Epoch 15/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1929 - mae: 0.1701 - mse: 0.1929 - val_loss: 0.1661 - val_mae: 0.1408 - val_mse: 0.1661 - learning_rate: 0.0010 - val_custom_mse: 0.4915 - val_custom_mae: 0.3508\n",
            "Epoch 16/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1920 - mae: 0.1700 - mse: 0.1920 - val_loss: 0.1662 - val_mae: 0.1407 - val_mse: 0.1662 - learning_rate: 0.0010 - val_custom_mse: 0.4917 - val_custom_mae: 0.3506\n",
            "Epoch 17/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1920 - mae: 0.1699 - mse: 0.1920 - val_loss: 0.1662 - val_mae: 0.1410 - val_mse: 0.1662 - learning_rate: 0.0010 - val_custom_mse: 0.4917 - val_custom_mae: 0.3510\n",
            "Epoch 18/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1921 - mae: 0.1700 - mse: 0.1921 - val_loss: 0.1662 - val_mae: 0.1411 - val_mse: 0.1662 - learning_rate: 0.0010 - val_custom_mse: 0.4918 - val_custom_mae: 0.3514\n",
            "Epoch 19/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1924 - mae: 0.1700 - mse: 0.1924 - val_loss: 0.1660 - val_mae: 0.1411 - val_mse: 0.1660 - learning_rate: 0.0010 - val_custom_mse: 0.4914 - val_custom_mae: 0.3510\n",
            "Epoch 20/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1916 - mae: 0.1700 - mse: 0.1916 - val_loss: 0.1659 - val_mae: 0.1411 - val_mse: 0.1659 - learning_rate: 0.0010 - val_custom_mse: 0.4913 - val_custom_mae: 0.3512\n",
            "Epoch 21/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1908 - mae: 0.1699 - mse: 0.1908 - val_loss: 0.1660 - val_mae: 0.1409 - val_mse: 0.1660 - learning_rate: 0.0010 - val_custom_mse: 0.4918 - val_custom_mae: 0.3513\n",
            "Epoch 22/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1912 - mae: 0.1700 - mse: 0.1912 - val_loss: 0.1658 - val_mae: 0.1408 - val_mse: 0.1658 - learning_rate: 0.0010 - val_custom_mse: 0.4909 - val_custom_mae: 0.3505\n",
            "Epoch 23/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1910 - mae: 0.1699 - mse: 0.1910 - val_loss: 0.1659 - val_mae: 0.1408 - val_mse: 0.1659 - learning_rate: 0.0010 - val_custom_mse: 0.4916 - val_custom_mae: 0.3508\n",
            "Epoch 24/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1921 - mae: 0.1700 - mse: 0.1921 - val_loss: 0.1660 - val_mae: 0.1409 - val_mse: 0.1660 - learning_rate: 0.0010 - val_custom_mse: 0.4917 - val_custom_mae: 0.3512\n",
            "Epoch 25/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1914 - mae: 0.1699 - mse: 0.1914 - val_loss: 0.1658 - val_mae: 0.1409 - val_mse: 0.1658 - learning_rate: 0.0010 - val_custom_mse: 0.4911 - val_custom_mae: 0.3507\n",
            "Epoch 26/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1918 - mae: 0.1699 - mse: 0.1918 - val_loss: 0.1657 - val_mae: 0.1407 - val_mse: 0.1657 - learning_rate: 0.0010 - val_custom_mse: 0.4909 - val_custom_mae: 0.3505\n",
            "Epoch 27/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1914 - mae: 0.1699 - mse: 0.1914 - val_loss: 0.1655 - val_mae: 0.1405 - val_mse: 0.1655 - learning_rate: 0.0010 - val_custom_mse: 0.4905 - val_custom_mae: 0.3502\n",
            "Epoch 28/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1914 - mae: 0.1699 - mse: 0.1914 - val_loss: 0.1654 - val_mae: 0.1406 - val_mse: 0.1654 - learning_rate: 0.0010 - val_custom_mse: 0.4901 - val_custom_mae: 0.3501\n",
            "Epoch 29/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1911 - mae: 0.1699 - mse: 0.1911 - val_loss: 0.1655 - val_mae: 0.1403 - val_mse: 0.1655 - learning_rate: 0.0010 - val_custom_mse: 0.4908 - val_custom_mae: 0.3500\n",
            "Epoch 30/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1915 - mae: 0.1699 - mse: 0.1915 - val_loss: 0.1654 - val_mae: 0.1404 - val_mse: 0.1654 - learning_rate: 0.0010 - val_custom_mse: 0.4902 - val_custom_mae: 0.3502\n",
            "Epoch 31/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1905 - mae: 0.1699 - mse: 0.1905 - val_loss: 0.1656 - val_mae: 0.1404 - val_mse: 0.1656 - learning_rate: 0.0010 - val_custom_mse: 0.4910 - val_custom_mae: 0.3502\n",
            "Epoch 32/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1904 - mae: 0.1698 - mse: 0.1904 - val_loss: 0.1654 - val_mae: 0.1405 - val_mse: 0.1654 - learning_rate: 0.0010 - val_custom_mse: 0.4906 - val_custom_mae: 0.3504\n",
            "Epoch 33/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1916 - mae: 0.1698 - mse: 0.1916 - val_loss: 0.1652 - val_mae: 0.1403 - val_mse: 0.1652 - learning_rate: 0.0010 - val_custom_mse: 0.4900 - val_custom_mae: 0.3499\n",
            "Epoch 34/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1906 - mae: 0.1699 - mse: 0.1906 - val_loss: 0.1653 - val_mae: 0.1403 - val_mse: 0.1653 - learning_rate: 0.0010 - val_custom_mse: 0.4901 - val_custom_mae: 0.3497\n",
            "Epoch 35/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1914 - mae: 0.1698 - mse: 0.1914 - val_loss: 0.1652 - val_mae: 0.1402 - val_mse: 0.1652 - learning_rate: 0.0010 - val_custom_mse: 0.4898 - val_custom_mae: 0.3494\n",
            "Epoch 36/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1909 - mae: 0.1699 - mse: 0.1909 - val_loss: 0.1654 - val_mae: 0.1406 - val_mse: 0.1654 - learning_rate: 0.0010 - val_custom_mse: 0.4908 - val_custom_mae: 0.3509\n",
            "Epoch 37/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1910 - mae: 0.1699 - mse: 0.1910 - val_loss: 0.1652 - val_mae: 0.1404 - val_mse: 0.1652 - learning_rate: 0.0010 - val_custom_mse: 0.4899 - val_custom_mae: 0.3498\n",
            "Epoch 38/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1905 - mae: 0.1699 - mse: 0.1905 - val_loss: 0.1653 - val_mae: 0.1403 - val_mse: 0.1653 - learning_rate: 0.0010 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 39/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1913 - mae: 0.1699 - mse: 0.1913 - val_loss: 0.1652 - val_mae: 0.1404 - val_mse: 0.1652 - learning_rate: 0.0010 - val_custom_mse: 0.4900 - val_custom_mae: 0.3501\n",
            "Epoch 40/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1914 - mae: 0.1699 - mse: 0.1914 - val_loss: 0.1655 - val_mae: 0.1405 - val_mse: 0.1655 - learning_rate: 0.0010 - val_custom_mse: 0.4909 - val_custom_mae: 0.3504\n",
            "Epoch 41/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1909 - mae: 0.1698 - mse: 0.1909 - val_loss: 0.1649 - val_mae: 0.1400 - val_mse: 0.1649 - learning_rate: 0.0010 - val_custom_mse: 0.4894 - val_custom_mae: 0.3491\n",
            "Epoch 42/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1910 - mae: 0.1699 - mse: 0.1910 - val_loss: 0.1652 - val_mae: 0.1400 - val_mse: 0.1652 - learning_rate: 0.0010 - val_custom_mse: 0.4901 - val_custom_mae: 0.3492\n",
            "Epoch 43/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1910 - mae: 0.1699 - mse: 0.1910 - val_loss: 0.1652 - val_mae: 0.1404 - val_mse: 0.1652 - learning_rate: 0.0010 - val_custom_mse: 0.4901 - val_custom_mae: 0.3499\n",
            "Epoch 44/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1906 - mae: 0.1699 - mse: 0.1906 - val_loss: 0.1652 - val_mae: 0.1403 - val_mse: 0.1652 - learning_rate: 0.0010 - val_custom_mse: 0.4903 - val_custom_mae: 0.3498\n",
            "Epoch 45/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1909 - mae: 0.1699 - mse: 0.1909 - val_loss: 0.1652 - val_mae: 0.1406 - val_mse: 0.1652 - learning_rate: 0.0010 - val_custom_mse: 0.4902 - val_custom_mae: 0.3505\n",
            "Epoch 46/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1907 - mae: 0.1699 - mse: 0.1907 - val_loss: 0.1652 - val_mae: 0.1404 - val_mse: 0.1652 - learning_rate: 0.0010 - val_custom_mse: 0.4903 - val_custom_mae: 0.3502\n",
            "Epoch 47/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1917 - mae: 0.1699 - mse: 0.1917 - val_loss: 0.1651 - val_mae: 0.1402 - val_mse: 0.1651 - learning_rate: 0.0010 - val_custom_mse: 0.4901 - val_custom_mae: 0.3497\n",
            "Epoch 48/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1911 - mae: 0.1699 - mse: 0.1911 - val_loss: 0.1653 - val_mae: 0.1402 - val_mse: 0.1653 - learning_rate: 0.0010 - val_custom_mse: 0.4908 - val_custom_mae: 0.3500\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1905 - mae: 0.1699 - mse: 0.1905 - val_loss: 0.1654 - val_mae: 0.1404 - val_mse: 0.1654 - learning_rate: 0.0010 - val_custom_mse: 0.4910 - val_custom_mae: 0.3507\n",
            "Epoch 50/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1902 - mae: 0.1694 - mse: 0.1902 - val_loss: 0.1650 - val_mae: 0.1399 - val_mse: 0.1650 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4900 - val_custom_mae: 0.3500\n",
            "Epoch 51/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1907 - mae: 0.1693 - mse: 0.1907 - val_loss: 0.1649 - val_mae: 0.1399 - val_mse: 0.1649 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4897 - val_custom_mae: 0.3499\n",
            "Epoch 52/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1905 - mae: 0.1693 - mse: 0.1905 - val_loss: 0.1649 - val_mae: 0.1399 - val_mse: 0.1649 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4897 - val_custom_mae: 0.3499\n",
            "Epoch 53/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1905 - mae: 0.1694 - mse: 0.1905 - val_loss: 0.1649 - val_mae: 0.1399 - val_mse: 0.1649 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4897 - val_custom_mae: 0.3499\n",
            "Epoch 54/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1900 - mae: 0.1693 - mse: 0.1900 - val_loss: 0.1650 - val_mae: 0.1399 - val_mse: 0.1650 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4898 - val_custom_mae: 0.3500\n",
            "Epoch 55/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1899 - mae: 0.1693 - mse: 0.1899 - val_loss: 0.1650 - val_mae: 0.1399 - val_mse: 0.1650 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4899 - val_custom_mae: 0.3500\n",
            "Epoch 56/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1906 - mae: 0.1694 - mse: 0.1906 - val_loss: 0.1650 - val_mae: 0.1399 - val_mse: 0.1650 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4899 - val_custom_mae: 0.3499\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1897 - mae: 0.1693 - mse: 0.1897 - val_loss: 0.1650 - val_mae: 0.1400 - val_mse: 0.1650 - learning_rate: 2.0000e-04 - val_custom_mse: 0.4899 - val_custom_mae: 0.3501\n",
            "Epoch 58/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1900 - mae: 0.1693 - mse: 0.1900 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4902 - val_custom_mae: 0.3501\n",
            "Epoch 59/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1900 - mae: 0.1692 - mse: 0.1900 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4902 - val_custom_mae: 0.3500\n",
            "Epoch 60/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1891 - mae: 0.1693 - mse: 0.1891 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4902 - val_custom_mae: 0.3500\n",
            "Epoch 61/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1896 - mae: 0.1692 - mse: 0.1896 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4902 - val_custom_mae: 0.3501\n",
            "Epoch 62/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1895 - mae: 0.1692 - mse: 0.1895 - val_loss: 0.1650 - val_mae: 0.1399 - val_mse: 0.1650 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4901 - val_custom_mae: 0.3500\n",
            "Epoch 63/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1905 - mae: 0.1693 - mse: 0.1905 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4902 - val_custom_mae: 0.3501\n",
            "Epoch 64/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1901 - mae: 0.1693 - mse: 0.1901 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4902 - val_custom_mae: 0.3500\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 65: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1895 - mae: 0.1693 - mse: 0.1895 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 4.0000e-05 - val_custom_mse: 0.4902 - val_custom_mae: 0.3501\n",
            "Epoch 66/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1893 - mae: 0.1692 - mse: 0.1893 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4903 - val_custom_mae: 0.3501\n",
            "Epoch 67/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1897 - mae: 0.1692 - mse: 0.1897 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 68/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1895 - mae: 0.1692 - mse: 0.1895 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 69/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1898 - mae: 0.1692 - mse: 0.1898 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 70/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1894 - mae: 0.1692 - mse: 0.1894 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 71/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1894 - mae: 0.1692 - mse: 0.1894 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 72/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1898 - mae: 0.1692 - mse: 0.1898 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1900 - mae: 0.1693 - mse: 0.1900 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 8.0000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 74/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1897 - mae: 0.1692 - mse: 0.1897 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 75/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1896 - mae: 0.1692 - mse: 0.1896 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 76/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1901 - mae: 0.1692 - mse: 0.1901 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 77/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1902 - mae: 0.1692 - mse: 0.1902 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 78/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1891 - mae: 0.1692 - mse: 0.1891 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 79/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1897 - mae: 0.1692 - mse: 0.1897 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 80/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1895 - mae: 0.1692 - mse: 0.1895 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1899 - mae: 0.1692 - mse: 0.1899 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.6000e-06 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 82/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1902 - mae: 0.1692 - mse: 0.1902 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4904 - val_custom_mae: 0.3501\n",
            "Epoch 83/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1899 - mae: 0.1692 - mse: 0.1899 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 84/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1891 - mae: 0.1692 - mse: 0.1891 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 85/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1898 - mae: 0.1692 - mse: 0.1898 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 86/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1896 - mae: 0.1692 - mse: 0.1896 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 87/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1898 - mae: 0.1692 - mse: 0.1898 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 88/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1895 - mae: 0.1692 - mse: 0.1895 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1897 - mae: 0.1692 - mse: 0.1897 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 3.2000e-07 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 90/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1901 - mae: 0.1692 - mse: 0.1901 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 91/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1893 - mae: 0.1692 - mse: 0.1893 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 92/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1896 - mae: 0.1692 - mse: 0.1896 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 93/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1894 - mae: 0.1692 - mse: 0.1894 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 94/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1893 - mae: 0.1692 - mse: 0.1893 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 95/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1902 - mae: 0.1692 - mse: 0.1902 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 96/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1895 - mae: 0.1692 - mse: 0.1895 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1897 - mae: 0.1692 - mse: 0.1897 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 6.4000e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 98/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1900 - mae: 0.1692 - mse: 0.1900 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 99/100\n",
            "1111/1111 - 7s - 6ms/step - loss: 0.1893 - mae: 0.1692 - mse: 0.1893 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Epoch 100/100\n",
            "1111/1111 - 7s - 7ms/step - loss: 0.1898 - mae: 0.1692 - mse: 0.1898 - val_loss: 0.1651 - val_mae: 0.1399 - val_mse: 0.1651 - learning_rate: 1.2800e-08 - val_custom_mse: 0.4904 - val_custom_mae: 0.3502\n",
            "Running experiment: horizon=720, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_38', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1099/1099 - 10s - 9ms/step - loss: 0.5681 - mae: 0.3675 - mse: 0.5681 - val_loss: 0.4362 - val_mae: 0.3119 - val_mse: 0.4362 - learning_rate: 0.0010 - val_custom_mse: 0.6018 - val_custom_mae: 0.4125\n",
            "Epoch 2/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4221 - mae: 0.3110 - mse: 0.4221 - val_loss: 0.4226 - val_mae: 0.3022 - val_mse: 0.4226 - learning_rate: 0.0010 - val_custom_mse: 0.5929 - val_custom_mae: 0.4093\n",
            "Epoch 3/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4099 - mae: 0.3042 - mse: 0.4099 - val_loss: 0.4194 - val_mae: 0.2995 - val_mse: 0.4194 - learning_rate: 0.0010 - val_custom_mse: 0.5918 - val_custom_mae: 0.4080\n",
            "Epoch 4/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4028 - mae: 0.3009 - mse: 0.4028 - val_loss: 0.4167 - val_mae: 0.2970 - val_mse: 0.4167 - learning_rate: 0.0010 - val_custom_mse: 0.5900 - val_custom_mae: 0.4066\n",
            "Epoch 5/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3981 - mae: 0.2983 - mse: 0.3981 - val_loss: 0.4151 - val_mae: 0.2951 - val_mse: 0.4151 - learning_rate: 0.0010 - val_custom_mse: 0.5887 - val_custom_mae: 0.4055\n",
            "Epoch 6/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3948 - mae: 0.2961 - mse: 0.3948 - val_loss: 0.4148 - val_mae: 0.2944 - val_mse: 0.4148 - learning_rate: 0.0010 - val_custom_mse: 0.5887 - val_custom_mae: 0.4054\n",
            "Epoch 7/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3925 - mae: 0.2949 - mse: 0.3925 - val_loss: 0.4140 - val_mae: 0.2938 - val_mse: 0.4140 - learning_rate: 0.0010 - val_custom_mse: 0.5877 - val_custom_mae: 0.4048\n",
            "Epoch 8/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3911 - mae: 0.2941 - mse: 0.3911 - val_loss: 0.4131 - val_mae: 0.2928 - val_mse: 0.4131 - learning_rate: 0.0010 - val_custom_mse: 0.5864 - val_custom_mae: 0.4035\n",
            "Epoch 9/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3901 - mae: 0.2937 - mse: 0.3901 - val_loss: 0.4134 - val_mae: 0.2932 - val_mse: 0.4134 - learning_rate: 0.0010 - val_custom_mse: 0.5869 - val_custom_mae: 0.4038\n",
            "Epoch 10/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3895 - mae: 0.2935 - mse: 0.3895 - val_loss: 0.4122 - val_mae: 0.2925 - val_mse: 0.4122 - learning_rate: 0.0010 - val_custom_mse: 0.5851 - val_custom_mae: 0.4025\n",
            "Epoch 11/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3890 - mae: 0.2934 - mse: 0.3890 - val_loss: 0.4132 - val_mae: 0.2932 - val_mse: 0.4132 - learning_rate: 0.0010 - val_custom_mse: 0.5865 - val_custom_mae: 0.4031\n",
            "Epoch 12/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3887 - mae: 0.2934 - mse: 0.3887 - val_loss: 0.4119 - val_mae: 0.2922 - val_mse: 0.4119 - learning_rate: 0.0010 - val_custom_mse: 0.5847 - val_custom_mae: 0.4017\n",
            "Epoch 13/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3885 - mae: 0.2934 - mse: 0.3885 - val_loss: 0.4122 - val_mae: 0.2928 - val_mse: 0.4122 - learning_rate: 0.0010 - val_custom_mse: 0.5849 - val_custom_mae: 0.4021\n",
            "Epoch 14/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3883 - mae: 0.2934 - mse: 0.3883 - val_loss: 0.4127 - val_mae: 0.2931 - val_mse: 0.4127 - learning_rate: 0.0010 - val_custom_mse: 0.5857 - val_custom_mae: 0.4024\n",
            "Epoch 15/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3882 - mae: 0.2933 - mse: 0.3882 - val_loss: 0.4125 - val_mae: 0.2930 - val_mse: 0.4125 - learning_rate: 0.0010 - val_custom_mse: 0.5855 - val_custom_mae: 0.4023\n",
            "Epoch 16/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3881 - mae: 0.2934 - mse: 0.3881 - val_loss: 0.4127 - val_mae: 0.2930 - val_mse: 0.4127 - learning_rate: 0.0010 - val_custom_mse: 0.5857 - val_custom_mae: 0.4023\n",
            "Epoch 17/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3880 - mae: 0.2933 - mse: 0.3880 - val_loss: 0.4124 - val_mae: 0.2930 - val_mse: 0.4124 - learning_rate: 0.0010 - val_custom_mse: 0.5853 - val_custom_mae: 0.4022\n",
            "Epoch 18/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3880 - mae: 0.2933 - mse: 0.3880 - val_loss: 0.4131 - val_mae: 0.2932 - val_mse: 0.4131 - learning_rate: 0.0010 - val_custom_mse: 0.5862 - val_custom_mae: 0.4025\n",
            "Epoch 19/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3879 - mae: 0.2933 - mse: 0.3879 - val_loss: 0.4130 - val_mae: 0.2930 - val_mse: 0.4130 - learning_rate: 0.0010 - val_custom_mse: 0.5861 - val_custom_mae: 0.4022\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3878 - mae: 0.2933 - mse: 0.3878 - val_loss: 0.4128 - val_mae: 0.2931 - val_mse: 0.4128 - learning_rate: 0.0010 - val_custom_mse: 0.5858 - val_custom_mae: 0.4024\n",
            "Epoch 21/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3868 - mae: 0.2923 - mse: 0.3868 - val_loss: 0.4124 - val_mae: 0.2927 - val_mse: 0.4124 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5853 - val_custom_mae: 0.4021\n",
            "Epoch 22/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3867 - mae: 0.2921 - mse: 0.3867 - val_loss: 0.4120 - val_mae: 0.2924 - val_mse: 0.4120 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5848 - val_custom_mae: 0.4017\n",
            "Epoch 23/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3867 - mae: 0.2921 - mse: 0.3867 - val_loss: 0.4120 - val_mae: 0.2924 - val_mse: 0.4120 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5848 - val_custom_mae: 0.4018\n",
            "Epoch 24/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3866 - mae: 0.2921 - mse: 0.3866 - val_loss: 0.4122 - val_mae: 0.2925 - val_mse: 0.4122 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5850 - val_custom_mae: 0.4019\n",
            "Epoch 25/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3866 - mae: 0.2921 - mse: 0.3866 - val_loss: 0.4121 - val_mae: 0.2925 - val_mse: 0.4121 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5849 - val_custom_mae: 0.4019\n",
            "Epoch 26/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3866 - mae: 0.2921 - mse: 0.3866 - val_loss: 0.4123 - val_mae: 0.2926 - val_mse: 0.4123 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5851 - val_custom_mae: 0.4020\n",
            "Epoch 27/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3866 - mae: 0.2921 - mse: 0.3866 - val_loss: 0.4120 - val_mae: 0.2924 - val_mse: 0.4120 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5848 - val_custom_mae: 0.4018\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3865 - mae: 0.2921 - mse: 0.3865 - val_loss: 0.4124 - val_mae: 0.2926 - val_mse: 0.4124 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5852 - val_custom_mae: 0.4020\n",
            "Epoch 29/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3863 - mae: 0.2919 - mse: 0.3863 - val_loss: 0.4123 - val_mae: 0.2927 - val_mse: 0.4123 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5851 - val_custom_mae: 0.4021\n",
            "Epoch 30/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3863 - mae: 0.2919 - mse: 0.3863 - val_loss: 0.4123 - val_mae: 0.2927 - val_mse: 0.4123 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5852 - val_custom_mae: 0.4022\n",
            "Epoch 31/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3863 - mae: 0.2919 - mse: 0.3863 - val_loss: 0.4124 - val_mae: 0.2927 - val_mse: 0.4124 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5852 - val_custom_mae: 0.4022\n",
            "Epoch 32/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3863 - mae: 0.2919 - mse: 0.3863 - val_loss: 0.4124 - val_mae: 0.2927 - val_mse: 0.4124 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5853 - val_custom_mae: 0.4023\n",
            "Epoch 33/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3863 - mae: 0.2919 - mse: 0.3863 - val_loss: 0.4124 - val_mae: 0.2927 - val_mse: 0.4124 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5853 - val_custom_mae: 0.4023\n",
            "Epoch 34/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3863 - mae: 0.2919 - mse: 0.3863 - val_loss: 0.4124 - val_mae: 0.2927 - val_mse: 0.4124 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5852 - val_custom_mae: 0.4023\n",
            "Epoch 35/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3863 - mae: 0.2919 - mse: 0.3863 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5853 - val_custom_mae: 0.4023\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3863 - mae: 0.2919 - mse: 0.3863 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5853 - val_custom_mae: 0.4023\n",
            "Epoch 37/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5853 - val_custom_mae: 0.4023\n",
            "Epoch 38/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5853 - val_custom_mae: 0.4023\n",
            "Epoch 39/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5853 - val_custom_mae: 0.4023\n",
            "Epoch 40/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5853 - val_custom_mae: 0.4023\n",
            "Epoch 41/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4023\n",
            "Epoch 42/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5853 - val_custom_mae: 0.4023\n",
            "Epoch 43/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4124 - val_mae: 0.2928 - val_mse: 0.4124 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 45/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 46/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 47/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 48/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 49/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 50/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 51/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 53/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 54/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 55/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 56/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 57/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 58/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 59/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 61/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 62/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 63/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 64/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 65/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 66/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 67/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 69/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 70/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 71/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 72/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 73/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 74/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 75/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 76: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 77/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 78/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 79/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 80/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 81/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 82/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 83/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 85/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 86/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 87/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 88/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 89/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 90/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 91/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 93/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 94/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 95/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 96/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 97/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 98/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 99/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 100: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.3862 - mae: 0.2919 - mse: 0.3862 - val_loss: 0.4125 - val_mae: 0.2928 - val_mse: 0.4125 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5854 - val_custom_mae: 0.4024\n",
            "Running experiment: horizon=720, dropout_rate=0.1\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'flow_mixer_39', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1099/1099 - 12s - 11ms/step - loss: 0.6012 - mae: 0.3873 - mse: 0.6012 - val_loss: 0.4351 - val_mae: 0.3120 - val_mse: 0.4351 - learning_rate: 0.0010 - val_custom_mse: 0.5995 - val_custom_mae: 0.4128\n",
            "Epoch 2/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4285 - mae: 0.3212 - mse: 0.4285 - val_loss: 0.4239 - val_mae: 0.3031 - val_mse: 0.4239 - learning_rate: 0.0010 - val_custom_mse: 0.5933 - val_custom_mae: 0.4099\n",
            "Epoch 3/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4177 - mae: 0.3145 - mse: 0.4177 - val_loss: 0.4206 - val_mae: 0.3022 - val_mse: 0.4206 - learning_rate: 0.0010 - val_custom_mse: 0.5913 - val_custom_mae: 0.4091\n",
            "Epoch 4/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4132 - mae: 0.3131 - mse: 0.4132 - val_loss: 0.4180 - val_mae: 0.3008 - val_mse: 0.4180 - learning_rate: 0.0010 - val_custom_mse: 0.5892 - val_custom_mae: 0.4076\n",
            "Epoch 5/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4106 - mae: 0.3122 - mse: 0.4106 - val_loss: 0.4172 - val_mae: 0.3003 - val_mse: 0.4172 - learning_rate: 0.0010 - val_custom_mse: 0.5888 - val_custom_mae: 0.4072\n",
            "Epoch 6/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4092 - mae: 0.3116 - mse: 0.4092 - val_loss: 0.4163 - val_mae: 0.3000 - val_mse: 0.4163 - learning_rate: 0.0010 - val_custom_mse: 0.5879 - val_custom_mae: 0.4068\n",
            "Epoch 7/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4080 - mae: 0.3110 - mse: 0.4080 - val_loss: 0.4156 - val_mae: 0.2996 - val_mse: 0.4156 - learning_rate: 0.0010 - val_custom_mse: 0.5869 - val_custom_mae: 0.4061\n",
            "Epoch 8/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4067 - mae: 0.3106 - mse: 0.4067 - val_loss: 0.4153 - val_mae: 0.2993 - val_mse: 0.4153 - learning_rate: 0.0010 - val_custom_mse: 0.5866 - val_custom_mae: 0.4057\n",
            "Epoch 9/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4060 - mae: 0.3103 - mse: 0.4060 - val_loss: 0.4153 - val_mae: 0.2991 - val_mse: 0.4153 - learning_rate: 0.0010 - val_custom_mse: 0.5866 - val_custom_mae: 0.4054\n",
            "Epoch 10/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4058 - mae: 0.3101 - mse: 0.4058 - val_loss: 0.4150 - val_mae: 0.2990 - val_mse: 0.4150 - learning_rate: 0.0010 - val_custom_mse: 0.5862 - val_custom_mae: 0.4051\n",
            "Epoch 11/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4051 - mae: 0.3099 - mse: 0.4051 - val_loss: 0.4144 - val_mae: 0.2987 - val_mse: 0.4144 - learning_rate: 0.0010 - val_custom_mse: 0.5854 - val_custom_mae: 0.4046\n",
            "Epoch 12/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4043 - mae: 0.3099 - mse: 0.4043 - val_loss: 0.4147 - val_mae: 0.2991 - val_mse: 0.4147 - learning_rate: 0.0010 - val_custom_mse: 0.5858 - val_custom_mae: 0.4050\n",
            "Epoch 13/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4043 - mae: 0.3098 - mse: 0.4043 - val_loss: 0.4150 - val_mae: 0.2992 - val_mse: 0.4150 - learning_rate: 0.0010 - val_custom_mse: 0.5863 - val_custom_mae: 0.4050\n",
            "Epoch 14/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4041 - mae: 0.3097 - mse: 0.4041 - val_loss: 0.4152 - val_mae: 0.2994 - val_mse: 0.4152 - learning_rate: 0.0010 - val_custom_mse: 0.5866 - val_custom_mae: 0.4052\n",
            "Epoch 15/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4041 - mae: 0.3097 - mse: 0.4041 - val_loss: 0.4144 - val_mae: 0.2987 - val_mse: 0.4144 - learning_rate: 0.0010 - val_custom_mse: 0.5855 - val_custom_mae: 0.4045\n",
            "Epoch 16/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4036 - mae: 0.3096 - mse: 0.4036 - val_loss: 0.4140 - val_mae: 0.2986 - val_mse: 0.4140 - learning_rate: 0.0010 - val_custom_mse: 0.5849 - val_custom_mae: 0.4043\n",
            "Epoch 17/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4039 - mae: 0.3096 - mse: 0.4039 - val_loss: 0.4142 - val_mae: 0.2986 - val_mse: 0.4142 - learning_rate: 0.0010 - val_custom_mse: 0.5852 - val_custom_mae: 0.4042\n",
            "Epoch 18/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4039 - mae: 0.3095 - mse: 0.4039 - val_loss: 0.4144 - val_mae: 0.2987 - val_mse: 0.4144 - learning_rate: 0.0010 - val_custom_mse: 0.5855 - val_custom_mae: 0.4044\n",
            "Epoch 19/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4034 - mae: 0.3095 - mse: 0.4034 - val_loss: 0.4146 - val_mae: 0.2988 - val_mse: 0.4146 - learning_rate: 0.0010 - val_custom_mse: 0.5859 - val_custom_mae: 0.4045\n",
            "Epoch 20/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4034 - mae: 0.3094 - mse: 0.4034 - val_loss: 0.4139 - val_mae: 0.2982 - val_mse: 0.4139 - learning_rate: 0.0010 - val_custom_mse: 0.5849 - val_custom_mae: 0.4039\n",
            "Epoch 21/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4032 - mae: 0.3094 - mse: 0.4032 - val_loss: 0.4142 - val_mae: 0.2986 - val_mse: 0.4142 - learning_rate: 0.0010 - val_custom_mse: 0.5853 - val_custom_mae: 0.4043\n",
            "Epoch 22/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4029 - mae: 0.3094 - mse: 0.4029 - val_loss: 0.4139 - val_mae: 0.2984 - val_mse: 0.4139 - learning_rate: 0.0010 - val_custom_mse: 0.5849 - val_custom_mae: 0.4041\n",
            "Epoch 23/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4027 - mae: 0.3094 - mse: 0.4027 - val_loss: 0.4139 - val_mae: 0.2982 - val_mse: 0.4139 - learning_rate: 0.0010 - val_custom_mse: 0.5849 - val_custom_mae: 0.4039\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4034 - mae: 0.3094 - mse: 0.4034 - val_loss: 0.4140 - val_mae: 0.2982 - val_mse: 0.4140 - learning_rate: 0.0010 - val_custom_mse: 0.5850 - val_custom_mae: 0.4039\n",
            "Epoch 25/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4022 - mae: 0.3087 - mse: 0.4022 - val_loss: 0.4143 - val_mae: 0.2979 - val_mse: 0.4143 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5855 - val_custom_mae: 0.4038\n",
            "Epoch 26/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4022 - mae: 0.3086 - mse: 0.4022 - val_loss: 0.4143 - val_mae: 0.2978 - val_mse: 0.4143 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5856 - val_custom_mae: 0.4038\n",
            "Epoch 27/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4021 - mae: 0.3086 - mse: 0.4021 - val_loss: 0.4144 - val_mae: 0.2978 - val_mse: 0.4144 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5857 - val_custom_mae: 0.4038\n",
            "Epoch 28/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4020 - mae: 0.3086 - mse: 0.4020 - val_loss: 0.4144 - val_mae: 0.2980 - val_mse: 0.4144 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5857 - val_custom_mae: 0.4039\n",
            "Epoch 29/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4022 - mae: 0.3086 - mse: 0.4022 - val_loss: 0.4142 - val_mae: 0.2978 - val_mse: 0.4142 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5855 - val_custom_mae: 0.4037\n",
            "Epoch 30/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4018 - mae: 0.3086 - mse: 0.4018 - val_loss: 0.4143 - val_mae: 0.2978 - val_mse: 0.4143 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5856 - val_custom_mae: 0.4038\n",
            "Epoch 31/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4019 - mae: 0.3086 - mse: 0.4019 - val_loss: 0.4144 - val_mae: 0.2978 - val_mse: 0.4144 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5857 - val_custom_mae: 0.4038\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4020 - mae: 0.3086 - mse: 0.4020 - val_loss: 0.4141 - val_mae: 0.2977 - val_mse: 0.4141 - learning_rate: 2.0000e-04 - val_custom_mse: 0.5853 - val_custom_mae: 0.4036\n",
            "Epoch 33/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4018 - mae: 0.3085 - mse: 0.4018 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5859 - val_custom_mae: 0.4038\n",
            "Epoch 34/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5859 - val_custom_mae: 0.4038\n",
            "Epoch 35/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4022 - mae: 0.3084 - mse: 0.4022 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5860 - val_custom_mae: 0.4038\n",
            "Epoch 36/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4017 - mae: 0.3085 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5859 - val_custom_mae: 0.4038\n",
            "Epoch 37/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4018 - mae: 0.3084 - mse: 0.4018 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5859 - val_custom_mae: 0.4038\n",
            "Epoch 38/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4021 - mae: 0.3084 - mse: 0.4021 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5860 - val_custom_mae: 0.4038\n",
            "Epoch 39/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4014 - mae: 0.3084 - mse: 0.4014 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5860 - val_custom_mae: 0.4038\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4016 - mae: 0.3085 - mse: 0.4016 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 4.0000e-05 - val_custom_mse: 0.5860 - val_custom_mae: 0.4038\n",
            "Epoch 41/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4014 - mae: 0.3084 - mse: 0.4014 - val_loss: 0.4146 - val_mae: 0.2978 - val_mse: 0.4146 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5860 - val_custom_mae: 0.4038\n",
            "Epoch 42/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4016 - mae: 0.3084 - mse: 0.4016 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4038\n",
            "Epoch 43/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4019 - mae: 0.3084 - mse: 0.4019 - val_loss: 0.4146 - val_mae: 0.2978 - val_mse: 0.4146 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5860 - val_custom_mae: 0.4038\n",
            "Epoch 44/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4019 - mae: 0.3084 - mse: 0.4019 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5860 - val_custom_mae: 0.4038\n",
            "Epoch 45/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4017 - mae: 0.3084 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4038\n",
            "Epoch 46/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4016 - mae: 0.3084 - mse: 0.4016 - val_loss: 0.4145 - val_mae: 0.2978 - val_mse: 0.4145 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4038\n",
            "Epoch 47/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4020 - mae: 0.3084 - mse: 0.4020 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4021 - mae: 0.3084 - mse: 0.4021 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 8.0000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 49/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4014 - mae: 0.3084 - mse: 0.4014 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 50/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4018 - mae: 0.3084 - mse: 0.4018 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 51/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4017 - mae: 0.3084 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5860 - val_custom_mae: 0.4037\n",
            "Epoch 52/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4019 - mae: 0.3084 - mse: 0.4019 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 53/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4020 - mae: 0.3084 - mse: 0.4020 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 54/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4016 - mae: 0.3084 - mse: 0.4016 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 55/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4016 - mae: 0.3084 - mse: 0.4016 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4018 - mae: 0.3084 - mse: 0.4018 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.6000e-06 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 57/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4018 - mae: 0.3084 - mse: 0.4018 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 58/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4019 - mae: 0.3084 - mse: 0.4019 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 59/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4019 - mae: 0.3084 - mse: 0.4019 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 60/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4013 - mae: 0.3084 - mse: 0.4013 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 61/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4017 - mae: 0.3084 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 62/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4024 - mae: 0.3084 - mse: 0.4024 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 63/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4016 - mae: 0.3084 - mse: 0.4016 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 3.2000e-07 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 65/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4020 - mae: 0.3084 - mse: 0.4020 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 66/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 67/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4017 - mae: 0.3084 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 68/100\n",
            "1099/1099 - 7s - 7ms/step - loss: 0.4016 - mae: 0.3084 - mse: 0.4016 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 69/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4021 - mae: 0.3084 - mse: 0.4021 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 70/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4017 - mae: 0.3084 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 71/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4013 - mae: 0.3084 - mse: 0.4013 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 6.4000e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 73/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4017 - mae: 0.3084 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 74/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4014 - mae: 0.3084 - mse: 0.4014 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 75/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4017 - mae: 0.3084 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 76/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4024 - mae: 0.3084 - mse: 0.4024 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 77/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4018 - mae: 0.3084 - mse: 0.4018 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 78/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4010 - mae: 0.3084 - mse: 0.4010 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 79/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 80: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4011 - mae: 0.3084 - mse: 0.4011 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.2800e-08 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 81/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4018 - mae: 0.3084 - mse: 0.4018 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 82/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 83/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 84/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4016 - mae: 0.3084 - mse: 0.4016 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 85/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4016 - mae: 0.3084 - mse: 0.4016 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 86/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4021 - mae: 0.3084 - mse: 0.4021 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 87/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4018 - mae: 0.3084 - mse: 0.4018 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4014 - mae: 0.3084 - mse: 0.4014 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 2.5600e-09 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 89/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4019 - mae: 0.3084 - mse: 0.4019 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 90/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 91/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 92/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4017 - mae: 0.3084 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 93/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4019 - mae: 0.3084 - mse: 0.4019 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 94/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4019 - mae: 0.3084 - mse: 0.4019 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 95/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4018 - mae: 0.3084 - mse: 0.4018 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4020 - mae: 0.3084 - mse: 0.4020 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 5.1200e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 97/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4014 - mae: 0.3084 - mse: 0.4014 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 98/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4017 - mae: 0.3084 - mse: 0.4017 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 99/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "Epoch 100/100\n",
            "1099/1099 - 7s - 6ms/step - loss: 0.4015 - mae: 0.3084 - mse: 0.4015 - val_loss: 0.4145 - val_mae: 0.2977 - val_mse: 0.4145 - learning_rate: 1.0240e-10 - val_custom_mse: 0.5859 - val_custom_mae: 0.4037\n",
            "\n",
            " Weather Final Results:\n",
            "\n",
            "MSE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%\n",
            "Horizon                \n",
            "96       0.1427  0.1455\n",
            "192      0.1846  0.1869\n",
            "336      0.2348  0.2365\n",
            "720      0.3055  0.3059\n",
            "\n",
            "MAE Results:\n",
            "==================================================\n",
            "          DR=0%  DR=10%\n",
            "Horizon                \n",
            "96       0.1936  0.1979\n",
            "192      0.2354  0.2385\n",
            "336      0.2758  0.2779\n",
            "720      0.3262  0.3268\n",
            "\n",
            "Results saved to: ./flowmixer_results/Weather_experiment_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Run the experiments\n",
        "data_name='Weather'\n",
        "results = run_experiments(data_name, horizons=[96,192,336,720], dropout_rates=[0.0, 0.1], revin=2, seq_len_=1024, learning_rate=1e-3, mopt='adamw')\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\n {data_name} Final Results:\")\n",
        "df = pd.DataFrame(results)\n",
        "# Assuming results is a list of dictionaries with horizon, dropout, MSE, and MAE values\n",
        "display_results_tables(results[0])\n",
        "\n",
        "\n",
        "# The results are already saved in CSV format after each experiment\n",
        "print(f\"\\nResults saved to: ./flowmixer_results/{data_name}_experiment_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Electricity"
      ],
      "metadata": {
        "id": "yoFwq-u_hTKS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249ac870-44fa-4710-8527-b0423352e2b0",
        "id": "5zQDExJWhSEa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: horizon=96, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "477/477 - 146s - 306ms/step - loss: 0.3271 - mae: 0.3460 - mse: 0.3271 - val_loss: 0.3069 - val_mae: 0.3261 - val_mse: 0.3069 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2746 - val_custom_mae: 0.3076\n",
            "Epoch 2/100\n",
            "477/477 - 138s - 290ms/step - loss: 0.2570 - mae: 0.3206 - mse: 0.2570 - val_loss: 0.2501 - val_mae: 0.3081 - val_mse: 0.2501 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2341 - val_custom_mae: 0.2962\n",
            "Epoch 3/100\n",
            "477/477 - 130s - 273ms/step - loss: 0.2164 - mae: 0.3020 - mse: 0.2164 - val_loss: 0.2158 - val_mae: 0.2907 - val_mse: 0.2158 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2157 - val_custom_mae: 0.2878\n",
            "Epoch 4/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.1871 - mae: 0.2828 - mse: 0.1871 - val_loss: 0.1866 - val_mae: 0.2721 - val_mse: 0.1866 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2014 - val_custom_mae: 0.2791\n",
            "Epoch 5/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.1610 - mae: 0.2629 - mse: 0.1610 - val_loss: 0.1600 - val_mae: 0.2528 - val_mse: 0.1600 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1883 - val_custom_mae: 0.2699\n",
            "Epoch 6/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.1380 - mae: 0.2439 - mse: 0.1380 - val_loss: 0.1373 - val_mae: 0.2352 - val_mse: 0.1373 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1764 - val_custom_mae: 0.2611\n",
            "Epoch 7/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.1186 - mae: 0.2267 - mse: 0.1186 - val_loss: 0.1181 - val_mae: 0.2192 - val_mse: 0.1181 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1663 - val_custom_mae: 0.2537\n",
            "Epoch 8/100\n",
            "477/477 - 130s - 273ms/step - loss: 0.1021 - mae: 0.2108 - mse: 0.1021 - val_loss: 0.1017 - val_mae: 0.2041 - val_mse: 0.1017 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1578 - val_custom_mae: 0.2475\n",
            "Epoch 9/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0878 - mae: 0.1958 - mse: 0.0878 - val_loss: 0.0875 - val_mae: 0.1898 - val_mse: 0.0875 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1502 - val_custom_mae: 0.2417\n",
            "Epoch 10/100\n",
            "477/477 - 130s - 273ms/step - loss: 0.0754 - mae: 0.1815 - mse: 0.0754 - val_loss: 0.0749 - val_mae: 0.1760 - val_mse: 0.0749 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1437 - val_custom_mae: 0.2367\n",
            "Epoch 11/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0645 - mae: 0.1678 - mse: 0.0645 - val_loss: 0.0639 - val_mae: 0.1625 - val_mse: 0.0639 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1381 - val_custom_mae: 0.2322\n",
            "Epoch 12/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0548 - mae: 0.1545 - mse: 0.0548 - val_loss: 0.0540 - val_mae: 0.1494 - val_mse: 0.0540 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1331 - val_custom_mae: 0.2280\n",
            "Epoch 13/100\n",
            "477/477 - 130s - 273ms/step - loss: 0.0462 - mae: 0.1416 - mse: 0.0462 - val_loss: 0.0453 - val_mae: 0.1366 - val_mse: 0.0453 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1288 - val_custom_mae: 0.2242\n",
            "Epoch 14/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0387 - mae: 0.1291 - mse: 0.0387 - val_loss: 0.0377 - val_mae: 0.1243 - val_mse: 0.0377 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1251 - val_custom_mae: 0.2209\n",
            "Epoch 15/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0322 - mae: 0.1172 - mse: 0.0322 - val_loss: 0.0311 - val_mae: 0.1125 - val_mse: 0.0311 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1221 - val_custom_mae: 0.2180\n",
            "Epoch 16/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0266 - mae: 0.1058 - mse: 0.0266 - val_loss: 0.0255 - val_mae: 0.1013 - val_mse: 0.0255 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1196 - val_custom_mae: 0.2157\n",
            "Epoch 17/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0219 - mae: 0.0950 - mse: 0.0219 - val_loss: 0.0208 - val_mae: 0.0907 - val_mse: 0.0208 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1175 - val_custom_mae: 0.2135\n",
            "Epoch 18/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0180 - mae: 0.0849 - mse: 0.0180 - val_loss: 0.0169 - val_mae: 0.0808 - val_mse: 0.0169 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1159 - val_custom_mae: 0.2119\n",
            "Epoch 19/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0147 - mae: 0.0755 - mse: 0.0147 - val_loss: 0.0137 - val_mae: 0.0717 - val_mse: 0.0137 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1146 - val_custom_mae: 0.2103\n",
            "Epoch 20/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0122 - mae: 0.0669 - mse: 0.0122 - val_loss: 0.0112 - val_mae: 0.0633 - val_mse: 0.0112 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1136 - val_custom_mae: 0.2093\n",
            "Epoch 21/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0101 - mae: 0.0590 - mse: 0.0101 - val_loss: 0.0092 - val_mae: 0.0557 - val_mse: 0.0092 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1130 - val_custom_mae: 0.2087\n",
            "Epoch 22/100\n",
            "477/477 - 130s - 273ms/step - loss: 0.0085 - mae: 0.0519 - mse: 0.0085 - val_loss: 0.0077 - val_mae: 0.0490 - val_mse: 0.0077 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1124 - val_custom_mae: 0.2080\n",
            "Epoch 23/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0073 - mae: 0.0457 - mse: 0.0073 - val_loss: 0.0066 - val_mae: 0.0431 - val_mse: 0.0066 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1121 - val_custom_mae: 0.2077\n",
            "Epoch 24/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0065 - mae: 0.0405 - mse: 0.0065 - val_loss: 0.0058 - val_mae: 0.0383 - val_mse: 0.0058 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1119 - val_custom_mae: 0.2074\n",
            "Epoch 25/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0059 - mae: 0.0362 - mse: 0.0059 - val_loss: 0.0052 - val_mae: 0.0342 - val_mse: 0.0052 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1118 - val_custom_mae: 0.2072\n",
            "Epoch 26/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0054 - mae: 0.0327 - mse: 0.0054 - val_loss: 0.0048 - val_mae: 0.0310 - val_mse: 0.0048 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1117 - val_custom_mae: 0.2068\n",
            "Epoch 27/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0051 - mae: 0.0298 - mse: 0.0051 - val_loss: 0.0045 - val_mae: 0.0283 - val_mse: 0.0045 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1115 - val_custom_mae: 0.2066\n",
            "Epoch 28/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0049 - mae: 0.0275 - mse: 0.0049 - val_loss: 0.0043 - val_mae: 0.0262 - val_mse: 0.0043 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1114 - val_custom_mae: 0.2066\n",
            "Epoch 29/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0047 - mae: 0.0256 - mse: 0.0047 - val_loss: 0.0042 - val_mae: 0.0244 - val_mse: 0.0042 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1114 - val_custom_mae: 0.2065\n",
            "Epoch 30/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0046 - mae: 0.0240 - mse: 0.0046 - val_loss: 0.0040 - val_mae: 0.0229 - val_mse: 0.0040 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1113 - val_custom_mae: 0.2063\n",
            "Epoch 31/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0045 - mae: 0.0226 - mse: 0.0045 - val_loss: 0.0040 - val_mae: 0.0216 - val_mse: 0.0040 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1112 - val_custom_mae: 0.2061\n",
            "Epoch 32/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0045 - mae: 0.0214 - mse: 0.0045 - val_loss: 0.0039 - val_mae: 0.0204 - val_mse: 0.0039 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1112 - val_custom_mae: 0.2061\n",
            "Epoch 33/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0044 - mae: 0.0204 - mse: 0.0044 - val_loss: 0.0038 - val_mae: 0.0195 - val_mse: 0.0038 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1110 - val_custom_mae: 0.2057\n",
            "Epoch 34/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0043 - mae: 0.0195 - mse: 0.0043 - val_loss: 0.0038 - val_mae: 0.0186 - val_mse: 0.0038 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1109 - val_custom_mae: 0.2055\n",
            "Epoch 35/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0043 - mae: 0.0187 - mse: 0.0043 - val_loss: 0.0037 - val_mae: 0.0179 - val_mse: 0.0037 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1109 - val_custom_mae: 0.2058\n",
            "Epoch 36/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0043 - mae: 0.0181 - mse: 0.0043 - val_loss: 0.0037 - val_mae: 0.0172 - val_mse: 0.0037 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1109 - val_custom_mae: 0.2056\n",
            "Epoch 37/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0042 - mae: 0.0174 - mse: 0.0042 - val_loss: 0.0037 - val_mae: 0.0166 - val_mse: 0.0037 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1109 - val_custom_mae: 0.2056\n",
            "Epoch 38/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0042 - mae: 0.0169 - mse: 0.0042 - val_loss: 0.0037 - val_mae: 0.0161 - val_mse: 0.0037 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1108 - val_custom_mae: 0.2055\n",
            "Epoch 39/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0042 - mae: 0.0164 - mse: 0.0042 - val_loss: 0.0036 - val_mae: 0.0156 - val_mse: 0.0036 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1107 - val_custom_mae: 0.2052\n",
            "Epoch 40/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0042 - mae: 0.0160 - mse: 0.0042 - val_loss: 0.0036 - val_mae: 0.0152 - val_mse: 0.0036 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1109 - val_custom_mae: 0.2057\n",
            "Epoch 41/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0042 - mae: 0.0156 - mse: 0.0042 - val_loss: 0.0036 - val_mae: 0.0148 - val_mse: 0.0036 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1110 - val_custom_mae: 0.2058\n",
            "Epoch 42/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0042 - mae: 0.0152 - mse: 0.0042 - val_loss: 0.0036 - val_mae: 0.0144 - val_mse: 0.0036 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1107 - val_custom_mae: 0.2052\n",
            "Epoch 43/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0148 - mse: 0.0041 - val_loss: 0.0036 - val_mae: 0.0141 - val_mse: 0.0036 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1108 - val_custom_mae: 0.2052\n",
            "Epoch 44/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0145 - mse: 0.0041 - val_loss: 0.0036 - val_mae: 0.0138 - val_mse: 0.0036 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1107 - val_custom_mae: 0.2052\n",
            "Epoch 45/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0142 - mse: 0.0041 - val_loss: 0.0036 - val_mae: 0.0135 - val_mse: 0.0036 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1106 - val_custom_mae: 0.2051\n",
            "Epoch 46/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0140 - mse: 0.0041 - val_loss: 0.0036 - val_mae: 0.0132 - val_mse: 0.0036 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1106 - val_custom_mae: 0.2051\n",
            "Epoch 47/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0137 - mse: 0.0041 - val_loss: 0.0036 - val_mae: 0.0130 - val_mse: 0.0036 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1107 - val_custom_mae: 0.2052\n",
            "Epoch 48/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0135 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0128 - val_mse: 0.0035 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1106 - val_custom_mae: 0.2050\n",
            "Epoch 49/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0133 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0125 - val_mse: 0.0035 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1106 - val_custom_mae: 0.2052\n",
            "Epoch 50/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0131 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0123 - val_mse: 0.0035 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1107 - val_custom_mae: 0.2051\n",
            "Epoch 51/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0129 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0122 - val_mse: 0.0035 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1107 - val_custom_mae: 0.2051\n",
            "Epoch 52/100\n",
            "477/477 - 130s - 273ms/step - loss: 0.0041 - mae: 0.0127 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0120 - val_mse: 0.0035 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1106 - val_custom_mae: 0.2051\n",
            "Epoch 53/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0126 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0118 - val_mse: 0.0035 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1105 - val_custom_mae: 0.2048\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0124 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0117 - val_mse: 0.0035 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1105 - val_custom_mae: 0.2049\n",
            "Epoch 55/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0123 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0116 - val_mse: 0.0035 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1104 - val_custom_mae: 0.2045\n",
            "Epoch 56/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0122 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0116 - val_mse: 0.0035 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 57/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0122 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0115 - val_mse: 0.0035 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 58/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0122 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0115 - val_mse: 0.0035 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 59/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0122 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0115 - val_mse: 0.0035 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 60/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0121 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0115 - val_mse: 0.0035 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 61/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0121 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0114 - val_mse: 0.0035 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0121 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0114 - val_mse: 0.0035 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 63/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0114 - val_mse: 0.0035 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 64/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0114 - val_mse: 0.0035 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 65/100\n",
            "477/477 - 130s - 273ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0114 - val_mse: 0.0035 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 66/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0114 - val_mse: 0.0035 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 67/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0114 - val_mse: 0.0035 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 68/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0114 - val_mse: 0.0035 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 69/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0114 - val_mse: 0.0035 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
            "477/477 - 130s - 273ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 71/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 72/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 73/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 74/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 75/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 76/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 77/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 79/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 80/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 81/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 82/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 83/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 84/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 85/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 3.199999980552093e-08.\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 87/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 88/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 89/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 90/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 91/100\n",
            "477/477 - 130s - 272ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 92/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 93/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 6.399999818995639e-09.\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 95/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 96/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 97/100\n",
            "477/477 - 129s - 271ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 98/100\n",
            "477/477 - 129s - 270ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 99/100\n",
            "477/477 - 129s - 270ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Epoch 100/100\n",
            "477/477 - 129s - 270ms/step - loss: 0.0041 - mae: 0.0120 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0113 - val_mse: 0.0035 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1103 - val_custom_mae: 0.2045\n",
            "Running experiment: horizon=192, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "474/474 - 136s - 288ms/step - loss: 0.2988 - mae: 0.3344 - mse: 0.2988 - val_loss: 0.2907 - val_mae: 0.3172 - val_mse: 0.2907 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2587 - val_custom_mae: 0.2994\n",
            "Epoch 2/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.2532 - mae: 0.3140 - mse: 0.2532 - val_loss: 0.2446 - val_mae: 0.2995 - val_mse: 0.2446 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2359 - val_custom_mae: 0.2927\n",
            "Epoch 3/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.2144 - mae: 0.2950 - mse: 0.2144 - val_loss: 0.2095 - val_mae: 0.2835 - val_mse: 0.2095 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2202 - val_custom_mae: 0.2872\n",
            "Epoch 4/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.1844 - mae: 0.2772 - mse: 0.1844 - val_loss: 0.1818 - val_mae: 0.2683 - val_mse: 0.1818 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2083 - val_custom_mae: 0.2825\n",
            "Epoch 5/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.1597 - mae: 0.2601 - mse: 0.1597 - val_loss: 0.1576 - val_mae: 0.2515 - val_mse: 0.1576 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1984 - val_custom_mae: 0.2778\n",
            "Epoch 6/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.1380 - mae: 0.2428 - mse: 0.1380 - val_loss: 0.1362 - val_mae: 0.2347 - val_mse: 0.1362 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1890 - val_custom_mae: 0.2720\n",
            "Epoch 7/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.1194 - mae: 0.2266 - mse: 0.1194 - val_loss: 0.1181 - val_mae: 0.2192 - val_mse: 0.1181 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1798 - val_custom_mae: 0.2656\n",
            "Epoch 8/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.1035 - mae: 0.2114 - mse: 0.1035 - val_loss: 0.1024 - val_mae: 0.2045 - val_mse: 0.1024 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1715 - val_custom_mae: 0.2594\n",
            "Epoch 9/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0897 - mae: 0.1968 - mse: 0.0897 - val_loss: 0.0888 - val_mae: 0.1905 - val_mse: 0.0888 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1640 - val_custom_mae: 0.2539\n",
            "Epoch 10/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0777 - mae: 0.1828 - mse: 0.0777 - val_loss: 0.0767 - val_mae: 0.1770 - val_mse: 0.0767 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1575 - val_custom_mae: 0.2489\n",
            "Epoch 11/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0671 - mae: 0.1694 - mse: 0.0671 - val_loss: 0.0659 - val_mae: 0.1638 - val_mse: 0.0659 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1517 - val_custom_mae: 0.2444\n",
            "Epoch 12/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0577 - mae: 0.1564 - mse: 0.0577 - val_loss: 0.0563 - val_mae: 0.1510 - val_mse: 0.0563 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1466 - val_custom_mae: 0.2403\n",
            "Epoch 13/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0495 - mae: 0.1439 - mse: 0.0495 - val_loss: 0.0479 - val_mae: 0.1386 - val_mse: 0.0479 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1423 - val_custom_mae: 0.2367\n",
            "Epoch 14/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0423 - mae: 0.1320 - mse: 0.0423 - val_loss: 0.0406 - val_mae: 0.1268 - val_mse: 0.0406 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1385 - val_custom_mae: 0.2334\n",
            "Epoch 15/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0361 - mae: 0.1205 - mse: 0.0361 - val_loss: 0.0343 - val_mae: 0.1155 - val_mse: 0.0343 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1352 - val_custom_mae: 0.2305\n",
            "Epoch 16/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0308 - mae: 0.1097 - mse: 0.0308 - val_loss: 0.0289 - val_mae: 0.1048 - val_mse: 0.0289 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1325 - val_custom_mae: 0.2280\n",
            "Epoch 17/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0263 - mae: 0.0994 - mse: 0.0263 - val_loss: 0.0244 - val_mae: 0.0947 - val_mse: 0.0244 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1303 - val_custom_mae: 0.2259\n",
            "Epoch 18/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0225 - mae: 0.0898 - mse: 0.0225 - val_loss: 0.0207 - val_mae: 0.0853 - val_mse: 0.0207 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1286 - val_custom_mae: 0.2241\n",
            "Epoch 19/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0194 - mae: 0.0809 - mse: 0.0194 - val_loss: 0.0176 - val_mae: 0.0765 - val_mse: 0.0176 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1271 - val_custom_mae: 0.2226\n",
            "Epoch 20/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0169 - mae: 0.0726 - mse: 0.0169 - val_loss: 0.0152 - val_mae: 0.0685 - val_mse: 0.0152 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1260 - val_custom_mae: 0.2213\n",
            "Epoch 21/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0150 - mae: 0.0651 - mse: 0.0150 - val_loss: 0.0133 - val_mae: 0.0612 - val_mse: 0.0133 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1252 - val_custom_mae: 0.2204\n",
            "Epoch 22/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0134 - mae: 0.0584 - mse: 0.0134 - val_loss: 0.0118 - val_mae: 0.0548 - val_mse: 0.0118 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1246 - val_custom_mae: 0.2196\n",
            "Epoch 23/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0123 - mae: 0.0525 - mse: 0.0123 - val_loss: 0.0107 - val_mae: 0.0492 - val_mse: 0.0107 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1241 - val_custom_mae: 0.2190\n",
            "Epoch 24/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0115 - mae: 0.0474 - mse: 0.0115 - val_loss: 0.0099 - val_mae: 0.0444 - val_mse: 0.0099 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1238 - val_custom_mae: 0.2187\n",
            "Epoch 25/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0109 - mae: 0.0432 - mse: 0.0109 - val_loss: 0.0094 - val_mae: 0.0405 - val_mse: 0.0094 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1236 - val_custom_mae: 0.2183\n",
            "Epoch 26/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0104 - mae: 0.0397 - mse: 0.0104 - val_loss: 0.0090 - val_mae: 0.0373 - val_mse: 0.0090 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1235 - val_custom_mae: 0.2181\n",
            "Epoch 27/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0101 - mae: 0.0369 - mse: 0.0101 - val_loss: 0.0087 - val_mae: 0.0346 - val_mse: 0.0087 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1234 - val_custom_mae: 0.2179\n",
            "Epoch 28/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0099 - mae: 0.0346 - mse: 0.0099 - val_loss: 0.0085 - val_mae: 0.0325 - val_mse: 0.0085 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1233 - val_custom_mae: 0.2179\n",
            "Epoch 29/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0097 - mae: 0.0327 - mse: 0.0097 - val_loss: 0.0083 - val_mae: 0.0307 - val_mse: 0.0083 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1232 - val_custom_mae: 0.2175\n",
            "Epoch 30/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0096 - mae: 0.0311 - mse: 0.0096 - val_loss: 0.0082 - val_mae: 0.0292 - val_mse: 0.0082 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1232 - val_custom_mae: 0.2175\n",
            "Epoch 31/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0095 - mae: 0.0298 - mse: 0.0095 - val_loss: 0.0081 - val_mae: 0.0279 - val_mse: 0.0081 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1231 - val_custom_mae: 0.2174\n",
            "Epoch 32/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0095 - mae: 0.0286 - mse: 0.0095 - val_loss: 0.0081 - val_mae: 0.0269 - val_mse: 0.0081 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1231 - val_custom_mae: 0.2173\n",
            "Epoch 33/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0094 - mae: 0.0277 - mse: 0.0094 - val_loss: 0.0080 - val_mae: 0.0259 - val_mse: 0.0080 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1230 - val_custom_mae: 0.2171\n",
            "Epoch 34/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0094 - mae: 0.0268 - mse: 0.0094 - val_loss: 0.0080 - val_mae: 0.0251 - val_mse: 0.0080 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1230 - val_custom_mae: 0.2170\n",
            "Epoch 35/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0093 - mae: 0.0260 - mse: 0.0093 - val_loss: 0.0079 - val_mae: 0.0244 - val_mse: 0.0079 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1230 - val_custom_mae: 0.2171\n",
            "Epoch 36/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0093 - mae: 0.0254 - mse: 0.0093 - val_loss: 0.0079 - val_mae: 0.0237 - val_mse: 0.0079 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1229 - val_custom_mae: 0.2170\n",
            "Epoch 37/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0093 - mae: 0.0248 - mse: 0.0093 - val_loss: 0.0079 - val_mae: 0.0232 - val_mse: 0.0079 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1230 - val_custom_mae: 0.2170\n",
            "Epoch 38/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0092 - mae: 0.0243 - mse: 0.0092 - val_loss: 0.0079 - val_mae: 0.0226 - val_mse: 0.0079 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1229 - val_custom_mae: 0.2168\n",
            "Epoch 39/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0092 - mae: 0.0238 - mse: 0.0092 - val_loss: 0.0078 - val_mae: 0.0222 - val_mse: 0.0078 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1229 - val_custom_mae: 0.2167\n",
            "Epoch 40/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0092 - mae: 0.0234 - mse: 0.0092 - val_loss: 0.0078 - val_mae: 0.0218 - val_mse: 0.0078 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1228 - val_custom_mae: 0.2169\n",
            "Epoch 41/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0092 - mae: 0.0230 - mse: 0.0092 - val_loss: 0.0078 - val_mae: 0.0214 - val_mse: 0.0078 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1228 - val_custom_mae: 0.2168\n",
            "Epoch 42/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0092 - mae: 0.0227 - mse: 0.0092 - val_loss: 0.0078 - val_mae: 0.0211 - val_mse: 0.0078 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1228 - val_custom_mae: 0.2166\n",
            "Epoch 43/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0092 - mae: 0.0223 - mse: 0.0092 - val_loss: 0.0078 - val_mae: 0.0207 - val_mse: 0.0078 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1228 - val_custom_mae: 0.2166\n",
            "Epoch 44/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0091 - mae: 0.0220 - mse: 0.0091 - val_loss: 0.0078 - val_mae: 0.0205 - val_mse: 0.0078 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1228 - val_custom_mae: 0.2167\n",
            "Epoch 45/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0218 - mse: 0.0091 - val_loss: 0.0078 - val_mae: 0.0202 - val_mse: 0.0078 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1228 - val_custom_mae: 0.2167\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0215 - mse: 0.0091 - val_loss: 0.0078 - val_mae: 0.0200 - val_mse: 0.0078 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1228 - val_custom_mae: 0.2167\n",
            "Epoch 47/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0214 - mse: 0.0091 - val_loss: 0.0078 - val_mae: 0.0199 - val_mse: 0.0078 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1227 - val_custom_mae: 0.2164\n",
            "Epoch 48/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0091 - mae: 0.0213 - mse: 0.0091 - val_loss: 0.0078 - val_mae: 0.0198 - val_mse: 0.0078 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1226 - val_custom_mae: 0.2164\n",
            "Epoch 49/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0213 - mse: 0.0091 - val_loss: 0.0078 - val_mae: 0.0198 - val_mse: 0.0078 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1226 - val_custom_mae: 0.2164\n",
            "Epoch 50/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0212 - mse: 0.0091 - val_loss: 0.0078 - val_mae: 0.0197 - val_mse: 0.0078 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1227 - val_custom_mae: 0.2164\n",
            "Epoch 51/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0212 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0197 - val_mse: 0.0077 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1226 - val_custom_mae: 0.2164\n",
            "Epoch 52/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0211 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0196 - val_mse: 0.0077 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1227 - val_custom_mae: 0.2164\n",
            "Epoch 53/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0211 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0196 - val_mse: 0.0077 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1226 - val_custom_mae: 0.2164\n",
            "Epoch 54/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0211 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0196 - val_mse: 0.0077 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0210 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0195 - val_mse: 0.0077 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1226 - val_custom_mae: 0.2164\n",
            "Epoch 56/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0210 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0195 - val_mse: 0.0077 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1226 - val_custom_mae: 0.2163\n",
            "Epoch 57/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0210 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0195 - val_mse: 0.0077 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1226 - val_custom_mae: 0.2163\n",
            "Epoch 58/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0210 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0195 - val_mse: 0.0077 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1226 - val_custom_mae: 0.2163\n",
            "Epoch 59/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0091 - mae: 0.0210 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0195 - val_mse: 0.0077 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 60/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0195 - val_mse: 0.0077 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1226 - val_custom_mae: 0.2163\n",
            "Epoch 61/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0195 - val_mse: 0.0077 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1226 - val_custom_mae: 0.2163\n",
            "Epoch 62/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0195 - val_mse: 0.0077 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1226 - val_custom_mae: 0.2163\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 64/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 65/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 66/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 67/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 68/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 69/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 70/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 72/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 73/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 74/100\n",
            "474/474 - 130s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 75/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 76/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 77/100\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 78/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.199999980552093e-08.\n",
            "474/474 - 130s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 80/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 81/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 82/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 83/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 84/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 85/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 86/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 6.399999818995639e-09.\n",
            "474/474 - 130s - 274ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 88/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 89/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 90/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 91/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 92/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 93/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 94/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.279999928271991e-09.\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 96/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 97/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 98/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 99/100\n",
            "474/474 - 129s - 272ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Epoch 100/100\n",
            "474/474 - 129s - 273ms/step - loss: 0.0091 - mae: 0.0209 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0194 - val_mse: 0.0077 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1227 - val_custom_mae: 0.2163\n",
            "Running experiment: horizon=336, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "469/469 - 148s - 316ms/step - loss: 0.2517 - mae: 0.3112 - mse: 0.2517 - val_loss: 0.2460 - val_mae: 0.2956 - val_mse: 0.2460 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2176 - val_custom_mae: 0.2802\n",
            "Epoch 2/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.2206 - mae: 0.2936 - mse: 0.2206 - val_loss: 0.2183 - val_mae: 0.2805 - val_mse: 0.2183 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2090 - val_custom_mae: 0.2754\n",
            "Epoch 3/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.1925 - mae: 0.2765 - mse: 0.1925 - val_loss: 0.1926 - val_mae: 0.2662 - val_mse: 0.1926 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2016 - val_custom_mae: 0.2712\n",
            "Epoch 4/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.1690 - mae: 0.2613 - mse: 0.1690 - val_loss: 0.1698 - val_mae: 0.2523 - val_mse: 0.1698 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1948 - val_custom_mae: 0.2674\n",
            "Epoch 5/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.1491 - mae: 0.2470 - mse: 0.1491 - val_loss: 0.1495 - val_mae: 0.2386 - val_mse: 0.1495 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1884 - val_custom_mae: 0.2638\n",
            "Epoch 6/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.1317 - mae: 0.2331 - mse: 0.1317 - val_loss: 0.1316 - val_mae: 0.2253 - val_mse: 0.1316 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1823 - val_custom_mae: 0.2605\n",
            "Epoch 7/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.1167 - mae: 0.2200 - mse: 0.1167 - val_loss: 0.1160 - val_mae: 0.2126 - val_mse: 0.1160 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1765 - val_custom_mae: 0.2572\n",
            "Epoch 8/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.1036 - mae: 0.2076 - mse: 0.1036 - val_loss: 0.1023 - val_mae: 0.2002 - val_mse: 0.1023 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1710 - val_custom_mae: 0.2539\n",
            "Epoch 9/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0919 - mae: 0.1953 - mse: 0.0919 - val_loss: 0.0899 - val_mae: 0.1878 - val_mse: 0.0899 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1659 - val_custom_mae: 0.2508\n",
            "Epoch 10/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0811 - mae: 0.1828 - mse: 0.0811 - val_loss: 0.0786 - val_mae: 0.1754 - val_mse: 0.0786 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1613 - val_custom_mae: 0.2480\n",
            "Epoch 11/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0712 - mae: 0.1701 - mse: 0.0712 - val_loss: 0.0685 - val_mae: 0.1630 - val_mse: 0.0685 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1570 - val_custom_mae: 0.2452\n",
            "Epoch 12/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0625 - mae: 0.1579 - mse: 0.0625 - val_loss: 0.0596 - val_mae: 0.1512 - val_mse: 0.0596 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1533 - val_custom_mae: 0.2429\n",
            "Epoch 13/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0549 - mae: 0.1462 - mse: 0.0549 - val_loss: 0.0518 - val_mae: 0.1399 - val_mse: 0.0518 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1501 - val_custom_mae: 0.2408\n",
            "Epoch 14/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0483 - mae: 0.1353 - mse: 0.0483 - val_loss: 0.0451 - val_mae: 0.1292 - val_mse: 0.0451 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1473 - val_custom_mae: 0.2389\n",
            "Epoch 15/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0427 - mae: 0.1249 - mse: 0.0427 - val_loss: 0.0393 - val_mae: 0.1190 - val_mse: 0.0393 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1451 - val_custom_mae: 0.2375\n",
            "Epoch 16/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0379 - mae: 0.1151 - mse: 0.0379 - val_loss: 0.0344 - val_mae: 0.1093 - val_mse: 0.0344 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1433 - val_custom_mae: 0.2362\n",
            "Epoch 17/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0338 - mae: 0.1059 - mse: 0.0338 - val_loss: 0.0303 - val_mae: 0.1001 - val_mse: 0.0303 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1420 - val_custom_mae: 0.2353\n",
            "Epoch 18/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0304 - mae: 0.0971 - mse: 0.0304 - val_loss: 0.0269 - val_mae: 0.0914 - val_mse: 0.0269 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1409 - val_custom_mae: 0.2345\n",
            "Epoch 19/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0276 - mae: 0.0889 - mse: 0.0276 - val_loss: 0.0241 - val_mae: 0.0834 - val_mse: 0.0241 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1403 - val_custom_mae: 0.2341\n",
            "Epoch 20/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0254 - mae: 0.0814 - mse: 0.0254 - val_loss: 0.0219 - val_mae: 0.0760 - val_mse: 0.0219 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1396 - val_custom_mae: 0.2335\n",
            "Epoch 21/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0236 - mae: 0.0745 - mse: 0.0236 - val_loss: 0.0202 - val_mae: 0.0694 - val_mse: 0.0202 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1394 - val_custom_mae: 0.2335\n",
            "Epoch 22/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0222 - mae: 0.0684 - mse: 0.0222 - val_loss: 0.0188 - val_mae: 0.0634 - val_mse: 0.0188 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1392 - val_custom_mae: 0.2334\n",
            "Epoch 23/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0212 - mae: 0.0630 - mse: 0.0212 - val_loss: 0.0179 - val_mae: 0.0582 - val_mse: 0.0179 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1392 - val_custom_mae: 0.2333\n",
            "Epoch 24/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0204 - mae: 0.0583 - mse: 0.0204 - val_loss: 0.0172 - val_mae: 0.0537 - val_mse: 0.0172 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1392 - val_custom_mae: 0.2333\n",
            "Epoch 25/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0199 - mae: 0.0543 - mse: 0.0199 - val_loss: 0.0167 - val_mae: 0.0500 - val_mse: 0.0167 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1392 - val_custom_mae: 0.2334\n",
            "Epoch 26/100\n",
            "469/469 - 127s - 270ms/step - loss: 0.0195 - mae: 0.0509 - mse: 0.0195 - val_loss: 0.0163 - val_mae: 0.0469 - val_mse: 0.0163 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1393 - val_custom_mae: 0.2333\n",
            "Epoch 27/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0192 - mae: 0.0482 - mse: 0.0192 - val_loss: 0.0161 - val_mae: 0.0443 - val_mse: 0.0161 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1393 - val_custom_mae: 0.2334\n",
            "Epoch 28/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0190 - mae: 0.0460 - mse: 0.0190 - val_loss: 0.0159 - val_mae: 0.0423 - val_mse: 0.0159 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1394 - val_custom_mae: 0.2334\n",
            "Epoch 29/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0189 - mae: 0.0442 - mse: 0.0189 - val_loss: 0.0158 - val_mae: 0.0407 - val_mse: 0.0158 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1394 - val_custom_mae: 0.2334\n",
            "Epoch 30/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0188 - mae: 0.0428 - mse: 0.0188 - val_loss: 0.0157 - val_mae: 0.0394 - val_mse: 0.0157 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1394 - val_custom_mae: 0.2333\n",
            "Epoch 31/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0187 - mae: 0.0416 - mse: 0.0187 - val_loss: 0.0156 - val_mae: 0.0383 - val_mse: 0.0156 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1393 - val_custom_mae: 0.2332\n",
            "Epoch 32/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0187 - mae: 0.0406 - mse: 0.0187 - val_loss: 0.0156 - val_mae: 0.0374 - val_mse: 0.0156 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1393 - val_custom_mae: 0.2332\n",
            "Epoch 33/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0186 - mae: 0.0398 - mse: 0.0186 - val_loss: 0.0155 - val_mae: 0.0366 - val_mse: 0.0155 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1392 - val_custom_mae: 0.2330\n",
            "Epoch 34/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0186 - mae: 0.0391 - mse: 0.0186 - val_loss: 0.0155 - val_mae: 0.0359 - val_mse: 0.0155 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1392 - val_custom_mae: 0.2330\n",
            "Epoch 35/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0185 - mae: 0.0385 - mse: 0.0185 - val_loss: 0.0154 - val_mae: 0.0353 - val_mse: 0.0154 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1392 - val_custom_mae: 0.2329\n",
            "Epoch 36/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0185 - mae: 0.0380 - mse: 0.0185 - val_loss: 0.0154 - val_mae: 0.0348 - val_mse: 0.0154 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1392 - val_custom_mae: 0.2329\n",
            "Epoch 37/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0185 - mae: 0.0375 - mse: 0.0185 - val_loss: 0.0154 - val_mae: 0.0344 - val_mse: 0.0154 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1391 - val_custom_mae: 0.2328\n",
            "Epoch 38/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0185 - mae: 0.0371 - mse: 0.0185 - val_loss: 0.0154 - val_mae: 0.0340 - val_mse: 0.0154 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1391 - val_custom_mae: 0.2328\n",
            "Epoch 39/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0184 - mae: 0.0367 - mse: 0.0184 - val_loss: 0.0154 - val_mae: 0.0336 - val_mse: 0.0154 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1391 - val_custom_mae: 0.2328\n",
            "Epoch 40/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0184 - mae: 0.0364 - mse: 0.0184 - val_loss: 0.0154 - val_mae: 0.0333 - val_mse: 0.0154 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1391 - val_custom_mae: 0.2328\n",
            "Epoch 41/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0184 - mae: 0.0360 - mse: 0.0184 - val_loss: 0.0153 - val_mae: 0.0330 - val_mse: 0.0153 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1391 - val_custom_mae: 0.2327\n",
            "Epoch 42/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0184 - mae: 0.0358 - mse: 0.0184 - val_loss: 0.0153 - val_mae: 0.0327 - val_mse: 0.0153 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1390 - val_custom_mae: 0.2327\n",
            "Epoch 43/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0184 - mae: 0.0355 - mse: 0.0184 - val_loss: 0.0153 - val_mae: 0.0325 - val_mse: 0.0153 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1391 - val_custom_mae: 0.2327\n",
            "Epoch 44/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0184 - mae: 0.0353 - mse: 0.0184 - val_loss: 0.0153 - val_mae: 0.0323 - val_mse: 0.0153 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1389 - val_custom_mae: 0.2327\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "469/469 - 127s - 271ms/step - loss: 0.0184 - mae: 0.0351 - mse: 0.0184 - val_loss: 0.0153 - val_mae: 0.0321 - val_mse: 0.0153 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1390 - val_custom_mae: 0.2326\n",
            "Epoch 46/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0349 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0320 - val_mse: 0.0153 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1387 - val_custom_mae: 0.2324\n",
            "Epoch 47/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0349 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0319 - val_mse: 0.0153 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1387 - val_custom_mae: 0.2324\n",
            "Epoch 48/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0183 - mae: 0.0349 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0319 - val_mse: 0.0153 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1387 - val_custom_mae: 0.2324\n",
            "Epoch 49/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0183 - mae: 0.0348 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0318 - val_mse: 0.0153 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1387 - val_custom_mae: 0.2324\n",
            "Epoch 50/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0348 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0318 - val_mse: 0.0153 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1387 - val_custom_mae: 0.2324\n",
            "Epoch 51/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0347 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0318 - val_mse: 0.0153 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1387 - val_custom_mae: 0.2324\n",
            "Epoch 52/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0347 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0317 - val_mse: 0.0153 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1387 - val_custom_mae: 0.2324\n",
            "Epoch 53/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0347 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0317 - val_mse: 0.0153 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0317 - val_mse: 0.0153 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1387 - val_custom_mae: 0.2324\n",
            "Epoch 55/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 56/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 57/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 58/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 59/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 60/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 61/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 63/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 64/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 65/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0346 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 66/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 67/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 68/100\n",
            "469/469 - 127s - 270ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 69/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 70: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 71/100\n",
            "469/469 - 127s - 270ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 72/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 73/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 74/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 75/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 76/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 77/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 3.199999980552093e-08.\n",
            "469/469 - 128s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 79/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 80/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 81/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 82/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 83/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 84/100\n",
            "469/469 - 127s - 270ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 85/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 6.399999818995639e-09.\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 87/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 88/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 89/100\n",
            "469/469 - 127s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 90/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 91/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 92/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 93/100\n",
            "469/469 - 128s - 272ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 94: ReduceLROnPlateau reducing learning rate to 1.279999928271991e-09.\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 95/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 96/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 97/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 98/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 99/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Epoch 100/100\n",
            "469/469 - 127s - 271ms/step - loss: 0.0183 - mae: 0.0345 - mse: 0.0183 - val_loss: 0.0153 - val_mae: 0.0316 - val_mse: 0.0153 - learning_rate: 1.2800e-09 - val_custom_mse: 0.1387 - val_custom_mae: 0.2323\n",
            "Running experiment: horizon=720, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "457/457 - 128s - 281ms/step - loss: 0.4489 - mae: 0.4314 - mse: 0.4489 - val_loss: 0.4129 - val_mae: 0.4022 - val_mse: 0.4129 - learning_rate: 1.0000e-04 - val_custom_mse: 0.3773 - val_custom_mae: 0.3790\n",
            "Epoch 2/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.3507 - mae: 0.3907 - mse: 0.3507 - val_loss: 0.3241 - val_mae: 0.3678 - val_mse: 0.3241 - learning_rate: 1.0000e-04 - val_custom_mse: 0.3252 - val_custom_mae: 0.3631\n",
            "Epoch 3/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.2874 - mae: 0.3601 - mse: 0.2874 - val_loss: 0.2744 - val_mae: 0.3445 - val_mse: 0.2744 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2981 - val_custom_mae: 0.3526\n",
            "Epoch 4/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.2494 - mae: 0.3372 - mse: 0.2494 - val_loss: 0.2419 - val_mae: 0.3262 - val_mse: 0.2419 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2809 - val_custom_mae: 0.3443\n",
            "Epoch 5/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.2215 - mae: 0.3177 - mse: 0.2215 - val_loss: 0.2151 - val_mae: 0.3078 - val_mse: 0.2151 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2674 - val_custom_mae: 0.3365\n",
            "Epoch 6/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.1979 - mae: 0.2991 - mse: 0.1979 - val_loss: 0.1916 - val_mae: 0.2897 - val_mse: 0.1916 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2550 - val_custom_mae: 0.3284\n",
            "Epoch 7/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.1775 - mae: 0.2816 - mse: 0.1775 - val_loss: 0.1712 - val_mae: 0.2726 - val_mse: 0.1712 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2437 - val_custom_mae: 0.3206\n",
            "Epoch 8/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.1598 - mae: 0.2653 - mse: 0.1598 - val_loss: 0.1532 - val_mae: 0.2567 - val_mse: 0.1532 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2335 - val_custom_mae: 0.3134\n",
            "Epoch 9/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.1444 - mae: 0.2501 - mse: 0.1444 - val_loss: 0.1373 - val_mae: 0.2417 - val_mse: 0.1373 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2246 - val_custom_mae: 0.3071\n",
            "Epoch 10/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.1308 - mae: 0.2358 - mse: 0.1308 - val_loss: 0.1233 - val_mae: 0.2276 - val_mse: 0.1233 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2167 - val_custom_mae: 0.3013\n",
            "Epoch 11/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.1188 - mae: 0.2223 - mse: 0.1188 - val_loss: 0.1109 - val_mae: 0.2141 - val_mse: 0.1109 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2096 - val_custom_mae: 0.2960\n",
            "Epoch 12/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.1083 - mae: 0.2096 - mse: 0.1083 - val_loss: 0.0998 - val_mae: 0.2013 - val_mse: 0.0998 - learning_rate: 1.0000e-04 - val_custom_mse: 0.2034 - val_custom_mae: 0.2913\n",
            "Epoch 13/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0990 - mae: 0.1974 - mse: 0.0990 - val_loss: 0.0900 - val_mae: 0.1889 - val_mse: 0.0900 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1978 - val_custom_mae: 0.2869\n",
            "Epoch 14/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0907 - mae: 0.1858 - mse: 0.0907 - val_loss: 0.0814 - val_mae: 0.1771 - val_mse: 0.0814 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1930 - val_custom_mae: 0.2829\n",
            "Epoch 15/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0836 - mae: 0.1747 - mse: 0.0836 - val_loss: 0.0739 - val_mae: 0.1659 - val_mse: 0.0739 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1889 - val_custom_mae: 0.2795\n",
            "Epoch 16/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0773 - mae: 0.1643 - mse: 0.0773 - val_loss: 0.0674 - val_mae: 0.1553 - val_mse: 0.0674 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1851 - val_custom_mae: 0.2763\n",
            "Epoch 17/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0720 - mae: 0.1545 - mse: 0.0720 - val_loss: 0.0620 - val_mae: 0.1454 - val_mse: 0.0620 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1821 - val_custom_mae: 0.2734\n",
            "Epoch 18/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0675 - mae: 0.1453 - mse: 0.0675 - val_loss: 0.0574 - val_mae: 0.1361 - val_mse: 0.0574 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1793 - val_custom_mae: 0.2709\n",
            "Epoch 19/100\n",
            "457/457 - 123s - 268ms/step - loss: 0.0637 - mae: 0.1367 - mse: 0.0637 - val_loss: 0.0536 - val_mae: 0.1275 - val_mse: 0.0536 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1772 - val_custom_mae: 0.2687\n",
            "Epoch 20/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0606 - mae: 0.1288 - mse: 0.0606 - val_loss: 0.0504 - val_mae: 0.1195 - val_mse: 0.0504 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1754 - val_custom_mae: 0.2669\n",
            "Epoch 21/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0580 - mae: 0.1215 - mse: 0.0580 - val_loss: 0.0479 - val_mae: 0.1123 - val_mse: 0.0479 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1739 - val_custom_mae: 0.2654\n",
            "Epoch 22/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0560 - mae: 0.1149 - mse: 0.0560 - val_loss: 0.0459 - val_mae: 0.1056 - val_mse: 0.0459 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1727 - val_custom_mae: 0.2640\n",
            "Epoch 23/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0544 - mae: 0.1090 - mse: 0.0544 - val_loss: 0.0443 - val_mae: 0.0998 - val_mse: 0.0443 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1719 - val_custom_mae: 0.2630\n",
            "Epoch 24/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0532 - mae: 0.1038 - mse: 0.0532 - val_loss: 0.0432 - val_mae: 0.0948 - val_mse: 0.0432 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1711 - val_custom_mae: 0.2624\n",
            "Epoch 25/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0522 - mae: 0.0994 - mse: 0.0522 - val_loss: 0.0423 - val_mae: 0.0905 - val_mse: 0.0423 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1706 - val_custom_mae: 0.2615\n",
            "Epoch 26/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0516 - mae: 0.0957 - mse: 0.0516 - val_loss: 0.0416 - val_mae: 0.0869 - val_mse: 0.0416 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1701 - val_custom_mae: 0.2611\n",
            "Epoch 27/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0510 - mae: 0.0925 - mse: 0.0510 - val_loss: 0.0412 - val_mae: 0.0839 - val_mse: 0.0412 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1698 - val_custom_mae: 0.2606\n",
            "Epoch 28/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0507 - mae: 0.0900 - mse: 0.0507 - val_loss: 0.0408 - val_mae: 0.0814 - val_mse: 0.0408 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1694 - val_custom_mae: 0.2602\n",
            "Epoch 29/100\n",
            "457/457 - 123s - 268ms/step - loss: 0.0504 - mae: 0.0878 - mse: 0.0504 - val_loss: 0.0405 - val_mae: 0.0793 - val_mse: 0.0405 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1692 - val_custom_mae: 0.2600\n",
            "Epoch 30/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0502 - mae: 0.0860 - mse: 0.0502 - val_loss: 0.0404 - val_mae: 0.0776 - val_mse: 0.0404 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1691 - val_custom_mae: 0.2598\n",
            "Epoch 31/100\n",
            "457/457 - 123s - 268ms/step - loss: 0.0500 - mae: 0.0845 - mse: 0.0500 - val_loss: 0.0402 - val_mae: 0.0761 - val_mse: 0.0402 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1689 - val_custom_mae: 0.2594\n",
            "Epoch 32/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0499 - mae: 0.0833 - mse: 0.0499 - val_loss: 0.0401 - val_mae: 0.0749 - val_mse: 0.0401 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1688 - val_custom_mae: 0.2592\n",
            "Epoch 33/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0498 - mae: 0.0822 - mse: 0.0498 - val_loss: 0.0400 - val_mae: 0.0738 - val_mse: 0.0400 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1686 - val_custom_mae: 0.2590\n",
            "Epoch 34/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0497 - mae: 0.0812 - mse: 0.0497 - val_loss: 0.0399 - val_mae: 0.0729 - val_mse: 0.0399 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1686 - val_custom_mae: 0.2590\n",
            "Epoch 35/100\n",
            "457/457 - 123s - 268ms/step - loss: 0.0497 - mae: 0.0804 - mse: 0.0497 - val_loss: 0.0398 - val_mae: 0.0720 - val_mse: 0.0398 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1684 - val_custom_mae: 0.2587\n",
            "Epoch 36/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0496 - mae: 0.0797 - mse: 0.0496 - val_loss: 0.0398 - val_mae: 0.0714 - val_mse: 0.0398 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1684 - val_custom_mae: 0.2588\n",
            "Epoch 37/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0495 - mae: 0.0791 - mse: 0.0495 - val_loss: 0.0397 - val_mae: 0.0708 - val_mse: 0.0397 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1683 - val_custom_mae: 0.2588\n",
            "Epoch 38/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0495 - mae: 0.0786 - mse: 0.0495 - val_loss: 0.0397 - val_mae: 0.0702 - val_mse: 0.0397 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1683 - val_custom_mae: 0.2586\n",
            "Epoch 39/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0495 - mae: 0.0781 - mse: 0.0495 - val_loss: 0.0396 - val_mae: 0.0697 - val_mse: 0.0396 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1682 - val_custom_mae: 0.2584\n",
            "Epoch 40/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0494 - mae: 0.0776 - mse: 0.0494 - val_loss: 0.0396 - val_mae: 0.0693 - val_mse: 0.0396 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1682 - val_custom_mae: 0.2586\n",
            "Epoch 41/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0494 - mae: 0.0773 - mse: 0.0494 - val_loss: 0.0396 - val_mae: 0.0689 - val_mse: 0.0396 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1683 - val_custom_mae: 0.2584\n",
            "Epoch 42/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0494 - mae: 0.0769 - mse: 0.0494 - val_loss: 0.0396 - val_mae: 0.0685 - val_mse: 0.0396 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1682 - val_custom_mae: 0.2586\n",
            "Epoch 43/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0494 - mae: 0.0766 - mse: 0.0494 - val_loss: 0.0395 - val_mae: 0.0682 - val_mse: 0.0395 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1680 - val_custom_mae: 0.2584\n",
            "Epoch 44/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0493 - mae: 0.0763 - mse: 0.0493 - val_loss: 0.0395 - val_mae: 0.0679 - val_mse: 0.0395 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1680 - val_custom_mae: 0.2582\n",
            "Epoch 45/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0493 - mae: 0.0760 - mse: 0.0493 - val_loss: 0.0395 - val_mae: 0.0676 - val_mse: 0.0395 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1681 - val_custom_mae: 0.2583\n",
            "Epoch 46/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0493 - mae: 0.0758 - mse: 0.0493 - val_loss: 0.0395 - val_mae: 0.0673 - val_mse: 0.0395 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1680 - val_custom_mae: 0.2581\n",
            "Epoch 47/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0493 - mae: 0.0755 - mse: 0.0493 - val_loss: 0.0395 - val_mae: 0.0671 - val_mse: 0.0395 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1680 - val_custom_mae: 0.2584\n",
            "Epoch 48/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0493 - mae: 0.0753 - mse: 0.0493 - val_loss: 0.0395 - val_mae: 0.0669 - val_mse: 0.0395 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1680 - val_custom_mae: 0.2583\n",
            "Epoch 49/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0493 - mae: 0.0751 - mse: 0.0493 - val_loss: 0.0395 - val_mae: 0.0667 - val_mse: 0.0395 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1681 - val_custom_mae: 0.2584\n",
            "Epoch 50/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0493 - mae: 0.0749 - mse: 0.0493 - val_loss: 0.0395 - val_mae: 0.0665 - val_mse: 0.0395 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1679 - val_custom_mae: 0.2583\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "457/457 - 122s - 266ms/step - loss: 0.0493 - mae: 0.0748 - mse: 0.0493 - val_loss: 0.0395 - val_mae: 0.0663 - val_mse: 0.0395 - learning_rate: 1.0000e-04 - val_custom_mse: 0.1680 - val_custom_mae: 0.2583\n",
            "Epoch 52/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0492 - mae: 0.0746 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0662 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2579\n",
            "Epoch 53/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0492 - mae: 0.0745 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0661 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2579\n",
            "Epoch 54/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0745 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0661 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2579\n",
            "Epoch 55/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0492 - mae: 0.0745 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0661 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 56/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0492 - mae: 0.0744 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0660 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2579\n",
            "Epoch 57/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0492 - mae: 0.0744 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0660 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2579\n",
            "Epoch 58/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0492 - mae: 0.0744 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0660 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2579\n",
            "Epoch 59/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0744 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0659 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 60/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0743 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0659 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2579\n",
            "Epoch 61/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0743 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0659 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 62/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0492 - mae: 0.0743 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0659 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 63/100\n",
            "457/457 - 122s - 268ms/step - loss: 0.0492 - mae: 0.0742 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0658 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 64/100\n",
            "457/457 - 122s - 267ms/step - loss: 0.0492 - mae: 0.0742 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0658 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 65: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0742 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0658 - val_mse: 0.0394 - learning_rate: 2.0000e-05 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 66/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0742 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0658 - val_mse: 0.0394 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 67/100\n",
            "457/457 - 121s - 266ms/step - loss: 0.0492 - mae: 0.0742 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 68/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 69/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 70/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 71/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 72/100\n",
            "457/457 - 121s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 4.0000e-06 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 74/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 75/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 76/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 77/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 78/100\n",
            "457/457 - 121s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 79/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 80/100\n",
            "457/457 - 121s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 81: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
            "457/457 - 121s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 8.0000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 82/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 83/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 84/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 85/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 86/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 87/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 88/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 3.199999980552093e-08.\n",
            "457/457 - 121s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 1.6000e-07 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 90/100\n",
            "457/457 - 121s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 91/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 92/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 93/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 94/100\n",
            "457/457 - 121s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 95/100\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 96/100\n",
            "457/457 - 121s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 6.399999818995639e-09.\n",
            "457/457 - 121s - 265ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 3.2000e-08 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 98/100\n",
            "457/457 - 121s - 264ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 99/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "Epoch 100/100\n",
            "457/457 - 122s - 266ms/step - loss: 0.0492 - mae: 0.0741 - mse: 0.0492 - val_loss: 0.0394 - val_mae: 0.0657 - val_mse: 0.0394 - learning_rate: 6.4000e-09 - val_custom_mse: 0.1679 - val_custom_mae: 0.2578\n",
            "\n",
            " ECL Final Results:\n",
            "\n",
            "MSE Results:\n",
            "==================================================\n",
            "          DR=0%\n",
            "Horizon        \n",
            "96       0.1304\n",
            "192      0.1449\n",
            "336      0.1619\n",
            "720      0.1967\n",
            "\n",
            "MAE Results:\n",
            "==================================================\n",
            "          DR=0%\n",
            "Horizon        \n",
            "96       0.2264\n",
            "192      0.2397\n",
            "336      0.2574\n",
            "720      0.2869\n",
            "\n",
            "Results saved to: ./flowmixer_results/ECL_experiment_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Run the experiments\n",
        "data_name='ECL'\n",
        "results = run_experiments(data_name,horizons=[96, 192, 336, 720], dropout_rates=[0.0],revin=1, learning_rate=1e-4, mopt='adamw', seq_len_=1024*3 )\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\n {data_name} Final Results:\")\n",
        "df = pd.DataFrame(results)\n",
        "# Assuming results is a list of dictionaries with horizon, dropout, MSE, and MAE values\n",
        "display_results_tables(results[0])\n",
        "\n",
        "\n",
        "# The results are already saved in CSV format after each experiment\n",
        "print(f\"\\nResults saved to: ./flowmixer_results/{data_name}_experiment_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traffic"
      ],
      "metadata": {
        "id": "Ufilxvxj2Dzq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA9kPJTNyu6E",
        "outputId": "20fe8f9b-d2c2-4019-cb57-c0e13a1ff302"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment: horizon=96, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "253/253 - 103s - 409ms/step - loss: 0.5703 - mae: 0.4043 - mse: 0.5703 - val_loss: 0.3984 - val_mae: 0.3391 - val_mse: 0.3984 - learning_rate: 0.0010 - val_custom_mse: 0.6558 - val_custom_mae: 0.3788\n",
            "Epoch 2/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.2483 - mae: 0.2732 - mse: 0.2483 - val_loss: 0.1789 - val_mae: 0.2289 - val_mse: 0.1789 - learning_rate: 0.0010 - val_custom_mse: 0.4775 - val_custom_mae: 0.3116\n",
            "Epoch 3/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.1072 - mae: 0.1789 - mse: 0.1072 - val_loss: 0.0810 - val_mae: 0.1526 - val_mse: 0.0810 - learning_rate: 0.0010 - val_custom_mse: 0.3917 - val_custom_mae: 0.2752\n",
            "Epoch 4/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.0496 - mae: 0.1199 - mse: 0.0496 - val_loss: 0.0402 - val_mae: 0.1041 - val_mse: 0.0402 - learning_rate: 0.0010 - val_custom_mse: 0.3520 - val_custom_mae: 0.2537\n",
            "Epoch 5/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0254 - mae: 0.0827 - mse: 0.0254 - val_loss: 0.0226 - val_mae: 0.0732 - val_mse: 0.0226 - learning_rate: 0.0010 - val_custom_mse: 0.3338 - val_custom_mae: 0.2432\n",
            "Epoch 6/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.0147 - mae: 0.0590 - mse: 0.0147 - val_loss: 0.0146 - val_mae: 0.0530 - val_mse: 0.0146 - learning_rate: 0.0010 - val_custom_mse: 0.3252 - val_custom_mae: 0.2363\n",
            "Epoch 7/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.0099 - mae: 0.0438 - mse: 0.0099 - val_loss: 0.0110 - val_mae: 0.0401 - val_mse: 0.0110 - learning_rate: 0.0010 - val_custom_mse: 0.3215 - val_custom_mae: 0.2329\n",
            "Epoch 8/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.0078 - mae: 0.0341 - mse: 0.0078 - val_loss: 0.0094 - val_mae: 0.0320 - val_mse: 0.0094 - learning_rate: 0.0010 - val_custom_mse: 0.3204 - val_custom_mae: 0.2325\n",
            "Epoch 9/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0068 - mae: 0.0277 - mse: 0.0068 - val_loss: 0.0086 - val_mae: 0.0265 - val_mse: 0.0086 - learning_rate: 0.0010 - val_custom_mse: 0.3198 - val_custom_mae: 0.2324\n",
            "Epoch 10/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0062 - mae: 0.0232 - mse: 0.0062 - val_loss: 0.0082 - val_mae: 0.0227 - val_mse: 0.0082 - learning_rate: 0.0010 - val_custom_mse: 0.3202 - val_custom_mae: 0.2340\n",
            "Epoch 11/100\n",
            "253/253 - 92s - 362ms/step - loss: 0.0060 - mae: 0.0201 - mse: 0.0060 - val_loss: 0.0080 - val_mae: 0.0199 - val_mse: 0.0080 - learning_rate: 0.0010 - val_custom_mse: 0.3196 - val_custom_mae: 0.2322\n",
            "Epoch 12/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.0058 - mae: 0.0179 - mse: 0.0058 - val_loss: 0.0079 - val_mae: 0.0179 - val_mse: 0.0079 - learning_rate: 0.0010 - val_custom_mse: 0.3197 - val_custom_mae: 0.2327\n",
            "Epoch 13/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0057 - mae: 0.0162 - mse: 0.0057 - val_loss: 0.0078 - val_mae: 0.0164 - val_mse: 0.0078 - learning_rate: 0.0010 - val_custom_mse: 0.3196 - val_custom_mae: 0.2329\n",
            "Epoch 14/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.0056 - mae: 0.0149 - mse: 0.0056 - val_loss: 0.0077 - val_mae: 0.0151 - val_mse: 0.0077 - learning_rate: 0.0010 - val_custom_mse: 0.3192 - val_custom_mae: 0.2315\n",
            "Epoch 15/100\n",
            "253/253 - 92s - 362ms/step - loss: 0.0056 - mae: 0.0139 - mse: 0.0056 - val_loss: 0.0077 - val_mae: 0.0142 - val_mse: 0.0077 - learning_rate: 0.0010 - val_custom_mse: 0.3197 - val_custom_mae: 0.2332\n",
            "Epoch 16/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0056 - mae: 0.0130 - mse: 0.0056 - val_loss: 0.0076 - val_mae: 0.0133 - val_mse: 0.0076 - learning_rate: 0.0010 - val_custom_mse: 0.3197 - val_custom_mae: 0.2330\n",
            "Epoch 17/100\n",
            "253/253 - 92s - 362ms/step - loss: 0.0055 - mae: 0.0123 - mse: 0.0055 - val_loss: 0.0076 - val_mae: 0.0126 - val_mse: 0.0076 - learning_rate: 0.0010 - val_custom_mse: 0.3199 - val_custom_mae: 0.2333\n",
            "Epoch 18/100\n",
            "253/253 - 92s - 362ms/step - loss: 0.0055 - mae: 0.0117 - mse: 0.0055 - val_loss: 0.0076 - val_mae: 0.0120 - val_mse: 0.0076 - learning_rate: 0.0010 - val_custom_mse: 0.3200 - val_custom_mae: 0.2334\n",
            "Epoch 19/100\n",
            "253/253 - 91s - 362ms/step - loss: 0.0055 - mae: 0.0112 - mse: 0.0055 - val_loss: 0.0076 - val_mae: 0.0114 - val_mse: 0.0076 - learning_rate: 0.0010 - val_custom_mse: 0.3193 - val_custom_mae: 0.2316\n",
            "Epoch 20/100\n",
            "253/253 - 92s - 362ms/step - loss: 0.0055 - mae: 0.0107 - mse: 0.0055 - val_loss: 0.0076 - val_mae: 0.0110 - val_mse: 0.0076 - learning_rate: 0.0010 - val_custom_mse: 0.3193 - val_custom_mae: 0.2315\n",
            "Epoch 21/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0055 - mae: 0.0103 - mse: 0.0055 - val_loss: 0.0075 - val_mae: 0.0105 - val_mse: 0.0075 - learning_rate: 0.0010 - val_custom_mse: 0.3192 - val_custom_mae: 0.2308\n",
            "Epoch 22/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0055 - mae: 0.0099 - mse: 0.0055 - val_loss: 0.0076 - val_mae: 0.0103 - val_mse: 0.0076 - learning_rate: 0.0010 - val_custom_mse: 0.3206 - val_custom_mae: 0.2358\n",
            "Epoch 23/100\n",
            "253/253 - 91s - 362ms/step - loss: 0.0055 - mae: 0.0096 - mse: 0.0055 - val_loss: 0.0075 - val_mae: 0.0099 - val_mse: 0.0075 - learning_rate: 0.0010 - val_custom_mse: 0.3195 - val_custom_mae: 0.2317\n",
            "Epoch 24/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0055 - mae: 0.0093 - mse: 0.0055 - val_loss: 0.0075 - val_mae: 0.0096 - val_mse: 0.0075 - learning_rate: 0.0010 - val_custom_mse: 0.3192 - val_custom_mae: 0.2307\n",
            "Epoch 25/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0055 - mae: 0.0091 - mse: 0.0055 - val_loss: 0.0075 - val_mae: 0.0094 - val_mse: 0.0075 - learning_rate: 0.0010 - val_custom_mse: 0.3198 - val_custom_mae: 0.2326\n",
            "Epoch 26/100\n",
            "253/253 - 91s - 359ms/step - loss: 0.0055 - mae: 0.0088 - mse: 0.0055 - val_loss: 0.0075 - val_mae: 0.0091 - val_mse: 0.0075 - learning_rate: 0.0010 - val_custom_mse: 0.3196 - val_custom_mae: 0.2324\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "253/253 - 91s - 359ms/step - loss: 0.0054 - mae: 0.0086 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0090 - val_mse: 0.0075 - learning_rate: 0.0010 - val_custom_mse: 0.3204 - val_custom_mae: 0.2344\n",
            "Epoch 28/100\n",
            "253/253 - 92s - 362ms/step - loss: 0.0054 - mae: 0.0084 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0087 - val_mse: 0.0075 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3183 - val_custom_mae: 0.2282\n",
            "Epoch 29/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0083 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0087 - val_mse: 0.0075 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3182 - val_custom_mae: 0.2281\n",
            "Epoch 30/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0083 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0086 - val_mse: 0.0075 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3183 - val_custom_mae: 0.2283\n",
            "Epoch 31/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0082 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0086 - val_mse: 0.0075 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3183 - val_custom_mae: 0.2283\n",
            "Epoch 32/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0082 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0086 - val_mse: 0.0075 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3183 - val_custom_mae: 0.2283\n",
            "Epoch 33/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0082 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0085 - val_mse: 0.0075 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3184 - val_custom_mae: 0.2287\n",
            "Epoch 34/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0082 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0085 - val_mse: 0.0075 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3186 - val_custom_mae: 0.2292\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0081 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0085 - val_mse: 0.0075 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3184 - val_custom_mae: 0.2286\n",
            "Epoch 36/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.0054 - mae: 0.0081 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3182 - val_custom_mae: 0.2278\n",
            "Epoch 37/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.0054 - mae: 0.0081 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3182 - val_custom_mae: 0.2277\n",
            "Epoch 38/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0081 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3181 - val_custom_mae: 0.2277\n",
            "Epoch 39/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0081 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3182 - val_custom_mae: 0.2277\n",
            "Epoch 40/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3182 - val_custom_mae: 0.2277\n",
            "Epoch 41/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3182 - val_custom_mae: 0.2277\n",
            "Epoch 42/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3182 - val_custom_mae: 0.2277\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 43: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3182 - val_custom_mae: 0.2277\n",
            "Epoch 44/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 45/100\n",
            "253/253 - 92s - 362ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 46/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 47/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 48/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 49/100\n",
            "253/253 - 92s - 365ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 50/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "253/253 - 91s - 359ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 52/100\n",
            "253/253 - 92s - 364ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 53/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 54/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 55/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 56/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 57/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 58/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 60/100\n",
            "253/253 - 92s - 362ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 61/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 62/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 63/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 64/100\n",
            "253/253 - 91s - 362ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 65/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 66/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 68/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 69/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 70/100\n",
            "253/253 - 92s - 363ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 71/100\n",
            "253/253 - 92s - 362ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 72/100\n",
            "253/253 - 91s - 359ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 73/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 74/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 75: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "253/253 - 92s - 362ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 76/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 77/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 78/100\n",
            "253/253 - 91s - 362ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 79/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 80/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 81/100\n",
            "253/253 - 91s - 359ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 82/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 84/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 85/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 86/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 87/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 88/100\n",
            "253/253 - 91s - 359ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 89/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 90/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 92/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 93/100\n",
            "253/253 - 91s - 361ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 94/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 95/100\n",
            "253/253 - 91s - 359ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 96/100\n",
            "253/253 - 91s - 359ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 97/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 98/100\n",
            "253/253 - 91s - 360ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "253/253 - 91s - 359ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Epoch 100/100\n",
            "253/253 - 91s - 359ms/step - loss: 0.0054 - mae: 0.0080 - mse: 0.0054 - val_loss: 0.0075 - val_mae: 0.0084 - val_mse: 0.0075 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3181 - val_custom_mae: 0.2276\n",
            "Running experiment: horizon=192, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "250/250 - 101s - 403ms/step - loss: 0.4454 - mae: 0.3253 - mse: 0.4454 - val_loss: 0.3439 - val_mae: 0.2869 - val_mse: 0.3439 - learning_rate: 0.0010 - val_custom_mse: 0.5392 - val_custom_mae: 0.3169\n",
            "Epoch 2/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.2127 - mae: 0.2328 - mse: 0.2127 - val_loss: 0.1469 - val_mae: 0.1953 - val_mse: 0.1469 - learning_rate: 0.0010 - val_custom_mse: 0.4136 - val_custom_mae: 0.2736\n",
            "Epoch 3/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0888 - mae: 0.1546 - mse: 0.0888 - val_loss: 0.0666 - val_mae: 0.1298 - val_mse: 0.0666 - learning_rate: 0.0010 - val_custom_mse: 0.3587 - val_custom_mae: 0.2506\n",
            "Epoch 4/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0416 - mae: 0.1030 - mse: 0.0416 - val_loss: 0.0360 - val_mae: 0.0882 - val_mse: 0.0360 - learning_rate: 0.0010 - val_custom_mse: 0.3369 - val_custom_mae: 0.2388\n",
            "Epoch 5/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0236 - mae: 0.0710 - mse: 0.0236 - val_loss: 0.0239 - val_mae: 0.0619 - val_mse: 0.0239 - learning_rate: 0.0010 - val_custom_mse: 0.3296 - val_custom_mae: 0.2339\n",
            "Epoch 6/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0164 - mae: 0.0511 - mse: 0.0164 - val_loss: 0.0191 - val_mae: 0.0460 - val_mse: 0.0191 - learning_rate: 0.0010 - val_custom_mse: 0.3281 - val_custom_mae: 0.2325\n",
            "Epoch 7/100\n",
            "250/250 - 90s - 360ms/step - loss: 0.0137 - mae: 0.0394 - mse: 0.0137 - val_loss: 0.0172 - val_mae: 0.0365 - val_mse: 0.0172 - learning_rate: 0.0010 - val_custom_mse: 0.3274 - val_custom_mae: 0.2303\n",
            "Epoch 8/100\n",
            "250/250 - 89s - 357ms/step - loss: 0.0125 - mae: 0.0324 - mse: 0.0125 - val_loss: 0.0164 - val_mae: 0.0306 - val_mse: 0.0164 - learning_rate: 0.0010 - val_custom_mse: 0.3272 - val_custom_mae: 0.2305\n",
            "Epoch 9/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0120 - mae: 0.0277 - mse: 0.0120 - val_loss: 0.0159 - val_mae: 0.0266 - val_mse: 0.0159 - learning_rate: 0.0010 - val_custom_mse: 0.3267 - val_custom_mae: 0.2300\n",
            "Epoch 10/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0117 - mae: 0.0244 - mse: 0.0117 - val_loss: 0.0157 - val_mae: 0.0237 - val_mse: 0.0157 - learning_rate: 0.0010 - val_custom_mse: 0.3266 - val_custom_mae: 0.2299\n",
            "Epoch 11/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0115 - mae: 0.0221 - mse: 0.0115 - val_loss: 0.0156 - val_mae: 0.0217 - val_mse: 0.0156 - learning_rate: 0.0010 - val_custom_mse: 0.3267 - val_custom_mae: 0.2307\n",
            "Epoch 12/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0114 - mae: 0.0203 - mse: 0.0114 - val_loss: 0.0155 - val_mae: 0.0201 - val_mse: 0.0155 - learning_rate: 0.0010 - val_custom_mse: 0.3267 - val_custom_mae: 0.2307\n",
            "Epoch 13/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0114 - mae: 0.0190 - mse: 0.0114 - val_loss: 0.0155 - val_mae: 0.0190 - val_mse: 0.0155 - learning_rate: 0.0010 - val_custom_mse: 0.3263 - val_custom_mae: 0.2301\n",
            "Epoch 14/100\n",
            "250/250 - 89s - 357ms/step - loss: 0.0113 - mae: 0.0180 - mse: 0.0113 - val_loss: 0.0154 - val_mae: 0.0181 - val_mse: 0.0154 - learning_rate: 0.0010 - val_custom_mse: 0.3265 - val_custom_mae: 0.2309\n",
            "Epoch 15/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0113 - mae: 0.0172 - mse: 0.0113 - val_loss: 0.0154 - val_mae: 0.0173 - val_mse: 0.0154 - learning_rate: 0.0010 - val_custom_mse: 0.3262 - val_custom_mae: 0.2309\n",
            "Epoch 16/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0113 - mae: 0.0166 - mse: 0.0113 - val_loss: 0.0154 - val_mae: 0.0167 - val_mse: 0.0154 - learning_rate: 0.0010 - val_custom_mse: 0.3261 - val_custom_mae: 0.2306\n",
            "Epoch 17/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0113 - mae: 0.0161 - mse: 0.0113 - val_loss: 0.0154 - val_mae: 0.0162 - val_mse: 0.0154 - learning_rate: 0.0010 - val_custom_mse: 0.3262 - val_custom_mae: 0.2308\n",
            "Epoch 18/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0113 - mae: 0.0156 - mse: 0.0113 - val_loss: 0.0153 - val_mae: 0.0158 - val_mse: 0.0153 - learning_rate: 0.0010 - val_custom_mse: 0.3261 - val_custom_mae: 0.2311\n",
            "Epoch 19/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0112 - mae: 0.0153 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0154 - val_mse: 0.0153 - learning_rate: 0.0010 - val_custom_mse: 0.3259 - val_custom_mae: 0.2302\n",
            "Epoch 20/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0112 - mae: 0.0150 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0152 - val_mse: 0.0153 - learning_rate: 0.0010 - val_custom_mse: 0.3262 - val_custom_mae: 0.2313\n",
            "Epoch 21/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0112 - mae: 0.0147 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0149 - val_mse: 0.0153 - learning_rate: 0.0010 - val_custom_mse: 0.3258 - val_custom_mae: 0.2306\n",
            "Epoch 22/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0112 - mae: 0.0145 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0147 - val_mse: 0.0153 - learning_rate: 0.0010 - val_custom_mse: 0.3260 - val_custom_mae: 0.2308\n",
            "Epoch 23/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0112 - mae: 0.0143 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0145 - val_mse: 0.0153 - learning_rate: 0.0010 - val_custom_mse: 0.3263 - val_custom_mae: 0.2318\n",
            "Epoch 24/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0112 - mae: 0.0141 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0143 - val_mse: 0.0153 - learning_rate: 0.0010 - val_custom_mse: 0.3264 - val_custom_mae: 0.2315\n",
            "Epoch 25/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0112 - mae: 0.0139 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0142 - val_mse: 0.0153 - learning_rate: 0.0010 - val_custom_mse: 0.3261 - val_custom_mae: 0.2310\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "250/250 - 90s - 358ms/step - loss: 0.0112 - mae: 0.0138 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0140 - val_mse: 0.0153 - learning_rate: 0.0010 - val_custom_mse: 0.3258 - val_custom_mae: 0.2303\n",
            "Epoch 27/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0112 - mae: 0.0136 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0138 - val_mse: 0.0153 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3253 - val_custom_mae: 0.2283\n",
            "Epoch 28/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0112 - mae: 0.0135 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0138 - val_mse: 0.0153 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3251 - val_custom_mae: 0.2280\n",
            "Epoch 29/100\n",
            "250/250 - 89s - 358ms/step - loss: 0.0112 - mae: 0.0135 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0138 - val_mse: 0.0153 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3251 - val_custom_mae: 0.2281\n",
            "Epoch 30/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0112 - mae: 0.0135 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0137 - val_mse: 0.0153 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3251 - val_custom_mae: 0.2281\n",
            "Epoch 31/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0112 - mae: 0.0135 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0137 - val_mse: 0.0153 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3253 - val_custom_mae: 0.2284\n",
            "Epoch 32/100\n",
            "250/250 - 89s - 354ms/step - loss: 0.0112 - mae: 0.0134 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0137 - val_mse: 0.0153 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3252 - val_custom_mae: 0.2282\n",
            "Epoch 33/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0112 - mae: 0.0134 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0137 - val_mse: 0.0153 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3252 - val_custom_mae: 0.2285\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "250/250 - 89s - 355ms/step - loss: 0.0112 - mae: 0.0134 - mse: 0.0112 - val_loss: 0.0153 - val_mae: 0.0137 - val_mse: 0.0153 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3252 - val_custom_mae: 0.2285\n",
            "Epoch 35/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 36/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3250 - val_custom_mae: 0.2280\n",
            "Epoch 37/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 38/100\n",
            "250/250 - 89s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 39/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 40/100\n",
            "250/250 - 89s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 41/100\n",
            "250/250 - 89s - 357ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3250 - val_custom_mae: 0.2280\n",
            "Epoch 43/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 44/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 45/100\n",
            "250/250 - 89s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 46/100\n",
            "250/250 - 90s - 360ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 47/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 48/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 49/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 51/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 52/100\n",
            "250/250 - 89s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 53/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2279\n",
            "Epoch 54/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 55/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 56/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 57/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 59/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 60/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 61/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 62/100\n",
            "250/250 - 89s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 63/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 64/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 65/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 67/100\n",
            "250/250 - 89s - 357ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 68/100\n",
            "250/250 - 89s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 69/100\n",
            "250/250 - 90s - 358ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 70/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 71/100\n",
            "250/250 - 90s - 359ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 72/100\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 73/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "250/250 - 89s - 355ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 75/100\n",
            "250/250 - 89s - 356ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 76/100\n",
            "250/250 - 89s - 357ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 77/100\n",
            "250/250 - 89s - 357ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 78/100\n",
            "250/250 - 89s - 357ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 79/100\n",
            "250/250 - 90s - 360ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 80/100\n",
            "250/250 - 91s - 363ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 81/100\n",
            "250/250 - 91s - 365ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 82: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "250/250 - 91s - 364ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 83/100\n",
            "250/250 - 91s - 363ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 84/100\n",
            "250/250 - 92s - 370ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 85/100\n",
            "250/250 - 92s - 370ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 86/100\n",
            "250/250 - 92s - 369ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 87/100\n",
            "250/250 - 92s - 368ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 88/100\n",
            "250/250 - 91s - 363ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 89/100\n",
            "250/250 - 91s - 363ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "250/250 - 91s - 364ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 91/100\n",
            "250/250 - 91s - 363ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 92/100\n",
            "250/250 - 92s - 366ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 93/100\n",
            "250/250 - 91s - 365ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 94/100\n",
            "250/250 - 91s - 364ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 95/100\n",
            "250/250 - 91s - 366ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 96/100\n",
            "250/250 - 91s - 366ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 97/100\n",
            "250/250 - 91s - 364ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "250/250 - 91s - 364ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 99/100\n",
            "250/250 - 91s - 364ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Epoch 100/100\n",
            "250/250 - 90s - 360ms/step - loss: 0.0111 - mae: 0.0133 - mse: 0.0111 - val_loss: 0.0153 - val_mae: 0.0136 - val_mse: 0.0153 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3250 - val_custom_mae: 0.2278\n",
            "Running experiment: horizon=336, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "246/246 - 99s - 402ms/step - loss: 0.3560 - mae: 0.2507 - mse: 0.3560 - val_loss: 0.2895 - val_mae: 0.2293 - val_mse: 0.2895 - learning_rate: 0.0010 - val_custom_mse: 0.4560 - val_custom_mae: 0.2563\n",
            "Epoch 2/100\n",
            "246/246 - 87s - 353ms/step - loss: 0.1859 - mae: 0.1947 - mse: 0.1859 - val_loss: 0.1314 - val_mae: 0.1667 - val_mse: 0.1314 - learning_rate: 0.0010 - val_custom_mse: 0.3800 - val_custom_mae: 0.2418\n",
            "Epoch 3/100\n",
            "246/246 - 88s - 356ms/step - loss: 0.0795 - mae: 0.1322 - mse: 0.0795 - val_loss: 0.0635 - val_mae: 0.1111 - val_mse: 0.0635 - learning_rate: 0.0010 - val_custom_mse: 0.3528 - val_custom_mae: 0.2354\n",
            "Epoch 4/100\n",
            "246/246 - 89s - 361ms/step - loss: 0.0403 - mae: 0.0886 - mse: 0.0403 - val_loss: 0.0404 - val_mae: 0.0762 - val_mse: 0.0404 - learning_rate: 0.0010 - val_custom_mse: 0.3468 - val_custom_mae: 0.2355\n",
            "Epoch 5/100\n",
            "246/246 - 88s - 359ms/step - loss: 0.0274 - mae: 0.0622 - mse: 0.0274 - val_loss: 0.0327 - val_mae: 0.0553 - val_mse: 0.0327 - learning_rate: 0.0010 - val_custom_mse: 0.3460 - val_custom_mae: 0.2369\n",
            "Epoch 6/100\n",
            "246/246 - 88s - 358ms/step - loss: 0.0232 - mae: 0.0472 - mse: 0.0232 - val_loss: 0.0303 - val_mae: 0.0435 - val_mse: 0.0303 - learning_rate: 0.0010 - val_custom_mse: 0.3471 - val_custom_mae: 0.2367\n",
            "Epoch 7/100\n",
            "246/246 - 88s - 358ms/step - loss: 0.0218 - mae: 0.0389 - mse: 0.0218 - val_loss: 0.0295 - val_mae: 0.0370 - val_mse: 0.0295 - learning_rate: 0.0010 - val_custom_mse: 0.3483 - val_custom_mae: 0.2379\n",
            "Epoch 8/100\n",
            "246/246 - 88s - 358ms/step - loss: 0.0213 - mae: 0.0342 - mse: 0.0213 - val_loss: 0.0289 - val_mae: 0.0330 - val_mse: 0.0289 - learning_rate: 0.0010 - val_custom_mse: 0.3465 - val_custom_mae: 0.2367\n",
            "Epoch 9/100\n",
            "246/246 - 88s - 357ms/step - loss: 0.0210 - mae: 0.0312 - mse: 0.0210 - val_loss: 0.0286 - val_mae: 0.0304 - val_mse: 0.0286 - learning_rate: 0.0010 - val_custom_mse: 0.3445 - val_custom_mae: 0.2365\n",
            "Epoch 10/100\n",
            "246/246 - 88s - 358ms/step - loss: 0.0208 - mae: 0.0291 - mse: 0.0208 - val_loss: 0.0284 - val_mae: 0.0287 - val_mse: 0.0284 - learning_rate: 0.0010 - val_custom_mse: 0.3439 - val_custom_mae: 0.2375\n",
            "Epoch 11/100\n",
            "246/246 - 88s - 360ms/step - loss: 0.0207 - mae: 0.0276 - mse: 0.0207 - val_loss: 0.0282 - val_mae: 0.0272 - val_mse: 0.0282 - learning_rate: 0.0010 - val_custom_mse: 0.3423 - val_custom_mae: 0.2355\n",
            "Epoch 12/100\n",
            "246/246 - 88s - 359ms/step - loss: 0.0206 - mae: 0.0264 - mse: 0.0206 - val_loss: 0.0282 - val_mae: 0.0261 - val_mse: 0.0282 - learning_rate: 0.0010 - val_custom_mse: 0.3418 - val_custom_mae: 0.2348\n",
            "Epoch 13/100\n",
            "246/246 - 87s - 354ms/step - loss: 0.0206 - mae: 0.0256 - mse: 0.0206 - val_loss: 0.0282 - val_mae: 0.0254 - val_mse: 0.0282 - learning_rate: 0.0010 - val_custom_mse: 0.3424 - val_custom_mae: 0.2362\n",
            "Epoch 14/100\n",
            "246/246 - 88s - 357ms/step - loss: 0.0205 - mae: 0.0249 - mse: 0.0205 - val_loss: 0.0281 - val_mae: 0.0247 - val_mse: 0.0281 - learning_rate: 0.0010 - val_custom_mse: 0.3420 - val_custom_mae: 0.2353\n",
            "Epoch 15/100\n",
            "246/246 - 87s - 353ms/step - loss: 0.0205 - mae: 0.0244 - mse: 0.0205 - val_loss: 0.0281 - val_mae: 0.0243 - val_mse: 0.0281 - learning_rate: 0.0010 - val_custom_mse: 0.3423 - val_custom_mae: 0.2361\n",
            "Epoch 16/100\n",
            "246/246 - 87s - 352ms/step - loss: 0.0205 - mae: 0.0239 - mse: 0.0205 - val_loss: 0.0281 - val_mae: 0.0239 - val_mse: 0.0281 - learning_rate: 0.0010 - val_custom_mse: 0.3421 - val_custom_mae: 0.2363\n",
            "Epoch 17/100\n",
            "246/246 - 87s - 353ms/step - loss: 0.0205 - mae: 0.0236 - mse: 0.0205 - val_loss: 0.0281 - val_mae: 0.0235 - val_mse: 0.0281 - learning_rate: 0.0010 - val_custom_mse: 0.3419 - val_custom_mae: 0.2353\n",
            "Epoch 18/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0205 - mae: 0.0233 - mse: 0.0205 - val_loss: 0.0281 - val_mae: 0.0234 - val_mse: 0.0281 - learning_rate: 0.0010 - val_custom_mse: 0.3423 - val_custom_mae: 0.2363\n",
            "Epoch 19/100\n",
            "246/246 - 86s - 352ms/step - loss: 0.0205 - mae: 0.0231 - mse: 0.0205 - val_loss: 0.0281 - val_mae: 0.0231 - val_mse: 0.0281 - learning_rate: 0.0010 - val_custom_mse: 0.3420 - val_custom_mae: 0.2353\n",
            "Epoch 20/100\n",
            "246/246 - 87s - 352ms/step - loss: 0.0205 - mae: 0.0229 - mse: 0.0205 - val_loss: 0.0281 - val_mae: 0.0229 - val_mse: 0.0281 - learning_rate: 0.0010 - val_custom_mse: 0.3418 - val_custom_mae: 0.2350\n",
            "Epoch 21/100\n",
            "246/246 - 85s - 347ms/step - loss: 0.0205 - mae: 0.0228 - mse: 0.0205 - val_loss: 0.0281 - val_mae: 0.0228 - val_mse: 0.0281 - learning_rate: 0.0010 - val_custom_mse: 0.3421 - val_custom_mae: 0.2356\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "246/246 - 86s - 349ms/step - loss: 0.0205 - mae: 0.0227 - mse: 0.0205 - val_loss: 0.0281 - val_mae: 0.0226 - val_mse: 0.0281 - learning_rate: 0.0010 - val_custom_mse: 0.3420 - val_custom_mae: 0.2354\n",
            "Epoch 23/100\n",
            "246/246 - 87s - 353ms/step - loss: 0.0204 - mae: 0.0224 - mse: 0.0204 - val_loss: 0.0280 - val_mae: 0.0224 - val_mse: 0.0280 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3409 - val_custom_mae: 0.2326\n",
            "Epoch 24/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0204 - mae: 0.0223 - mse: 0.0204 - val_loss: 0.0280 - val_mae: 0.0224 - val_mse: 0.0280 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3410 - val_custom_mae: 0.2331\n",
            "Epoch 25/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0204 - mae: 0.0223 - mse: 0.0204 - val_loss: 0.0280 - val_mae: 0.0223 - val_mse: 0.0280 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3410 - val_custom_mae: 0.2331\n",
            "Epoch 26/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0204 - mae: 0.0223 - mse: 0.0204 - val_loss: 0.0280 - val_mae: 0.0224 - val_mse: 0.0280 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3412 - val_custom_mae: 0.2336\n",
            "Epoch 27/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0204 - mae: 0.0223 - mse: 0.0204 - val_loss: 0.0280 - val_mae: 0.0223 - val_mse: 0.0280 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3411 - val_custom_mae: 0.2334\n",
            "Epoch 28/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0204 - mae: 0.0222 - mse: 0.0204 - val_loss: 0.0280 - val_mae: 0.0223 - val_mse: 0.0280 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3411 - val_custom_mae: 0.2335\n",
            "Epoch 29/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0204 - mae: 0.0222 - mse: 0.0204 - val_loss: 0.0280 - val_mae: 0.0223 - val_mse: 0.0280 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3412 - val_custom_mae: 0.2337\n",
            "Epoch 30/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0204 - mae: 0.0222 - mse: 0.0204 - val_loss: 0.0280 - val_mae: 0.0223 - val_mse: 0.0280 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3413 - val_custom_mae: 0.2339\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "246/246 - 86s - 350ms/step - loss: 0.0204 - mae: 0.0222 - mse: 0.0204 - val_loss: 0.0280 - val_mae: 0.0223 - val_mse: 0.0280 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3412 - val_custom_mae: 0.2336\n",
            "Epoch 32/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3410 - val_custom_mae: 0.2330\n",
            "Epoch 33/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3409 - val_custom_mae: 0.2330\n",
            "Epoch 34/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3409 - val_custom_mae: 0.2330\n",
            "Epoch 35/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3409 - val_custom_mae: 0.2330\n",
            "Epoch 36/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3410 - val_custom_mae: 0.2331\n",
            "Epoch 37/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3410 - val_custom_mae: 0.2331\n",
            "Epoch 38/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3410 - val_custom_mae: 0.2330\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3410 - val_custom_mae: 0.2332\n",
            "Epoch 40/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 41/100\n",
            "246/246 - 87s - 352ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 42/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 43/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 44/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 45/100\n",
            "246/246 - 86s - 351ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 46/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 48/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 49/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 50/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 51/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 52/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 53/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 54/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 56/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 57/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 58/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 59/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 60/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 61/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 62/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 64/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 65/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0221 - val_mse: 0.0280 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 66/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0221 - val_mse: 0.0280 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 67/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 68/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0221 - val_mse: 0.0280 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 69/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0221 - val_mse: 0.0280 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 70/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0221 - val_mse: 0.0280 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 72/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 73/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 74/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 75/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 76/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 77/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 78/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 80/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 81/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 82/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 83/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 84/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 85/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 86/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 88/100\n",
            "246/246 - 87s - 352ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 89/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 90/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 91/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 92/100\n",
            "246/246 - 86s - 348ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 93/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 94/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 96/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 97/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 98/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 99/100\n",
            "246/246 - 86s - 349ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Epoch 100/100\n",
            "246/246 - 86s - 350ms/step - loss: 0.0203 - mae: 0.0221 - mse: 0.0203 - val_loss: 0.0280 - val_mae: 0.0222 - val_mse: 0.0280 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3409 - val_custom_mae: 0.2329\n",
            "Running experiment: horizon=720, dropout_rate=0.0\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_custom_mse` which is not available. Available metrics are: loss,mae,mse,val_loss,val_mae,val_mse\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234/234 - 89s - 382ms/step - loss: 0.6009 - mae: 0.4182 - mse: 0.6009 - val_loss: 0.4552 - val_mae: 0.3613 - val_mse: 0.4552 - learning_rate: 0.0010 - val_custom_mse: 0.6672 - val_custom_mae: 0.3894\n",
            "Epoch 2/100\n",
            "234/234 - 78s - 333ms/step - loss: 0.2965 - mae: 0.2968 - mse: 0.2965 - val_loss: 0.2451 - val_mae: 0.2586 - val_mse: 0.2451 - learning_rate: 0.0010 - val_custom_mse: 0.5276 - val_custom_mae: 0.3350\n",
            "Epoch 3/100\n",
            "234/234 - 78s - 333ms/step - loss: 0.1584 - mae: 0.2116 - mse: 0.1584 - val_loss: 0.1505 - val_mae: 0.1892 - val_mse: 0.1505 - learning_rate: 0.0010 - val_custom_mse: 0.4581 - val_custom_mae: 0.3001\n",
            "Epoch 4/100\n",
            "234/234 - 78s - 334ms/step - loss: 0.0992 - mae: 0.1574 - mse: 0.0992 - val_loss: 0.1077 - val_mae: 0.1434 - val_mse: 0.1077 - learning_rate: 0.0010 - val_custom_mse: 0.4225 - val_custom_mae: 0.2771\n",
            "Epoch 5/100\n",
            "234/234 - 78s - 333ms/step - loss: 0.0725 - mae: 0.1224 - mse: 0.0725 - val_loss: 0.0877 - val_mae: 0.1135 - val_mse: 0.0877 - learning_rate: 0.0010 - val_custom_mse: 0.4051 - val_custom_mae: 0.2651\n",
            "Epoch 6/100\n",
            "234/234 - 78s - 332ms/step - loss: 0.0598 - mae: 0.0994 - mse: 0.0598 - val_loss: 0.0781 - val_mae: 0.0939 - val_mse: 0.0781 - learning_rate: 0.0010 - val_custom_mse: 0.3970 - val_custom_mae: 0.2604\n",
            "Epoch 7/100\n",
            "234/234 - 78s - 333ms/step - loss: 0.0536 - mae: 0.0841 - mse: 0.0536 - val_loss: 0.0732 - val_mae: 0.0803 - val_mse: 0.0732 - learning_rate: 0.0010 - val_custom_mse: 0.3919 - val_custom_mae: 0.2561\n",
            "Epoch 8/100\n",
            "234/234 - 78s - 333ms/step - loss: 0.0506 - mae: 0.0740 - mse: 0.0506 - val_loss: 0.0707 - val_mae: 0.0712 - val_mse: 0.0707 - learning_rate: 0.0010 - val_custom_mse: 0.3894 - val_custom_mae: 0.2531\n",
            "Epoch 9/100\n",
            "234/234 - 78s - 335ms/step - loss: 0.0491 - mae: 0.0672 - mse: 0.0491 - val_loss: 0.0696 - val_mae: 0.0653 - val_mse: 0.0696 - learning_rate: 0.0010 - val_custom_mse: 0.3883 - val_custom_mae: 0.2518\n",
            "Epoch 10/100\n",
            "234/234 - 78s - 334ms/step - loss: 0.0484 - mae: 0.0627 - mse: 0.0484 - val_loss: 0.0691 - val_mae: 0.0616 - val_mse: 0.0691 - learning_rate: 0.0010 - val_custom_mse: 0.3883 - val_custom_mae: 0.2523\n",
            "Epoch 11/100\n",
            "234/234 - 78s - 333ms/step - loss: 0.0480 - mae: 0.0595 - mse: 0.0480 - val_loss: 0.0686 - val_mae: 0.0585 - val_mse: 0.0686 - learning_rate: 0.0010 - val_custom_mse: 0.3869 - val_custom_mae: 0.2497\n",
            "Epoch 12/100\n",
            "234/234 - 78s - 335ms/step - loss: 0.0477 - mae: 0.0573 - mse: 0.0477 - val_loss: 0.0686 - val_mae: 0.0569 - val_mse: 0.0686 - learning_rate: 0.0010 - val_custom_mse: 0.3876 - val_custom_mae: 0.2513\n",
            "Epoch 13/100\n",
            "234/234 - 78s - 334ms/step - loss: 0.0476 - mae: 0.0557 - mse: 0.0476 - val_loss: 0.0683 - val_mae: 0.0551 - val_mse: 0.0683 - learning_rate: 0.0010 - val_custom_mse: 0.3867 - val_custom_mae: 0.2493\n",
            "Epoch 14/100\n",
            "234/234 - 78s - 331ms/step - loss: 0.0475 - mae: 0.0545 - mse: 0.0475 - val_loss: 0.0684 - val_mae: 0.0542 - val_mse: 0.0684 - learning_rate: 0.0010 - val_custom_mse: 0.3875 - val_custom_mae: 0.2510\n",
            "Epoch 15/100\n",
            "234/234 - 78s - 333ms/step - loss: 0.0474 - mae: 0.0536 - mse: 0.0474 - val_loss: 0.0682 - val_mae: 0.0530 - val_mse: 0.0682 - learning_rate: 0.0010 - val_custom_mse: 0.3867 - val_custom_mae: 0.2492\n",
            "Epoch 16/100\n",
            "234/234 - 78s - 332ms/step - loss: 0.0474 - mae: 0.0528 - mse: 0.0474 - val_loss: 0.0683 - val_mae: 0.0525 - val_mse: 0.0683 - learning_rate: 0.0010 - val_custom_mse: 0.3871 - val_custom_mae: 0.2501\n",
            "Epoch 17/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0474 - mae: 0.0522 - mse: 0.0474 - val_loss: 0.0683 - val_mae: 0.0520 - val_mse: 0.0683 - learning_rate: 0.0010 - val_custom_mse: 0.3874 - val_custom_mae: 0.2510\n",
            "Epoch 18/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0473 - mae: 0.0517 - mse: 0.0473 - val_loss: 0.0682 - val_mae: 0.0515 - val_mse: 0.0682 - learning_rate: 0.0010 - val_custom_mse: 0.3872 - val_custom_mae: 0.2506\n",
            "Epoch 19/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0473 - mae: 0.0514 - mse: 0.0473 - val_loss: 0.0683 - val_mae: 0.0511 - val_mse: 0.0683 - learning_rate: 0.0010 - val_custom_mse: 0.3875 - val_custom_mae: 0.2513\n",
            "Epoch 20/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0473 - mae: 0.0510 - mse: 0.0473 - val_loss: 0.0683 - val_mae: 0.0508 - val_mse: 0.0683 - learning_rate: 0.0010 - val_custom_mse: 0.3875 - val_custom_mae: 0.2513\n",
            "Epoch 21/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0473 - mae: 0.0506 - mse: 0.0473 - val_loss: 0.0683 - val_mae: 0.0506 - val_mse: 0.0683 - learning_rate: 0.0010 - val_custom_mse: 0.3878 - val_custom_mae: 0.2521\n",
            "Epoch 22/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0473 - mae: 0.0504 - mse: 0.0473 - val_loss: 0.0683 - val_mae: 0.0504 - val_mse: 0.0683 - learning_rate: 0.0010 - val_custom_mse: 0.3879 - val_custom_mae: 0.2524\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "234/234 - 77s - 330ms/step - loss: 0.0473 - mae: 0.0502 - mse: 0.0473 - val_loss: 0.0683 - val_mae: 0.0501 - val_mse: 0.0683 - learning_rate: 0.0010 - val_custom_mse: 0.3879 - val_custom_mae: 0.2522\n",
            "Epoch 24/100\n",
            "234/234 - 78s - 333ms/step - loss: 0.0470 - mae: 0.0494 - mse: 0.0470 - val_loss: 0.0680 - val_mae: 0.0493 - val_mse: 0.0680 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3862 - val_custom_mae: 0.2480\n",
            "Epoch 25/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0470 - mae: 0.0493 - mse: 0.0470 - val_loss: 0.0680 - val_mae: 0.0493 - val_mse: 0.0680 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3865 - val_custom_mae: 0.2487\n",
            "Epoch 26/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0470 - mae: 0.0493 - mse: 0.0470 - val_loss: 0.0680 - val_mae: 0.0493 - val_mse: 0.0680 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3864 - val_custom_mae: 0.2487\n",
            "Epoch 27/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0470 - mae: 0.0493 - mse: 0.0470 - val_loss: 0.0681 - val_mae: 0.0493 - val_mse: 0.0681 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3865 - val_custom_mae: 0.2490\n",
            "Epoch 28/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0470 - mae: 0.0492 - mse: 0.0470 - val_loss: 0.0680 - val_mae: 0.0492 - val_mse: 0.0680 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3864 - val_custom_mae: 0.2486\n",
            "Epoch 29/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0470 - mae: 0.0492 - mse: 0.0470 - val_loss: 0.0680 - val_mae: 0.0492 - val_mse: 0.0680 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3864 - val_custom_mae: 0.2488\n",
            "Epoch 30/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0470 - mae: 0.0492 - mse: 0.0470 - val_loss: 0.0680 - val_mae: 0.0492 - val_mse: 0.0680 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3865 - val_custom_mae: 0.2488\n",
            "Epoch 31/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0470 - mae: 0.0492 - mse: 0.0470 - val_loss: 0.0681 - val_mae: 0.0493 - val_mse: 0.0681 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3869 - val_custom_mae: 0.2499\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "234/234 - 77s - 330ms/step - loss: 0.0470 - mae: 0.0491 - mse: 0.0470 - val_loss: 0.0681 - val_mae: 0.0492 - val_mse: 0.0681 - learning_rate: 2.0000e-04 - val_custom_mse: 0.3866 - val_custom_mae: 0.2493\n",
            "Epoch 33/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0490 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3864 - val_custom_mae: 0.2486\n",
            "Epoch 34/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3864 - val_custom_mae: 0.2486\n",
            "Epoch 35/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3864 - val_custom_mae: 0.2486\n",
            "Epoch 36/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0491 - val_mse: 0.0680 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3865 - val_custom_mae: 0.2488\n",
            "Epoch 37/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3865 - val_custom_mae: 0.2487\n",
            "Epoch 38/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3865 - val_custom_mae: 0.2488\n",
            "Epoch 39/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3864 - val_custom_mae: 0.2487\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 4.0000e-05 - val_custom_mse: 0.3865 - val_custom_mae: 0.2488\n",
            "Epoch 41/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2488\n",
            "Epoch 42/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2488\n",
            "Epoch 43/100\n",
            "234/234 - 77s - 329ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 44/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2488\n",
            "Epoch 45/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2488\n",
            "Epoch 46/100\n",
            "234/234 - 78s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2488\n",
            "Epoch 47/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "234/234 - 77s - 329ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 8.0000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 49/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 50/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 51/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 52/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 53/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 54/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 55/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.6000e-06 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 57/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 58/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 59/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 60/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 61/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 62/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 63/100\n",
            "234/234 - 78s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "234/234 - 78s - 332ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 3.2000e-07 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 65/100\n",
            "234/234 - 78s - 332ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 66/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 67/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 68/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 69/100\n",
            "234/234 - 77s - 329ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 70/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 71/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 6.4000e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 73/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 74/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 75/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 76/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 77/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 78/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 79/100\n",
            "234/234 - 78s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 80: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "234/234 - 78s - 332ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.2800e-08 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 81/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 82/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 83/100\n",
            "234/234 - 78s - 332ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 84/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 85/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 86/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 87/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 2.5600e-09 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 89/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 90/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 91/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 92/100\n",
            "234/234 - 77s - 331ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 93/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 94/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 95/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 96: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 5.1200e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 97/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 98/100\n",
            "234/234 - 77s - 330ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 99/100\n",
            "234/234 - 77s - 329ms/step - loss: 0.0469 - mae: 0.0489 - mse: 0.0469 - val_loss: 0.0680 - val_mae: 0.0490 - val_mse: 0.0680 - learning_rate: 1.0240e-10 - val_custom_mse: 0.3865 - val_custom_mae: 0.2489\n",
            "Epoch 100/100\n"
          ]
        }
      ],
      "source": [
        "# Run the experiments\n",
        "data_name='TrafficL'\n",
        "results = run_experiments(data_name,horizons=[96, 192, 336, 720], dropout_rates=[0.0],revin=2, learning_rate=1e-3, mopt='adamw', seq_len_=4096 )\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\n {data_name} Final Results:\")\n",
        "df = pd.DataFrame(results)\n",
        "# Assuming results is a list of dictionaries with horizon, dropout, MSE, and MAE values\n",
        "display_results_tables(results[0])\n",
        "\n",
        "\n",
        "# The results are already saved in CSV format after each experiment\n",
        "print(f\"\\nResults saved to: ./flowmixer_results/{data_name}_experiment_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_results_tables(results[0]) # Traffic Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6OsK1zVv9nW",
        "outputId": "38fa8459-9cb1-4209-ce27-906bfca05b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MSE Results:\n",
            "==================================================\n",
            "          DR=0%\n",
            "Horizon        \n",
            "96       0.3771\n",
            "192      0.3886\n",
            "336      0.4015\n",
            "720      0.4343\n",
            "\n",
            "MAE Results:\n",
            "==================================================\n",
            "          DR=0%\n",
            "Horizon        \n",
            "96       0.2641\n",
            "192      0.2684\n",
            "336      0.2748\n",
            "720      0.2919\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "6rCpXxlC0W9z",
        "sRvz2gelw7pM",
        "v4DHt_Bg0avt",
        "hFsq4RP91fx_",
        "h325ZvXK1r7y",
        "yoFwq-u_hTKS",
        "Ufilxvxj2Dzq"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}